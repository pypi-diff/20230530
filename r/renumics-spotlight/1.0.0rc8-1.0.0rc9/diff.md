# Comparing `tmp/renumics_spotlight-1.0.0rc8-py3-none-any.whl.zip` & `tmp/renumics_spotlight-1.0.0rc9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,87 +1,88 @@
-Zip file size: 1017950 bytes, number of entries: 85
--rw-r--r--  2.0 unx      438 b- defN 80-Jan-01 00:00 renumics/spotlight/__init__.py
+Zip file size: 1018778 bytes, number of entries: 86
+-rw-r--r--  2.0 unx      452 b- defN 80-Jan-01 00:00 renumics/spotlight/__init__.py
 -rw-r--r--  2.0 unx       78 b- defN 80-Jan-01 00:00 renumics/spotlight/__version__.py
 -rw-r--r--  2.0 unx       83 b- defN 80-Jan-01 00:00 renumics/spotlight/_build_variant.py
 -rw-r--r--  2.0 unx      308 b- defN 80-Jan-01 00:00 renumics/spotlight/appdirs.py
--rw-r--r--  2.0 unx      908 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/__init__.py
+-rw-r--r--  2.0 unx     1379 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/apis/__init__.py
--rw-r--r--  2.0 unx     1110 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/apis/config.py
--rw-r--r--  2.0 unx     1532 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/apis/filebrowser.py
--rw-r--r--  2.0 unx     1789 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/apis/layout.py
--rw-r--r--  2.0 unx    10969 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/apis/table.py
--rw-r--r--  2.0 unx      392 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/apis/user.py
 -rw-r--r--  2.0 unx      396 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/apis/websocket.py
--rw-r--r--  2.0 unx     3828 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/app.py
+-rw-r--r--  2.0 unx     3630 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/app.py
 -rw-r--r--  2.0 unx     1126 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/cache.py
 -rw-r--r--  2.0 unx     2608 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/config.py
--rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/core/__init__.py
--rw-r--r--  2.0 unx      836 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/core/user.py
--rw-r--r--  2.0 unx    10723 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/data_source.py
--rw-r--r--  2.0 unx     3993 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/exceptions.py
--rw-r--r--  2.0 unx    15341 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/hdf5_data_source.py
+-rw-r--r--  2.0 unx    11189 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/data_source.py
+-rw-r--r--  2.0 unx     4525 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/exceptions.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/middlewares/__init__.py
 -rw-r--r--  2.0 unx     1177 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/middlewares/timing.py
--rw-r--r--  2.0 unx    16792 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/pandas_data_source.py
 -rw-r--r--  2.0 unx     3638 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/server.py
 -rw-r--r--  2.0 unx      395 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/asset-manifest.json
 -rwxr-xr-x  2.0 unx     1579 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/favicon.png
 -rw-r--r--  2.0 unx      502 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/index.html
 -rw-r--r--  2.0 unx     9808 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/logo.svg
 -rw-r--r--  2.0 unx      586 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/manifest.json
 -rw-r--r--  2.0 unx       67 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/robots.txt
 -rw-r--r--  2.0 unx      506 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/spotlight.svg
 -rw-r--r--  2.0 unx    23379 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/css/main.515252a3.css
 -rw-r--r--  2.0 unx    65624 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/js/384.87f65e3c.chunk.js
 -rw-r--r--  2.0 unx      149 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/js/384.87f65e3c.chunk.js.LICENSE.txt
 -rw-r--r--  2.0 unx     2241 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/js/886.810d122e.chunk.js
--rw-r--r--  2.0 unx  3167667 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/js/main.27112c26.js
--rw-r--r--  2.0 unx    13460 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/js/main.27112c26.js.LICENSE.txt
+-rw-r--r--  2.0 unx  3167691 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js
+-rw-r--r--  2.0 unx    13460 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js.LICENSE.txt
 -rw-r--r--  2.0 unx      126 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/tasks/__init__.py
 -rw-r--r--  2.0 unx      121 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/tasks/exceptions.py
 -rw-r--r--  2.0 unx     3670 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/tasks/reduction.py
 -rw-r--r--  2.0 unx      325 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/tasks/task.py
 -rw-r--r--  2.0 unx     3286 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/tasks/task_manager.py
 -rw-r--r--  2.0 unx     9299 b- defN 80-Jan-01 00:00 renumics/spotlight/backend/websockets.py
 -rw-r--r--  2.0 unx     3609 b- defN 80-Jan-01 00:00 renumics/spotlight/cli.py
--rw-r--r--  2.0 unx   143164 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/__init__.py
--rw-r--r--  2.0 unx      133 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/descriptors/__init__.py
--rw-r--r--  2.0 unx     3173 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/descriptors/catch22.py
--rw-r--r--  2.0 unx     1810 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/descriptors/pca.py
--rw-r--r--  2.0 unx     7053 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/descriptors/utils.py
+-rw-r--r--  2.0 unx   139918 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/__init__.py
+-rw-r--r--  2.0 unx     4530 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/descriptors/__init__.py
+-rw-r--r--  2.0 unx     7053 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/descriptors/data_alignment.py
 -rw-r--r--  2.0 unx     1395 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/exceptions.py
 -rw-r--r--  2.0 unx     2576 b- defN 80-Jan-01 00:00 renumics/spotlight/dataset/typing.py
--rw-r--r--  2.0 unx    37402 b- defN 80-Jan-01 00:00 renumics/spotlight/dtypes/__init__.py
+-rw-r--r--  2.0 unx    37681 b- defN 80-Jan-01 00:00 renumics/spotlight/dtypes/__init__.py
 -rw-r--r--  2.0 unx      427 b- defN 80-Jan-01 00:00 renumics/spotlight/dtypes/exceptions.py
 -rw-r--r--  2.0 unx     5569 b- defN 80-Jan-01 00:00 renumics/spotlight/dtypes/triangulation.py
 -rw-r--r--  2.0 unx     2440 b- defN 80-Jan-01 00:00 renumics/spotlight/dtypes/typing.py
 -rw-r--r--  2.0 unx      888 b- defN 80-Jan-01 00:00 renumics/spotlight/environ.py
 -rw-r--r--  2.0 unx      368 b- defN 80-Jan-01 00:00 renumics/spotlight/io/__init__.py
 -rw-r--r--  2.0 unx     6899 b- defN 80-Jan-01 00:00 renumics/spotlight/io/audio.py
 -rw-r--r--  2.0 unx     5534 b- defN 80-Jan-01 00:00 renumics/spotlight/io/gltf.py
--rw-r--r--  2.0 unx     5448 b- defN 80-Jan-01 00:00 renumics/spotlight/io/pandas.py
+-rw-r--r--  2.0 unx     5502 b- defN 80-Jan-01 00:00 renumics/spotlight/io/pandas.py
 -rw-r--r--  2.0 unx     9092 b- defN 80-Jan-01 00:00 renumics/spotlight/layout/__init__.py
 -rw-r--r--  2.0 unx      340 b- defN 80-Jan-01 00:00 renumics/spotlight/layout/default.py
 -rw-r--r--  2.0 unx     1529 b- defN 80-Jan-01 00:00 renumics/spotlight/layout/nodes.py
 -rw-r--r--  2.0 unx     6513 b- defN 80-Jan-01 00:00 renumics/spotlight/layout/widgets.py
 -rw-r--r--  2.0 unx      847 b- defN 80-Jan-01 00:00 renumics/spotlight/licensing/README.md
 -rw-r--r--  2.0 unx     1099 b- defN 80-Jan-01 00:00 renumics/spotlight/licensing/__init__.py
 -rw-r--r--  2.0 unx    34453 b- defN 80-Jan-01 00:00 renumics/spotlight/licensing/terms_and_conditions.py
 -rw-r--r--  2.0 unx     5326 b- defN 80-Jan-01 00:00 renumics/spotlight/licensing/verification.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 renumics/spotlight/notebook/__init__.py
 -rwxr-xr-x  2.0 unx     2554 b- defN 80-Jan-01 00:00 renumics/spotlight/notebook/cli.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 renumics/spotlight/notebook/theme/__init__.py
 -rw-r--r--  2.0 unx      279 b- defN 80-Jan-01 00:00 renumics/spotlight/notebook/theme/custom.css
 -rw-r--r--  2.0 unx      591 b- defN 80-Jan-01 00:00 renumics/spotlight/notebook/theme/logo.svg
--rw-r--r--  2.0 unx      523 b- defN 80-Jan-01 00:00 renumics/spotlight/plugin_loader.py
--rw-r--r--  2.0 unx       98 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/__init__.py
+-rw-r--r--  2.0 unx      721 b- defN 80-Jan-01 00:00 renumics/spotlight/plugin_loader.py
+-rw-r--r--  2.0 unx      888 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/api/__init__.py
+-rw-r--r--  2.0 unx     1136 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/api/config.py
+-rw-r--r--  2.0 unx     1532 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/api/filebrowser.py
+-rw-r--r--  2.0 unx     1789 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/api/layout.py
+-rw-r--r--  2.0 unx     6463 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/api/table.py
+-rw-r--r--  2.0 unx      955 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/api/user.py
+-rw-r--r--  2.0 unx    11695 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/hdf5_data_source.py
+-rw-r--r--  2.0 unx    12699 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/core/pandas_data_source.py
+-rw-r--r--  2.0 unx      511 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/pro/__init__.py
+-rw-r--r--  2.0 unx     5111 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/pro/api/table.py
+-rw-r--r--  2.0 unx     4318 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/pro/hdf5_data_source.py
+-rw-r--r--  2.0 unx     4141 b- defN 80-Jan-01 00:00 renumics/spotlight/plugins/pro/pandas_data_source.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 renumics/spotlight/py.typed
 -rw-r--r--  2.0 unx      462 b- defN 80-Jan-01 00:00 renumics/spotlight/settings.py
 -rw-r--r--  2.0 unx     1303 b- defN 80-Jan-01 00:00 renumics/spotlight/typing.py
--rw-r--r--  2.0 unx    12925 b- defN 80-Jan-01 00:00 renumics/spotlight/viewer.py
+-rw-r--r--  2.0 unx    11945 b- defN 80-Jan-01 00:00 renumics/spotlight/viewer.py
 -rw-r--r--  2.0 unx     1630 b- defN 80-Jan-01 00:00 renumics/spotlight/webbrowser.py
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc8.dist-info/WHEEL
--rw-r--r--  2.0 unx      113 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc8.dist-info/entry_points.txt
--rw-r--r--  2.0 unx     4852 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc8.dist-info/METADATA
--rw-r--r--  2.0 unx     1066 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc8.dist-info/LICENSE
-?rw-r--r--  2.0 unx     8318 b- defN 16-Jan-01 00:00 renumics_spotlight-1.0.0rc8.dist-info/RECORD
-85 files, 3702744 bytes uncompressed, 1004338 bytes compressed:  72.9%
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc9.dist-info/WHEEL
+-rw-r--r--  2.0 unx      113 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc9.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx     4813 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc9.dist-info/METADATA
+-rw-r--r--  2.0 unx     1066 b- defN 80-Jan-01 00:00 renumics_spotlight-1.0.0rc9.dist-info/LICENSE
+?rw-r--r--  2.0 unx     8474 b- defN 16-Jan-01 00:00 renumics_spotlight-1.0.0rc9.dist-info/RECORD
+86 files, 3702268 bytes uncompressed, 1004896 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -12,65 +12,38 @@
 
 Filename: renumics/spotlight/backend/__init__.py
 Comment: 
 
 Filename: renumics/spotlight/backend/apis/__init__.py
 Comment: 
 
-Filename: renumics/spotlight/backend/apis/config.py
-Comment: 
-
-Filename: renumics/spotlight/backend/apis/filebrowser.py
-Comment: 
-
-Filename: renumics/spotlight/backend/apis/layout.py
-Comment: 
-
-Filename: renumics/spotlight/backend/apis/table.py
-Comment: 
-
-Filename: renumics/spotlight/backend/apis/user.py
-Comment: 
-
 Filename: renumics/spotlight/backend/apis/websocket.py
 Comment: 
 
 Filename: renumics/spotlight/backend/app.py
 Comment: 
 
 Filename: renumics/spotlight/backend/cache.py
 Comment: 
 
 Filename: renumics/spotlight/backend/config.py
 Comment: 
 
-Filename: renumics/spotlight/backend/core/__init__.py
-Comment: 
-
-Filename: renumics/spotlight/backend/core/user.py
-Comment: 
-
 Filename: renumics/spotlight/backend/data_source.py
 Comment: 
 
 Filename: renumics/spotlight/backend/exceptions.py
 Comment: 
 
-Filename: renumics/spotlight/backend/hdf5_data_source.py
-Comment: 
-
 Filename: renumics/spotlight/backend/middlewares/__init__.py
 Comment: 
 
 Filename: renumics/spotlight/backend/middlewares/timing.py
 Comment: 
 
-Filename: renumics/spotlight/backend/pandas_data_source.py
-Comment: 
-
 Filename: renumics/spotlight/backend/server.py
 Comment: 
 
 Filename: renumics/spotlight/backend/statics/asset-manifest.json
 Comment: 
 
 Filename: renumics/spotlight/backend/statics/favicon.png
@@ -99,18 +72,18 @@
 
 Filename: renumics/spotlight/backend/statics/static/js/384.87f65e3c.chunk.js.LICENSE.txt
 Comment: 
 
 Filename: renumics/spotlight/backend/statics/static/js/886.810d122e.chunk.js
 Comment: 
 
-Filename: renumics/spotlight/backend/statics/static/js/main.27112c26.js
+Filename: renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js
 Comment: 
 
-Filename: renumics/spotlight/backend/statics/static/js/main.27112c26.js.LICENSE.txt
+Filename: renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js.LICENSE.txt
 Comment: 
 
 Filename: renumics/spotlight/backend/tasks/__init__.py
 Comment: 
 
 Filename: renumics/spotlight/backend/tasks/exceptions.py
 Comment: 
@@ -132,21 +105,15 @@
 
 Filename: renumics/spotlight/dataset/__init__.py
 Comment: 
 
 Filename: renumics/spotlight/dataset/descriptors/__init__.py
 Comment: 
 
-Filename: renumics/spotlight/dataset/descriptors/catch22.py
-Comment: 
-
-Filename: renumics/spotlight/dataset/descriptors/pca.py
-Comment: 
-
-Filename: renumics/spotlight/dataset/descriptors/utils.py
+Filename: renumics/spotlight/dataset/descriptors/data_alignment.py
 Comment: 
 
 Filename: renumics/spotlight/dataset/exceptions.py
 Comment: 
 
 Filename: renumics/spotlight/dataset/typing.py
 Comment: 
@@ -219,14 +186,50 @@
 
 Filename: renumics/spotlight/plugin_loader.py
 Comment: 
 
 Filename: renumics/spotlight/plugins/core/__init__.py
 Comment: 
 
+Filename: renumics/spotlight/plugins/core/api/__init__.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/core/api/config.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/core/api/filebrowser.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/core/api/layout.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/core/api/table.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/core/api/user.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/core/hdf5_data_source.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/core/pandas_data_source.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/pro/__init__.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/pro/api/table.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/pro/hdf5_data_source.py
+Comment: 
+
+Filename: renumics/spotlight/plugins/pro/pandas_data_source.py
+Comment: 
+
 Filename: renumics/spotlight/py.typed
 Comment: 
 
 Filename: renumics/spotlight/settings.py
 Comment: 
 
 Filename: renumics/spotlight/typing.py
@@ -234,23 +237,23 @@
 
 Filename: renumics/spotlight/viewer.py
 Comment: 
 
 Filename: renumics/spotlight/webbrowser.py
 Comment: 
 
-Filename: renumics_spotlight-1.0.0rc8.dist-info/WHEEL
+Filename: renumics_spotlight-1.0.0rc9.dist-info/WHEEL
 Comment: 
 
-Filename: renumics_spotlight-1.0.0rc8.dist-info/entry_points.txt
+Filename: renumics_spotlight-1.0.0rc9.dist-info/entry_points.txt
 Comment: 
 
-Filename: renumics_spotlight-1.0.0rc8.dist-info/METADATA
+Filename: renumics_spotlight-1.0.0rc9.dist-info/METADATA
 Comment: 
 
-Filename: renumics_spotlight-1.0.0rc8.dist-info/LICENSE
+Filename: renumics_spotlight-1.0.0rc9.dist-info/LICENSE
 Comment: 
 
-Filename: renumics_spotlight-1.0.0rc8.dist-info/RECORD
+Filename: renumics_spotlight-1.0.0rc9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## renumics/spotlight/__init__.py

```diff
@@ -14,15 +14,15 @@
     Sequence1D,
     Video,
     Window,
 )
 from .viewer import Viewer, close, viewers, show
 from .plugin_loader import load_plugins
 
-load_plugins()
+__plugins__ = load_plugins()
 
 __all__ = [
     "show",
     "close",
     "viewers",
     "Viewer",
 ]
```

## renumics/spotlight/__version__.py

```diff
@@ -1,6 +1,6 @@
 """
 Package version.
 
 Replaced at build time.
 """
-__version__ = "1.0.0-pre.8"
+__version__ = "1.0.0-pre.9"
```

## renumics/spotlight/backend/__init__.py

```diff
@@ -1,37 +1,62 @@
 """
     This module provides all backend code.
     Based on FastAPI
 """
 
 import os
 from pathlib import Path
-from typing import Union, Optional
+from typing import Any, Type, Union, Optional, Dict, Callable
 
 import pandas as pd
 
 from ..dtypes.typing import ColumnTypeMapping
+from ..typing import is_pathtype
 
 from .data_source import DataSource
-from .pandas_data_source import PandasDataSource
-from .hdf5_data_source import Hdf5DataSource
-from .exceptions import InvalidPath
+from .exceptions import InvalidDataSource
+
+data_sources: Dict[Union[str, Type], Type[DataSource]] = {}
+
+
+def add_datasource(source: Union[str, Type], klass: Type[DataSource]) -> None:
+    """
+    Add a datasource for the given file extension or type
+    """
+    data_sources[source] = klass
+
+
+def datasource(
+    source: Union[str, Type]
+) -> Callable[[Type[DataSource]], Type[DataSource]]:
+    """
+    Decorator to add a data source.
+    See `add_datasource`
+    """
+
+    def func(klass: Type[DataSource]) -> Type[DataSource]:
+        add_datasource(source, klass)
+        return klass
+
+    return func
 
 
 def create_datasource(
     source: Union[pd.DataFrame, os.PathLike, str],
     dtype: Optional[ColumnTypeMapping] = None,
 ) -> DataSource:
     """
     open the specified data source
     """
-    if isinstance(source, pd.DataFrame):
-        return PandasDataSource(df=source, dtype=dtype)
-
-    if Path(source).suffix == ".csv":
-        df = pd.read_csv(source)
-        return PandasDataSource(df=df, dtype=dtype)
 
-    if Path(source).suffix == ".h5":
-        return Hdf5DataSource(table_file=source)
+    key: Any = None
+    if is_pathtype(source):
+        key = Path(source).suffix
+    else:
+        key = type(source)
+
+    try:
+        data_source = data_sources[key]
+    except KeyError as e:
+        raise InvalidDataSource() from e
 
-    raise InvalidPath(source)
+    return data_source(source, dtype)
```

## renumics/spotlight/backend/app.py

```diff
@@ -12,30 +12,22 @@
 from fastapi.responses import JSONResponse
 from fastapi.staticfiles import StaticFiles
 from loguru import logger
 from renumics.spotlight.backend.exceptions import Problem
 
 from renumics.spotlight.licensing import spotlight_license
 from renumics.spotlight.layout.nodes import Layout
-
+from renumics.spotlight.plugin_loader import load_plugins
 from renumics.spotlight.licensing.verification import LicensedFeature
-
 from renumics.spotlight.backend.data_source import DataSource
-
 from renumics.spotlight.typing import PathType
-
 from renumics.spotlight.settings import settings
-from .apis import (
-    table,
-    user,
-    filebrowser,
-    layout as layout_api,
-    config as config_api,
-    websocket,
-)
+
+from .apis import websocket
+
 from .tasks.task_manager import TaskManager
 from .middlewares.timing import add_timing_middleware
 from .config import Config
 from .websockets import WebsocketManager
 
 
 class SpotlightApp(FastAPI):
@@ -65,21 +57,16 @@
     app.spotlight_license = spotlight_license
     app.data_source = None
     app.task_manager = TaskManager()
     app.config = Config()
     app.layout = None
     app.project_root = Path.cwd()
 
-    # setup routes
-    app.include_router(table.router, prefix="/api/table")
-    app.include_router(user.router, prefix="/api/user")
-    app.include_router(filebrowser.router, prefix="/api/browse")
+    # setup websocket route
     app.include_router(websocket.router, prefix="/api")
-    app.include_router(layout_api.router, prefix="/api/layout")
-    app.include_router(config_api.router, prefix="/api/config")
 
     @app.exception_handler(Exception)
     async def _(_: Request, e: Exception) -> JSONResponse:
         if settings.dev:
             logger.exception(e)
         else:
             logger.info(e)
@@ -101,14 +88,19 @@
                 "title": problem.title,
                 "detail": problem.detail,
                 "type": type(problem).__name__,
             },
             status_code=problem.status_code,
         )
 
+    plugins = load_plugins()
+    for plugin in plugins.values():
+        if hasattr(plugin, "on_startup"):
+            plugin.on_startup(app)
+
     @app.on_event("startup")
     def _() -> None:
         loop = asyncio.get_running_loop()
         app.websocket_manager = WebsocketManager(loop)
 
     @app.on_event("shutdown")
     def _() -> None:
```

## renumics/spotlight/backend/data_source.py

```diff
@@ -9,27 +9,28 @@
 
 import pandas as pd
 import numpy as np
 from pydantic.dataclasses import dataclass
 
 from renumics.spotlight.io import audio
 from renumics.spotlight.typing import PathOrURLType, PathType, is_iterable
-from renumics.spotlight.dataset import _prepare_path_or_url
+from renumics.spotlight.dataset import prepare_path_or_url
 from renumics.spotlight.dataset.exceptions import (
     ColumnExistsError,
     ColumnNotExistsError,
 )
 from renumics.spotlight.dtypes import Audio
 from renumics.spotlight.dtypes.typing import (
     ColumnType,
+    ColumnTypeMapping,
     FileBasedColumnType,
     get_column_type_name,
 )
 from .cache import Cache
-from .exceptions import GenerationIDMismatch, NoRowFound
+from .exceptions import DatasetNotEditable, GenerationIDMismatch, NoRowFound
 
 cache = Cache("external-data")
 
 
 @dataclasses.dataclass
 class Attrs:
     """
@@ -75,21 +76,34 @@
     author: str
     edited_at: str
 
 
 class DataSource(ABC):
     """abstract base class for different data sources"""
 
+    @abstractmethod
+    def __init__(self, source: Any, dtype: Optional[ColumnTypeMapping]):
+        """
+        Create Data Source from matching source and dtype mapping.
+        """
+
     @property
     @abstractmethod
     def column_names(self) -> List[str]:
         """
         Dataset's available column names.
         """
 
+    @property
+    def df(self) -> Optional[pd.DataFrame]:
+        """
+        Get the source data as a pandas dataframe if possible
+        """
+        return None
+
     @abstractmethod
     def __len__(self) -> int:
         """
         Get the table's length.
         """
 
     @abstractmethod
@@ -155,45 +169,45 @@
             return waveform
         except KeyError:
             ...
         waveform = audio.get_waveform(io.BytesIO(blob))
         cache[cache_key] = waveform
         return waveform
 
-    @abstractmethod
     def replace_cells(
         self, column_name: str, indices: List[int], value: Any
     ) -> CellsUpdate:
         """
         replace multiple cell's value
         """
+        raise DatasetNotEditable()
 
-    @abstractmethod
     def delete_column(self, name: str) -> None:
         """
         remove a column from the table
         """
+        raise DatasetNotEditable()
 
-    @abstractmethod
     def delete_row(self, index: int) -> None:
         """
         remove a row from the table
         """
+        raise DatasetNotEditable()
 
-    @abstractmethod
     def duplicate_row(self, index: int) -> int:
         """
         duplicate a row in the table
         """
+        raise DatasetNotEditable()
 
-    @abstractmethod
     def append_column(self, name: str, dtype_name: str) -> Column:
         """
         add a column to the table
         """
+        raise DatasetNotEditable()
 
     def _assert_index_exists(self, index: int) -> None:
         if index < -len(self) or index >= len(self):
             raise NoRowFound(index)
 
     def _assert_indices_exist(self, indices: List[int]) -> None:
         indices_array = np.array(indices)
@@ -344,15 +358,15 @@
     target_format: Optional[str] = None,
     workdir: PathType = ".",
 ) -> np.void:
     """
     Decode an external value as expected by the rest of the backend.
     """
     # pylint: disable=too-many-return-statements
-    path_or_url = _prepare_path_or_url(path_or_url, workdir)
+    path_or_url = prepare_path_or_url(path_or_url, workdir)
     if column_type is Audio:
         file = audio.prepare_input_file(path_or_url, reusable=True)
         # `file` is a filepath of type `str` or an URL downloaded as `io.BytesIO`.
         input_format, input_codec = audio.get_format_codec(file)
         if not isinstance(file, str):
             file.seek(0)
         if target_format is None:
```

## renumics/spotlight/backend/exceptions.py

```diff
@@ -28,14 +28,25 @@
 
     def __init__(self, path: PathType) -> None:
         super().__init__(
             "Invalid path", f"Path {path} is not valid.", status.HTTP_403_FORBIDDEN
         )
 
 
+class InvalidDataSource(Problem):
+    """The data source can't be opened"""
+
+    def __init__(self) -> None:
+        super().__init__(
+            "Invalid data source",
+            "Can't open supplied data source.",
+            status.HTTP_403_FORBIDDEN,
+        )
+
+
 class NoTableFileFound(Problem):
     """raised when the table file could not be found"""
 
     def __init__(self, path: PathType) -> None:
         super().__init__(
             "Dataset file not found",
             f"File {path} does not exist.",
@@ -142,7 +153,18 @@
         self.dtype = dtype
         self.value = value
         super().__init__(
             "Type conversion failed",
             f"Value of type {type(value)} cannot be converted to type {dtype}.",
             status.HTTP_422_UNPROCESSABLE_ENTITY,
         )
+
+
+class DatasetNotEditable(Problem):
+    """The dataset is not editable"""
+
+    def __init__(self) -> None:
+        super().__init__(
+            "Dataset not editable",
+            "The dataset is not editable.",
+            status.HTTP_403_FORBIDDEN,
+        )
```

## renumics/spotlight/backend/statics/asset-manifest.json

### Pretty-printed

 * *Similarity: 0.8083333333333333%*

 * *Differences: {"'entrypoints'": "{insert: [(1, 'static/js/main.2c4f3aef.js')], delete: [1]}",*

 * * "'files'": "{'main.js': './static/js/main.2c4f3aef.js'}"}*

```diff
@@ -1,13 +1,13 @@
 {
     "entrypoints": [
         "static/css/main.515252a3.css",
-        "static/js/main.27112c26.js"
+        "static/js/main.2c4f3aef.js"
     ],
     "files": {
         "index.html": "./index.html",
         "main.css": "./static/css/main.515252a3.css",
-        "main.js": "./static/js/main.27112c26.js",
+        "main.js": "./static/js/main.2c4f3aef.js",
         "static/js/384.87f65e3c.chunk.js": "./static/js/384.87f65e3c.chunk.js",
         "static/js/886.810d122e.chunk.js": "./static/js/886.810d122e.chunk.js"
     }
 }
```

## renumics/spotlight/backend/statics/index.html

```diff
@@ -1 +1 @@
-<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="./favicon.png"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><title>Spotlight</title><script defer="defer" src="./static/js/main.27112c26.js"></script><link href="./static/css/main.515252a3.css" rel="stylesheet"></head><body oncontextmenu="return!1"><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div></body></html>
+<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="./favicon.png"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><title>Spotlight</title><script defer="defer" src="./static/js/main.2c4f3aef.js"></script><link href="./static/css/main.515252a3.css" rel="stylesheet"></head><body oncontextmenu="return!1"><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div></body></html>
```

## renumics/spotlight/dataset/__init__.py

```diff
@@ -1,16 +1,14 @@
 """
 This module provides Spotlight dataset.
 """
 # pylint: disable=too-many-lines
-import ast
 import os
 import shutil
 import uuid
-import warnings
 from datetime import datetime
 from tempfile import TemporaryDirectory
 from typing import (
     Any,
     Callable,
     Dict,
     Iterable,
@@ -30,14 +28,19 @@
 import prettytable
 import trimesh
 import validators
 from loguru import logger
 from typing_extensions import Literal, TypeGuard
 
 from renumics.spotlight import __version__
+from renumics.spotlight.io.pandas import (
+    infer_dtypes,
+    prepare_column,
+    is_string_mask,
+)
 from renumics.spotlight.licensing import username, verify_license_or_exit
 from renumics.spotlight.typing import (
     BoolType,
     IndexType,
     Indices1DType,
     PathOrURLType,
     PathType,
@@ -52,27 +55,25 @@
     Sequence1D,
     Image,
     Audio,
     Category,
     Video,
     Window,
 )
+from renumics.spotlight.dtypes.exceptions import UnsupportedDType
 from renumics.spotlight.dtypes.typing import (
     ColumnType,
     ColumnTypeMapping,
-    COLUMN_TYPES_BY_NAME,
     FileBasedColumnType,
     get_column_type,
     get_column_type_name,
-    is_column_type,
     is_file_based_column_type,
 )
 from . import exceptions
 from .typing import (
-    NAME_BY_COLUMN_TYPE,
     REF_COLUMN_TYPE_NAMES,
     SimpleColumnType,
     RefColumnType,
     ExternalColumnType,
     BoolColumnInputType,
     IntColumnInputType,
     FloatColumnInputType,
@@ -93,208 +94,40 @@
 )
 
 INTERNAL_COLUMN_NAMES = ["__last_edited_by__", "__last_edited_at__"]
 
 _EncodedColumnType = Optional[Union[bool, int, float, str, np.ndarray, h5py.Reference]]
 
 
-def _get_current_datetime() -> datetime:
+def get_current_datetime() -> datetime:
     """
     Get current datetime with timezone.
     """
     return datetime.now().astimezone()
 
 
-def _prepare_path_or_url(
-    path_or_url: PathOrURLType, workdir: PathType
-) -> PathOrURLType:
+def prepare_path_or_url(path_or_url: PathOrURLType, workdir: PathType) -> str:
     """
-    For a relative path, resolve it relative to the `workdir`.
+    For a relative path, prefix it with the `workdir`.
     For an absolute path or an URL, do nothing.
     """
-    path_or_url = str(path_or_url)
-    if not validators.url(path_or_url):
-        return os.path.join(workdir, path_or_url)
-    return path_or_url
-
-
-def _prepare_pandas_column(
-    column: pd.Series,
-    dtype: Type[ColumnType],
-    workdir: Optional[PathType] = None,
-) -> pd.Series:
-    """
-    Prepare pandas column for conversion to the given `dtype`.
-    """
-    if workdir is not None:
-        workdir = os.path.abspath(workdir)
-    if issubclass(dtype, (np.ndarray, Window, _BaseData)):
-        null_mask = column.isnull()
-        if issubclass(dtype, _BaseFileBasedData):
-            # For file based data, we expect filepaths or URLs only.
-            column = column.astype(str, copy=True)
-            null_mask |= column == ""
-            if workdir is not None:
-                column[~null_mask] = column[~null_mask].apply(
-                    lambda x: _prepare_path_or_url(x, workdir)
-                )
-            column[null_mask] = None
-            return column
-        if dtype in (np.ndarray, Embedding, Sequence1D, Window):
-            column = column.astype(object, copy=True)
-            null_mask |= column == ""
-            str_mask = (column.map(type) == str) & ~null_mask
-            column[str_mask] = column[str_mask].apply(ast.literal_eval)
-            column[null_mask] = None
-            return column
-        raise exceptions.InvalidDTypeError(
-            f"Column type {dtype.__name__} not supported yet."
-        )
-    if dtype is datetime:
-        return pd.to_datetime(column)
-    if dtype is Category:
-        return column.astype("category", copy=True)
-    if dtype is str:
-        null_cells = column.isnull()
-        return column.astype(str, copy=True).mask(null_cells, None)
-    return column.astype(dtype, copy=True)
-
-
-def _asarray(values: Any) -> np.ndarray:
-    """
-    Try to convert data to an array.
-    """
-    warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
-    data = np.array(values)
-    warnings.filterwarnings("default", category=np.VisibleDeprecationWarning)
-    return data
-
-
-def _convert_pandas_column(
-    column: pd.Series,
-    dtype: Optional[Type[ColumnType]] = None,
-    workdir: Optional[PathType] = None,
-) -> Tuple[Optional[np.ndarray], Optional[Type[ColumnType]], Dict[str, Any]]:
-    """
-    convert pandas column to array importable into spotlight
-    """
-    # pylint: disable=too-many-branches, too-many-return-statements
-    if dtype is not None:
-        column = _prepare_pandas_column(column, dtype, workdir)
-        if issubclass(dtype, (np.ndarray, Window, _BaseData)):
-            return column.to_numpy(), dtype, {"optional": column.hasnans}
-
-    if pd.api.types.is_bool_dtype(column.dtype):  # `column` fails in pandas 1.4.0
-        return column.to_numpy(), bool, {}
-    if pd.api.types.is_categorical_dtype(column):
-        if "" in column.cat.categories and column.hasnans:
-            logger.warning(
-                "Skipped categorical column, "
-                'because it contains NANs and "", '
-                'which both would be mapped to "" in spotlight.'
-            )
-            return None, None, {}
-        return (
-            column.astype(str).replace("nan", "").to_numpy(),
-            Category,
-            {"optional": column.hasnans, "categories": column.cat.categories.to_list()},
-        )
-
-    if pd.api.types.is_integer_dtype(column):
-        return column.to_numpy(), int, {}
-    if pd.api.types.is_float_dtype(column):
-        return column.to_numpy(), float, {}
-    if pd.api.types.is_datetime64_any_dtype(column):
-        # Some information can be lost due to the numpy conversion.
-        return (
-            column.to_numpy().astype("datetime64[us]"),
-            datetime,
-            {"optional": column.hasnans},
-        )
-    if (
-        pd.api.types.is_string_dtype(column)
-        and ((column.map(type) == str) | (column.isna())).all()
-    ):
-        # `is_string_dtype` only checks `object` dtype, it's not enough.
-        return (
-            column.replace([np.nan], [None]).to_numpy(),
-            str,
-            {"optional": column.hasnans},
-        )
-
-    null_mask = column.isnull()
-    try:
-        data = _asarray(column[~null_mask].to_list())
-    except Exception:  # pylint: disable=broad-except
-        ...
-    else:
-        if (
-            data.ndim == 2
-            and len(data) == (~null_mask).sum()
-            and issubclass(data.dtype.type, (np.floating, np.integer))
-        ):
-            if null_mask.any():
-                # Object array filled with `None`'s by default.
-                full_data = np.empty(len(column), dtype=object)
-                full_data[np.where(~null_mask)[0]] = list(data)
-                return full_data, Embedding, {}
-            return data, Embedding, {}
-
-    return None, None, {}
-
-
-def _get_column_type(
-    x: Union[str, h5py.Dataset, h5py.AttributeManager]
-) -> Type[ColumnType]:
-    """
-    Get column type by its name, or extract it from `h5py` entities.
-    """
-    if isinstance(x, str):
-        return get_column_type(x)
-    if isinstance(x, h5py.Dataset):
-        return get_column_type(x.attrs["type"])
-    if isinstance(x, h5py.AttributeManager):
-        return get_column_type(x["type"])
-    raise TypeError(
-        f"Argument is expected to ba an instance of type `str`, `h5py.Dataset` "
-        f"or `h5py.AttributeManager`, but `x` of type {type(x)} received."
-    )
-
-
-def _is_ref_column_type_name(type_name: str) -> bool:
-    """
-    Check if a column type name is reference column type name.
-    """
-    return type_name in REF_COLUMN_TYPE_NAMES
-
-
-def _get_column_name(column: h5py.Dataset) -> str:
-    """
-    Get name of a column.
-    """
-    return column.name.split("/")[-1]
-
-
-def _is_ref_column(column: h5py.Dataset) -> bool:
-    """
-    Check if a column is ref column.
-    """
-    return column.attrs["type"] in REF_COLUMN_TYPE_NAMES and (
-        h5py.check_string_dtype(column.dtype) or h5py.check_ref_dtype(column.dtype)
-    )
+    path_or_url_str = str(path_or_url)
+    if validators.url(path_or_url_str):
+        return path_or_url_str
+    return os.path.join(workdir, path_or_url_str)
 
 
-def _escape_dataset_name(name: str) -> str:
+def escape_dataset_name(name: str) -> str:
     r"""
     Replace "\" with "\\" and "/" with "\s".
     """
     return name.replace("\\", "\\\\").replace("/", r"\s")
 
 
-def _unescape_dataset_name(escaped_name: str) -> str:
+def unescape_dataset_name(escaped_name: str) -> str:
     r"""
     Replace "\\" with "\" and "\s" with "/".
     """
     name = ""
     i = 0
     while i < len(escaped_name):
         char = escaped_name[i]
@@ -523,15 +356,15 @@
             self._assert_index_exists(item)
             item = cast(int, item)
             if item < 0:
                 item += self._length
             for column_name in self.keys() + INTERNAL_COLUMN_NAMES:
                 column = self._h5_file[column_name]
                 raw_values = column[item + 1 :]
-                if _get_column_type(column) is Embedding:
+                if self._get_column_type(column) is Embedding:
                     raw_values = list(raw_values)
                 column[item:-1] = raw_values
                 column.resize(self._length - 1, axis=0)
             self._length -= 1
         else:
             raise exceptions.InvalidIndexError(
                 f"`item` argument should be a string or an index/indices, but"
@@ -757,15 +590,15 @@
 
     def close(self) -> None:
         """
         Close file.
         """
         if not self._closed:
             if self._is_writable():
-                current_time = _get_current_datetime().isoformat()
+                current_time = get_current_datetime().isoformat()
                 raw_attrs = self._h5_file.attrs
                 # Version could be `None`, but *shouldn't* be.
                 raw_attrs["version"] = __version__
                 raw_attrs["last_edited_by"] = username
                 raw_attrs["last_edited_at"] = current_time
                 if "created" not in raw_attrs:
                     raw_attrs["created"] = __version__
@@ -804,20 +637,20 @@
         """
         Iterate through dataset rows.
         """
         self._assert_is_opened()
         if isinstance(column_names, str):
             self._assert_column_exists(column_names)
             column = self._h5_file[column_names]
-            column_type = _get_column_type(column)
+            column_type = self._get_column_type(column)
             if column.attrs.get("external", False):
                 for value in column:
                     column_type = cast(Type[ExternalColumnType], column_type)
                     yield self._decode_external_value(value, column_type)
-            elif _is_ref_column(column):
+            elif self._is_ref_column(column):
                 for ref in column:
                     column_type = cast(Type[RefColumnType], column_type)
                     yield self._decode_ref_value(ref, column_type, column_names)
             else:
                 for value in column:
                     column_type = cast(Type[SimpleColumnType], column_type)
                     yield self._decode_simple_value(value, column, column_type)
@@ -881,34 +714,65 @@
             >>> with Dataset("docs/example.h5", "r") as dataset:
             ...     print(len(dataset))
             ...     print(sorted(dataset.keys()))
             3
             ['bools', 'datetimes', 'floats', 'ints', 'strings']
         """
         self._assert_is_writable()
-        dtype = dtype or {}
+
         if index:
             df = df.reset_index(level=df.index.names)
 
+        inferred_dtype = infer_dtypes(df, dtype)
+
         for column_name in df.columns:
-            column, column_dtype, attributes = _convert_pandas_column(
-                df[column_name],
-                dtype=dtype[column_name] if column_name in dtype else None,
-                workdir=workdir,
-            )
-            if column is not None and column_dtype is not None:
-                self.append_column(column_name, column_dtype, column, **attributes)
+            try:
+                column = df[column_name]
+                column_type = inferred_dtype[column_name]
 
-        not_imported_columns = df.columns.difference(self.keys())
-        if len(not_imported_columns) > 0:
-            logger.warning(
-                'Columns "'
-                + '", "'.join(not_imported_columns)
-                + '" not imported to the dataset. Please append them manually.'
-            )
+                column = prepare_column(column, column_type)
+
+                if workdir is not None and is_file_based_column_type(dtype):
+                    # For file-based data types, relative paths should be resolved.
+                    str_mask = is_string_mask(column)
+                    column[str_mask] = column[str_mask].apply(
+                        lambda x: prepare_path_or_url(x, workdir)
+                    )
+
+                attrs = {}
+
+                if column_type is Category:
+                    attrs["categories"] = column.cat.categories.to_list()
+                    values = column.to_numpy()
+                    # `pandas` uses `NaN`s for unknown values, we use `None`.
+                    values = np.where(pd.isna(values), np.array(None), values)
+                elif column_type is datetime:
+                    values = column.to_numpy("datetime64[us]")
+                else:
+                    values = column.to_numpy()
+
+                if is_file_based_column_type(column_type):
+                    attrs["external"] = False
+                    attrs["lookup"] = False
+
+                self.append_column(
+                    column_name,
+                    column_type,
+                    values,
+                    hidden=column_name.startswith("_"),
+                    optional=column_type not in (bool, int),
+                    **attrs,
+                )
+            except Exception as e:  # pylint: disable=broad-except
+                if column_name in (dtype or {}):
+                    raise e
+                logger.warning(
+                    f"Column '{column_name}' not imported from "
+                    f"`pandas.DataFrame` because of the following error:\n{e}"
+                )
 
     def from_csv(
         self,
         filepath: PathType,
         dtype: Optional[ColumnTypeMapping] = None,
         columns: Optional[Iterable[str]] = None,
         workdir: Optional[PathType] = None,
@@ -1941,15 +1805,15 @@
         length = len(self)
         if index < 0:
             index += length
         for column_name in self.keys() + INTERNAL_COLUMN_NAMES:
             column = self._h5_file[column_name]
             column.resize(length + 1, axis=0)
             raw_values = column[index:-1]
-            if _get_column_type(column) is Embedding:
+            if self._get_column_type(column) is Embedding:
                 raw_values = list(raw_values)
             column[index + 1 :] = raw_values
         self._length += 1
         try:
             self._set_row(index, values)
         except Exception as e:
             del self[index]
@@ -1984,16 +1848,16 @@
         """
         # pylint: disable=too-many-return-statements
         self._assert_is_opened()
         self._assert_column_exists(column_name, internal=True)
         column = self._h5_file[column_name]
         raw_values = column[()]
 
-        column_type = _get_column_type(column)
-        if _is_ref_column(column):
+        column_type = self._get_column_type(column)
+        if self._is_ref_column(column):
             return ~raw_values.astype(bool)
         if column_type is datetime:
             return np.array([raw_value in ["", b""] for raw_value in raw_values])
         if column_type is float:
             return np.isnan(raw_values)
         if column_type is Category:
             return raw_values == -1
@@ -2058,15 +1922,15 @@
                         column.shape,
                         column.dtype,
                         maxshape=column.maxshape,
                     )
                     for attr_name, attr in column.attrs.items():
                         new_column.attrs[attr_name] = attr
                     raw_values = column[()]
-                    if _is_ref_column(column):
+                    if self._is_ref_column(column):
                         if h5py.check_string_dtype(column.dtype):
                             # New-style string refs.
                             for ref in raw_values:
                                 if ref:
                                     h5_dataset = self._resolve_ref(ref, column_name)
                                     if h5_dataset.name not in h5_file:
                                         h5_file.create_dataset(
@@ -2084,15 +1948,15 @@
                                         new_h5_dataset = h5_file.create_dataset(
                                             h5_dataset.name, data=h5_dataset[()]
                                         )
                                     refs.append(new_h5_dataset.ref)
                                 else:
                                     refs.append(None)
                             raw_values = refs
-                    if _get_column_type(column) is Embedding:
+                    if self._get_column_type(column) is Embedding:
                         raw_values = list(raw_values)
                     new_column[:] = raw_values
             self.close()
             shutil.move(new_dataset, os.path.realpath(self._filepath))
             self.open()
 
     @overload
@@ -2211,15 +2075,15 @@
                 f"`item` argument should be a string, but value {name} of type "
                 f"`{type(name)}` received.`"
             )
         self._assert_column_exists(name, internal=True)
 
         column = self._h5_file[name]
         column_attrs = column.attrs
-        column_type = _get_column_type(column_attrs)
+        column_type = self._get_column_type(column_attrs)
         allowed_attributes = self._user_column_attributes(column_type)
 
         attrs: Dict[
             str,
             Optional[
                 Union[
                     bool,
@@ -2268,15 +2132,15 @@
 
         return attrs
 
     def _assert_valid_attribute(
         self, attribute_name: str, attribute_value: ColumnInputType, column_name: str
     ) -> None:
         column = self._h5_file.get(column_name)
-        column_type = _get_column_type(column)
+        column_type = self._get_column_type(column)
 
         allowed_attributes = self._user_column_attributes(column_type)
         if attribute_name not in allowed_attributes:
             raise exceptions.InvalidAttributeError(
                 f'Setting an attribute with the name "{attribute_name}" for column '
                 f'"{column_name}" is not allowed. '
                 f'Allowed attribute names for "{column_type}" '
@@ -2371,15 +2235,15 @@
         attrs["optional"] = optional
         attrs["description"] = description
         attrs["tags"] = tags
 
         attrs = {k: v for k, v in attrs.items() if v is not None}
 
         column = self._h5_file[name]
-        column_type = _get_column_type(column)
+        column_type = self._get_column_type(column)
 
         if "lookup" in attrs:
             lookup = attrs["lookup"]
             if lookup in (True, np.bool_(True)):
                 attrs["lookup"] = {}
             elif lookup in (False, np.bool_(False)):
                 if "lookup_values" in column.attrs:
@@ -2480,15 +2344,15 @@
             # Set new default value.
             try:
                 if default is None and old_default is None:
                     default = self._default_default(column_type)
                     if (
                         default is None
                         and column_type is Embedding
-                        and not _is_ref_column(column)
+                        and not self._is_ref_column(column)
                     ):
                         # For a non-ref `Embedding` column, replace `None` with an empty array.
                         default = np.empty(0, column.dtype.metadata["vlen"])
                 if column_type is Category and default != "":
                     if default not in column.attrs["category_keys"]:
                         column.attrs["category_values"] = np.append(
                             column.attrs["category_values"],
@@ -2642,15 +2506,15 @@
         self,
         column: h5py.Dataset,
         values: Union[ColumnInputType, Iterable[ColumnInputType]],
         indices: Union[None, slice, List[Union[int, bool]], np.ndarray] = None,
         preserve_values: bool = False,
     ) -> None:
         # pylint: disable=too-many-branches, too-many-statements
-        column_name = _get_column_name(column)
+        column_name = self._get_column_name(column)
         row_wise_filling_message = (
             f"Dataset has initialized, but unfilled columns and should be "
             f"filled row-wise, but values received for column "
             f'"{column_name}".'
         )
         attrs = column.attrs
 
@@ -2712,15 +2576,15 @@
                     )
             elif indices_length == 0:
                 # Set an empty column with 0 values, i.e. do nothing.
                 return
             else:
                 # Reorder values according to the given indices.
                 encoded_values = encoded_values[values_indices]
-            if _get_column_type(column) is Embedding:
+            if self._get_column_type(column) is Embedding:
                 encoded_values = list(encoded_values)
         elif values is not None:
             # A single value is given. `Window` and `Embedding` values should
             # never go here, because they are always iterable, even a single value.
             if self._length == 0:
                 if len(self._column_names) == 1:
                     target_column_length = indices_length = 1
@@ -2815,19 +2679,19 @@
                     f"to the dataset with the length {self._length}."
                 ) from e
             indices, mapping = np.unique(indices, return_inverse=True)
             values = column[indices][mapping]
         return self._decode_values(values, column)
 
     def _decode_values(self, values: np.ndarray, column: h5py.Dataset) -> np.ndarray:
-        column_type = _get_column_type(column)
+        column_type = self._get_column_type(column)
         if column.attrs.get("external", False):
             column_type = cast(Type[ExternalColumnType], column_type)
             return self._decode_external_values(values, column_type)
-        if _is_ref_column(column):
+        if self._is_ref_column(column):
             column_type = cast(Type[RefColumnType], column_type)
             return self._decode_ref_values(values, column, column_type)
         column_type = cast(Type[SimpleColumnType], column_type)
         return self._decode_simple_values(values, column, column_type)
 
     @staticmethod
     def _decode_simple_values(
@@ -2854,15 +2718,15 @@
             values[null_mask] = None
         # For column types `bool`, `int`, `float` or `Window`, return the array as-is.
         return values
 
     def _decode_ref_values(
         self, values: np.ndarray, column: h5py.Dataset, column_type: Type[RefColumnType]
     ) -> np.ndarray:
-        column_name = _get_column_name(column)
+        column_name = self._get_column_name(column)
         if column_type in (np.ndarray, Embedding):
             # `np.array([<...>], dtype=object)` creation does not work for
             # some cases and erases dtypes of sub-arrays, so we use assignment.
             decoded_values = np.empty(len(values), dtype=object)
             decoded_values[:] = [
                 self._decode_ref_value(ref, column_type, column_name) for ref in values
             ]
@@ -2904,15 +2768,15 @@
         """
         names = []
         lengths = []
         for name in self._h5_file:
             h5_dataset = self._h5_file[name]
             if isinstance(h5_dataset, h5py.Dataset):
                 try:
-                    _get_column_type(h5_dataset)
+                    self._get_column_type(h5_dataset)
                 except (KeyError, exceptions.InvalidDTypeError):
                     continue
                 else:
                     names.append(name)
                     shape = h5_dataset.shape
                     lengths.append(shape[0] if shape else 0)
         max_count = 0
@@ -2937,39 +2801,39 @@
                 f"this length taken as the dataset's columns."
             )
         return column_names, length
 
     def _encode_values(
         self, values: Iterable[ColumnInputType], column: h5py.Dataset
     ) -> np.ndarray:
-        if _is_ref_column(column):
+        if self._is_ref_column(column):
             values = cast(Iterable[RefColumnInputType], values)
             return self._encode_ref_values(values, column)
         values = cast(Iterable[SimpleColumnInputType], values)
         return self._encode_simple_values(values, column)
 
     def _encode_simple_values(
         self, values: Iterable[SimpleColumnInputType], column: h5py.Dataset
     ) -> np.ndarray:
         # pylint: disable=too-many-branches, too-many-return-statements
-        column_type = cast(Type[SimpleColumnType], _get_column_type(column))
+        column_type = cast(Type[SimpleColumnType], self._get_column_type(column))
         if column_type is Category:
             mapping = dict(
                 zip(column.attrs["category_keys"], column.attrs["category_values"])
             )
             if column.attrs.get("optional", False):
                 default = column.attrs.get("default", -1)
                 mapping[None] = default
                 if default == -1:
                     mapping[""] = -1
             try:
                 # Map values and save as the right int type.
                 return np.array([mapping[x] for x in values], dtype=column.dtype)
             except KeyError as e:
-                column_name = _get_column_name(column)
+                column_name = self._get_column_name(column)
                 raise exceptions.InvalidValueError(
                     f'Values for the categorical column "{column_name}" '
                     f"contain unknown categories."
                 ) from e
         if column_type is datetime:
             if _check_valid_array(values, column_type):
                 encoded_values = np.array(
@@ -2991,15 +2855,15 @@
                     return np.broadcast_to(values, (1, 2))
                 if len(encoded_values) == 0:
                     # An empty array, reshape for compatibility.
                     return np.broadcast_to(values, (0, 2))
             elif encoded_values.ndim == 2 and encoded_values.shape[1] == 2:
                 # An array with valid windows.
                 return encoded_values
-            column_name = _get_column_name(column)
+            column_name = self._get_column_name(column)
             raise exceptions.InvalidShapeError(
                 f'Input values to `Window` column "{column_name}" should have '
                 f"one of shapes (2,) (a single window) or (n, 2) (multiple "
                 f"windows), but values with shape {encoded_values.shape} received."
             )
         if column_type is Embedding:
             if _check_valid_array(values, column_type):
@@ -3024,15 +2888,15 @@
             encoded_values[:] = values_list
             encoded_values = self._replace_none(encoded_values, column)
             return encoded_values
         # column type is `bool`, `int`, `float` or `str`.
         encoded_values = self._asarray(values, column, column_type)
         if encoded_values.ndim == 1:
             return encoded_values
-        column_name = _get_column_name(column)
+        column_name = self._get_column_name(column)
         raise exceptions.InvalidShapeError(
             f'Input values to `{column_type}` column "{column_name}" should '
             f"be 1-dimensional, but values with shape {encoded_values.shape} "
             f"received."
         )
 
     def _encode_ref_values(
@@ -3062,15 +2926,15 @@
         encoded_values = np.array(values, dtype=object)
         encoded_values = self._replace_none(encoded_values, column)
         try:
             # At the moment, `None`s are already replaced, so try optimistic
             # dtype conversion.
             return np.array(encoded_values.tolist(), dtype=column.dtype)
         except TypeError as e:
-            column_name = _get_column_name(column)
+            column_name = self._get_column_name(column)
             raise exceptions.InvalidValueError(
                 f'Values for the column "{column_name}" of type {column_type} '
                 f"are not convertible to the dtype {column.dtype}."
             ) from e
 
     @staticmethod
     def _replace_none(values: np.ndarray, column: h5py.Dataset) -> np.ndarray:
@@ -3151,27 +3015,27 @@
         self, value: ColumnInputType, column: h5py.Dataset
     ) -> _EncodedColumnType:
         """
         Encode a value for writing into a column, *but* do not replace `None`s
         with default value (only check that this exists), since batch replace
         should be faster.
         """
-        column_name = _get_column_name(column)
+        column_name = self._get_column_name(column)
         attrs = column.attrs
         if value is None:
             if attrs.get("optional", False):
                 return value
             raise exceptions.InvalidDTypeError(
                 f'No value given for the non-optional column "{column_name}".'
             )
         if attrs.get("external", False):
             value = cast(PathOrURLType, value)
             return self._encode_external_value(value, column)
-        column_type = _get_column_type(attrs)
-        if _is_ref_column(column):
+        column_type = self._get_column_type(attrs)
+        if self._is_ref_column(column):
             value = cast(RefColumnInputType, value)
             return self._encode_ref_value(value, column, column_type, column_name)
         value = cast(SimpleColumnInputType, value)
         return self._encode_simple_value(value, column, column_type, column_name)
 
     def _encode_simple_value(
         self,
@@ -3272,15 +3136,15 @@
             value = np.asarray(value)
         # `value` can be a `np.ndarray` or a `np.void`.
         if isinstance(value, np.ndarray):
             # Check dtype.
             self._assert_valid_or_set_value_dtype(value.dtype, column)
             if column_type is Embedding:
                 self._assert_valid_or_set_embedding_shape(value.shape, column)
-        dataset_name = str(uuid.uuid4()) if key is None else _escape_dataset_name(key)
+        dataset_name = str(uuid.uuid4()) if key is None else escape_dataset_name(key)
         h5_dataset = self._h5_file.create_dataset(
             f"__group__/{column_name}/{dataset_name}", data=value
         )
         if h5py.check_ref_dtype(column.dtype):
             ref = h5_dataset.ref  # Legacy handling.
         else:
             ref = dataset_name
@@ -3302,15 +3166,15 @@
         """
         Encode an external value, i.e. an URL or a path.
         Value *should not* be a `None`.
 
         Column *must* be an external column (H5 dataset with string dtype).
         """
         if not isinstance(value, (str, os.PathLike)):
-            column_name = _get_column_name(column)
+            column_name = self._get_column_name(column)
             raise exceptions.InvalidDTypeError(
                 f'For the external column "{column_name}" values should '
                 f"contain only URLs and/or paths (`str` or `os.PathLike`), but "
                 f"value {value} of type {type(value)} received."
             )
         value = str(value)
         attrs = column.attrs
@@ -3340,15 +3204,15 @@
             self._write_lookup(
                 attrs,
                 lookup_keys + [value],
                 np.concatenate(  # pylint: disable=unexpected-keyword-arg
                     (attrs["lookup_values"], [value]),
                     dtype=column.dtype,
                 ),
-                _get_column_name(column),
+                self._get_column_name(column),
             )
         return value
 
     @staticmethod
     def _assert_valid_value_type(
         value: ColumnInputType, column_type: Type[ColumnType], column_name: str
     ) -> None:
@@ -3363,23 +3227,23 @@
     def _decode_value(
         self,
         value: Union[
             np.bool_, np.integer, np.floating, bytes, str, np.ndarray, h5py.Reference
         ],
         column: h5py.Dataset,
     ) -> Optional[ColumnType]:
-        column_type = _get_column_type(column)
+        column_type = self._get_column_type(column)
         if column.attrs.get("external", False):
             value = cast(bytes, value)
             column_type = cast(Type[ExternalColumnType], column_type)
             return self._decode_external_value(value, column_type)
-        if _is_ref_column(column):
+        if self._is_ref_column(column):
             value = cast(Union[bytes, h5py.Reference], value)
             column_type = cast(Type[RefColumnType], column_type)
-            column_name = _get_column_name(column)
+            column_name = self._get_column_name(column)
             return self._decode_ref_value(value, column_type, column_name)
         value = cast(Union[np.bool_, np.integer, np.floating, bytes, np.ndarray], value)
         column_type = cast(Type[SimpleColumnType], column_type)
         return self._decode_simple_value(value, column, column_type)
 
     @staticmethod
     def _decode_simple_value(
@@ -3439,15 +3303,15 @@
         value: Union[str, bytes],
         column_type: Type[ExternalColumnType],
     ) -> Optional[ExternalColumnType]:
         if not value:
             return None
         if isinstance(value, bytes):
             value = value.decode("utf-8")
-        file = _prepare_path_or_url(value, os.path.dirname(self._filepath))
+        file = prepare_path_or_url(value, os.path.dirname(self._filepath))
         try:
             return column_type.from_file(file)
         except Exception:  # pylint: disable=broad-except
             # No matter what happens, we should not crash, but warn instead.
             logger.warning(
                 f"File or URL {value} either does not exist or could not be "
                 f"loaded by the class `spotlight.{column_type.__name__}`."
@@ -3456,15 +3320,15 @@
             )
             return None
 
     def _append_internal_columns(self) -> None:
         """
         Append internal columns to the first created or imported dataset.
         """
-        internal_column_values = [username, _get_current_datetime()]
+        internal_column_values = [username, get_current_datetime()]
         for column_name, value in zip(INTERNAL_COLUMN_NAMES, internal_column_values):
             try:
                 column = self._h5_file[column_name]
             except KeyError:
                 # Internal column does not exist, create.
                 value = cast(Union[str, datetime], value)
                 self.append_column(
@@ -3493,15 +3357,15 @@
         self, index: Optional[Union[IndexType, Indices1DType]] = None
     ) -> None:
         """
         Update internal columns.
 
         Indices should be prepared (slice with positive step or unique sorted sequence).
         """
-        internal_column_values = [username, _get_current_datetime().isoformat()]
+        internal_column_values = [username, get_current_datetime().isoformat()]
         for column_name, value in zip(INTERNAL_COLUMN_NAMES, internal_column_values):
             if column_name not in self._h5_file:
                 continue
             column = self._h5_file[column_name]
             column_length = len(column)
             if column_length != self._length:
                 column.resize(self._length, axis=0)
@@ -3540,14 +3404,48 @@
         if isinstance(ref, bytes):
             ref = ref.decode("utf-8")
         if isinstance(ref, str):
             return self._h5_file[f"__group__/{column_name}/{ref}"]
         return self._h5_file[ref]
 
     @staticmethod
+    def _get_column_type(
+        x: Union[str, h5py.Dataset, h5py.AttributeManager]
+    ) -> Type[ColumnType]:
+        """
+        Get column type by its name, or extract it from `h5py` entities.
+        """
+        if isinstance(x, str):
+            return get_column_type(x)
+        if isinstance(x, h5py.Dataset):
+            return get_column_type(x.attrs["type"])
+        if isinstance(x, h5py.AttributeManager):
+            return get_column_type(x["type"])
+        raise TypeError(
+            f"Argument is expected to ba an instance of type `str`, `h5py.Dataset` "
+            f"or `h5py.AttributeManager`, but `x` of type {type(x)} received."
+        )
+
+    @staticmethod
+    def _get_column_name(column: h5py.Dataset) -> str:
+        """
+        Get name of a column.
+        """
+        return column.name.split("/")[-1]
+
+    @staticmethod
+    def _is_ref_column(column: h5py.Dataset) -> bool:
+        """
+        Check if a column is ref column.
+        """
+        return column.attrs["type"] in REF_COLUMN_TYPE_NAMES and (
+            h5py.check_string_dtype(column.dtype) or h5py.check_ref_dtype(column.dtype)
+        )
+
+    @staticmethod
     def _check_mode(mode: str) -> None:
         """
         Check an open mode.
         """
         if mode not in ("r", "r+", "w", "w-", "x", "a"):
             raise exceptions.InvalidModeError(
                 f'Open mode should be one of "r", "r+", "w", "w-"/"x" or "a" '
@@ -3620,53 +3518,53 @@
                 f"type `{type(index)}` received."
             )
         if index < -self._length or index >= self._length:
             raise exceptions.InvalidIndexError(
                 f"Row {index} does not exist, dataset has length {self._length}."
             )
 
-    @staticmethod
-    def _assert_valid_or_set_value_dtype(dtype: np.dtype, column: h5py.Dataset) -> None:
+    def _assert_valid_or_set_value_dtype(
+        self, dtype: np.dtype, column: h5py.Dataset
+    ) -> None:
         attrs = column.attrs
         if "value_dtype" in attrs:
             if dtype.str != attrs["value_dtype"]:
-                column_name = _get_column_name(column)
+                column_name = self._get_column_name(column)
                 raise exceptions.InvalidDTypeError(
                     f'Values for {attrs["type"]} column "{column_name}" '
                     f"should have dtype `{np.dtype(attrs['value_dtype'])}`, "
                     f"but value with dtype `{dtype}` received."
                 )
         elif issubclass(dtype.type, (np.bool_, np.number)):
             attrs["value_dtype"] = dtype.str
         else:
-            column_name = _get_column_name(column)
+            column_name = self._get_column_name(column)
             raise exceptions.InvalidDTypeError(
                 f'Values for {attrs["type"]} column "{column_name}" should '
                 f"have numeric or boolean dtype, but value with dtype {dtype} "
                 f"received."
             )
 
-    @staticmethod
     def _assert_valid_or_set_embedding_shape(
-        shape: Tuple[int, ...], column: h5py.Dataset
+        self, shape: Tuple[int, ...], column: h5py.Dataset
     ) -> None:
         attrs = column.attrs
         if shape == (0,) and attrs.get("optional", False):
             # Do not check shape if an empty array given for an optional column.
             return
         if "value_shape" in attrs:
             if shape != attrs["value_shape"]:
-                column_name = _get_column_name(column)
+                column_name = self._get_column_name(column)
                 raise exceptions.InvalidShapeError(
                     f'Values for `Embedding` column "{column_name}" '
                     f'should have shape {attrs["value_shape"]}, but '
                     f"value with shape {shape} received."
                 )
         elif len(shape) == 1 and shape[0] > 0:
             attrs["value_shape"] = shape
         else:
-            column_name = _get_column_name(column)
+            column_name = self._get_column_name(column)
             raise exceptions.InvalidShapeError(
                 f'Values for `Embedding` column "{column_name}" should '
                 f"have shape `(num_features,)`, `num_features > 0`, "
                 f"but value with shape {shape} received."
             )
```

## renumics/spotlight/dataset/descriptors/__init__.py

```diff
@@ -1,5 +1,131 @@
 """make descriptor methods more available
 """
-from .pca import pca
-from .catch22 import catch22
-from .utils import align_column_data
+from typing import List, Optional, Tuple
+
+import numpy as np
+import pycatch22
+from sklearn.decomposition import PCA
+from sklearn.preprocessing import StandardScaler
+
+from renumics.spotlight.dataset import Dataset
+from renumics.spotlight.dataset.exceptions import ColumnExistsError, InvalidDTypeError
+from renumics.spotlight.dtypes import Audio, Sequence1D
+from .data_alignment import align_column_data
+
+
+def pca(
+    dataset: Dataset,
+    column: str,
+    n_components: int = 8,
+    inplace: bool = False,
+    suffix: str = "pca",
+    overwrite: bool = False,
+) -> Optional[Tuple[np.ndarray, np.ndarray]]:
+    """
+    Generate PCA embeddings for the given column of a dataset and
+    optionally write them back into dataset.
+    """
+    # pylint: disable=too-many-arguments
+    embedding_column_name = f"{column}-{suffix}"
+    if inplace and not overwrite and embedding_column_name in dataset.keys():
+        raise ColumnExistsError(
+            f'Column "{embedding_column_name}" already exists. Either set '
+            f"another `suffix` argument or set `overwrite` argument to `True`."
+        )
+    data, mask = align_column_data(dataset, column, allow_nan=False)
+
+    if inplace:
+        if overwrite and embedding_column_name in dataset.keys():
+            del dataset[embedding_column_name]
+        dataset.append_embedding_column(embedding_column_name, optional=True)
+    if len(data) == 0:
+        if inplace:
+            return None
+        return np.empty((0, n_components), dtype=data.dtype), np.full(
+            len(dataset), False
+        )
+
+    data = StandardScaler().fit_transform(data)
+    embeddings = PCA(n_components=n_components).fit_transform(data)
+    if inplace:
+        dataset[embedding_column_name, mask] = embeddings
+        return None
+    return embeddings, mask
+
+
+def get_catch22_feature_names(catch24: bool = False) -> List[str]:
+    """
+    Get Catch22 feature names in the same order as returned by :func:`catch22`.
+    """
+    # Run Catch22 with dummy data to get feature names.
+    return pycatch22.catch22_all([0], catch24)["names"]
+
+
+def catch22(
+    dataset: Dataset,
+    column: str,
+    catch24: bool = False,
+    inplace: bool = False,
+    suffix: Optional[str] = None,
+    overwrite: bool = False,
+    as_float_columns: bool = False,
+) -> Optional[Tuple[np.ndarray, np.ndarray]]:
+    """
+    Generate Catch22 embeddings for the given column of a dataset and
+    optionally write them back into dataset.
+    """
+    # pylint: disable=too-many-arguments, too-many-branches
+    if suffix is None:
+        suffix = "catch24" if catch24 else "catch22"
+    column_type = dataset.get_column_type(column)
+    if column_type not in (Audio, Sequence1D):
+        raise InvalidDTypeError(
+            f"catch22 is only applicable to columns of type `Audio` and "
+            f'`Sequence1D`, but column "{column}" of type {column_type} received.'
+        )
+
+    column_names = []
+    if as_float_columns:
+        for name in get_catch22_feature_names(catch24):
+            column_names.append("-".join((column, suffix, name)))
+    else:
+        column_names.append(f"{column}-{suffix}")
+    if inplace and not overwrite:
+        for name in column_names:
+            if name in dataset.keys():
+                raise ColumnExistsError(
+                    f'Column "{name}" already exists. Either set another '
+                    f"`suffix` argument or set `overwrite` argument to `True`."
+                )
+    data, mask = align_column_data(dataset, column, allow_nan=False)
+
+    if inplace:
+        if overwrite:
+            for name in column_names:
+                if name in dataset.keys():
+                    del dataset[name]
+        for name in column_names:
+            if as_float_columns:
+                dataset.append_float_column(name, optional=True)
+            else:
+                dataset.append_embedding_column(name, optional=True)
+    if len(data) == 0:
+        if inplace:
+            return None
+        return np.empty((0, 24 if catch24 else 22), dtype=data.dtype), np.full(
+            len(dataset), False
+        )
+
+    embeddings = np.array(
+        [pycatch22.catch22_all(sample, catch24)["values"] for sample in data],
+        dtype=float,
+    )
+
+    if inplace:
+        if as_float_columns:
+            for name, values in zip(column_names, embeddings.T):
+                dataset[name, mask] = values
+        else:
+            dataset[column_names[0], mask] = embeddings
+        return None
+    return embeddings, mask
```

## renumics/spotlight/dtypes/__init__.py

```diff
@@ -703,17 +703,21 @@
 class Audio(_BaseFileBasedData):
     """
     An Audio Signal that will be saved in encoded form.
 
     All formats and codecs supported by AV are supported for read.
 
     Attributes:
-        data: Array-like with shape `(num_samples, num_channels)` with `num_channels` <= 5
-            min/max and bits-per-sample will be determined by the data-type
-            (uint8, int32, int16, float32).
+        data: Array-like with shape `(num_samples, num_channels)`
+            with `num_channels` <= 5.
+            If `data` has a float dtype, its values should be between -1 and 1.
+            If `data` has an int dtype, its values should be between minimum and
+            maximum possible values for the particular int dtype.
+            If `data` has an unsigned int dtype, ist values should be between 0
+            and maximum possible values for the particular unsigned int dtype.
         sampling_rate: Sampling rate (samples per seconds)
 
     Example:
         >>> import numpy as np
         >>> from renumics.spotlight import Dataset, Audio
         >>> samplerate = 44100
         >>> fs = 100 # 100 Hz audio signal
```

## renumics/spotlight/io/pandas.py

```diff
@@ -1,30 +1,27 @@
 """
 This module contains helpers for importing `pandas.DataFrame`s.
 """
 
 import ast
-import os.path
 from contextlib import suppress
 from datetime import datetime
 from typing import Any, Optional, Type
 
 import pandas as pd
-import validators
 
 from renumics.spotlight.dtypes import Category
-from renumics.spotlight.dtypes.exceptions import NotADType
+from renumics.spotlight.dtypes.exceptions import NotADType, UnsupportedDType
 from renumics.spotlight.dtypes.typing import (
     COLUMN_TYPES_BY_NAME,
     ColumnType,
+    ColumnTypeMapping,
     is_column_type,
-    is_file_based_column_type,
     is_scalar_column_type,
 )
-from renumics.spotlight.typing import PathOrURLType, PathType
 
 
 def is_empty(value: Any) -> bool:
     """
     Check if value is `NA` or an empty string.
     """
     # We need to check for a string type before checking for an empty string
@@ -37,25 +34,14 @@
     Try to evaluate a literal expression, otherwise return value as is.
     """
     with suppress(Exception):
         return ast.literal_eval(x)
     return x
 
 
-def prepare_path_or_url(path_or_url: PathOrURLType, workdir: PathType) -> str:
-    """
-    For a relative path, prefix it with the `workdir`.
-    For an absolute path or an URL, do nothing.
-    """
-    path_or_url_str = str(path_or_url)
-    if validators.url(path_or_url_str):
-        return path_or_url_str
-    return os.path.join(workdir, path_or_url_str)
-
-
 def infer_dtype(column: pd.Series) -> Type[ColumnType]:
     """
     Get an equivalent Spotlight data type for a `pandas` column, if possible.
 
     At the moment, only scalar data types can be inferred.
 
     Nullable boolean and integer `pandas` dtypes have no equivalent Spotlight
@@ -84,15 +70,39 @@
         return datetime
     # `is_string_dtype` only checks `object` dtype, it's not enough.
     if (
         pd.api.types.is_string_dtype(column)
         and ((column.map(type) == str) | (column.isna())).all()
     ):
         return str
-    raise ValueError("Column dtype cannot be inferred automatically.")
+    raise UnsupportedDType("Column dtype cannot be inferred automatically.")
+
+
+def infer_dtypes(
+    df: pd.DataFrame, dtype: Optional[ColumnTypeMapping]
+) -> ColumnTypeMapping:
+    """
+    Check column types from the given `dtype` and complete it with auto inferred
+    column types for the given `pandas.DataFrame`.
+    """
+    inferred_dtype = dtype or {}
+    for column_name, column_type in inferred_dtype.items():
+        if not is_column_type(column_type):
+            raise NotADType(
+                f"Given column type {column_type} for column '{column_name}' "
+                f"is not a valid Spotlight column type."
+            )
+    for column_name in df:
+        if column_name not in inferred_dtype:
+            try:
+                column_type = infer_dtype(df[column_name])
+            except UnsupportedDType:
+                column_type = str
+            inferred_dtype[column_name] = column_type
+    return inferred_dtype
 
 
 def is_string_mask(column: pd.Series) -> pd.Series:
     """
     Return mask of column's elements of type string.
     """
     return column.map(type) == str
@@ -111,25 +121,22 @@
     """
     column = column.mask(column.isna(), None).astype("category")
     if str_categories:
         return column.cat.rename_categories(column.cat.categories.astype(str))
     return column
 
 
-def prepare_column(
-    column: pd.Series, dtype: Type[ColumnType], workdir: Optional[PathType] = None
-) -> pd.Series:
+def prepare_column(column: pd.Series, dtype: Type[ColumnType]) -> pd.Series:
     """
     Convert a `pandas` column to the desired `dtype` and prepare some values,
     but still as `pandas` column.
 
     Args:
         column: A `pandas` column to prepare.
         dtype: Target data type.
-        workdir: Directory to prefix relative paths. For file-based data only.
 
     Returns:
         Prepared `pandas` column.
 
     Raises:
         TypeError: If `dtype` is not a Spotlight data type.
     """
@@ -165,13 +172,8 @@
     na_mask = column.isna()
 
     # When `pandas` reads a csv, arrays and lists are read as literal strings,
     # try to interpret them.
     str_mask = is_string_mask(column)
     column[str_mask] = column[str_mask].apply(try_literal_eval)
 
-    if workdir is not None and is_file_based_column_type(dtype):
-        # For file-based data types, relative paths should be resolved.
-        str_mask = is_string_mask(column)
-        column[str_mask] = column[str_mask].apply(prepare_path_or_url)
-
     return column.mask(na_mask, None)
```

## renumics/spotlight/plugin_loader.py

```diff
@@ -1,22 +1,28 @@
 """
     Facilities for plugin loading and registration.
 """
 
 import importlib
 import pkgutil
+from functools import lru_cache
 
 import renumics.spotlight.plugins as plugins_namespace
 
 
+@lru_cache()
 def load_plugins() -> dict:
     """
     Automatically load and register plugins
     inside the renumics.spotlight.plugins namespace package.
     """
     plugins = {}
 
     for _, name, _ in pkgutil.iter_modules(plugins_namespace.__path__):
         plugin = importlib.import_module(plugins_namespace.__name__ + "." + name)
         plugins[name] = plugin
 
+    for plugin in sorted(plugins.values(), key=lambda m: m.__priority__):
+        if hasattr(plugin, "__activate__"):
+            plugin.__activate__()
+
     return plugins
```

## renumics/spotlight/plugins/core/__init__.py

```diff
@@ -1,6 +1,36 @@
 """
     Spotlight core plugin
     Provides core datatypes and sources.
 """
 
+from fastapi import FastAPI
+from .api import (
+    table as table_api,
+    user as user_api,
+    filebrowser as file_api,
+    config as config_api,
+    layout as layout_api,
+)
+
+
 __version__ = "0.0.1"
+__priority__ = 0
+
+
+def __activate__() -> None:
+    """
+    register data sources
+    """
+    # pylint: disable=import-outside-toplevel, unused-import
+    from . import pandas_data_source, hdf5_data_source
+
+
+def on_startup(app: FastAPI) -> None:
+    """
+    register api routes on app startup
+    """
+    app.include_router(layout_api.router, prefix="/api/layout")
+    app.include_router(table_api.router, prefix="/api/table")
+    app.include_router(file_api.router, prefix="/api/browse")
+    app.include_router(user_api.router, prefix="/api/user")
+    app.include_router(config_api.router, prefix="/api/config")
```

## renumics/spotlight/viewer.py

```diff
@@ -38,35 +38,33 @@
     >>> import pandas as pd
     >>> from renumics import spotlight
     >>> df = pd.DataFrame({"a":[0, 1, 2], "b":["x", "y", "z"]})
     >>> viewer = spotlight.show(df, "127.0.0.1", port=5001, no_browser=True, wait=False)
     Spotlight running on http://127.0.0.1:5001/
     >>> viewer
     http://127.0.0.1:5001/
-    >>> viewer.get_df()["a"].to_list()
+    >>> viewer.df["a"].to_list()
     [0, 1, 2]
     >>> spotlight.close()
 
 """
 
 import os
 from pathlib import Path
 import threading
 from typing import List, Union, Optional, Dict, Type
 
 import pandas as pd
-from loguru import logger
 from typing_extensions import Literal
 import ipywidgets as widgets
 import IPython.display
 
 import __main__
 from renumics.spotlight.webbrowser import launch_browser_in_thread
 from renumics.spotlight.dataset import ColumnType
-from renumics.spotlight.backend.pandas_data_source import PandasDataSource
 from renumics.spotlight.layout import _LayoutLike, parse
 from renumics.spotlight.backend.server import create_server, Server
 from renumics.spotlight.backend.websockets import RefreshMessage
 from renumics.spotlight.backend import create_datasource
 
 from renumics.spotlight.dtypes.typing import ColumnTypeMapping
 
@@ -220,46 +218,23 @@
 
     def refresh(self) -> None:
         """
         Refresh the corresponding Spotlight instance in a browser.
         """
         self._server.app.websocket_manager.broadcast(RefreshMessage())
 
-    def get_df(self) -> Optional[pd.DataFrame]:
+    @property
+    def df(self) -> Optional[pd.DataFrame]:
         """
         Get served `DataFrame` if a `DataFrame` is served, `None` otherwise.
         """
-        data_source = self._server.app.data_source
-        if isinstance(data_source, PandasDataSource):
-            return data_source.df.copy()
+        if self._server.app.data_source:
+            return self._server.app.data_source.df
         return None
 
-    def update_df(
-        self,
-        df: Optional[pd.DataFrame] = None,
-        dtype: Optional[Dict[str, Type[ColumnType]]] = None,
-    ) -> None:
-        """
-        Replace `df` and/or its `dtype` if a `DataFrame` is served, do nothing
-        otherwise.
-        """
-        if df is None and dtype is None:
-            logger.warning("Neither `df` nor `dtype` is set, nothing to update.")
-            return
-        data_source = self._server.app.data_source
-        if not isinstance(data_source, PandasDataSource):
-            logger.warning("Current data source is not a `DataFrame`.")
-            return
-        if df is None:
-            df = data_source.df
-        elif dtype is None:
-            dtype = data_source.dtype
-        self._server.app.data_source = create_datasource(df, dtype)
-        self.refresh()
-
     def __repr__(self) -> str:
         return f"http://{self.host}:{self.port}/"
 
     def _ipython_display_(self) -> None:
         if self._server.should_exit:
             return
```

## Comparing `renumics/spotlight/backend/apis/config.py` & `renumics/spotlight/plugins/core/api/config.py`

 * *Files 24% similar despite different names*

```diff
@@ -3,15 +3,16 @@
 """
 
 from typing import Optional
 
 from fastapi import APIRouter, Request
 from pydantic import BaseModel  # pylint: disable=no-name-in-module
 
-from ..config import ConfigValue
+from renumics.spotlight.backend.config import ConfigValue
+
 
 router = APIRouter(tags=["config"])
 
 
 @router.get("/{name}", response_model=Optional[ConfigValue], operation_id="get")
 async def get_value(name: str, request: Request) -> Optional[ConfigValue]:
     """
```

## Comparing `renumics/spotlight/backend/apis/filebrowser.py` & `renumics/spotlight/plugins/core/api/filebrowser.py`

 * *Files identical despite different names*

## Comparing `renumics/spotlight/backend/apis/layout.py` & `renumics/spotlight/plugins/core/api/layout.py`

 * *Files identical despite different names*

## Comparing `renumics/spotlight/backend/apis/table.py` & `renumics/spotlight/plugins/core/pandas_data_source.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,428 +1,340 @@
 """
-table api endpoints
+access pandas DataFrame table data
 """
 from datetime import datetime
-from pathlib import Path
-from typing import Any, Dict, Optional, List
+from functools import lru_cache
+from typing import Any, Dict, List, Optional, Union, Type, cast
 
-from fastapi import APIRouter, Request
-from fastapi.responses import Response, ORJSONResponse
-from pydantic import BaseModel  # pylint: disable=no-name-in-module
-
-from renumics.spotlight.dtypes.typing import get_column_type_name
-from renumics.spotlight.licensing import (
-    LicensedFeature,
-    username,
-    verify_license_or_exit,
+import numpy as np
+import pandas as pd
+import trimesh
+from loguru import logger
+
+from renumics.spotlight.dtypes import Category, Embedding, Mesh, Window
+from renumics.spotlight.dtypes.exceptions import InvalidFile, UnsupportedDType
+from renumics.spotlight.dtypes.typing import (
+    ColumnType,
+    ColumnTypeMapping,
+    get_column_type_name,
+    is_array_based_column_type,
+    is_file_based_column_type,
+    is_scalar_column_type,
 )
-
-from renumics.spotlight.typing import PathType
-
-from renumics.spotlight.backend import create_datasource
-from ..exceptions import InvalidPath
-from ..data_source import (
-    Column as DatasetColumn,
+from renumics.spotlight.io.pandas import (
+    infer_dtypes,
+    is_empty,
+    prepare_column,
+    to_categorical,
+    try_literal_eval,
+)
+from renumics.spotlight.backend import datasource
+from renumics.spotlight.backend.data_source import (
+    Column,
     DataSource,
-    sanitize_values,
-    idx_column,
-    last_edited_at_column,
-    last_edited_by_column,
+    read_external_value,
 )
+from renumics.spotlight.backend.exceptions import ConversionFailed
+from renumics.spotlight.typing import PathType, is_pathtype
 
 
-class Column(BaseModel):
+@datasource(pd.DataFrame)
+@datasource(".csv")
+class PandasDataSource(DataSource):
     """
-    a single table column
+    access pandas DataFrame table data
     """
 
-    # pylint: disable=too-few-public-methods
+    _generation_id: int
+    _uid: str
+    _df: pd.DataFrame
+    _dtype: ColumnTypeMapping
+    _inferred_dtype: ColumnTypeMapping
 
-    name: str
-    index: Optional[int]
-    hidden: bool
-    editable: bool
-    optional: bool
-    role: str
-    values: List[Any]
-    references: Optional[List[bool]]
-    y_label: Optional[str]
-    x_label: Optional[str]
-    description: Optional[str]
-    tags: Optional[List[str]]
-    categories: Optional[Dict[str, int]]
-    embedding_length: Optional[int]
+    def __init__(
+        self,
+        source: Union[PathType, pd.DataFrame],
+        dtype: Optional[ColumnTypeMapping] = None,
+    ):
+        if is_pathtype(source):
+            df = pd.read_csv(source)
+        else:
+            df = source
+
+        self._generation_id = 0
+        self._uid = str(id(df))
+        self._df = df.copy()
+        self._dtype = dtype.copy() if dtype else {}
+        self._inferred_dtype = infer_dtypes(self._df, self._dtype)
+
+    @property
+    def column_names(self) -> List[str]:
+        return self._df.columns.to_list()
 
-    @classmethod
-    def from_dataset_column(cls, column: DatasetColumn) -> "Column":
+    @property
+    def df(self) -> pd.DataFrame:
         """
-        Instantiate column from a dataset column.
+        Get **a copy** of the served `DataFrame`.
         """
-        return cls(
-            name=column.name,
-            index=column.order,
-            hidden=column.hidden,
-            editable=column.editable,
-            optional=column.optional,
-            role=get_column_type_name(column.type),
-            values=sanitize_values(column.values),
-            references=sanitize_values(column.references),
-            x_label=column.x_label,
-            y_label=column.y_label,
-            description=column.description,
-            tags=column.tags,
-            categories=column.categories,
-            embedding_length=column.embedding_length,
-        )
-
+        return self._df.copy()
 
-# pylint: disable=too-few-public-methods
-class Table(BaseModel):
-    """
-    a table slice
-    """
-
-    uid: str
-    filename: str
-    columns: List[Column]
-    max_rows_hit: bool
-    max_columns_hit: bool
-    generation_id: int
-
-
-router = APIRouter()
-
-
-@router.get(
-    "/",
-    response_model=Table,
-    response_class=ORJSONResponse,
-    tags=["table"],
-    operation_id="get_table",
-)
-def get_table(request: Request) -> ORJSONResponse:
-    """
-    table slice api endpoint
-    """
-    table: Optional[DataSource] = request.app.data_source
-    verify_license_or_exit()
-    if table is None:
-        return ORJSONResponse(
-            Table(
-                uid="",
-                filename="",
-                columns=[],
-                max_rows_hit=False,
-                max_columns_hit=False,
-                generation_id=-1,
-            ).dict()
+    @property
+    def dtype(self) -> Optional[ColumnTypeMapping]:
+        """
+        Get **a copy** of dict with the desired data types.
+        """
+        if self._dtype is None:
+            return None
+        return self._dtype.copy()
+
+    def __len__(self) -> int:
+        return len(self._df)
+
+    def get_generation_id(self) -> int:
+        return self._generation_id
+
+    def get_uid(self) -> str:
+        return self._uid
+
+    def get_name(self) -> str:
+        return "pd.DataFrame"
+
+    def get_columns(self, column_names: Optional[List[str]] = None) -> List[Column]:
+        if column_names is None:
+            column_names = self._df.columns.to_list()
+        columns = []
+        for column_name in column_names:
+            try:
+                column = self.get_column(column_name, None)
+            except Exception as e:  # pylint: disable=broad-except
+                if column_name in self._dtype:
+                    raise e
+                logger.warning(
+                    f"Column '{column_name}' not imported from "
+                    f"`pandas.DataFrame` because of the following error:\n{e}"
+                )
+            else:
+                columns.append(column)
+
+        return columns
+
+    def get_column(self, column_name: str, indices: Optional[List[int]]) -> Column:
+        # pylint: disable=too-many-branches, too-many-statements
+        self._assert_column_exists(column_name)
+
+        dtype = self._inferred_dtype[column_name]
+        column = self._df[column_name]
+        if indices is not None:
+            column = column.iloc[indices]
+        column = prepare_column(column, dtype)
+
+        categories = None
+        embedding_length = None
+        references = None
+
+        if dtype is Category:
+            # `NaN` category is listed neither in `pandas`, not in our format.
+            categories = {
+                category: i for i, category in enumerate(column.cat.categories)
+            }
+            column = column.cat.codes
+            values = column.to_numpy()
+        elif dtype is datetime:
+            # We expect datetimes as ISO strings; empty strings instead of `NaT`s.
+            column = column.dt.strftime("%Y-%m-%dT%H:%M:%S.%f%z")
+            column = column.mask(column.isna(), "")
+            values = column.to_numpy()
+        elif dtype is str:
+            # Replace `NA`s with empty strings.
+            column = column.mask(column.isna(), "")
+            values = column.to_numpy()
+        elif is_scalar_column_type(dtype):
+            values = column.to_numpy()
+        elif dtype is Window:
+            # Replace all `NA` values with `[NaN, NaN]`.
+            na_mask = column.isna()
+            column.iloc[na_mask] = pd.Series(
+                [[float("nan"), float("nan")]] * na_mask.sum()
+            )
+            # This will fail, if arrays in column cells aren't aligned.
+            try:
+                values = np.asarray(column.to_list(), dtype=float)
+            except ValueError as e:
+                raise ValueError(
+                    f"For the window column '{column_name}', column cells "
+                    f"should be sequences of shape (2,) or `NA`s, but "
+                    f"unaligned sequences received."
+                ) from e
+            if values.ndim != 2 or values.shape[1] != 2:
+                raise ValueError(
+                    f"For the window column '{column_name}', column cells "
+                    f"should be sequences of shape (2,) or `NA`s, but "
+                    f"sequences of shape {values.shape[1:]} received."
+                )
+        elif dtype is Embedding:
+            na_mask = column.isna()
+            # This will fail, if arrays in column cells aren't aligned.
+            try:
+                embeddings = np.asarray(column[~na_mask].to_list(), dtype=float)
+            except ValueError as e:
+                raise ValueError(
+                    f"For the window column '{column_name}', column cells "
+                    f"should be sequences of the same shape (n,) or `NA`s, but "
+                    f"unaligned sequences received."
+                ) from e
+            if embeddings.ndim != 2 or embeddings.shape[1] == 0:
+                raise ValueError(
+                    f"For the window column '{column_name}', column cells "
+                    f"should be sequences of the same shape (n,) or `NA`s, but "
+                    f"sequences of shape {values.shape[1:]} received."
+                )
+            if na_mask.any():
+                values = np.empty(len(column), dtype=object)
+                values[np.where(~na_mask)[0]] = list(embeddings)
+            else:
+                values = embeddings
+            embedding_length = embeddings.shape[1]
+        else:
+            # A reference column. `dtype` is one of `np.ndarray`, `Audio`,
+            # `Image`, `Mesh`, `Sequence1D` or `Video`. Don't try to check or
+            # convert values at the moment.
+            na_mask = column.isna()
+            references = na_mask.to_numpy()
+            if is_file_based_column_type(dtype):
+                # Strings are paths or URLs, let them inplace. Replace
+                # non-strings with empty strings.
+                column = column.mask(~(column.map(type) == str), "")
+                values = column.to_numpy()
+            else:
+                values = np.full(len(column), "")
+        return Column(
+            type_name=get_column_type_name(dtype),
+            type=dtype,
+            order=None,
+            hidden=column_name.startswith("_"),
+            optional=True,
+            description=None,
+            tags=[],
+            editable=dtype in (bool, int, float, str, Category, Window),
+            categories=categories,
+            x_label=None,
+            y_label=None,
+            embedding_length=embedding_length,
+            has_lookup=False,
+            is_external=False,
+            name=column_name,
+            values=values,
+            references=references,
         )
-    spotlight_license: LicensedFeature = request.app.spotlight_license
-
-    columns = table.get_columns()
-    max_columns_hit = False
-    if spotlight_license.column_limit is not None:
-        if len(columns) > spotlight_license.column_limit:
-            max_columns_hit = True
-            columns = columns[: spotlight_license.column_limit]
-
-    columns.extend(table.get_internal_columns())
-
-    max_rows_hit = False
-    row_count = len(table)
-    if spotlight_license.row_limit is not None:
-        if len(table) > spotlight_license.row_limit:
-            max_rows_hit = True
-            row_count = spotlight_license.row_limit
-            for column in columns:
-                column.values = column.values[: spotlight_license.row_limit]
-                if column.references is not None:
-                    column.references = column.references[: spotlight_license.row_limit]
-
-    columns.append(idx_column(row_count))
-    if not any(column.name == "__last_edited_at__" for column in columns):
-        columns.append(last_edited_at_column(row_count, datetime.now()))
-    if not any(column.name == "__last_edited_by__" for column in columns):
-        columns.append(last_edited_by_column(row_count, username))
-
-    return ORJSONResponse(
-        Table(
-            uid=table.get_uid(),
-            filename=table.get_name(),
-            columns=[Column.from_dataset_column(column) for column in columns],
-            max_rows_hit=max_rows_hit,
-            max_columns_hit=max_columns_hit,
-            generation_id=table.get_generation_id(),
-        ).dict()
-    )
-
-
-@router.get(
-    "/{column}/{row}",
-    tags=["table"],
-    operation_id="get_cell",
-)
-async def get_table_cell(
-    column: str, row: int, generation_id: int, request: Request
-) -> Any:
-    """
-    table cell api endpoint
-    """
-    table: DataSource = request.app.data_source
-    table.check_generation_id(generation_id)
-
-    cell_data = table.get_cell_data(column, row)
-    value = sanitize_values(cell_data)
-
-    if isinstance(value, (bytes, str)):
-        return Response(value, media_type="application/octet-stream")
-
-    return value
-
-
-@router.get(
-    "/{column}/{row}/waveform",
-    response_model=Optional[List[float]],
-    tags=["table"],
-    operation_id="get_waveform",
-)
-async def get_waveform(
-    column: str, row: int, generation_id: int, request: Request
-) -> Optional[List[float]]:
-    """
-    table cell api endpoint
-    """
-    table: DataSource = request.app.data_source
-    table.check_generation_id(generation_id)
-
-    waveform = table.get_waveform(column, row)
-
-    return sanitize_values(waveform)
-
-
-class Cells(BaseModel):
-    """
-    Multiple Cells with the same value
-    """
-
-    column: str
-    rows: List[int]
-    value: Any
-
-
-# pylint: disable=too-few-public-methods
-class CellsUpdateResponse(Cells):
-    """
-    A cell update (cell + edit information)
-    """
-
-    author: str
-    edited_at: Optional[datetime]
-    generation_id: int
-
-
-class CellsUpdateRequest(BaseModel):
-    """
-    Table Cell update request model
-    """
-
-    rows: List[int]
-    value: Any
-
-
-class AddColumnRequest(BaseModel):
-    """
-    Add Column request model
-    """
-
-    dtype: str
-
-
-class MutationResponse(BaseModel):
-    """
-    Common response model for all mutational endpoints.
-    """
-
-    generation_id: int
-
-
-class AddColumnResponse(MutationResponse):
-    """
-    Add column response model.
-    """
 
-    column: Column
-
-
-class DuplicateRowResponse(MutationResponse):
-    """
-    Duplicate row response model.
-    """
-
-    row: int
-
-
-@router.put(
-    "/{column}",
-    response_model=CellsUpdateResponse,
-    tags=["table"],
-    operation_id="put_cells",
-)
-async def put_table_cells(
-    column: str,
-    generation_id: int,
-    update_request: CellsUpdateRequest,
-    request: Request,
-) -> CellsUpdateResponse:
-    """
-    replace multiple cell's data
-
-    :raises NoColumnFound: if the column was not found in the dataset
-    :raises NoRowFound: if one of the rows was not found in the dataset
-    """
-    table: DataSource = request.app.data_source
-    table.check_generation_id(generation_id)
-
-    cells_update = table.replace_cells(
-        column, update_request.rows, update_request.value
-    )
-
-    return CellsUpdateResponse(
-        column=column,
-        rows=update_request.rows,
-        value=sanitize_values(cells_update.value),
-        author=sanitize_values(cells_update.author),
-        edited_at=sanitize_values(cells_update.edited_at),
-        generation_id=table.get_generation_id(),
-    )
-
-
-@router.delete(
-    "/{column}",
-    response_model=MutationResponse,
-    tags=["table"],
-    operation_id="delete_column",
-)
-async def delete_column(
-    column: str, generation_id: int, request: Request
-) -> MutationResponse:
-    """
-    remove a column from the datasets
-
-    :raises NoColumnFound: if a column with the name does not exist
-    """
-    table: DataSource = request.app.data_source
-    table.check_generation_id(generation_id)
-
-    table.delete_column(column)
-
-    return MutationResponse(generation_id=table.get_generation_id())
-
-
-@router.post(
-    "/{column}",
-    response_model=AddColumnResponse,
-    tags=["table"],
-    operation_id="add_column",
-)
-async def add_column(
-    column: str,
-    generation_id: int,
-    add_column_request: AddColumnRequest,
-    request: Request,
-) -> AddColumnResponse:
-    """
-    add an editable column
-
-    :raises ColumnExistsError: if a column with the same name exists in the dataset
-    """
-    table: DataSource = request.app.data_source
-    table.check_generation_id(generation_id)
-
-    new_column = table.append_column(column, add_column_request.dtype)
-
-    spotlight_license: LicensedFeature = request.app.spotlight_license
-
-    if (
-        spotlight_license.row_limit is not None
-        and len(table) > spotlight_license.row_limit
-    ):
-        new_column.values = new_column.values[: spotlight_license.row_limit]
-        if new_column.references is not None:
-            new_column.references = new_column.references[: spotlight_license.row_limit]
-
-    return AddColumnResponse(
-        generation_id=table.get_generation_id(),
-        column=Column.from_dataset_column(new_column),
-    )
-
-
-@router.delete(
-    "/rows/{row}",
-    response_model=MutationResponse,
-    tags=["table"],
-    operation_id="delete_row",
-)
-async def delete_row(
-    row: int, generation_id: int, request: Request
-) -> MutationResponse:
-    """
-    remove a row from the datasets
-
-    :raises NoRowFound: if a row is not found
-    """
-    table: DataSource = request.app.data_source
-    table.check_generation_id(generation_id)
-
-    table.delete_row(row)
-
-    return MutationResponse(generation_id=table.get_generation_id())
-
-
-@router.post(
-    "/rows/{row}",
-    response_model=DuplicateRowResponse,
-    tags=["table"],
-    operation_id="create_row",
-)
-async def insert_row(
-    row: int, generation_id: int, request: Request
-) -> DuplicateRowResponse:
-    """
-    create a new row in the dataset by duplicating a given row
-
-    :raises NoRowFound: if a row is not found
-    """
-    table: DataSource = request.app.data_source
-    table.check_generation_id(generation_id)
-
-    row_index = table.duplicate_row(row)
-
-    return DuplicateRowResponse(generation_id=table.get_generation_id(), row=row_index)
-
-
-def is_path_relative_to(path: PathType, parent: PathType) -> bool:
-    """
-    Is the path a subpath of the parent
-    """
-    try:
-        Path(path).relative_to(parent)
-        return True
-    except ValueError:
-        return False
-
-
-@router.post("/open/{path:path}", tags=["table"], operation_id="open")
-async def open_table(path: str, request: Request) -> None:
-    """
-    Open the specified table file
-
-    :raises InvalidPath: if the supplied path is outside the project root
-                         or points to an incompatible file
-    """
-    full_path = Path(request.app.project_root) / path
+    def get_cell_data(self, column_name: str, row_index: int) -> Any:
+        """
+        Return the value of a single cell, warn if not possible.
+        """
+        # pylint: disable=too-many-return-statements, too-many-branches
+        self._assert_column_exists(column_name)
+        self._assert_index_exists(row_index)
+
+        column_index = self._df.columns.get_loc(column_name)
+        raw_value = self._df.iloc[row_index, column_index]
+        dtype = self._inferred_dtype[column_name]
+
+        if dtype is Category:
+            if pd.isna(raw_value):
+                return -1
+            categories = self._get_column_categories(column_name, as_string=True)
+            return categories.get(raw_value, -1)
+        if dtype is datetime:
+            value = pd.to_datetime(raw_value).to_numpy("datetime64[us]").tolist()
+            # `tolist()` returns `None` for all `NaT`s, `datetime` otherwise.
+            if value is None:
+                return ""
+            return value.isoformat()
+        if dtype is str:
+            if pd.isna(raw_value):
+                return ""
+            return str(raw_value)
+        if is_scalar_column_type(dtype):
+            # `dtype` is `bool`, `int` or `float`.
+            dtype = cast(Type[Union[bool, int, float]], dtype)
+            try:
+                return dtype(raw_value)
+            except (TypeError, ValueError) as e:
+                raise ConversionFailed(dtype, raw_value) from e
+        if dtype is np.ndarray:
+            if isinstance(raw_value, str):
+                raw_value = try_literal_eval(raw_value)
+            if is_empty(raw_value):
+                return None
+            return np.asarray(raw_value)
+        if dtype is Window:
+            if isinstance(raw_value, str):
+                raw_value = try_literal_eval(raw_value)
+            if is_empty(raw_value):
+                return np.full(2, np.nan)
+            try:
+                value = np.asarray(raw_value, dtype=float)
+            except (TypeError, ValueError) as e:
+                raise ConversionFailed(dtype, raw_value) from e
+            if value.ndim != 1 or len(value) != 2:
+                raise ValueError(
+                    f"Window column cells should be sequences of shape (2,), "
+                    f"but a sequence of shape {value.shape} received for the "
+                    f"column '{column_name}'."
+                )
+        if is_empty(raw_value):
+            return None
+        if isinstance(raw_value, str):
+            raw_value = try_literal_eval(raw_value)
+        if isinstance(raw_value, trimesh.Trimesh) and dtype is Mesh:
+            value = Mesh.from_trimesh(raw_value)
+            return value.encode()
+        if isinstance(raw_value, str) and is_file_based_column_type(dtype):
+            try:
+                return read_external_value(str(raw_value), dtype)
+            except Exception as e:
+                raise ConversionFailed(dtype, raw_value) from e
+        if not isinstance(raw_value, dtype) and is_array_based_column_type(dtype):
+            try:
+                value = dtype(raw_value)
+            except (InvalidFile, TypeError, ValueError) as e:
+                raise ConversionFailed(dtype, raw_value) from e
+            return value.encode()
+        if isinstance(raw_value, dtype):
+            return raw_value.encode()
+        raise ConversionFailed(dtype, raw_value)
+
+    def _get_default_value(self, dtype: Type[ColumnType]) -> Any:
+        if dtype is int:
+            return 0
+        if dtype is bool:
+            return False
+        if dtype is str:
+            return ""
+        if dtype is Window:
+            return [float("nan"), float("nan")]
+        # `dtype` is `float`, `datetime`, `np.ndarray`, or a custom data type.
+        return np.nan
+
+    @lru_cache(maxsize=128)
+    def _get_column_categories(
+        self, column_name: str, as_string: bool = False
+    ) -> Dict[str, int]:
+        """
+        Get categories of a categorical column.
 
-    # assert that the path is inside our project root
-    if not is_path_relative_to(full_path, request.app.project_root):
-        raise InvalidPath(path)
+        If `as_string` is True, convert the categories to their string
+        representation.
 
-    request.app.data_source = create_datasource(full_path, dtype=None)
+        At the moment, there is no way to add a new category in Spotlight, so we
+        rely on the previously cached ones.
+        """
+        self._assert_column_exists(column_name)
+        dtype = self._inferred_dtype[column_name]
+        if dtype is not Category:
+            raise UnsupportedDType(
+                f"Column categories exist for categorical columns only, but "
+                f"column '{column_name}' of type {dtype} received."
+            )
+        column = self._df[column_name]
+        column = to_categorical(column, str_categories=as_string)
+        return {category: i for i, category in enumerate(column.cat.categories)}
```

## Comparing `renumics/spotlight/backend/core/user.py` & `renumics/spotlight/plugins/core/api/user.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,40 @@
 """
-user core commands
+user api endpoints
 """
+
 from datetime import date
 from typing import Optional
+from fastapi import APIRouter, Request
 from pydantic import BaseModel  # pylint: disable=no-name-in-module
 
-from renumics.spotlight.licensing import LicensedFeature
+router = APIRouter()
 
 # pylint: disable=too-few-public-methods
 class User(BaseModel):
     """
     a User
     """
 
     user_name: str
     expiration_date: date
     row_limit: Optional[int]
     column_limit: Optional[int]
     has_test_license: bool
 
 
-def get_user(spotlight_license: LicensedFeature) -> User:
+@router.get(
+    "/", response_model=User, tags=["user"], summary="User Api", operation_id="get_user"
+)
+async def _(request: Request) -> User:
     """
-    read user from license file and return it
+    user api endpoint
     """
 
+    spotlight_license = request.app.spotlight_license
     return User(
         user_name=spotlight_license.users[0],
         expiration_date=spotlight_license.expiration_date,
         row_limit=spotlight_license.row_limit,
         column_limit=spotlight_license.column_limit,
         has_test_license=spotlight_license.is_test,
     )
```

## Comparing `renumics/spotlight/backend/hdf5_data_source.py` & `renumics/spotlight/plugins/core/hdf5_data_source.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,52 +6,50 @@
 from pathlib import Path
 from typing import Any, Dict, List, Optional, cast, Union, Type
 from dataclasses import asdict
 
 import h5py
 import numpy as np
 
-from renumics.spotlight.dataset.exceptions import InvalidIndexError
 from renumics.spotlight.dtypes import Category, Embedding
-from renumics.spotlight.dtypes.typing import get_column_type, FileBasedColumnType
+from renumics.spotlight.dtypes.typing import (
+    ColumnTypeMapping,
+    get_column_type,
+    FileBasedColumnType,
+)
 from renumics.spotlight.licensing import FeatureNotLicensed
 from renumics.spotlight.typing import PathType, IndexType
 from renumics.spotlight.dataset import (
     Dataset,
     INTERNAL_COLUMN_NAMES,
-    _get_column_name,
-    _get_column_type,
-    _is_ref_column,
-    _unescape_dataset_name,
+    unescape_dataset_name,
 )
 
-
-from .data_source import (
-    CellsUpdate,
+from renumics.spotlight.backend.data_source import (
     DataSource,
     Attrs,
     Column,
     read_external_value,
 )
-from .exceptions import (
+from renumics.spotlight.backend.exceptions import (
     NoTableFileFound,
     CouldNotOpenTableFile,
     InvalidLicense,
     NoRowFound,
-    ColumnNotEditable,
-    InvalidCategory,
     InvalidExternalData,
 )
 
+from renumics.spotlight.backend import datasource
+
 
 def unescape_dataset_names(refs: np.ndarray) -> np.ndarray:
     """
     Unescape multiple dataset names.
     """
-    return np.array([_unescape_dataset_name(value) for value in refs])
+    return np.array([unescape_dataset_name(value) for value in refs])
 
 
 def decode_attrs(raw_attrs: h5py.AttributeManager) -> Attrs:
     """
     Get relevant subset of column attributes.
     """
     column_type_name = raw_attrs.get("type", "unknown")
@@ -125,24 +123,24 @@
         self._assert_column_exists(column_name, internal=True)
         self._assert_index_exists(index)
         column = self._h5_file[column_name]
         value = column[index]
         if isinstance(value, bytes):
             value = value.decode("utf-8")
         if column.attrs.get("external", False):
-            column_type = _get_column_type(column)
+            column_type = self._get_column_type(column)
             target_format = column.attrs.get("format", None)
             try:
                 column_type = cast(Type[FileBasedColumnType], column_type)
                 return read_external_value(
                     value, column_type, target_format, os.path.dirname(self._filepath)
                 )
             except Exception as e:
                 raise InvalidExternalData(value) from e
-        if _is_ref_column(column):
+        if self._is_ref_column(column):
             return self._resolve_ref(value, column_name)[()] if value else None
         return value
 
     def read_column(
         self,
         column_name: str,
         max_elements_per_cell: int = 2048,
@@ -152,15 +150,15 @@
         Read a dataset column for serialization.
         """
         # pylint: disable=too-many-branches, too-many-nested-blocks
         self._assert_column_exists(column_name, internal=True)
 
         column = self._h5_file[column_name]
         attrs = decode_attrs(column.attrs)
-        is_ref_column = _is_ref_column(column)
+        is_ref_column = self._is_ref_column(column)
         is_string_dtype = h5py.check_string_dtype(column.dtype)
 
         raw_values: np.ndarray
         if indices is None:
             raw_values = column[:]
         else:
             raw_values = column[indices]
@@ -202,15 +200,15 @@
                     values = []
                     for ref in raw_values:
                         if ref:
                             h5_dataset: h5py.Dataset = self._h5_file[ref]
                             try:
                                 name = h5_dataset.attrs["key"]
                             except KeyError:
-                                name = _get_column_name(h5_dataset)
+                                name = self._get_column_name(h5_dataset)
                             values.append(name)
                         else:
                             values.append(None)
                     raw_values = np.array(values, dtype=object)
                 else:
                     raw_values = ref_placeholder_names(refs)
 
@@ -233,15 +231,15 @@
             self._assert_index_exists(to_index)
         for column_name in self.keys() + INTERNAL_COLUMN_NAMES:
             column = self._h5_file[column_name]
             column.resize(length + 1, axis=0)
             if to_index != length:
                 # Shift all values after the insertion position by one.
                 raw_values = column[int(to_index) : -1]
-                if _get_column_type(column) is Embedding:
+                if self._get_column_type(column) is Embedding:
                     raw_values = list(raw_values)
                 column[int(to_index) + 1 :] = raw_values
             column[int(to_index)] = column[from_index]
         self._length += 1
         self._update_generation_id()
 
     def read_attrs(self, column_name: str) -> Attrs:
@@ -271,24 +269,22 @@
         raw_values = np.empty(len(refs), dtype=object)
         raw_values[:] = [
             self._resolve_ref(ref, column_name)[()] if ref else None for ref in refs
         ]
         return raw_values
 
 
+@datasource(".h5")
 class Hdf5DataSource(DataSource):
     """
     access h5 table data
     """
 
-    def __init__(
-        self,
-        table_file: PathType,
-    ):
-        self._table_file = Path(table_file)
+    def __init__(self, source: PathType, dtype: ColumnTypeMapping):
+        self._table_file = Path(source)
 
     @property
     def column_names(self) -> List[str]:
         with self._open_table() as dataset:
             return dataset.keys()
 
     def __len__(self) -> int:
@@ -328,120 +324,14 @@
         """
         with self._open_table() as dataset:
             try:
                 return dataset.read_value(column_name, row_index)
             except IndexError as e:
                 raise NoRowFound(row_index) from e
 
-    def replace_cells(
-        self, column_name: str, indices: List[int], value: Any
-    ) -> CellsUpdate:
-        """
-        replace multiple cell's value
-        """
-
-        with self._open_table("r+") as dataset:
-            # we can't assign an int value to a float cell in spotlight atm
-            # but json numbers don't have distinct float and int types,
-            # so we convert ints to float values for now
-            attrs = dataset.read_attrs(column_name)
-            if not attrs.editable:
-                raise ColumnNotEditable(column_name)
-            if value is not None:
-                if attrs.type is float:
-                    value = float(value)
-                elif attrs.type is Category:
-                    categories = cast(Dict, attrs.categories)
-                    if value == -1:
-                        value = None
-                    else:
-                        try:
-                            value = list(categories.keys())[
-                                list(categories.values()).index(value)
-                            ]
-                        except ValueError as e:
-                            raise InvalidCategory() from e
-
-            try:
-                dataset[column_name, indices] = value
-                new_value = dataset.read_value(column_name, indices[0])
-            except IndexError as e:
-                raise NoRowFound(indices[0] if len(indices) == 1 else None) from e
-
-            edited_at = dataset.read_value("__last_edited_at__", indices[0])
-            author = dataset.read_value("__last_edited_by__", indices[0])
-
-            author = cast(str, author)
-            edited_at = cast(str, edited_at)
-            return CellsUpdate(value=new_value, author=author, edited_at=edited_at)
-
-    def delete_column(self, name: str) -> None:
-        """
-        remove a column from the table
-        """
-        with self._open_table("r+") as dataset:
-            del dataset[name]
-
-    def delete_row(self, index: int) -> None:
-        """
-        remove a row from the table
-        """
-
-        with self._open_table("r+") as dataset:
-            try:
-                del dataset[index]
-            except InvalidIndexError as e:
-                raise NoRowFound(index) from e
-
-    def duplicate_row(self, index: int) -> int:
-        """
-        duplicate a row in the table
-        """
-
-        with self._open_table("r+") as dataset:
-            try:
-                dataset.duplicate_row(index, index + 1)
-                return index + 1
-            except InvalidIndexError as e:
-                raise NoRowFound(index) from e
-
-    def append_column(self, name: str, dtype_name: str) -> Column:
-        """
-        add a column to the table
-        """
-
-        with self._open_table("r+") as dataset:
-            dtype = get_column_type(dtype_name)
-            order = dataset.min_order() - 1
-
-            if dtype is int:
-                dataset.append_column(
-                    name,
-                    dtype,
-                    optional=True,
-                    editable=True,
-                    default=0,
-                    order=order,
-                )
-            elif dtype is bool:
-                dataset.append_column(
-                    name,
-                    dtype,
-                    optional=True,
-                    editable=True,
-                    default=False,
-                    order=order,
-                )
-            else:
-                dataset.append_column(
-                    name, dtype, optional=True, editable=True, order=order
-                )
-
-            return dataset.read_column(name)
-
     def _open_table(self, mode: str = "r") -> H5Dataset:
         try:
             return H5Dataset(self._table_file, mode)
         except FileNotFoundError as e:
             raise NoTableFileFound(self._table_file) from e
         except OSError as e:
             raise CouldNotOpenTableFile(self._table_file) from e
```

## Comparing `renumics/spotlight/backend/statics/static/js/main.27112c26.js` & `renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js`

 * *Files 0% similar despite different names*

### js-beautify {}

```diff
@@ -1,8 +1,8 @@
-/*! For license information please see main.27112c26.js.LICENSE.txt */ ! function() {
+/*! For license information please see main.2c4f3aef.js.LICENSE.txt */ ! function() {
     var e = {
             17972: function(e, t, n) {
                 var r = n(20649);
 
                 function i(e, t) {
                     var n = new r(e, t);
                     return function(e) {
@@ -36058,28 +36058,28 @@
                 if ({
                         NODE_ENV: "production",
                         PUBLIC_URL: ".",
                         WDS_SOCKET_HOST: void 0,
                         WDS_SOCKET_PATH: void 0,
                         WDS_SOCKET_PORT: void 0,
                         FAST_REFRESH: !0,
-                        REACT_APP_VERSION: "1.0.0rc8",
+                        REACT_APP_VERSION: "1.0.0rc9",
                         REACT_APP_PUBLIC_URL: "./",
                         REACT_APP_PREVIEW_USER: "Volker Vorschau",
                         REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                         REACT_APP_VARIANT: "core"
                     }.NODE_DEBUG) {
                     var s = {
                         NODE_ENV: "production",
                         PUBLIC_URL: ".",
                         WDS_SOCKET_HOST: void 0,
                         WDS_SOCKET_PATH: void 0,
                         WDS_SOCKET_PORT: void 0,
                         FAST_REFRESH: !0,
-                        REACT_APP_VERSION: "1.0.0rc8",
+                        REACT_APP_VERSION: "1.0.0rc9",
                         REACT_APP_PUBLIC_URL: "./",
                         REACT_APP_PREVIEW_USER: "Volker Vorschau",
                         REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                         REACT_APP_VARIANT: "core"
                     }.NODE_DEBUG;
                     s = s.replace(/[|\\{}()[\]^$+?.]/g, "\\$&").replace(/\*/g, ".*").replace(/,/g, "$|^").toUpperCase(), a = new RegExp("^" + s + "$", "i")
                 }
@@ -45659,125 +45659,125 @@
             var Ee = "undefined" != typeof process && ({
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.REACT_APP_SC_ATTR || {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.SC_ATTR) || "data-styled",
                 Ce = "undefined" != typeof window && "HTMLElement" in window,
                 Me = Boolean("boolean" == typeof SC_DISABLE_SPEEDY ? SC_DISABLE_SPEEDY : "undefined" != typeof process && void 0 !== {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.REACT_APP_SC_DISABLE_SPEEDY && "" !== {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.REACT_APP_SC_DISABLE_SPEEDY ? "false" !== {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.REACT_APP_SC_DISABLE_SPEEDY && {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.REACT_APP_SC_DISABLE_SPEEDY : "undefined" != typeof process && void 0 !== {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.SC_DISABLE_SPEEDY && "" !== {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.SC_DISABLE_SPEEDY && ("false" !== {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.SC_DISABLE_SPEEDY && {
                     NODE_ENV: "production",
                     PUBLIC_URL: ".",
                     WDS_SOCKET_HOST: void 0,
                     WDS_SOCKET_PATH: void 0,
                     WDS_SOCKET_PORT: void 0,
                     FAST_REFRESH: !0,
-                    REACT_APP_VERSION: "1.0.0rc8",
+                    REACT_APP_VERSION: "1.0.0rc9",
                     REACT_APP_PUBLIC_URL: "./",
                     REACT_APP_PREVIEW_USER: "Volker Vorschau",
                     REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                     REACT_APP_VARIANT: "core"
                 }.SC_DISABLE_SPEEDY)),
                 Te = {};
 
@@ -46492,25 +46492,25 @@
                             }
                         }
                         return o
                     }
                 }(e, t) || Kt(e, t) || $t()
             }
             var Jt = {
-                    version: null !== (Ft = "1.0.0rc8") ? Ft : "dev",
+                    version: null !== (Ft = "1.0.0rc9") ? Ft : "dev",
                     variant: null !== (Ut = "core") ? Ut : "core",
                     publicUrl: ".",
                     apiUrl: {
                         NODE_ENV: "production",
                         PUBLIC_URL: ".",
                         WDS_SOCKET_HOST: void 0,
                         WDS_SOCKET_PATH: void 0,
                         WDS_SOCKET_PORT: void 0,
                         FAST_REFRESH: !0,
-                        REACT_APP_VERSION: "1.0.0rc8",
+                        REACT_APP_VERSION: "1.0.0rc9",
                         REACT_APP_PUBLIC_URL: "./",
                         REACT_APP_PREVIEW_USER: "Volker Vorschau",
                         REACT_APP_DOCS_URL: "https://docs.renumics.com/spotlight/",
                         REACT_APP_VARIANT: "core"
                     }.REACT_APP_API_BASE_PATH,
                     docsUrl: "https://spotlight.renumics.com",
                     repositoryUrl: "https://github.com/renumics/spotlight"
@@ -94565,15 +94565,15 @@
                             return (0, Wt.jsx)(K$, pn({
                                 column: t
                             }, n));
                         default:
                             return (0, Wt.jsx)(Wt.Fragment, {})
                     }
                 };
-            $$.displayName = "Select (Autocomplete)", $$.dataTypes = ["str", "Category"], $$.defaultHeight = 22, $$.minHeight = 22, $$.maxHeight = 48, $$.isEditor = !0;
+            $$.displayName = "Select (Autocomplete)", $$.dataTypes = ["str", "Category"], $$.defaultHeight = 28, $$.minHeight = 28, $$.maxHeight = 48, $$.isEditor = !0;
             var Z$ = $$,
                 J$ = n(23602),
                 Q$ = n.n(J$),
                 eZ = function() {
                     return eZ = Object.assign || function(e) {
                         for (var t, n = 1, r = arguments.length; n < r; n++)
                             for (var i in t = arguments[n]) Object.prototype.hasOwnProperty.call(t, i) && (e[i] = t[i]);
@@ -119822,15 +119822,15 @@
                     return (0, Wt.jsx)(cae, {
                         children: (0, Wt.jsx)(MU, {
                             value: t,
                             column: n
                         })
                     })
                 };
-            uae.dataTypes = ["int", "float", "bool", "str", "datetime", "Category"], uae.defaultHeight = 22, uae.minHeight = 22, uae.maxHeight = 64, uae.displayName = "Value";
+            uae.dataTypes = ["int", "float", "bool", "str", "datetime", "Category"], uae.defaultHeight = 28, uae.minHeight = 28, uae.maxHeight = 64, uae.displayName = "Value";
             var lae = uae,
                 cae = Ht("div").withConfig({
                     displayName: "ScalarView___StyledDiv",
                     componentId: "sc-1ovl1x0-0"
                 })({
                     fontSize: "0.875rem",
                     lineHeight: "1.25rem",
@@ -131445,15 +131445,15 @@
                             return (0, Wt.jsx)(Eye, pn({
                                 value: t,
                                 values: n
                             }, r))
                     }
                     return (0, Wt.jsx)(Wt.Fragment, {})
                 };
-            Mye.dataTypes = ["Category", "bool"], Mye.displayName = "Switch", Mye.minHeight = 24, Mye.defaultHeight = 32, Mye.maxHeight = 128, Mye.isEditor = !0;
+            Mye.dataTypes = ["Category", "bool"], Mye.displayName = "Switch", Mye.minHeight = 28, Mye.defaultHeight = 32, Mye.maxHeight = 128, Mye.isEditor = !0;
             var Tye, Aye = Mye,
                 Oye = Ht("button").withConfig({
                     displayName: "Switch___StyledButton",
                     componentId: "sc-6xzljz-1"
                 })(["", ""], (function(e) {
                     return e.$_css
                 })),
@@ -131783,35 +131783,35 @@
                         h = (0, re.useRef)(0),
                         p = (0, re.useRef)(0),
                         v = (0, re.useRef)(0),
                         m = Zye(ebe);
                     (0, re.useEffect)((function() {
                         var e = m.reduce((function(e, t) {
                             var n, r, i;
-                            return e[t.key] = null !== (n = null !== (r = a[t.key]) && void 0 !== r ? r : null === (i = Gye[t.view]) || void 0 === i ? void 0 : i.defaultHeight) && void 0 !== n ? n : 24, e
+                            return e[t.key] = null !== (n = null !== (r = a[t.key]) && void 0 !== r ? r : null === (i = Gye[t.view]) || void 0 === i ? void 0 : i.defaultHeight) && void 0 !== n ? n : 28, e
                         }), {});
                         xl(e, a) || s(e)
                     }), [m, a]);
                     var g = (0, re.useCallback)((function(e) {
                             var t, n;
-                            return null !== (t = a[null === (n = m[e]) || void 0 === n ? void 0 : n.key]) && void 0 !== t ? t : 24
+                            return null !== (t = a[null === (n = m[e]) || void 0 === n ? void 0 : n.key]) && void 0 !== t ? t : 28
                         }), [a, m]),
                         y = (0, re.useCallback)((function(e, t) {
                             var n = m[e].key;
                             f.current = n, d.current = t, h.current = t, p.current = a[n], v.current = p.current, c(!0)
                         }), [a, m]),
                         b = (0, re.useCallback)((function(e, t) {
                             var r = m.find((function(t) {
                                 return t.key === e
                             }));
                             if (void 0 !== r) {
-                                var i, o, a = null === (i = Gye[r.view]) || void 0 === i ? void 0 : i.maxHeight;
-                                void 0 !== a && (t = Math.min(a, t));
-                                var u = (null === (o = Gye[r.view]) || void 0 === o ? void 0 : o.minHeight) || 12;
-                                void 0 !== u && (t = Math.max(u, t))
+                                var i, o, a, u = null === (i = Gye[r.view]) || void 0 === i ? void 0 : i.maxHeight;
+                                void 0 !== u && (t = Math.min(u, t));
+                                var l = null !== (o = null === (a = Gye[r.view]) || void 0 === a ? void 0 : a.minHeight) && void 0 !== o ? o : 28;
+                                void 0 !== l && (t = Math.max(l, t))
                             }
                             v.current = t, s((function(t) {
                                 return pn(pn({}, t), {}, dn({}, e, v.current))
                             })), n(e)
                         }), [n, m]),
                         w = (0, re.useCallback)((function(e) {
                             var t;
```

## Comparing `renumics/spotlight/backend/statics/static/js/main.27112c26.js.LICENSE.txt` & `renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js.LICENSE.txt`

 * *Files identical despite different names*

## Comparing `renumics/spotlight/dataset/descriptors/utils.py` & `renumics/spotlight/dataset/descriptors/data_alignment.py`

 * *Files identical despite different names*

## Comparing `renumics_spotlight-1.0.0rc8.dist-info/METADATA` & `renumics_spotlight-1.0.0rc9.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: renumics-spotlight
-Version: 1.0.0rc8
+Version: 1.0.0rc9
 Summary: Visualize and maintain datasets to develop and understand data-driven algorithms.
 Home-page: https://spotlight.renumics.com/
 License: MIT
 Keywords: data curation,machine learning,data science,visualization,pandas,ai
 Author: Renumics GmbH
 Author-email: info@renumics.com
 Requires-Python: >=3.7,<3.11
@@ -42,27 +42,26 @@
 Requires-Dist: imagecodecs ; platform_machine != "arm64"
 Requires-Dist: imageio (>=2.18.0)
 Requires-Dist: importlib_resources (<5.8.0)
 Requires-Dist: ipywidgets
 Requires-Dist: loguru
 Requires-Dist: networkx
 Requires-Dist: notebook
-Requires-Dist: nptyping (<2.0.0)
 Requires-Dist: numpy
 Requires-Dist: orjson
 Requires-Dist: packaging
 Requires-Dist: pandas
 Requires-Dist: prettytable
+Requires-Dist: pycatch22
 Requires-Dist: pydantic
 Requires-Dist: pygltflib (>=1.15.1)
 Requires-Dist: requests
 Requires-Dist: rsa
 Requires-Dist: scikit-image
 Requires-Dist: scikit-learn
-Requires-Dist: sktime (>0.9.0)
 Requires-Dist: tqdm
 Requires-Dist: trimesh
 Requires-Dist: typing-extensions
 Requires-Dist: umap-learn
 Requires-Dist: uvicorn
 Requires-Dist: uvloop (>=0.17.0,<0.18.0) ; sys_platform == "darwin"
 Requires-Dist: uvloop (>=0.17.0,<0.18.0) ; sys_platform == "linux"
```

## Comparing `renumics_spotlight-1.0.0rc8.dist-info/LICENSE` & `renumics_spotlight-1.0.0rc9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `renumics_spotlight-1.0.0rc8.dist-info/RECORD` & `renumics_spotlight-1.0.0rc9.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,85 +1,86 @@
-renumics/spotlight/__init__.py,sha256=xLTJQzGM3B12EAOuWhu-6kp4q2Q_UnTUf1DMxMzHnl8,438
-renumics/spotlight/__version__.py,sha256=BLuowG2U5MXfVSWvx-F1cT9p2G8B4UbfKh3Y20rOVpQ,78
+renumics/spotlight/__init__.py,sha256=Y9oYZa3hGBK2hF-YlLGIw55Q7a8Gp63xLs0a-32F2yg,452
+renumics/spotlight/__version__.py,sha256=o_GKhmDaL9JGx7xdDQlYb-MDssYAcv35ZcOeOkmytq8,78
 renumics/spotlight/_build_variant.py,sha256=YC4SJ2H0vc1f5fSvmSgtl9coF3GX_sPA4leYxumAUAc,83
 renumics/spotlight/appdirs.py,sha256=af6CB1RFn1q4FWSiklv4PRsGWHjYVPYCuqMZkxB7bDE,308
-renumics/spotlight/backend/__init__.py,sha256=76mZmiE57C4nbqR9ZqktOhl77oZeS_Qf1SQMskBCr3c,908
+renumics/spotlight/backend/__init__.py,sha256=MoFyg_eyjdxbvHVhVq6DPw9aDD_kDEZnI12wZxzpVhQ,1379
 renumics/spotlight/backend/apis/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-renumics/spotlight/backend/apis/config.py,sha256=eMSZrO0HH26sXoQdrhPkwBXZWFVvjxpc4TmdnUd0Asg,1110
-renumics/spotlight/backend/apis/filebrowser.py,sha256=5t7Vvp_ePwT4aEyTGt0MZrsYIR-tuTron0ijHm28xAQ,1532
-renumics/spotlight/backend/apis/layout.py,sha256=_U4yitn44lVnHB6-xOtxEZZDeKcD3TcNGAq86q1v-tw,1789
-renumics/spotlight/backend/apis/table.py,sha256=V2bZMS18Y_Jy4hR5vFrmC3mc3mMXCz37IKfwiJvIWws,10969
-renumics/spotlight/backend/apis/user.py,sha256=b2L7IJ6N9CnfNbQuqqcNfubjQR78dgYjCQTHz34Id3A,392
 renumics/spotlight/backend/apis/websocket.py,sha256=aAxKtRqWs7NTIoWb74aAEqZR4qaAuXP96SkD68Dmv6Y,396
-renumics/spotlight/backend/app.py,sha256=AJFMqOheheePGgGxgguoJb_4xzT5AmvDzfkbKgGoLHY,3828
+renumics/spotlight/backend/app.py,sha256=9Zu1g9A5mMyV_88kImAIrdzgfRx36GBEupBKFSBOjAE,3630
 renumics/spotlight/backend/cache.py,sha256=b4wNkXaI8BQcU5l2DqyotC0-Xwv9XIPpH5qTCw5PkQ0,1126
 renumics/spotlight/backend/config.py,sha256=vPirtHCXGQty9m_IsAhu2RdfFzVelpL7MIX5EUAQeNU,2608
-renumics/spotlight/backend/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-renumics/spotlight/backend/core/user.py,sha256=q_JDNj9VEitS0jlknZlEXoPToIfKGF6xfyzBZOpeVtw,836
-renumics/spotlight/backend/data_source.py,sha256=xT88NbYJPge0a8FvOuClFVFrU6UxSlvNE-EmLrsDwxo,10723
-renumics/spotlight/backend/exceptions.py,sha256=XukuvtSnDtDJBuO9nWM5Ai5ShYdvus0wNqSYKTE4ZLo,3993
-renumics/spotlight/backend/hdf5_data_source.py,sha256=PsIHvdqGyIBT5gwX3BTOhTCPa83xRpuexx9AzbqE0Xc,15341
+renumics/spotlight/backend/data_source.py,sha256=2WUoCLppZwUttDEsX5-HrlvnyNqigu5WDmM7otVtt-o,11189
+renumics/spotlight/backend/exceptions.py,sha256=NCDAi_dBhsOa3SLm3D-qHVJ0L-aFs0CCc-GfzONlj8Q,4525
 renumics/spotlight/backend/middlewares/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 renumics/spotlight/backend/middlewares/timing.py,sha256=N4-w-kVh3TVkDZMMasXOtml1JIcX4aQe9oKWdVCXM0s,1177
-renumics/spotlight/backend/pandas_data_source.py,sha256=eHYLWne8Xt-ZCPhGiNstyexrAUsJ7pubR2JpexsYsPM,16792
 renumics/spotlight/backend/server.py,sha256=NEl-eE4KKd6M_9HxlJ7yC5PpH1qlP2YH_vhxMSArTxo,3638
-renumics/spotlight/backend/statics/asset-manifest.json,sha256=fyk8pzAAjO6UTdL9_q3nB_elBhSlF3GiA_rQh3kqzA0,395
+renumics/spotlight/backend/statics/asset-manifest.json,sha256=X4VjKA4onyHlpW-3-T6uqRKoZYqMMBuA14UbHd8Xoec,395
 renumics/spotlight/backend/statics/favicon.png,sha256=MSHp-1bdI-H1q9RoVgi4r3jUJV5YwQd-TpFsozEEJzU,1579
-renumics/spotlight/backend/statics/index.html,sha256=it0y1ifhrZYit70m_uLkH_P5AoL4FBEuQz4I6BFVf_s,502
+renumics/spotlight/backend/statics/index.html,sha256=5ibhfmUZMRVJ65g1Z9BmkwQ8kfE6Lld7Zb7y1-VDMf8,502
 renumics/spotlight/backend/statics/logo.svg,sha256=jWwWRyZLnf2eOImH1uAnzmLKnzlThuGHJPD6MLb3uFg,9808
 renumics/spotlight/backend/statics/manifest.json,sha256=VxTrMJ6AYHIzMT3FJwLA-59pAdA8NpzIht4KjlEiQ2k,586
 renumics/spotlight/backend/statics/robots.txt,sha256=kNJLw79pisHhc3OVAimMzKcq3x9WT6sF9IS4xI0crdI,67
 renumics/spotlight/backend/statics/spotlight.svg,sha256=Xf793OdQRubPsc0U72PpEhl1L92h3zO8KOMKdW3Zf1k,506
 renumics/spotlight/backend/statics/static/css/main.515252a3.css,sha256=Kje9RkfCqE9kcn1K8eAYLjd6kLzNG9Xoi-kK48-X_DI,23379
 renumics/spotlight/backend/statics/static/js/384.87f65e3c.chunk.js,sha256=GyM3FnKIPzIsSrYLfQtZhaVkPOSCVswdeRSc8BeOitA,65624
 renumics/spotlight/backend/statics/static/js/384.87f65e3c.chunk.js.LICENSE.txt,sha256=zfljztfSWg-YkBpUdke01uLb4Bl_14yHoFmoew5UL-I,149
 renumics/spotlight/backend/statics/static/js/886.810d122e.chunk.js,sha256=zF2XnXsqgcVLwB66n8VPAU-5SAucNxmqePT_AQ_l4IU,2241
-renumics/spotlight/backend/statics/static/js/main.27112c26.js,sha256=dhyGK1G_W5XBo0w9CJYPFjbs2IK0r8csFSLpcdUvaAo,3167667
-renumics/spotlight/backend/statics/static/js/main.27112c26.js.LICENSE.txt,sha256=2Qk25s3dreCxCKpR5INtyJ8HUOc10vF0klj4tUOkqA8,13460
+renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js,sha256=vG9syEPXlGDqpWQ34wqVONhUAl8_e2hzbTKZ7_eQ9E0,3167691
+renumics/spotlight/backend/statics/static/js/main.2c4f3aef.js.LICENSE.txt,sha256=2Qk25s3dreCxCKpR5INtyJ8HUOc10vF0klj4tUOkqA8,13460
 renumics/spotlight/backend/tasks/__init__.py,sha256=sETCb9EEd0h1921qGyZbfNgBnmunSWISNp5SDOm6so0,126
 renumics/spotlight/backend/tasks/exceptions.py,sha256=EOu0lk9I85lQs48875NSc1G-f_9lpUY-mo22Lb3tymw,121
 renumics/spotlight/backend/tasks/reduction.py,sha256=z9LVLPe7mAPLbbmbqIbWFQ_t4539L5H0SOgEC0vdZ5M,3670
 renumics/spotlight/backend/tasks/task.py,sha256=ifJQin8GMd_sswdVWeuakXIYVPR57ZFX9js36GgdMyk,325
 renumics/spotlight/backend/tasks/task_manager.py,sha256=c0ASUK5aTY_fEBPmo2nB6BSulPFWEKJkHRBMifj1opM,3286
 renumics/spotlight/backend/websockets.py,sha256=2TqaIOZNie8qco5ZiBr9hjgcLwZ6Y9aAT1j2ea6ikuI,9299
 renumics/spotlight/cli.py,sha256=Jw5PmQ0IG2wzPV65j8l7O4IhhUESDULLZ1CTzXKkOr8,3609
-renumics/spotlight/dataset/__init__.py,sha256=PPnSIwDvHNl5qLDygZUPGHW-hWRKGvHY0U7-gJ7nplw,143164
-renumics/spotlight/dataset/descriptors/__init__.py,sha256=j6jsyB8DwbpD9FoEb3lCDoqHHqrY1jqYN9SGiHaT8Nw,133
-renumics/spotlight/dataset/descriptors/catch22.py,sha256=2Hceqq6BaPFMpmbOA_GHItoACwNzmrnPMu2_Tawq9YM,3173
-renumics/spotlight/dataset/descriptors/pca.py,sha256=iZ6Q3BOeDuuFQP-JAU42srcFkcL7__2T4si51IUaMfM,1810
-renumics/spotlight/dataset/descriptors/utils.py,sha256=jq8Uj1zSmTi31TJYJWwjyagr1a3b0cIYMDt9tMxsbJE,7053
+renumics/spotlight/dataset/__init__.py,sha256=Z0k2NvL7e706d8YmWuX3mXraIWKEKaYNo4H7GORLsDQ,139918
+renumics/spotlight/dataset/descriptors/__init__.py,sha256=NbX32GiWier97vgNEsv0rR96Lfj7Bw-JHC-40Mu2Cqg,4530
+renumics/spotlight/dataset/descriptors/data_alignment.py,sha256=jq8Uj1zSmTi31TJYJWwjyagr1a3b0cIYMDt9tMxsbJE,7053
 renumics/spotlight/dataset/exceptions.py,sha256=hvTyOH4oQad5ef2O90jQ0Oqf0YiUT0Sz9_x4HzllgFo,1395
 renumics/spotlight/dataset/typing.py,sha256=88ZOS0bDLaRktNVmz0OGWH6c157_zHU1UYbkaGOpslg,2576
-renumics/spotlight/dtypes/__init__.py,sha256=ja5BuskFoIpeLOCg8Kfb1qrJjG2EBkcfPlE8tKugk8I,37402
+renumics/spotlight/dtypes/__init__.py,sha256=HthVe_FUY1u8xuyjV1E9UTcDTKb9yhk7dDjL3xTMzRg,37681
 renumics/spotlight/dtypes/exceptions.py,sha256=yc_UOzvlpC0aRntuSNc5ABCf8OKOTy2QwEMgJBhSN9E,427
 renumics/spotlight/dtypes/triangulation.py,sha256=ah_2zNkKgtA44npQP4NMX3SKRgSCsUy9LwREa4HlPcE,5569
 renumics/spotlight/dtypes/typing.py,sha256=VU88ESAzLzWKdVnJhMkGJvyiqydb0xZtwLDUjF5tDXU,2440
 renumics/spotlight/environ.py,sha256=EfLHpabvWlcIEuyNTMk6ihDFoHF0lhzxG_wtAqLeAf0,888
 renumics/spotlight/io/__init__.py,sha256=mDNEBaJoKTHrCTmrq1iMwZPxkwErQR945VAckdptsKo,368
 renumics/spotlight/io/audio.py,sha256=yU8Dgopm937KPSxqq1O8leMVTFvIdcV9CRRZdPtxFVY,6899
 renumics/spotlight/io/gltf.py,sha256=U-2syphBP9gmeGSEsPg8dxEhVohsNOWa_0yFikfLzFI,5534
-renumics/spotlight/io/pandas.py,sha256=JIYKce1BIbFJGiKGMTYKXmn2xBY3IWkIkdEkJteUjOI,5448
+renumics/spotlight/io/pandas.py,sha256=dfpW4isWdr9bH9ptSWUQdh26saFvgIjZ4WaWW80kZbM,5502
 renumics/spotlight/layout/__init__.py,sha256=xRPvpt8eYJDcRiJGz66jCJYxZz1g5yLrc-GeX7WYgw4,9092
 renumics/spotlight/layout/default.py,sha256=kfAUkIH2CUuf15Ym1CTsPQFOU-vlxPVSpEsyZYoIkXM,340
 renumics/spotlight/layout/nodes.py,sha256=EqS1wNEl_hNkAmB-xeFE8udLHQMr0ceGXZXnnsguk-s,1529
 renumics/spotlight/layout/widgets.py,sha256=0q7HzYEJA6AkeOPlfscgk__f_0jztLaLDkRkFQDDVZc,6513
 renumics/spotlight/licensing/README.md,sha256=tA8cAWPR4Mf3wUt5JjiM0B-u66hzZyZrJ_rcNx6zBvU,847
 renumics/spotlight/licensing/__init__.py,sha256=8dFt1nNOKywVcLTXL0u99bnAQuSFYVcSeHuun2IqKb8,1099
 renumics/spotlight/licensing/terms_and_conditions.py,sha256=R1-09WOdwqsuSlPGdhAOrALAg3eIhO8Jv2RTtXJVtf4,34453
 renumics/spotlight/licensing/verification.py,sha256=Ivc2w5wPa7hfHcAS0mLMPyB-POx_iZMmkjquexClLNc,5326
 renumics/spotlight/notebook/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 renumics/spotlight/notebook/cli.py,sha256=A9NTI3m7492eOs3B0aObETOhmcoFUidmBolwB11ITrA,2554
 renumics/spotlight/notebook/theme/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 renumics/spotlight/notebook/theme/custom.css,sha256=4RozHxBlIIxTH1mVyJ5o5hQpYJ8SRiDXBfnUO0PdOG4,279
 renumics/spotlight/notebook/theme/logo.svg,sha256=8MkylzZ0d-VVgW6d__9BzVN77i6Pv_oEYiY4ksXKvgE,591
-renumics/spotlight/plugin_loader.py,sha256=rxlNgXxAO3Qb4JIrPSib-FHydYWz4QOmhwZ7dejFAHU,523
-renumics/spotlight/plugins/core/__init__.py,sha256=_nlTBaaUJTiaKW8DUU7LsOxaw1JFabwHLZ6Oa4GGwjU,98
+renumics/spotlight/plugin_loader.py,sha256=BXxcrj_AWCkVgKWNBPNo7aAPCMVa8qZ-IlZsAbb_CE0,721
+renumics/spotlight/plugins/core/__init__.py,sha256=zx9K21KS1XMt_j3W3fuX1rRVRaagtpRUeMPTsyk4_n0,888
+renumics/spotlight/plugins/core/api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+renumics/spotlight/plugins/core/api/config.py,sha256=AhcX4hM1a6T8fIFMUrYoq9YVfSu8lrG7e14eWtdQVXM,1136
+renumics/spotlight/plugins/core/api/filebrowser.py,sha256=5t7Vvp_ePwT4aEyTGt0MZrsYIR-tuTron0ijHm28xAQ,1532
+renumics/spotlight/plugins/core/api/layout.py,sha256=_U4yitn44lVnHB6-xOtxEZZDeKcD3TcNGAq86q1v-tw,1789
+renumics/spotlight/plugins/core/api/table.py,sha256=ZmZLimKN-17yDRiecPfgyx2A2RzsjxgsACRnpl1tVog,6463
+renumics/spotlight/plugins/core/api/user.py,sha256=DJYe3jQAK_FR5HII_JEazeJBS2bOKtEw-UDXAxGPlMU,955
+renumics/spotlight/plugins/core/hdf5_data_source.py,sha256=gEqky0RUNUnfI_OCZps9QDRpyz8e9oTB34hpj3CjlN4,11695
+renumics/spotlight/plugins/core/pandas_data_source.py,sha256=6I8vsKopIIL4D91DvoibVNH8zP_MAGAxyTZAplJZNxg,12699
+renumics/spotlight/plugins/pro/__init__.py,sha256=WRst8yJK56rfgpDBAd5E6COmbuhVosa2OfBFuCrjPp8,511
+renumics/spotlight/plugins/pro/api/table.py,sha256=fQNWF3sQrk2Mvc_rEb1cNwJUh9pSfidqsSqiGxJmpRY,5111
+renumics/spotlight/plugins/pro/hdf5_data_source.py,sha256=jI4MeDphc6LxuQa7pT8ZEQ9argrD1snPPK65hK5sR0o,4318
+renumics/spotlight/plugins/pro/pandas_data_source.py,sha256=A1_MxIpK6mQXS_-a2J9u5M_QKnw-M0L8x8QlMbEC8fY,4141
 renumics/spotlight/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 renumics/spotlight/settings.py,sha256=-PAh56EY2luayzwKsBFq8xWzoLugKhbO2wBqKf58Pu8,462
 renumics/spotlight/typing.py,sha256=PAPm35caldjeKyiWsiccx17hvUdapxYJ-l4TFX0gbiI,1303
-renumics/spotlight/viewer.py,sha256=b0JWvzKF7IS1N_yiVPS3NHMZ5jCL00TgUPzjf-rXwjw,12925
+renumics/spotlight/viewer.py,sha256=2UDSkZpKL_BayOOa0Se_G6Zk9F61gqrwpIsJ3rDA9g0,11945
 renumics/spotlight/webbrowser.py,sha256=nbmO8sPpqjKA_V5AJQAUXagcw5UuPD0rGBmFlLiHQzk,1630
-renumics_spotlight-1.0.0rc8.dist-info/WHEEL,sha256=vVCvjcmxuUltf8cYhJ0sJMRDLr1XsPuxEId8YDzbyCY,88
-renumics_spotlight-1.0.0rc8.dist-info/entry_points.txt,sha256=OF4JRdDRdkTPm_yyIKJuZ0n4VD9RxIC2nF7a1u-uZCc,113
-renumics_spotlight-1.0.0rc8.dist-info/METADATA,sha256=KOxbQ_gG0wsH5yZkt4r9iXGU2WYa8SYdL9ApmUZFBIQ,4852
-renumics_spotlight-1.0.0rc8.dist-info/LICENSE,sha256=PyGfmsJccsZex9KO2NfQUUnKwOX8AF3m1aKB4I6hlZo,1066
-renumics_spotlight-1.0.0rc8.dist-info/RECORD,,
+renumics_spotlight-1.0.0rc9.dist-info/WHEEL,sha256=vVCvjcmxuUltf8cYhJ0sJMRDLr1XsPuxEId8YDzbyCY,88
+renumics_spotlight-1.0.0rc9.dist-info/entry_points.txt,sha256=OF4JRdDRdkTPm_yyIKJuZ0n4VD9RxIC2nF7a1u-uZCc,113
+renumics_spotlight-1.0.0rc9.dist-info/METADATA,sha256=j1x6GYKGAUghiPjoOaBwQ_wArkc3q5yi1DLm5njc1PI,4813
+renumics_spotlight-1.0.0rc9.dist-info/LICENSE,sha256=PyGfmsJccsZex9KO2NfQUUnKwOX8AF3m1aKB4I6hlZo,1066
+renumics_spotlight-1.0.0rc9.dist-info/RECORD,,
```

