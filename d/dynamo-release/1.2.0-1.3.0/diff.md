# Comparing `tmp/dynamo-release-1.2.0.tar.gz` & `tmp/dynamo-release-1.3.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/dynamo-release-1.2.0.tar", last modified: Thu Dec 15 21:39:03 2022, max compression
+gzip compressed data, was "dist/dynamo-release-1.3.0.tar", last modified: Tue May 30 20:03:33 2023, max compression
```

## Comparing `dynamo-release-1.2.0.tar` & `dynamo-release-1.3.0.tar`

### file list

```diff
@@ -1,162 +1,170 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/
--rw-r--r--   0 runner    (1001) docker     (123)     7360 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/PKG-INFO
--rwxr-xr-x   0 runner    (1001) docker     (123)     6302 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/
--rwxr-xr-x   0 runner    (1001) docker     (123)      734 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    29643 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/configuration.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    11861 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/data_io.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    34268 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/docrep.py
--rw-r--r--   0 runner    (1001) docker     (123)    12477 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/dynamo_logger.py
--rw-r--r--   0 runner    (1001) docker     (123)       71 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/est.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/estimation/
--rw-r--r--   0 runner    (1001) docker     (123)      153 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/estimation/csc/
--rw-r--r--   0 runner    (1001) docker     (123)      280 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/csc/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    26002 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/csc/utils_velocity.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    88165 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/csc/velocity.py
--rw-r--r--   0 runner    (1001) docker     (123)     5339 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/fit_jacobian.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/estimation/tsc/
--rw-r--r--   0 runner    (1001) docker     (123)      624 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/tsc/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    33692 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/tsc/estimation_kinetic.py
--rw-r--r--   0 runner    (1001) docker     (123)     2530 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/tsc/twostep.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    21537 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/tsc/utils_kinetic.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     8945 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/estimation/tsc/utils_moments.py
--rw-r--r--   0 runner    (1001) docker     (123)       69 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/ext.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/external/
--rwxr-xr-x   0 runner    (1001) docker     (123)      583 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3094 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/gseapy.py
--rw-r--r--   0 runner    (1001) docker     (123)    10622 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/hodge.py
--rw-r--r--   0 runner    (1001) docker     (123)    22646 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/pearson_residual_recipe.py
--rw-r--r--   0 runner    (1001) docker     (123)    14622 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/scifate.py
--rw-r--r--   0 runner    (1001) docker     (123)    20596 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/scribe.py
--rw-r--r--   0 runner    (1001) docker     (123)    11556 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/sctransform.py
--rw-r--r--   0 runner    (1001) docker     (123)     4455 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/external/utils.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     6194 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/get_version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/movie/
--rw-r--r--   0 runner    (1001) docker     (123)       93 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/movie/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    17146 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/movie/fate.py
--rw-r--r--   0 runner    (1001) docker     (123)      345 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/movie/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)       66 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/mv.py
--rw-r--r--   0 runner    (1001) docker     (123)       71 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/pd.py
--rwxr-xr-x   0 runner    (1001) docker     (123)       65 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/pl.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/plot/
--rwxr-xr-x   0 runner    (1001) docker     (123)     3269 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2855 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/cell_cycle.py
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/clustering.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    20570 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2949 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/dimension_reduction.py
--rwxr-xr-x   0 runner    (1001) docker     (123)   125212 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/dynamics.py
--rw-r--r--   0 runner    (1001) docker     (123)     8774 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/ezplots.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     6556 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/fate.py
--rwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/fate_utilities_deprecated.py
--rw-r--r--   0 runner    (1001) docker     (123)    58936 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/heatmaps.py
--rw-r--r--   0 runner    (1001) docker     (123)     6492 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/least_action_path.py
--rw-r--r--   0 runner    (1001) docker     (123)    14775 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/markers.py
--rw-r--r--   0 runner    (1001) docker     (123)    24840 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/networks.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    41060 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/preprocess.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1965 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/pseudotime.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     4740 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/scPotential.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    69474 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/scVectorField.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    44620 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/scatters.py
--rw-r--r--   0 runner    (1001) docker     (123)     6803 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/space.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     9772 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/state_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     7304 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/streamtube.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      544 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/theme.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    45526 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/time_series.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    54433 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/topography.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    56227 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    75821 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/utils_dynamics.py
--rw-r--r--   0 runner    (1001) docker     (123)     6014 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/utils_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)    51353 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/plot/vector_calculus.py
--rwxr-xr-x   0 runner    (1001) docker     (123)       74 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/pp.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/prediction/
--rwxr-xr-x   0 runner    (1001) docker     (123)     1192 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    33467 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/fate.py
--rw-r--r--   0 runner    (1001) docker     (123)    15184 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/least_action_path.py
--rw-r--r--   0 runner    (1001) docker     (123)    19434 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/perturbation.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    19776 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/state_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     8541 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/trajectory.py
--rw-r--r--   0 runner    (1001) docker     (123)     3954 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/trajectory_analysis.py
--rw-r--r--   0 runner    (1001) docker     (123)     3417 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/tscRNA_seq.py
--rw-r--r--   0 runner    (1001) docker     (123)    21035 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/prediction/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/preprocessing/
--rw-r--r--   0 runner    (1001) docker     (123)     2914 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/CnmfPreprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)    31611 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/Preprocessor.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1828 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17780 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/cell_cycle.py
--rw-r--r--   0 runner    (1001) docker     (123)     6216 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/dynast.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    73321 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/preprocess.py
--rw-r--r--   0 runner    (1001) docker     (123)    11243 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/preprocess_monocle_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    63827 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/preprocessor_utils.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    49307 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/preprocessing/utils.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     8736 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/sample_data.py
--rwxr-xr-x   0 runner    (1001) docker     (123)       71 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/sim.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/simulation/
--rwxr-xr-x   0 runner    (1001) docker     (123)    21614 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/simulation/Gillespie.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13786 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/simulation/ODE.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      582 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/simulation/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    14934 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/simulation/bif_os_inclusive_sim.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1459 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/simulation/evaluation.py
--rw-r--r--   0 runner    (1001) docker     (123)    33624 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/simulation/simulate_anndata.py
--rw-r--r--   0 runner    (1001) docker     (123)    15664 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/simulation/utils.py
--rwxr-xr-x   0 runner    (1001) docker     (123)       66 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tl.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/tools/
--rwxr-xr-x   0 runner    (1001) docker     (123)     6252 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/DDRTree_py.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    34759 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/Markov.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2538 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13110 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/_dynamics_deprecated.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    46294 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/cell_velocities.py
--rw-r--r--   0 runner    (1001) docker     (123)    23849 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/clustering.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    28350 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     6240 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/construct_velocity_tree.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     7974 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/dimension_reduction.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    78959 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/dynamics.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1887 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/dynamo_bk.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     4049 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/dynamo_fitting.py
--rw-r--r--   0 runner    (1001) docker     (123)    20154 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/graph_calculus.py
--rw-r--r--   0 runner    (1001) docker     (123)     3622 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/graph_operators.py
--rw-r--r--   0 runner    (1001) docker     (123)    11413 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/growth.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    33940 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/markers.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    19077 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/metric_velocity.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    46561 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/moments.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1716 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/multiomics.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3524 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/pseudotime.py
--rw-r--r--   0 runner    (1001) docker     (123)     8380 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/pseudotime_velocity.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     9165 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/psl_py.py
--rw-r--r--   0 runner    (1001) docker     (123)    42293 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/recipes.py
--rw-r--r--   0 runner    (1001) docker     (123)     6729 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/sampling.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2865 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/time_series.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    85092 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1442 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/utils_markers.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    17828 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/utils_moments_deprecated.py
--rw-r--r--   0 runner    (1001) docker     (123)     8910 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/utils_reduceDimension.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    17843 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/tools/velocyto_scvelo.py
--rw-r--r--   0 runner    (1001) docker     (123)     2310 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo/vectorfield/
--rwxr-xr-x   0 runner    (1001) docker     (123)     4872 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/Ao.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    17167 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/Bhattacharya.py
--rw-r--r--   0 runner    (1001) docker     (123)     2322 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/FixedPoints.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     6878 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/Wang.py
--rw-r--r--   0 runner    (1001) docker     (123)     1608 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11860 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/cell_vectors.py
--rw-r--r--   0 runner    (1001) docker     (123)    18241 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/clustering.py
--rw-r--r--   0 runner    (1001) docker     (123)     2006 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/least_action_path.py
--rw-r--r--   0 runner    (1001) docker     (123)     9492 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/networks.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    27468 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/scPotential.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    49525 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/scVectorField.py
--rw-r--r--   0 runner    (1001) docker     (123)     9269 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/stochastic_process.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    40347 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/topography.py
--rw-r--r--   0 runner    (1001) docker     (123)     4068 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/util_vector_calculus.py
--rw-r--r--   0 runner    (1001) docker     (123)    44974 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    90323 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/vector_calculus.py
--rw-r--r--   0 runner    (1001) docker     (123)     1993 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vectorfield/vfGraph_deprecated.py
--rw-r--r--   0 runner    (1001) docker     (123)       72 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/dynamo/vf.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo_release.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     7360 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo_release.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     4229 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo_release.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo_release.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      366 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo_release.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        7 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/dynamo_release.egg-info/top_level.txt
--rwxr-xr-x   0 runner    (1001) docker     (123)     2280 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)      171 2022-12-15 21:39:03.000000 dynamo-release-1.2.0/setup.cfg
--rwxr-xr-x   0 runner    (1001) docker     (123)     1932 2022-12-15 21:38:56.000000 dynamo-release-1.2.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/
+-rw-r--r--   0 runner    (1001) docker     (123)     7886 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/PKG-INFO
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6767 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      748 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    32578 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/configuration.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    11861 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/data_io.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    34268 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/docrep.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13186 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/dynamo_logger.py
+-rw-r--r--   0 runner    (1001) docker     (123)       71 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/est.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/estimation/
+-rw-r--r--   0 runner    (1001) docker     (123)      153 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/estimation/csc/
+-rw-r--r--   0 runner    (1001) docker     (123)      280 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/csc/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    26002 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/csc/utils_velocity.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    88165 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/csc/velocity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5339 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/fit_jacobian.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/estimation/tsc/
+-rw-r--r--   0 runner    (1001) docker     (123)      624 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/tsc/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    33692 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/tsc/estimation_kinetic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2530 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/tsc/twostep.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21537 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/tsc/utils_kinetic.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8945 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/estimation/tsc/utils_moments.py
+-rw-r--r--   0 runner    (1001) docker     (123)       69 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/ext.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/external/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      324 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/external/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3061 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/external/gseapy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10622 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/external/hodge.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14633 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/external/scifate.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20607 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/external/scribe.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4455 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/external/utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6440 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/get_version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/movie/
+-rw-r--r--   0 runner    (1001) docker     (123)       93 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/movie/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    17146 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/movie/fate.py
+-rw-r--r--   0 runner    (1001) docker     (123)      345 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/movie/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)       66 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/mv.py
+-rw-r--r--   0 runner    (1001) docker     (123)       71 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/pd.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)       65 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/pl.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/plot/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3387 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3436 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/cell_cycle.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6268 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/clustering.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    25248 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2528 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/dimension_reduction.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)   126140 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/dynamics.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17396 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/ezplots.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6928 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/fate.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/fate_utilities_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (123)    68251 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/heatmaps.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8381 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/least_action_path.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15497 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/markers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23916 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/networks.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    42698 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/preprocess.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2389 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/pseudotime.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5100 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/scPotential.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    77144 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/scVectorField.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    45133 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/scatters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5307 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/sctransform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7009 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/space.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    15705 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/state_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8112 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/streamtube.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      544 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/theme.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    47637 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/time_series.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    60256 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/topography.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    56243 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    75821 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/utils_dynamics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6013 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/utils_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51879 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/plot/vector_calculus.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)       74 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/pp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/prediction/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1192 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    31622 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/fate.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24687 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/least_action_path.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18467 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/perturbation.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    19549 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/state_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15081 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/trajectory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8150 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/trajectory_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3264 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/tscRNA_seq.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21198 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/prediction/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/preprocessing/
+-rw-r--r--   0 runner    (1001) docker     (123)     2914 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/CnmfPreprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36583 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/Preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23712 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/QC.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2144 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17784 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/cell_cycle.py
+-rw-r--r--   0 runner    (1001) docker     (123)    79367 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6216 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/dynast.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/preprocessing/external/
+-rw-r--r--   0 runner    (1001) docker     (123)      365 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/external/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/external/integration.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25574 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/external/pearson_residual_recipe.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16545 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/external/sctransform.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35475 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/gene_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16850 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14287 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/pca.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12122 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/transform.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    36269 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/preprocessing/utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8736 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/sample_data.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)       71 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/sim.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/simulation/
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21399 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/simulation/Gillespie.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18272 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/simulation/ODE.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      585 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/simulation/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    14934 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/simulation/bif_os_inclusive_sim.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1374 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/simulation/evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33644 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/simulation/simulate_anndata.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15585 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/simulation/utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)       66 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tl.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/tools/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7504 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/DDRTree_py.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    34759 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/Markov.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2538 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13110 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/_dynamics_deprecated.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    46051 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/cell_velocities.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31842 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/clustering.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    30327 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6973 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/construct_velocity_tree.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12409 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/dimension_reduction.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    83861 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/dynamics.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1887 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/dynamo_bk.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4049 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/dynamo_fitting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30473 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/graph_calculus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3634 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/graph_operators.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11769 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/growth.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    36505 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/markers.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    20934 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/metric_velocity.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    55606 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/moments.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1566 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/multiomics.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3642 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/pseudotime.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9672 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/pseudotime_velocity.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7636 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/psl_py.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41332 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/recipes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9013 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/sampling.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3391 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/time_series.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)   104052 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/utils_markers.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    17828 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/utils_moments_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12166 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/utils_reduceDimension.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18907 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/tools/velocyto_scvelo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2313 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo/vectorfield/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5135 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/Ao.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    17110 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/Bhattacharya.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3685 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/FixedPoints.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6564 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/Wang.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1633 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11461 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/cell_vectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20086 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/clustering.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4725 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/least_action_path.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9365 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/networks.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35881 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/rank_vf.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    28474 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/scPotential.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    57332 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/scVectorField.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8984 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/stochastic_process.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    52760 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/topography.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4068 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/util_vector_calculus.py
+-rw-r--r--   0 runner    (1001) docker     (123)    54129 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46453 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/vector_calculus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1993 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vectorfield/vfGraph_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (123)       72 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/dynamo/vf.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo_release.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     7886 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo_release.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     4475 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo_release.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo_release.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      981 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo_release.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/dynamo_release.egg-info/top_level.txt
+-rwxr-xr-x   0 runner    (1001) docker     (123)       30 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)      171 2023-05-30 20:03:33.000000 dynamo-release-1.3.0/setup.cfg
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1554 2023-05-30 20:03:23.000000 dynamo-release-1.3.0/setup.py
```

### Comparing `dynamo-release-1.2.0/PKG-INFO` & `dynamo-release-1.3.0/PKG-INFO`

 * *Files 7% similar despite different names*

```diff
@@ -1,40 +1,42 @@
 Metadata-Version: 2.1
 Name: dynamo-release
-Version: 1.2.0
+Version: 1.3.0
 Summary: Mapping Vector Field of Single Cells
 Home-page: https://github.com/aristoteleo/dynamo-release
 Author: Xiaojie Qiu, Yan Zhang, Ke Ni
 Author-email: xqiu.sc@gmail.com
 License: BSD
 Download-URL: https://github.com/aristoteleo/dynamo-release
 Description: <p align="center">
           <img height="150" src="https://dynamo-release.readthedocs.io/en/latest/_static/logo_with_word.png" />
         </p>
         
         ##
         
-        [![package](https://github.com/aristoteleo/dynamo-release/workflows/Python%20package/badge.svg)](https://github.com/aristoteleo/dynamo-release/runs/950435412) 
-        [![upload](https://github.com/aristoteleo/dynamo-release/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/dynamo-release/) 
+        [![package](https://github.com/aristoteleo/dynamo-release/workflows/Python%20package/badge.svg)](https://github.com/aristoteleo/dynamo-release)
         [![documentation](https://readthedocs.org/projects/dynamo-release/badge/?version=latest)](https://dynamo-release.readthedocs.io/en/latest/)
+        [![upload](https://img.shields.io/pypi/v/dynamo-release?logo=PyPI)](https://pypi.org/project/dynamo-release/) 
+        [![download](https://static.pepy.tech/badge/dynamo-release)](https://pepy.tech/project/dynamo-release)
+        [![star](https://img.shields.io/github/stars/aristoteleo/dynamo-release?logo=GitHub&color=red)](https://github.com/aristoteleo/dynamo-release/stargazers)
         ![build](https://github.com/aristoteleo/dynamo-release/actions/workflows/python-package.yml/badge.svg)
         ![test](https://github.com/aristoteleo/dynamo-release/actions/workflows/python-plain-run-test.yml/badge.svg)
         
-        
         ## **Dynamo**: Mapping Transcriptomic Vector Fields of Single Cells
         
         Inclusive model of expression dynamics with metabolic labeling based scRNA-seq / multiomics, vector field reconstruction, potential landscape mapping, differential geometry analyses, and most probably paths / *in silico* perturbation predictions.
         
-        [Installation](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html#how-to-install) - [Ten minutes to dynamo](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html) - [Tutorials](https://dynamo-release.readthedocs.io/en/latest/notebooks/Differential_geometry.html) - [API](https://dynamo-release.readthedocs.io/en/latest/API.html) - [Citation](https://www.biorxiv.org/content/10.1101/696724v2) - [Theory](https://dynamo-release.readthedocs.io/en/latest/notebooks/Primer.html)
+        [Installation](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html#how-to-install) - [Ten minutes to dynamo](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html) - [Tutorials](https://dynamo-release.readthedocs.io/en/latest/notebooks/Differential_geometry.html) - [API](https://dynamo-release.readthedocs.io/en/latest/API.html) - [Citation](https://www.sciencedirect.com/science/article/pii/S0092867421015774?via%3Dihub) - [Theory](https://dynamo-release.readthedocs.io/en/latest/notebooks/Primer.html)
         
         ![Dynamo](https://user-images.githubusercontent.com/7456281/152110270-7ee1b0ed-1205-495d-9d65-59c7984d2fa2.png)
         
         Single-cell (sc)RNA-seq, together with RNA velocity and metabolic labeling, reveals cellular states and transitions at unprecedented resolution. Fully exploiting these data, however, requires kinetic models capable of unveiling governing regulatory functions. Here, we introduce an analytical framework dynamo, which infers absolute RNA velocity, reconstructs continuous vector fields that predict cell fates, employs differential geometry to extract underlying regulations, and ultimately predicts optimal reprogramming paths and perturbation outcomes. We highlight dynamo’s power to overcome fundamental limitations of conventional splicing-based RNA velocity analyses to enable accurate velocity estimations on a metabolically labeled human hematopoiesis scRNA-seq dataset. Furthermore, differential geometry analyses reveal mechanisms driving early megakaryocyte appearance and elucidate asymmetrical regulation within the PU.1-GATA1 circuit. Leveraging the least-action-path method, dynamo accurately predicts drivers of numerous hematopoietic transitions. Finally, in silico perturbations predict cell-fate diversions induced by gene perturbations. Dynamo, thus, represents an important step in advancing quantitative and predictive theories of cell-state transitions.
         
         ## Highlights of dynamo
+        
         * Robust and accurate estimation of RNA velocities for regular scRNA-seq datasets:
             * Three methods for the velocity estimations (including the new negative binomial distribution based approach)
             * Improved kernels for transition matrix calculation and velocity projection 
             * Strategies to correct RNA velocity vectors (when your RNA velocity direction is problematic) 
         * Inclusive modeling of time-resolved metabolic labeling based scRNA-seq:
             * Overcome intrinsic limitation of the conventional splicing based RNA velocity analyses
             * Explicitly model RNA metabolic labeling, in conjunction with RNA bursting, transcription, splicing and degradation
@@ -44,27 +46,31 @@
             * Calculate RNA acceleration (reveals early drivers), curvature (reveals master regulators of fate decision points), divergence (stability of cell states) and RNA Jacobian (cell-state dependent regulatory networks) 
             * Various downstream differential geometry analyses to rank critical regulators/effectors,  and visualize regulatory networks at key fate decision points    
         * Non-trivial vector field predictions of cell fate transitions:
             * Least action path approach to predict the optimal paths and transcription factors of cell fate reprogrammings
             * In silico perturbation to predict the gene-wise perturbation effects and cell fate diversion after genetic perturbations
         
         ## News
-        * 2/1/2022: after 3.5+ years of perseverance, our dynamo paper is finally online in [Cell](https://www.sciencedirect.com/science/article/pii/S0092867421015774#tbl1) today!
-        * 2/15/2022: primers and tutorials on least action paths and in silico perturbation are released.
-        * 3/14/2022: Since today dynamo has its own logo! Here the arrow represents the RNA velocity vector field, while the helix the RNA molecule and the colored dots RNA metabolic labels (4sU labeling). See [readthedocs](https://dynamo-release.readthedocs.io/en/latest/index.html)
-        * 4/14/2022: dynamo 1.1.0 released
+        * 5/30/2023: dynamo 1.3.0 released!
+        * 3/1/2023: We welcome @Sichao25 to join the dynamo develop team!
+        * 1/28/2023: We welcome @Ukyeon to join the dynamo develop team! 
+        * 15/12/2022: *Thanks for @elfofmaxwell and @MukundhMurthy's contribution*. dynamo 1.2.0 released
         * 11/11/2022: the continuing development of dynamo and the Aristotle ecosystem will be supported by CZI. See [here](https://chanzuckerberg.com/eoss/proposals/predictive-modeling-of-single-cell-multiomics-over-time-and-space/)
-        * 15/12/2022: dynamo 1.2.0 released
+        * 4/14/2022: dynamo 1.1.0 released!
+        * 3/14/2022: Since today dynamo has its own logo! Here the arrow represents the RNA velocity vector field, while the helix the RNA molecule and the colored dots RNA metabolic labels (4sU labeling). See [readthedocs](https://dynamo-release.readthedocs.io/en/latest/index.html)
+        * 2/15/2022: primers and tutorials on least action paths and in silico perturbation are released.
+        * 2/1/2022: after 3.5+ years of perseverance, our dynamo paper is finally online in [Cell](https://www.sciencedirect.com/science/article/pii/S0092867421015774#tbl1) today!
         
         ## Discussion 
         Please use github issue tracker to report coding related [issues](https://github.com/aristoteleo/dynamo-release/issues) of dynamo. For community discussion of novel usage cases, analysis tips and biological interpretations of dynamo, please join our public slack workspace: [dynamo-discussion](https://join.slack.com/t/dynamo-discussionhq/shared_invite/zt-itnzjdxs-PV~C3Hr9uOArHZcmv622Kg) (Only a working email address is required from the slack side). 
         
         ## Contribution 
         If you want to contribute to the development of dynamo, please check out CONTRIBUTION instruction: [Contribution](https://github.com/aristoteleo/dynamo-release/blob/master/CONTRIBUTING.md)
         
 Keywords: VectorField,singlecell,velocity,scNT-seq,sci-fate,NASC-seq,scSLAMseq,potential
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
+Provides-Extra: docs
```

### Comparing `dynamo-release-1.2.0/README.md` & `dynamo-release-1.3.0/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 <p align="center">
   <img height="150" src="https://dynamo-release.readthedocs.io/en/latest/_static/logo_with_word.png" />
 </p>
 
 ##
 
-[![package](https://github.com/aristoteleo/dynamo-release/workflows/Python%20package/badge.svg)](https://github.com/aristoteleo/dynamo-release/runs/950435412) 
-[![upload](https://github.com/aristoteleo/dynamo-release/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/dynamo-release/) 
+[![package](https://github.com/aristoteleo/dynamo-release/workflows/Python%20package/badge.svg)](https://github.com/aristoteleo/dynamo-release)
 [![documentation](https://readthedocs.org/projects/dynamo-release/badge/?version=latest)](https://dynamo-release.readthedocs.io/en/latest/)
+[![upload](https://img.shields.io/pypi/v/dynamo-release?logo=PyPI)](https://pypi.org/project/dynamo-release/) 
+[![download](https://static.pepy.tech/badge/dynamo-release)](https://pepy.tech/project/dynamo-release)
+[![star](https://img.shields.io/github/stars/aristoteleo/dynamo-release?logo=GitHub&color=red)](https://github.com/aristoteleo/dynamo-release/stargazers)
 ![build](https://github.com/aristoteleo/dynamo-release/actions/workflows/python-package.yml/badge.svg)
 ![test](https://github.com/aristoteleo/dynamo-release/actions/workflows/python-plain-run-test.yml/badge.svg)
 
-
 ## **Dynamo**: Mapping Transcriptomic Vector Fields of Single Cells
 
 Inclusive model of expression dynamics with metabolic labeling based scRNA-seq / multiomics, vector field reconstruction, potential landscape mapping, differential geometry analyses, and most probably paths / *in silico* perturbation predictions.
 
-[Installation](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html#how-to-install) - [Ten minutes to dynamo](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html) - [Tutorials](https://dynamo-release.readthedocs.io/en/latest/notebooks/Differential_geometry.html) - [API](https://dynamo-release.readthedocs.io/en/latest/API.html) - [Citation](https://www.biorxiv.org/content/10.1101/696724v2) - [Theory](https://dynamo-release.readthedocs.io/en/latest/notebooks/Primer.html)
+[Installation](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html#how-to-install) - [Ten minutes to dynamo](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html) - [Tutorials](https://dynamo-release.readthedocs.io/en/latest/notebooks/Differential_geometry.html) - [API](https://dynamo-release.readthedocs.io/en/latest/API.html) - [Citation](https://www.sciencedirect.com/science/article/pii/S0092867421015774?via%3Dihub) - [Theory](https://dynamo-release.readthedocs.io/en/latest/notebooks/Primer.html)
 
 ![Dynamo](https://user-images.githubusercontent.com/7456281/152110270-7ee1b0ed-1205-495d-9d65-59c7984d2fa2.png)
 
 Single-cell (sc)RNA-seq, together with RNA velocity and metabolic labeling, reveals cellular states and transitions at unprecedented resolution. Fully exploiting these data, however, requires kinetic models capable of unveiling governing regulatory functions. Here, we introduce an analytical framework dynamo, which infers absolute RNA velocity, reconstructs continuous vector fields that predict cell fates, employs differential geometry to extract underlying regulations, and ultimately predicts optimal reprogramming paths and perturbation outcomes. We highlight dynamo’s power to overcome fundamental limitations of conventional splicing-based RNA velocity analyses to enable accurate velocity estimations on a metabolically labeled human hematopoiesis scRNA-seq dataset. Furthermore, differential geometry analyses reveal mechanisms driving early megakaryocyte appearance and elucidate asymmetrical regulation within the PU.1-GATA1 circuit. Leveraging the least-action-path method, dynamo accurately predicts drivers of numerous hematopoietic transitions. Finally, in silico perturbations predict cell-fate diversions induced by gene perturbations. Dynamo, thus, represents an important step in advancing quantitative and predictive theories of cell-state transitions.
 
 ## Highlights of dynamo
+
 * Robust and accurate estimation of RNA velocities for regular scRNA-seq datasets:
     * Three methods for the velocity estimations (including the new negative binomial distribution based approach)
     * Improved kernels for transition matrix calculation and velocity projection 
     * Strategies to correct RNA velocity vectors (when your RNA velocity direction is problematic) 
 * Inclusive modeling of time-resolved metabolic labeling based scRNA-seq:
     * Overcome intrinsic limitation of the conventional splicing based RNA velocity analyses
     * Explicitly model RNA metabolic labeling, in conjunction with RNA bursting, transcription, splicing and degradation
@@ -35,19 +37,22 @@
     * Calculate RNA acceleration (reveals early drivers), curvature (reveals master regulators of fate decision points), divergence (stability of cell states) and RNA Jacobian (cell-state dependent regulatory networks) 
     * Various downstream differential geometry analyses to rank critical regulators/effectors,  and visualize regulatory networks at key fate decision points    
 * Non-trivial vector field predictions of cell fate transitions:
     * Least action path approach to predict the optimal paths and transcription factors of cell fate reprogrammings
     * In silico perturbation to predict the gene-wise perturbation effects and cell fate diversion after genetic perturbations
 
 ## News
-* 2/1/2022: after 3.5+ years of perseverance, our dynamo paper is finally online in [Cell](https://www.sciencedirect.com/science/article/pii/S0092867421015774#tbl1) today!
-* 2/15/2022: primers and tutorials on least action paths and in silico perturbation are released.
-* 3/14/2022: Since today dynamo has its own logo! Here the arrow represents the RNA velocity vector field, while the helix the RNA molecule and the colored dots RNA metabolic labels (4sU labeling). See [readthedocs](https://dynamo-release.readthedocs.io/en/latest/index.html)
-* 4/14/2022: dynamo 1.1.0 released
+* 5/30/2023: dynamo 1.3.0 released!
+* 3/1/2023: We welcome @Sichao25 to join the dynamo develop team!
+* 1/28/2023: We welcome @Ukyeon to join the dynamo develop team! 
+* 15/12/2022: *Thanks for @elfofmaxwell and @MukundhMurthy's contribution*. dynamo 1.2.0 released
 * 11/11/2022: the continuing development of dynamo and the Aristotle ecosystem will be supported by CZI. See [here](https://chanzuckerberg.com/eoss/proposals/predictive-modeling-of-single-cell-multiomics-over-time-and-space/)
-* 15/12/2022: dynamo 1.2.0 released
+* 4/14/2022: dynamo 1.1.0 released!
+* 3/14/2022: Since today dynamo has its own logo! Here the arrow represents the RNA velocity vector field, while the helix the RNA molecule and the colored dots RNA metabolic labels (4sU labeling). See [readthedocs](https://dynamo-release.readthedocs.io/en/latest/index.html)
+* 2/15/2022: primers and tutorials on least action paths and in silico perturbation are released.
+* 2/1/2022: after 3.5+ years of perseverance, our dynamo paper is finally online in [Cell](https://www.sciencedirect.com/science/article/pii/S0092867421015774#tbl1) today!
 
 ## Discussion 
 Please use github issue tracker to report coding related [issues](https://github.com/aristoteleo/dynamo-release/issues) of dynamo. For community discussion of novel usage cases, analysis tips and biological interpretations of dynamo, please join our public slack workspace: [dynamo-discussion](https://join.slack.com/t/dynamo-discussionhq/shared_invite/zt-itnzjdxs-PV~C3Hr9uOArHZcmv622Kg) (Only a working email address is required from the slack side). 
 
 ## Contribution 
 If you want to contribute to the development of dynamo, please check out CONTRIBUTION instruction: [Contribution](https://github.com/aristoteleo/dynamo-release/blob/master/CONTRIBUTING.md)
```

### Comparing `dynamo-release-1.2.0/dynamo/__init__.py` & `dynamo-release-1.3.0/dynamo/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -29,11 +29,11 @@
     LoggerManager,
     main_critical,
     main_exception,
     main_info,
     main_tqdm,
     main_warning,
 )
-from .get_version import get_all_dependencies_version
+from .get_version import get_all_dependencies_version, session_info
 
 # alias
 config = configuration
```

### Comparing `dynamo-release-1.2.0/dynamo/configuration.py` & `dynamo-release-1.3.0/dynamo/configuration.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 import warnings
+from typing import List, Optional, Tuple, Union
 
 import colorcet
 import matplotlib
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 from anndata._core.anndata import AnnData
 from cycler import cycler
 from matplotlib import cm, colors, rcParams
 
-from .dynamo_logger import main_info, main_warning
+from .dynamo_logger import main_debug, main_info
 
 
 class DynamoAdataKeyManager:
     VAR_GENE_MEAN_KEY = "pp_gene_mean"
     VAR_GENE_VAR_KEY = "pp_gene_variance"
     VAR_GENE_HIGHLY_VARIABLE_KEY = "gene_highly_variable"
     VAR_GENE_HIGHLY_VARIABLE_SCORES = "gene_highly_variable_scores"
@@ -31,14 +32,15 @@
     # obsp adjacency matrix string constants
     OBSP_ADJ_MAT_DIST = "distances"
     OBSP_ADJ_MAT_CONNECTIVITY = "connectivities"
 
     # special key names frequently used in dynamo
     X_LAYER = "X"
     PROTEIN_LAYER = "protein"
+    X_PCA = "X_pca"
 
     def gen_new_layer_key(layer_name, key, sep="_") -> str:
         """utility function for returning a new key name for a specific layer. By convention layer_name should not have the separator as the last character."""
         if layer_name == "":
             return key
         if layer_name[-1] == sep:
             return layer_name + key
@@ -58,24 +60,27 @@
     def gen_layer_pearson_residual_key(layer: str):
         """Generate dynamo style keys for adata.uns[pp][key0_key1_key2...]"""
         return DynamoAdataKeyManager.gen_layer_pp_key(
             layer, DynamoAdataKeyManager.UNS_PP_PEARSON_RESIDUAL_NORMALIZATION
         )
 
     def select_layer_data(adata: AnnData, layer: str, copy=False) -> pd.DataFrame:
-        """select layer data based on layer key. The default layer is X layer in adata.
-        For layer-like data such as X stored in adata.X (but not in adata.layers) and protein data specified by dynamo convention,
-        this utility provides an unified interface for selecting layer data with shape n_obs x n_var."""
+        """This utility provides a unified interface for selecting layer data.
+
+        The default layer is X layer in adata with shape n_obs x n_var. For protein data it selects adata.obsm["protein"]
+        as specified by dynamo convention (the number of proteins are generally less than detected genes `n_var`).
+        For other layer data, select data based on layer key with shape n_obs x n_var.
+        """
         if layer is None:
             layer = DynamoAdataKeyManager.X_LAYER
         res_data = None
         if layer == DynamoAdataKeyManager.X_LAYER:
             res_data = adata.X
         elif layer == DynamoAdataKeyManager.PROTEIN_LAYER:
-            res_data = adata.obsm["protein"]
+            res_data = adata.obsm["protein"] if "protein" in adata.obsm_keys() else None
         else:
             res_data = adata.layers[layer]
         if copy:
             return res_data.copy()
         return res_data
 
     def set_layer_data(adata: AnnData, layer: str, vals: np.array, var_indices: np.array = None):
@@ -98,14 +103,16 @@
             return DynamoAdataKeyManager.PROTEIN_LAYER in adata.obsm
 
         return layer in adata.layers
 
     def get_available_layer_keys(adata, layers="all", remove_pp_layers=True, include_protein=True):
         """Get the list of available layers' keys. If `layers` is set to all, return a list of all available layers; if `layers` is set to a list, then the intersetion of available layers and `layers` will be returned."""
         layer_keys = list(adata.layers.keys())
+        if layers is None:  # layers=adata.uns["pp"]["experiment_layers"], in calc_sz_factor
+            layers = "X"
         if remove_pp_layers:
             layer_keys = [i for i in layer_keys if not i.startswith("X_")]
 
         if "protein" in adata.obsm.keys() and include_protein:
             layer_keys.extend(["X", "protein"])
         else:
             layer_keys.extend(["X"])
@@ -133,14 +140,67 @@
         splicing_and_labeling = ["X_uu", "X_ul", "X_su", "X_sl"]
 
         return only_splicing, only_labeling, splicing_and_labeling
 
     def init_uns_pp_namespace(adata: AnnData):
         adata.uns[DynamoAdataKeyManager.UNS_PP_KEY] = {}
 
+    def get_excluded_layers(X_total_layers: bool = False, splicing_total_layers: bool = False) -> List:
+        """Get a list of excluded layers based on the provided arguments.
+
+        When splicing_total_layers is False, the function normalize spliced and unspliced RNA separately using each
+        layer's size factors. When X_total_layers is False, the function normalize X (normally it corresponds to the
+        spliced RNA or total RNA for a conventional scRNA-seq or labeling scRNA-seq) using its own size factor.
+
+        Args:
+            X_total_layers: whether to also normalize adata.X by size factor from total RNA.
+            splicing_total_layers: whether to also normalize spliced / unspliced layers by size factor from total RNA.
+
+        Returns:
+            The list of layers to be excluded.
+        """
+        excluded_layers = []
+        if not X_total_layers:
+            excluded_layers.extend(["X"])
+        if not splicing_total_layers:
+            excluded_layers.extend(["spliced", "unspliced"])
+        return excluded_layers
+
+    def aggregate_layers_into_total(
+        _adata: AnnData,
+        layers: Union[str, List[str]] = "all",
+        total_layers: Optional[List[str]] = None,
+        extend_layers: bool = True,
+    ) -> Tuple[Optional[List[str]], Union[str, List[str]]]:
+        """Create a total layer in adata by aggregating multiple layers.
+
+        The size factor normalization function is able to calculate size factors from customized layers. Given list
+        of total_layers, this helper function will calculate a temporary `_total_` layer.
+
+        Args:
+            _adata: the Anndata object.
+            layers: the layer(s) to be normailized in the normailzation function.
+            total_layers: the layer(s) to sum up to get the total mRNA. For example, ["spliced", "unspliced"],
+                ["uu", "ul", "su", "sl"] or ["new", "old"], etc.
+            extend_layers: whether to extend the `_total_` layer to the list of layers.
+
+        Returns:
+            The tuple contains total layers and layers. Anndata object will be updated with `_total_` layer.
+        """
+        if not isinstance(total_layers, list):
+            total_layers = [total_layers]
+        if len(set(total_layers).difference(_adata.layers.keys())) == 0:
+            total = None
+            for t_key in total_layers:
+                total = _adata.layers[t_key] if total is None else total + _adata.layers[t_key]
+            _adata.layers["_total_"] = total
+            if extend_layers:
+                layers.extend(["_total_"])
+        return total_layers, layers
+
 
 # TODO discuss alias naming convention
 DKM = DynamoAdataKeyManager
 
 
 class DynamoVisConfig:
     def set_default_mode(background="white"):
@@ -782,9 +842,9 @@
         "axes.labelpad": 1,
     }
     matplotlib.rcParams.update(params)
 
 
 # initialize DynamoSaveConfig and DynamoVisConfig mode defaults
 DynamoAdataConfig.update_data_store_mode("full")
-main_info("setting visualization default mode in dynamo. Your customized matplotlib settings might be overritten.")
+main_debug("setting visualization default mode in dynamo. Your customized matplotlib settings might be overwritten.")
 DynamoVisConfig.set_default_mode()
```

### Comparing `dynamo-release-1.2.0/dynamo/data_io.py` & `dynamo-release-1.3.0/dynamo/data_io.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/docrep.py` & `dynamo-release-1.3.0/dynamo/docrep.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/dynamo_logger.py` & `dynamo-release-1.3.0/dynamo/dynamo_logger.py`

 * *Files 3% similar despite different names*

```diff
@@ -148,27 +148,43 @@
         message = format_logging_message(message, logging.CRITICAL, indent_level=indent_level)
         return self.logger.critical(message, *args, **kwargs)
 
     def error(self, message, indent_level=1, *args, **kwargs):
         message = format_logging_message(message, logging.ERROR, indent_level=indent_level)
         return self.logger.error(message, *args, **kwargs)
 
-    def info_insert_adata(self, key, adata_attr="obsm", indent_level=1, *args, **kwargs):
+    def info_insert_adata(self, key, adata_attr="obsm", log_level=logging.NOTSET, indent_level=1, *args, **kwargs):
         message = "<insert> %s to %s in AnnData Object." % (key, adata_attr)
-        message = format_logging_message(message, logging.INFO, indent_level=indent_level)
-        return self.logger.error(message, *args, **kwargs)
+        if log_level == logging.NOTSET or log_level == logging.DEBUG:
+            self.debug(message, indent_level=indent_level, *args, **kwargs)
+        elif log_level == logging.INFO:
+            self.info(message, indent_level=indent_level, *args, **kwargs)
+        elif log_level == logging.WARN:
+            self.warning(message, indent_level=indent_level, *args, **kwargs)
+        elif log_level == logging.ERROR:
+            self.error(message, indent_level=indent_level, *args, **kwargs)
+        elif log_level == logging.CRITICAL:
+            self.critical(message, indent_level=indent_level, *args, **kwargs)
+        else:
+            raise NotImplementedError
 
-    def info_insert_adata_var(self, key, indent_level=1, *args, **kwargs):
-        return self.info_insert_adata(self, key, adata_attr="var", indent_level=1, *args, **kwargs)
+    def info_insert_adata_var(self, key, log_level, indent_level, *args, **kwargs):
+        return self.info_insert_adata(
+            self, key, adata_attr="var", log_level=log_level, indent_level=indent_level, *args, **kwargs
+        )
 
-    def info_insert_adata_obsm(self, key, indent_level=1, *args, **kwargs):
-        return self.info_insert_adata(self, key, adata_attr="obsm", indent_level=1, *args, **kwargs)
+    def info_insert_adata_obsm(self, key, log_level, indent_level, *args, **kwargs):
+        return self.info_insert_adata(
+            self, key, adata_attr="obsm", log_level=log_level, indent_level=indent_level, *args, **kwargs
+        )
 
-    def info_insert_adata_uns(self, key, indent_level=1, *args, **kwargs):
-        return self.info_insert_adata(self, key, adata_attr="uns", indent_level=1, *args, **kwargs)
+    def info_insert_adata_uns(self, key, log_level, indent_level, *args, **kwargs):
+        return self.info_insert_adata(
+            self, key, adata_attr="uns", log_level=log_level, indent_level=indent_level, *args, **kwargs
+        )
 
     def log_time(self):
         now = time.time()
         self.time_passed = now - self.previous_timestamp
         self.previous_timestamp = now
         return self.time_passed
 
@@ -185,26 +201,25 @@
         )
         self.logger.info(message)
         self.logger_stream_handler.flush()
         self.logger_stream_handler.terminator = saved_terminator
 
     def finish_progress(self, progress_name="", time_unit="s", indent_level=1):
         self.log_time()
-        self.report_progress(percent=100, progress_name=progress_name)
+        # self.report_progress(percent=100, progress_name=progress_name)
 
         saved_terminator = self.logger_stream_handler.terminator
         self.logger_stream_handler.terminator = ""
-        self.logger.info("\n")
         self.logger_stream_handler.flush()
         self.logger_stream_handler.terminator = saved_terminator
 
         if time_unit == "s":
-            self.info("[%s] finished [%.4fs]" % (progress_name, self.time_passed), indent_level=indent_level)
+            self.info("[%s] completed [%.4fs]" % (progress_name, self.time_passed), indent_level=indent_level)
         elif time_unit == "ms":
-            self.info("[%s] finished [%.4fms]" % (progress_name, self.time_passed * 1e3), indent_level=indent_level)
+            self.info("[%s] completed [%.4fms]" % (progress_name, self.time_passed * 1e3), indent_level=indent_level)
         else:
             raise NotImplementedError
         # self.logger.info("|")
         self.logger_stream_handler.flush()
 
     def request_report_hook(self, bn: int, rs: int, ts: int) -> None:
         """A callback required by the request lib:
```

### Comparing `dynamo-release-1.2.0/dynamo/estimation/csc/utils_velocity.py` & `dynamo-release-1.3.0/dynamo/estimation/csc/utils_velocity.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/estimation/csc/velocity.py` & `dynamo-release-1.3.0/dynamo/estimation/csc/velocity.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/estimation/fit_jacobian.py` & `dynamo-release-1.3.0/dynamo/estimation/fit_jacobian.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/estimation/tsc/__init__.py` & `dynamo-release-1.3.0/dynamo/estimation/tsc/__init__.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/estimation/tsc/estimation_kinetic.py` & `dynamo-release-1.3.0/dynamo/estimation/tsc/estimation_kinetic.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/estimation/tsc/twostep.py` & `dynamo-release-1.3.0/dynamo/estimation/tsc/twostep.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/estimation/tsc/utils_kinetic.py` & `dynamo-release-1.3.0/dynamo/estimation/tsc/utils_kinetic.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/estimation/tsc/utils_moments.py` & `dynamo-release-1.3.0/dynamo/estimation/tsc/utils_moments.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/external/gseapy.py` & `dynamo-release-1.3.0/dynamo/external/gseapy.py`

 * *Files 6% similar despite different names*

```diff
@@ -66,15 +66,14 @@
     Path(outdir).mkdir(parents=True, exist_ok=True)
 
     enr = gp.enrichr(
         gene_list=genes,
         gene_sets=gene_sets,  # GO_Biological_Process_2018
         organism=organism,  # don't forget to set organism to the one you desired! e.g. Yeast
         background=background,
-        description=description,
         outdir=outdir,
         no_plot=no_plot,
         cutoff=cutoff,  # test dataset, use lower value from range(0,1)
         **kwargs,
     )
 
     return enr
```

### Comparing `dynamo-release-1.2.0/dynamo/external/hodge.py` & `dynamo-release-1.3.0/dynamo/external/hodge.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/external/pearson_residual_recipe.py` & `dynamo-release-1.3.0/dynamo/preprocessing/external/pearson_residual_recipe.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,65 +12,78 @@
 
 import numpy as np
 import pandas as pd
 import scipy.sparse as sp_sparse
 from anndata import AnnData
 from scipy.sparse import issparse
 
-from ..configuration import DKM
-from ..dynamo_logger import (
+from ...configuration import DKM
+from ...dynamo_logger import (
     LoggerManager,
     main_info,
     main_info_insert_adata_layer,
     main_warning,
 )
-from ..preprocessing.preprocessor_utils import (
-    filter_genes_by_outliers,
-    is_nonnegative_integer_arr,
-    seurat_get_mean_var,
-)
-from ..preprocessing.utils import pca_monocle
+from ...preprocessing.utils import is_nonnegative_integer_arr, seurat_get_mean_var
+from ..QC import filter_genes_by_outliers
 
 main_logger = LoggerManager.main_logger
 
-
+# TODO: Use compute_pearson_residuals function to calculate residuals
 def _highly_variable_pearson_residuals(
     adata: AnnData,
     theta: float = 100,
     clip: Optional[float] = None,
     n_top_genes: int = 1000,
     batch_key: Optional[str] = None,
     chunksize: int = 1000,
     check_values: bool = True,
     layer: Optional[str] = None,
     subset: bool = False,
     inplace: bool = True,
 ) -> Optional[pd.DataFrame]:
-    """\
-    Compute highly variable genes based on pearson residuals.
+    """Compute and annotate highly variable genes based on analytic Pearson residuals [Lause21]_.
 
-    Returns
-    -------
-    Depending on `inplace` returns calculated metrics (:class:`~pd.DataFrame`)
-    or updates `.var` with the following fields:
-
-    highly_variable : bool
-        boolean indicator of highly-variable genes
-    means : float
-        means per gene
-    variances : float
-        variance per gene
-    residual_variances : float
-        Residual variance per gene. Averaged in the case of multiple batches.
-    highly_variable_rank : float
-        Rank of the gene according to residual variance, median rank in the case of multiple batches
-    highly_variable_nbatches : int
-        If `batch_key` given, denotes in how many batches genes are detected as HVG
-    highly_variable_intersection : bool
-        If `batch_key` given, denotes the genes that are highly variable in all batches
+    For [Lause21]_, Pearson residuals of a negative binomial offset model (with overdispersion theta shared across
+    genes) are computed. By default, overdispersion theta=100 is used and residuals are clipped to sqrt(n). Finally,
+    genes are ranked by residual variance. Expects raw count input.
+
+    Args:
+        adata: an annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes.
+        theta: the negative binomial overdispersion parameter theta for Pearson residuals. Higher values correspond to
+            less overdispersion (var = mean + mean^2/theta), and `theta=np.Inf` corresponds to a Poisson model.
+        clip: the threshold to determine if and how residuals are clipped. If `None`, residuals are clipped to the
+            interval [-sqrt(n), sqrt(n)] where n is the number of cells in the dataset (default behavior). If any
+            scalar c, residuals are clipped to the interval [-c, c]. Set `clip=np.Inf` for no clipping.
+        n_top_genes: the number of highly-variable genes to keep.
+        batch_key: the key to indicate how highly-variable genes are selected within each batch separately and merged
+            later. This simple process avoids the selection of batch-specific genes and acts as a lightweight batch
+            correction method. Genes are first sorted by how many batches they are an HVG. Ties are broken by the median
+            rank (across batches) based on within-batch residual variance.
+        chunksize: the number of genes are processed at once while computing the Pearson residual variance. Choosing a
+            smaller value will reduce the required memory.
+        check_values: whether to check if counts in selected layer are integers. A Warning is returned if set to True.
+        layer: the layer to perform gene selection on.
+        subset: whether to inplace subset to highly-variable genes. If `True` otherwise merely indicate highly variable
+            genes.
+        inplace: whether to place calculated metrics in `.var` or return them.
+
+    Returns:
+        Depending on `inplace` returns calculated metrics (:class:`~pandas.DataFrame`) or updates `.var` with
+        the following fields:
+            highly_variable: boolean indicator of highly-variable genes.
+            means: means per gene.
+            variances: variance per gene.
+            residual_variances: For `recipe='pearson_residuals'`, residual variance per gene. Averaged in the case of
+                multiple batches.
+            highly_variable_rank: For `recipe='pearson_residuals'`, rank of the gene according to residual variance,
+                median rank in the case of multiple batches.
+            highly_variable_nbatches: If `batch_key` given, denotes in how many batches genes are detected as HVG.
+            highly_variable_intersection: If `batch_key` given, denotes the genes that are highly variable in all
+                batches.
     """
 
     # view_to_actual(adata)
     X = DKM.select_layer_data(adata, layer)
     _computed_on_prompt_str = layer if layer else "adata.X"
 
     # Check for raw counts
@@ -216,109 +229,33 @@
             df = df.drop(["highly_variable_nbatches", "highly_variable_intersection"], axis=1)
         if subset:
             df = df.iloc[df.highly_variable.values, :]
 
         return df
 
 
+# TODO: Move this function to a higher level. Now this function is called by
+# pearson_residual_recipe, but this function aims to support different recipe in
+# the future.
 def compute_highly_variable_genes(
     adata: AnnData,
     *,
     theta: float = 100,
     clip: Optional[float] = None,
     n_top_genes: Optional[int] = None,
     batch_key: Optional[str] = None,
     chunksize: int = 1000,
     recipe: str = "pearson_residuals",
     check_values: bool = True,
     layer: Optional[str] = None,
     subset: bool = False,
     inplace: bool = True,
 ) -> Optional[pd.DataFrame]:
-    """\
-    Annotate highly variable genes using analytic Pearson residuals [Lause21]_.
-
-    For [Lause21]_, Pearson residuals of a negative binomial offset model (with
-    overdispersion theta shared across genes) are computed. By default, overdispersion
-    theta=100 is used and residuals are clipped to sqrt(n). Finally, genes are ranked
-    by residual variance.
-
-    Expects raw count input.
-
-
-    Parameters
-    ----------
-    adata
-        The annotated data matrix of shape `n_obs` × `n_vars`.
-        Rows correspond to cells and columns to genes.
-    theta
-        The negative binomial overdispersion parameter theta for Pearson residuals.
-        Higher values correspond to less overdispersion (var = mean + mean^2/theta),
-        and `theta=np.Inf` corresponds to a Poisson model.
-    clip
-        If `flavor='pearson_residuals'`, determines if and how residuals are clipped:
-
-            * If `None`, residuals are clipped to the interval [-sqrt(n), sqrt(n)], \
-            where n is the number of cells in the dataset (default behavior).
-            * If any scalar c, residuals are clipped to the interval [-c, c]. Set \
-            `clip=np.Inf` for no clipping.
-
-    n_top_genes
-        Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or
-        `flavor='pearson_residuals'`.
-    batch_key
-        If specified, highly-variable genes are selected within each batch separately
-        and merged. This simple process avoids the selection of batch-specific genes
-        and acts as a lightweight batch correction method. Genes are first sorted by
-        how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are
-        broken by the median rank (across batches) based on within-batch residual
-        variance.
-    chunksize
-        If `flavor='pearson_residuals'`, this dertermines how many genes are processed at
-        once while computing the residual variance. Choosing a smaller value will reduce
-        the required memory.
-    flavor
-        Choose the flavor for identifying highly variable genes. In this experimental
-        version, only 'pearson_residuals' is functional.
-    check_values
-        Check if counts in selected layer are integers. A Warning is returned if set to
-        True. Only used if `flavor='pearson_residuals'`.
-    layer
-        If provided, use `adata.layers[layer]` for expression values instead of `adata.X`.
-    subset
-        Inplace subset to highly-variable genes if `True` otherwise merely indicate
-        highly variable genes.
-    inplace
-        Whether to place calculated metrics in `.var` or return them.
-
-    Returns
-    -------
-    Depending on `inplace` returns calculated metrics (:class:`~pandas.DataFrame`) or
-    updates `.var` with the following fields
-
-    highly_variable : bool
-        boolean indicator of highly-variable genes
-    means : float
-        means per gene
-    variances : float
-        variance per gene
-    residual_variances : float
-        For `flavor='pearson_residuals'`, residual variance per gene. Averaged in the
-        case of multiple batches.
-    highly_variable_rank : float
-        For `flavor='pearson_residuals'`, rank of the gene according to residual
-        variance, median rank in the case of multiple batches
-    highly_variable_nbatches : int
-        If `batch_key` given, denotes in how many batches genes are detected as HVG
-    highly_variable_intersection : bool
-        If `batch_key` given, denotes the genes that are highly variable in all batches
-
-    Notes
-    -----
-    Experimental version of `sc.pp.highly_variable_genes()`
+    """A wrapper calls corresponding recipe to identify highly variable genes. Currently only 'pearson_residuals'
+    is supported.
     """
 
     main_logger.info("extracting highly variable genes")
 
     if not isinstance(adata, AnnData):
         raise ValueError(
             "`pp.highly_variable_genes` expects an `AnnData` argument, "
@@ -340,16 +277,38 @@
             chunksize=chunksize,
             subset=subset,
             check_values=check_values,
             inplace=inplace,
         )
 
 
-def compute_pearson_residuals(X, theta, clip, check_values, copy=False):
+def compute_pearson_residuals(
+    X: np.ndarray,
+    theta: float = 100,
+    clip: Optional[float] = None,
+    check_values: bool = True,
+    copy: bool = False,
+) -> np.ndarray:
+    """Compute Pearson residuals from count data.
+
+    Pearson residuals are a measure of the deviation of observed counts from expected counts under a Poisson or negative
+    binomial model.
+
+    Args:
+        X: array_like count matrix, shape (n_cells, n_genes).
+        theta: the dispersion parameter for the negative binomial model. Must be positive.
+        clip: The maximum absolute value of the residuals. Residuals with absolute value larger than `clip` are clipped
+            to `clip`. If `None`,`clip` is set to the square root of the number of cells in `X`.
+        check_values: whether to check if `X` contains non-negative integers. If `True` and non-integer values are
+            found, a `UserWarning` is issued.
+        copy: whether to make a copy of `X`.
 
+    Returns:
+        The Pearson residuals.
+    """
     X = X.copy() if copy else X
 
     # check theta
     if theta <= 0:
         # TODO: would "underdispersion" with negative theta make sense?
         # then only theta=0 were undefined..
         raise ValueError("Pearson residuals require theta > 0")
@@ -381,105 +340,109 @@
 
     # clip
     residuals = np.clip(residuals, a_min=-clip, a_max=clip)
 
     return residuals
 
 
+# TODO: Read pearson residuals if they exist instead of calculating them again.
 def _normalize_single_layer_pearson_residuals(
     adata: AnnData,
     *,
     theta: float = 100,
     clip: Optional[float] = None,
     check_values: bool = True,
     layer: Optional[str] = None,
     var_select_genes_key: np.array = None,
     copy: bool = False,
-) -> Optional[Dict[str, np.ndarray]]:
-    """\
-    Applies analytic Pearson residual normalization, based on [Lause21]_.
-    The residuals are based on a negative binomial offset model with overdispersion
-    `theta` shared across genes. By default, residuals are clipped to sqrt(n) and
-    overdispersion `theta=100` is used.
-    Expects raw count input.
-    Params
-    ------
-    adata
-        The annotated data matrix of shape `n_obs` × `n_vars`.
-        Rows correspond to cells and columns to genes.
-    theta
-        The negative binomial overdispersion parameter theta for Pearson residuals.
-        Higher values correspond to less overdispersion (var = mean + mean^2/theta),
-        and `theta=np.Inf` corresponds to a Poisson model.
-    clip
-        Determines if and how residuals are clipped:
-            * If `None`, residuals are clipped to the interval [-sqrt(n), sqrt(n)], \
-            where n is the number of cells in the dataset (default behavior).
-            * If any scalar c, residuals are clipped to the interval [-c, c]. Set \
-            `clip=np.Inf` for no clipping.
-    check_values
-        Check if counts in selected layer are integers. A Warning is returned if set to
-        True.
-    layer
-        Layer to normalize instead of `X`. If `None`, `X` is normalized.
-    copy
-        Whether to modify copied input object. Not compatible with `inplace=False`.
-    Returns
-    -------
-        returns adata if `copy` is True, otherwise `None`.
-    """
+) -> Optional[AnnData]:
+    """Applies analytic Pearson residual normalization, based on [Lause21]_.
 
-    if copy:
-        adata = adata.copy()
-    # view_to_actual(adata)
+    The residuals are based on a negative binomial offset model with overdispersion `theta` shared across genes. By
+    default, residuals are clipped to sqrt(n) and overdispersion `theta=100` is used. Expects raw count input.
+
+    Args:
+        adata: the annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes.
+        theta: the negative binomial overdispersion parameter theta for Pearson residuals. Higher values correspond to
+            less overdispersion (var = mean + mean^2/theta), and `theta=np.Inf` corresponds to a Poisson model.
+        clip: the threshold to determine if and how residuals are clipped. If `None`, residuals are clipped to the
+            interval [-sqrt(n), sqrt(n)] where n is the number of cells in the dataset (default behavior). If any
+            scalar c, residuals are clipped to the interval [-c, c]. Set `clip=np.Inf` for no clipping.
+        check_values: whether to check if counts in selected layer are integers. A Warning is returned if set to True.
+        layer: the Layer to normalize instead of `X`.
+        copy: Whether to modify copied input object.
+
+    Returns:
+        If `copy` is True, a new AnnData object with normalized layers will be returned. If `copy` is False, modifies
+        the given AnnData object in place and returns None.
+    """
 
-    # if select_genes_key:
-    #     main_info("normalize selected genes...")
-    #     adata = adata[:, adata.var[select_genes_key]]
+    msg = "applying Pearson residuals to layer <%s>" % layer
+    main_logger.info(msg)
+    main_logger.log_time()
 
     if layer is None:
         layer = DKM.X_LAYER
+
+    if layer != DKM.X_LAYER:
+        main_warning(
+            f"Pearson residual is only recommended for X layer while you are applying on layer: {layer}, "
+            f"This will overwrite existing pearson residual params and create negative values in layers, "
+            f"which will cause error in the velocities calculation. Please run the pearson residual recipe by default "
+            f"if you plan to perform downstream analysis."
+        )
+        copy = True  # residuals for spliced/unspliced layers will be saved in X_splice/X_unspliced.
+
+    if copy:
+        adata = adata.copy()
+
     pp_pearson_store_key = DKM.gen_layer_pearson_residual_key(layer)
 
     selected_genes_bools = np.ones(adata.n_vars, dtype=bool)
     if var_select_genes_key:
         selected_genes_bools = adata.var[var_select_genes_key]
 
     adata_selected_genes = adata[:, selected_genes_bools]
-
     X = DKM.select_layer_data(adata_selected_genes, layer=layer)
 
-    msg = "applying Pearson residuals to layer <%s>" % (layer)
-    main_logger.info(msg)
-    main_logger.log_time()
-
     residuals = compute_pearson_residuals(X, theta, clip, check_values, copy=copy)
     pearson_residual_params_dict = dict(theta=theta, clip=clip, layer=layer)
 
-    if not copy:
-        main_logger.info("replacing layer <%s> with pearson residual normalized data." % (layer))
-        DKM.set_layer_data(adata, layer, residuals, selected_genes_bools)
-        adata.uns["pp"][pp_pearson_store_key] = pearson_residual_params_dict
-    else:
-        results_dict = dict(X=residuals, **pearson_residual_params_dict)
+    main_logger.info("replacing layer <%s> with pearson residual normalized data." % layer)
+    DKM.set_layer_data(adata, layer, residuals, selected_genes_bools)
+    adata.uns["pp"][pp_pearson_store_key] = pearson_residual_params_dict
 
-    main_logger.finish_progress(progress_name="pearson residual normalization")
+    main_logger.finish_progress(progress_name=f"pearson residual normalization for {layer}")
 
     if copy:
         return adata
 
 
 def normalize_layers_pearson_residuals(
     adata: AnnData,
     layers: list = ["X"],
     select_genes_layer="X",
     select_genes_key="use_for_pca",
     copy=False,
     **normalize_pearson_residual_args,
-):
+) -> None:
+    """Normalize the given layers of the AnnData object using Pearson residuals.
+
+    Args:
+        adata: AnnData object to normalize.
+        layers: the list of layers to normalize.
+        select_genes_layer: the layer to select highly variable genes.
+        select_genes_key: the key to use for selecting highly variable genes.
+        copy: Whether to create a copy of the AnnData object before editing it.
+        **normalize_pearson_residual_args: Additional arguments to pass to the
+            `_normalize_single_layer_pearson_residuals` function.
+
+    Returns:
+        None. Anndata object will be updated.
+    """
     if len(layers) == 0:
         main_warning("layers arg has zero length. return and do nothing in normalize_layers_pearson_residuals.")
     if not select_genes_layer in layers:
         main_warning(
             "select_genes_layer: %s not in layers, using layer: %s instead to select genes instead."
             % (select_genes_layer, layers[0])
         )
@@ -492,80 +455,57 @@
         if select_genes_layer == layer:
             temp_select_genes_key = select_genes_key
 
         temp_adata = _normalize_single_layer_pearson_residuals(
             adata, layer=layer, var_select_genes_key=temp_select_genes_key, copy=copy, **normalize_pearson_residual_args
         )
 
-        # copy is False
-        if temp_adata is None:
-            temp_adata = adata
-
-        if layer == DKM.X_LAYER:
-            # TODO: discuss if we need X set in layers
-            # X layer will only be used for X_pca
-            main_info("skipping set X as layer in adata.layers", indent_level=2)
-            continue
-        new_X_key = DKM.gen_layer_X_key(layer)
-        main_info_insert_adata_layer(new_X_key, indent_level=2)
-        adata.layers[new_X_key] = DKM.select_layer_data(temp_adata, layer)
+        if layer != DKM.X_LAYER:  # update 'X_' layers
+            new_X_key = DKM.gen_layer_X_key(layer)
+            main_info_insert_adata_layer(new_X_key, indent_level=2)
+            adata.layers[new_X_key] = DKM.select_layer_data(temp_adata, layer)
 
 
+# TODO: Combine this function with compute_highly_variable_genes.
 def select_genes_by_pearson_residuals(
     adata: AnnData,
     layer: str = None,
     theta: float = 100,
     clip: Optional[float] = None,
     n_top_genes: int = 2000,
     batch_key: Optional[str] = None,
     chunksize: int = 1000,
     check_values: bool = True,
     inplace: bool = True,
 ) -> Optional[Tuple[pd.DataFrame, pd.DataFrame]]:
-    """\
-    Gene selection and normalization based on [Lause21]_.
-    Applies gene selection based on Pearson residuals. On the resulting subset,
-    Expects raw count input.
-
-    Params
-    ------
-    adata
-        The annotated data matrix of shape `n_obs` × `n_vars`.
-        Rows correspond to cells and columns to genes.
-    theta
-        The negative binomial overdispersion parameter theta for Pearson residuals.
-        Higher values correspond to less overdispersion (var = mean + mean^2/theta),
-        and `theta=np.Inf` corresponds to a Poisson model.
-    clip
-        Determines if and how residuals are clipped:
-            * If `None`, residuals are clipped to the interval [-sqrt(n), sqrt(n)], \
-            where n is the number of cells in the dataset (default behavior).
-            * If any scalar c, residuals are clipped to the interval [-c, c]. Set \
-            `clip=np.Inf` for no clipping.
-    n_top_genes
-        Number of highly-variable genes to keep.
-    batch_key
-        If specified, highly-variable genes are selected within each batch separately
-        and merged. This simple process avoids the selection of batch-specific genes
-        and acts as a lightweight batch correction method. Genes are first sorted by
-        how many batches they are a HVG. Ties are broken by the median rank (across
-        batches) based on within-batch residual variance.
-    chunksize
-        This dertermines how many genes are processed at once while computing
-        the Pearson residual variance. Choosing a smaller value will reduce
-        the required memory.
-    n_pca_components
-        Number of principal components to compute in the PCA step.
-    check_values
-        Check if counts in selected layer are integers. A Warning is returned if set to
-        True.
-    inplace
-        Whether to place results in `adata` or return them.
-    Returns
-    -------
+    """Gene selection and normalization based on [Lause21]_.
+
+    This function applies gene selection based on Pearson residuals. Expects raw count input on the resulting subset.
+
+    Args:
+        adata: an annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes.
+        layer: the layer to perform gene selection on.
+        theta: the negative binomial overdispersion parameter theta for Pearson residuals. Higher values correspond to
+            less overdispersion (var = mean + mean^2/theta), and `theta=np.Inf` corresponds to a Poisson model.
+        clip: the threshold to determine if and how residuals are clipped. If `None`, residuals are clipped to the
+            interval [-sqrt(n), sqrt(n)] where n is the number of cells in the dataset (default behavior). If any
+            scalar c, residuals are clipped to the interval [-c, c]. Set `clip=np.Inf` for no clipping.
+        n_top_genes: the number of highly-variable genes to keep.
+        batch_key: the key to indicate how highly-variable genes are selected within each batch separately and merged
+            later. This simple process avoids the selection of batch-specific genes and acts as a lightweight batch
+            correction method. Genes are first sorted by how many batches they are an HVG. Ties are broken by the median
+            rank (across batches) based on within-batch residual variance.
+        chunksize: the number of genes are processed at once while computing the Pearson residual variance. Choosing a
+            smaller value will reduce the required memory.
+        check_values: whether to check if counts in selected layer are integers. A Warning is returned if set to True.
+        inplace: whether to place results in `adata` or return them.
+
+    Returns:
+        If inplace is 'True', the 'adata' will be updated without return values. Otherwise, the 'adata' object and
+        selected highly-variable genes will be returned.
     """
     if layer is None:
         layer = DKM.X_LAYER
     main_info("gene selection on layer: " + layer)
     if DKM.UNS_PP_KEY not in adata.uns:
         DKM.init_uns_pp_namespace(adata)
 
@@ -585,7 +525,49 @@
     else:
         hvg = compute_highly_variable_genes(adata, **hvg_params, inplace=False)
 
     if inplace:
         return None
     else:
         return adata, hvg
+
+
+def pearson_residuals(
+    adata: AnnData,
+    n_top_genes: Optional[int] = 3000,
+    subset: bool = False,
+    theta: float = 100,
+    clip: Optional[float] = None,
+    check_values: bool = True,
+) -> None:
+    """Preprocess UMI count data with analytic Pearson residuals.
+
+    Pearson residuals transform raw UMI counts into a representation where three aims are achieved:
+        1.Remove the technical variation that comes from differences in total counts between cells.
+        2.Stabilize the mean-variance relationship across genes, i.e. ensure that biological signal from both low and
+            high expression genes can contribute similarly to downstream processing.
+        3.Genes that are homogeneously expressed (like housekeeping genes) have small variance, while genes that are
+            differentially expressed (like marker genes) have high variance.
+
+    Args:
+        adata: An anndata object.
+        n_top_genes: Number of highly-variable genes to keep.
+        subset: Inplace subset to highly-variable genes if `True` otherwise merely indicate highly variable genes.
+        theta: The negative binomial overdispersion parameter theta for Pearson residuals. Higher values correspond to
+            less overdispersion (var = mean + mean^2/theta), and `theta=np.Inf` corresponds to a Poisson model.
+        clip: Determines if and how residuals are clipped:
+                * If `None`, residuals are clipped to the interval [-sqrt(n), sqrt(n)], where n is the number of cells
+                    in the dataset (default behavior).
+                * If any scalar c, residuals are clipped to the interval [-c, c]. Set `clip=np.Inf` for no clipping.
+        check_values: Check if counts in selected layer are integers. A Warning is returned if set to True.
+
+    Returns:
+        Updates adata with the field ``adata.obsm["pearson_residuals"]``, containing pearson_residuals.
+    """
+    if not (n_top_genes is None):
+        compute_highly_variable_genes(
+            adata, n_top_genes=n_top_genes, recipe="pearson_residuals", inplace=True, subset=subset
+        )
+
+    X = adata.X.copy()
+    residuals = compute_pearson_residuals(X, theta=theta, clip=clip, check_values=check_values)
+    adata.obsm["pearson_residuals"] = residuals
```

### Comparing `dynamo-release-1.2.0/dynamo/external/scifate.py` & `dynamo-release-1.3.0/dynamo/external/scifate.py`

 * *Files 0% similar despite different names*

```diff
@@ -165,15 +165,15 @@
     print(f"Cell number after filtering: {adata.n_obs}")
 
     # generate the expression matrix for downstream analysis
     new = adata.layers[nt_layers[0]]
     total = adata.layers[nt_layers[1]]
 
     # recalculate size factor
-    from ..preprocessing import calc_sz_factor_legacy
+    from ..preprocessing.deprecated import calc_sz_factor_legacy
 
     adata = calc_sz_factor_legacy(
         adata,
         method="mean-geometric-mean-total",
         round_exprs=True,
         total_layers=["total"],
     )
```

### Comparing `dynamo-release-1.2.0/dynamo/external/scribe.py` & `dynamo-release-1.3.0/dynamo/external/scribe.py`

 * *Files 0% similar despite different names*

```diff
@@ -147,15 +147,15 @@
     print(f"Cell number after filtering: {adata.n_obs}")
 
     new = adata.layers[nt_layers[0]]
     total = adata.layers[nt_layers[1]]
 
     if normalize:
         # recalculate size factor
-        from ..preprocessing import calc_sz_factor_legacy
+        from ..preprocessing.deprecated import calc_sz_factor_legacy
 
         adata = calc_sz_factor_legacy(
             adata,
             method="mean-geometric-mean-total",
             round_exprs=True,
             total_layers=["total"],
         )
```

### Comparing `dynamo-release-1.2.0/dynamo/external/sctransform.py` & `dynamo-release-1.3.0/dynamo/preprocessing/external/sctransform.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,92 +4,162 @@
 # Created Date: 12/16/2021
 # Description: pearson residuals based method for preprocessing single cell expression data
 # Original Code Repository: https://github.com/atarashansky/SCTransformPy
 # Sctransform Paper: Hafemeister, C., Satija, R. Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression.
 # =================================================================
 
 import os
-from multiprocessing import Manager, Pool
+from typing import Dict, Optional, Union
 
 import numpy as np
 import pandas as pd
 import scipy
 import scipy as sp
 import statsmodels.discrete.discrete_model
 import statsmodels.nonparametric.kernel_regression
 from anndata import AnnData
 from KDEpy import FFTKDE
 from scipy import stats
 
-from ..configuration import DKM
-from ..dynamo_logger import main_info, main_info_insert_adata_layer
+from ...configuration import DKM
+from ...dynamo_logger import main_info, main_info_insert_adata_layer, main_warning
+from ..utils import get_gene_selection_filter
 
 _EPS = np.finfo(float).eps
 
 
-def robust_scale_binned(y, x, breaks):
+def robust_scale_binned(
+    y: np.ndarray,
+    x: np.ndarray,
+    breaks: np.ndarray,
+) -> np.ndarray:
+    """Scale the values of y based on their medians and absolute deviations in each bin defined by the breaks of x.
+
+    The scaling factor for each bin is determined by the median absolute deviation (MAD) of the residuals, which are
+    defined as the differences between the y values and their median in each bin, divided by a constant factor of
+    1.4826. This scaling is robust to the presence of outliers in each bin.
+
+    Args:
+        y: the array of values to be scaled. Must have the same length as x.
+        x: the array of values to define the bins for scaling. Must have the same length as y.
+        breaks: the sequence of break points for binning the x values. Must be sorted in increasing order.
+
+    Returns:
+        An array of the same shape as y, with the scaled values.
+    """
     bins = np.digitize(x, breaks)
     binsu = np.unique(bins)
     res = np.zeros(bins.size)
     for i in range(binsu.size):
         yb = y[bins == binsu[i]]
         res[bins == binsu[i]] = (yb - np.median(yb)) / (1.4826 * np.median(np.abs(yb - np.median(yb))) + _EPS)
 
     return res
 
 
-def is_outlier(y, x, th=10):
+def is_outlier(
+    y: np.ndarray,
+    x: np.ndarray,
+    th: Union[int, float] = 10,
+) -> np.ndarray:
+    """Identify outliers in a dataset using robust scaling and a FFTKDE density estimate.
+
+    Args:
+       y: the array of values to test for outliers.
+       x: an array of values corresponding to the locations of `y`.
+       th: threshold value for outlier detection.
+
+    Returns:
+       Boolean array indicating whether each value in `y` is an outlier (`True`) or not (`False`).
+    """
     z = FFTKDE(kernel="gaussian", bw="ISJ").fit(x)
     z.evaluate()
     bin_width = (max(x) - min(x)) * z.bw / 2
     eps = _EPS * 10
 
     breaks1 = np.arange(min(x), max(x) + bin_width, bin_width)
     breaks2 = np.arange(min(x) - eps - bin_width / 2, max(x) + bin_width, bin_width)
     score1 = robust_scale_binned(y, x, breaks1)
     score2 = robust_scale_binned(y, x, breaks2)
     return np.abs(np.vstack((score1, score2))).min(0) > th
 
 
-def _parallel_init(igenes_bin_regress, iumi_bin, ign, imm, ips):
+def _parallel_init(
+    igenes_bin_regress: np.ndarray,
+    iumi_bin: np.ndarray,
+    ign: np.ndarray,
+    imm: np.ndarray,
+    ips: Dict,
+) -> None:
+    """Initialize global variables used in parallel processing."""
     global genes_bin_regress
     global umi_bin
     global gn
     global mm
     global ps
     genes_bin_regress = igenes_bin_regress
     umi_bin = iumi_bin
     gn = ign
     mm = imm
     ps = ips
 
 
-def _parallel_wrapper(j):
+def _parallel_wrapper(j: int) -> None:
+    """Helper function to fit Poisson regression to UMI counts."""
     name = gn[genes_bin_regress[j]]
     y = umi_bin[:, j].A.flatten()
     pr = statsmodels.discrete.discrete_model.Poisson(y, mm)
     res = pr.fit(disp=False)
     mu = res.predict()
     theta = theta_ml(y, mu)
     ps[name] = np.append(res.params, theta)
 
 
-def gmean(X: scipy.sparse.spmatrix, axis=0, eps=1):
+def gmean(
+    X: scipy.sparse.spmatrix,
+    axis: int = 0,
+    eps: Union[int, float] = 1,
+) -> np.ndarray:
+    """Compute the geometric mean of the non-zero elements in each column (or row) of a sparse matrix.
+
+    Args:
+        X: the sparse matrix of shape (n_samples, n_features) whose columns (or rows) are to be averaged.
+        axis: the axis along which to average the columns (or rows). By default, the function averages over columns
+            (axis=0).
+        eps: A small positive number added to the elements of the sparse matrix before taking the logarithm. This is
+            necessary to avoid taking the logarithm of zero.
+
+    Returns:
+        An array of shape (n_features,) containing the geometric means of the non-zero elements in each column (or row)
+        of the input sparse matrix X.
+    """
     X = X.copy()
     X = X.asfptype()
     assert np.all(X.sum(0) > 0)
     assert np.all(X.data > 0)
     X.data[:] = np.log(X.data + eps)
     res = np.exp(X.mean(axis).A.flatten()) - eps
 
     assert np.all(res > 0)
     return res
 
 
-def theta_ml(y, mu):
+def theta_ml(y: np.ndarray, mu: np.ndarray) -> float:
+    """Compute the maximum likelihood estimator of theta parameter.
+
+    This function uses an iterative algorithm to optimize the log-likelihood of the Poisson distribution with respect to
+    the theta parameter. The algorithm stops when the change in theta estimate is smaller than a threshold value.
+
+    Args:
+        y: the observed count data.
+        mu: the mean of the Poisson distribution.
+
+    Returns:
+        The maximum likelihood estimate of the theta parameter.
+    """
     n = y.size
     weights = np.ones(n)
     limit = 10
     eps = (_EPS) ** 0.25
 
     from scipy.special import polygamma, psi
 
@@ -111,42 +181,58 @@
         t0 += de
     t0 = max(t0, 0)
 
     return t0
 
 
 def sctransform_core(
-    adata,
-    layer=DKM.X_LAYER,
-    min_cells=5,
-    gmean_eps=1,
-    n_genes=2000,
-    n_cells=None,
-    bin_size=500,
-    bw_adjust=3,
-    inplace=True,
-):
-    """
-    A re-implementation of SCTransform from the Satija lab.
+    adata: AnnData,
+    layer: Optional[str] = DKM.X_LAYER,
+    min_cells: int = 5,
+    gmean_eps: Union[int, float] = 1,
+    n_genes: int = 2000,
+    n_cells: Optional[int] = None,
+    bin_size: int = 500,
+    bw_adjust: int = 3,
+    inplace: bool = True,
+) -> Optional[AnnData]:
+    """A re-implementation of SCTransform from the Satija lab.
+
+    Args:
+        adata: an Annotated data matrix.
+        layer: the name of the layer to perform sctransform
+        min_cells: minimum number of cells expressing a gene to be included in sctransform.
+        gmean_eps: epsilon value to add to the geometric mean to avoid log(0) when calculating the log of geometric
+            mean.
+        n_genes: number of genes to be used for sctransform.
+        n_cells: number of cells to be used for sctransform. If None, use all cells.
+        bin_size: size of bins in which to group genes during sctransform.
+        bw_adjust: bandwidth adjustment factor for kernel density estimation.
+        inplace: whether to perform the sctransform in-place or return a copy of the original data matrix.
+
+    Returns:
+        If inplace=True, adata is updated with results in layer `layer`.
+        If inplace=False, a new AnnData object with results will be returned.
     """
+    import multiprocessing
+    import sys
+
     main_info("sctransform adata on layer: %s" % (layer))
     X = DKM.select_layer_data(adata, layer).copy()
     X = sp.sparse.csr_matrix(X)
     X.eliminate_zeros()
     gene_names = np.array(list(adata.var_names))
     cell_names = np.array(list(adata.obs_names))
     genes_cell_count = X.sum(0).A.flatten()
     genes = np.where(genes_cell_count >= min_cells)[0]
     genes_ix = genes.copy()
 
     X = X[:, genes]
-    Xraw = X.copy()
     gene_names = gene_names[genes]
     genes = np.arange(X.shape[1])
-    genes_cell_count = X.sum(0).A.flatten()
 
     genes_log_gmean = np.log10(gmean(X, axis=0, eps=gmean_eps))
 
     # sample by n_cells, or use all cells
     if n_cells is not None and n_cells < X.shape[0]:
         cells_step1 = np.sort(np.random.choice(X.shape[0], replace=False, size=n_cells))
         genes_cell_count_step1 = X[cells_step1].sum(0).A.flatten()
@@ -184,24 +270,32 @@
             np.random.choice(genes_step1, size=n_genes, p=sampling_prob / sampling_prob.sum(), replace=False)
         )
         genes_log_gmean_step1 = np.log10(gmean(X[cells_step1, :][:, genes_step1], eps=gmean_eps))
 
     bin_ind = np.ceil(np.arange(1, genes_step1.size + 1) / bin_size)
     max_bin = max(bin_ind)
 
-    ps = Manager().dict()
+    ps = multiprocessing.Manager().dict()
+
+    # create a process context of fork that copy a Python process from an existing process.
+    if sys.platform != "win32":
+        ctx = multiprocessing.get_context("fork")  # this fixes loop on MacOS
+    else:
+        ctx = multiprocessing.get_context("spawn")
 
     for i in range(1, int(max_bin) + 1):
         genes_bin_regress = genes_step1[bin_ind == i]
         umi_bin = X[cells_step1, :][:, genes_bin_regress]
 
         mm = np.vstack((np.ones(data_step1.shape[0]), data_step1["log_umi"].values.flatten())).T
 
         pc_chunksize = umi_bin.shape[1] // os.cpu_count() + 1
-        pool = Pool(os.cpu_count(), _parallel_init, [genes_bin_regress, umi_bin, gene_names, mm, ps])
+
+        pool = ctx.Pool(os.cpu_count(), _parallel_init, [genes_bin_regress, umi_bin, gene_names, mm, ps])
+
         try:
             pool.map(_parallel_wrapper, range(umi_bin.shape[1]), chunksize=pc_chunksize)
         finally:
             pool.close()
             pool.join()
 
     ps = ps._getvalue()
@@ -250,18 +344,14 @@
         )
         full_model_pars[i] = kr.fit(data_predict=x_points)[0]
 
     theta = 10**genes_log_gmean / (10 ** full_model_pars["dispersion"].values - 1)
     full_model_pars["theta"] = theta
     del full_model_pars["dispersion"]
 
-    model_pars_outliers = outliers
-
-    regressor_data = np.vstack((np.ones(cell_attrs.shape[0]), cell_attrs["log_umi"].values)).T
-
     d = X.data
     x, y = X.nonzero()
     mud = np.exp(full_model_pars.values[:, 0][y] + full_model_pars.values[:, 1][y] * cell_attrs["log_umi"].values[x])
     vard = mud + mud**2 / full_model_pars["theta"].values.flatten()[y]
 
     X.data[:] = (d - mud) / vard**0.5
     X.data[X.data < 0] = 0
@@ -323,11 +413,27 @@
         z = pd.Series(index=gene_names, data=np.zeros(gene_names.size, dtype="int"))
         z[gene_names[genes_step1]] = 1
         adata_new.var["genes_step1_sct"] = z
         adata_new.var["log10_gmean_sct"] = genes_log_gmean
         return adata_new
 
 
-def sctransform(adata: AnnData, layers: str = [DKM.X_LAYER], output_layer: str = None, n_top_genes=2000, **kwargs):
-    """a wrapper calls sctransform_core and set dynamo style keys in adata"""
+def sctransform(
+    adata: AnnData, layers: str = [DKM.X_LAYER], output_layer: str = None, n_top_genes=2000, **kwargs
+) -> Optional[AnnData]:
+    """A wrapper calls sctransform_core and set dynamo style keys in adata"""
     for layer in layers:
+        if layer != DKM.X_LAYER:
+            main_warning(
+                f"Sctransform is only recommended for X layer while you are applying sctransform on layer: {layer}, "
+                f"This will overwrite existing sctransform params and create negative values in layers, "
+                f"which will cause error in the velocities calculation. Please run the sctransform recipe with default "
+                f"if you plan to perform downstream analysis."
+            )
         sctransform_core(adata, layer=layer, n_genes=n_top_genes, **kwargs)
+        if layer == DKM.X_LAYER:
+            if adata.X.shape[1] > n_top_genes:
+                X_squared = adata.X.copy()
+                X_squared.data **= 2
+                variance = X_squared.mean(0) - np.square(adata.X.mean(0))
+                adata.var["sct_score"] = variance.A1
+                adata.var["use_for_pca"] = get_gene_selection_filter(adata.var["sct_score"], n_top_genes=n_top_genes)
```

### Comparing `dynamo-release-1.2.0/dynamo/external/utils.py` & `dynamo-release-1.3.0/dynamo/external/utils.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/get_version.py` & `dynamo-release-1.3.0/dynamo/get_version.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 """
 A minimalistic version helper in the spirit of versioneer, that is able to run without build step using pkg_resources.
 Developed by P Angerer, see https://github.com/flying-sheep/get_version.
 """
-# __version__ is defined at the very end of this file.
-
+import logging
 import os
 import re
 from pathlib import Path
 from subprocess import PIPE, CalledProcessError, run
 from typing import List, NamedTuple, Optional, Union
 
+# __version__ is defined at the very end of this file.
+
+
 RE_VERSION = r"([\d.]+?)(?:\.dev(\d+))?(?:[_+-]([0-9a-zA-Z.]+))?"
 # RE_GIT_DESCRIBE = r"v?(?:([\d.]+)-(\d+)-g)?([0-9a-f]{7})(-dirty)?"
 RE_GIT_DESCRIBE = r"v?(?:([\d.]+)-(.+)-g)?([0-9a-f]{7})(-dirty)?"
 ON_RTD = os.environ.get("READTHEDOCS") == "True"
 
 
 def match_groups(regex, target):
@@ -203,12 +205,20 @@
     if display:
         pd.options.display.max_columns = None
         display(df)
     else:
         return df
 
 
-__version__ = get_version(__file__)
+def session_info():
+    try:
+        import session_info
+    except:
+        logging.error("session_info not installed! Please install it with `pip install -U session-info`")
+
+    session_info.show(html=False, dependencies=True)
 
 
+__version__ = get_version(__file__)
+
 if __name__ == "__main__":
     print(__version__)
```

### Comparing `dynamo-release-1.2.0/dynamo/movie/fate.py` & `dynamo-release-1.3.0/dynamo/movie/fate.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/plot/__init__.py` & `dynamo-release-1.3.0/dynamo/plot/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,14 +28,15 @@
     highest_frac_genes,
     loading,
     show_fraction,
     variance_explained,
 )
 from .scatters import scatters
 from .scPotential import show_landscape
+from .sctransform import sctransform_plot_fit, plot_residual_var
 from .scVectorField import (  # , plot_LIC_gray
     cell_wise_vectors,
     cell_wise_vectors_3d,
     grid_vectors,
     line_integral_conv,
     plot_energy,
     streamline_plot,
@@ -146,8 +147,10 @@
     "SchemeDivergeBWR",
     "streamline_clusters",
     "response",
     "plot_hill_function",
     "causality",
     "comb_logic",
     "hessian",
+    "sctransform_plot_fit",
+    "plot_residual_var",
 ]
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/cell_cycle.py` & `dynamo-release-1.3.0/dynamo/plot/cell_cycle.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,36 +1,46 @@
-from typing import Optional, Union
+from typing import Any, Dict, Optional, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 from anndata import AnnData
 from matplotlib.axes import Axes
 
 from ..tools.utils import update_dict
 from .utils import save_fig
 
 
 def cell_cycle_scores(
     adata: AnnData,
     cells: Optional[list] = None,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-) -> Union[None, Axes]:
-    """Plot a heatmap of cells ordered by cell cycle position
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-        cells: a list of cell ids used to subset the adata object.
-        save_show_or_return:
-            Whether to save, show or return the figure.
-        save_kwargs:
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[Axes]:
+    """Plot a heatmap of cells ordered by cell cycle position.
+
+    Args:
+        adata: an AnnData object.
+        cells: a list of cell ids used to subset the AnnData object. If None, all cells would be used. Defaults to None.
+        save_show_or_return: whether to save, show, or return the figure. Available flags are `"save"`, `"show"`, and
+            `"return"`. Defaults to "show".
+        save_kwargs: A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
             save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent":
-             True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-             properly modify those keys according to your needs.
+            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
+            properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        NotImplementedError: unavailable save_show_or_return
+
+    Returns:
+        Axes of the plotted figure if `save_show_or_return` is set to `"return"`; otherwise, return `None`.
     """
+
     import matplotlib.pyplot as plt
     import seaborn as sns
     from matplotlib.pyplot import colorbar
     from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable
 
     if cells is None:
         cell_cycle_scores = adata.obsm["cell_cycle_scores"].dropna()
@@ -58,25 +68,29 @@
     ax_divider = make_axes_locatable(ax)
     # define size and padding of axes for colorbar
     cax = ax_divider.append_axes("right", size="2%", pad="0.5%", aspect=4, anchor="NW")
     # make colorbar for heatmap.
     # Heatmap returns an axes obj but you need to get a mappable obj (get_children)
     colorbar(ax.get_children()[0], cax=cax, ticks=[-0.9, 0, 0.9])
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_direct_graph",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
+
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/connectivity.py` & `dynamo-release-1.3.0/dynamo/plot/connectivity.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,22 +1,31 @@
 """
-# code are largely adapted from https://github.com/lmcinnes/umap/blob/7e051d8f3c4adca90ca81eb45f6a9d1372c076cf/umap/plot.py
+code are largely adapted from 
+https://github.com/lmcinnes/umap/blob/7e051d8f3c4adca90ca81eb45f6a9d1372c076cf/umap/plot.py
 The code base will be extended extensively to consider the following cases:
     1. nneighbors: kNN graph constructed from umap/scKDTree/annoy, etc
     2. mutual kNN shared between spliced or unspliced layer
     3. principal graph that learnt from DDRTree, L1graph or other principal graph algorithms
     4. regulatory network learnt from Scribe
     5. spatial kNN graph
     6. others
 """
 
 
-from typing import Optional, Union
+from typing import Any, Dict, List, Optional, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
+import logging
 from warnings import warn
 
+import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 import scipy
 from anndata import AnnData
 from matplotlib.axes import Axes
 from matplotlib.figure import Figure
 
@@ -32,30 +41,23 @@
     _select_font_color,
     save_fig,
 )
 
 docstrings = DocstringProcessor()
 
 
-def _plt_connectivity(coord: dict, connectivity: scipy.sparse.csr_matrix):
+def _plt_connectivity(coord: dict, connectivity: scipy.sparse.csr_matrix) -> None:
     """Plot connectivity graph via networkx and matplotlib.
 
-    Parameters
-    ----------
-        coord: `dict`
-            A dictionary where the keys are the graph node names and values are the corresponding coordinates of the node.
-        connectivity: `scipy.sparse.csr_matrix`
-            A csr sparse matrix of the cell connectivities.
-
-    Returns
-    -------
-        Nothing but a connectivity graph plot built upon networkx and matplotlib.
+    Args:
+        coord: a dictionary where the keys are the graph node names and values are the corresponding coordinates of the
+            node.
+        connectivity: a csr sparse matrix of the cell connectivities.
     """
 
-    import matplotlib.pyplot as plt
     import networkx as nx
 
     if_symmetric = (abs(connectivity - connectivity.T) > 1e-10).nnz == 0
 
     G = (
         nx.from_scipy_sparse_matrix(connectivity, create_using=nx.Graph())
         if if_symmetric
@@ -90,163 +92,159 @@
 
 
 @docstrings.get_sectionsf("con_base")
 def connectivity_base(
     x: int,
     y: int,
     edge_df: pd.DataFrame,
-    highlights: Optional[list] = None,
-    edge_bundling: Optional[str] = None,
+    highlights: Optional[List[str]] = None,
+    edge_bundling: Optional[Literal["hammer"]] = None,
     edge_cmap: str = "gray_r",
     show_points: bool = True,
     labels: Optional[list] = None,
     values: Optional[list] = None,
-    theme: Optional[str] = None,
+    theme: Optional[
+        Literal[
+            "blue",
+            "red",
+            "green",
+            "inferno",
+            "fire",
+            "viridis",
+            "darkblue",
+            "darkgreen",
+            "darkred",
+        ]
+    ] = None,
     cmap: str = "Blues",
     color_key: Union[dict, list, None] = None,
     color_key_cmap: str = "Spectral",
     background: str = "black",
     figsize: tuple = (7, 5),
     ax: Optional[Axes] = None,
-    sort: str = "raw",
-    save_show_or_return: str = "return",
-    save_kwargs: dict = {},
-) -> Union[None, Axes]:
-    """Plot connectivity relationships of the underlying UMAP
-    simplicial set data structure. Internally UMAP will make
-    use of what can be viewed as a weighted graph. This graph
-    can be plotted using the layout provided by UMAP as a
-    potential diagnostic view of the embedding. Currently this only works
-    for 2D embeddings. While there are many optional parameters
-    to further control and tailor the plotting, you need only
-    pass in the trained/fit umap model to get results. This plot
-    utility will attempt to do the hard work of avoiding
-    overplotting issues and provide options for plotting the
-    points as well as using edge bundling for graph visualization.
-
-    Parameters
-    ----------
-        x: `int`
-            The first component of the embedding.
-        y: `int`
-            The second component of the embedding.
-        edge_df `pd.DataFrame`
-            The dataframe denotes the graph edge pairs. The three columns
-            include 'source', 'target' and 'weight'.
-        highlights: `list`, `list of list` or None (default: `None`)
-            The list that cells will be restricted to.
-        edge_bundling: string or None (optional, default None)
-            The edge bundling method to use. Currently supported
-            are None or 'hammer'. See the datashader docs
-            on graph visualization for more details.
-        edge_cmap: string (default 'gray_r')
-            The name of a matplotlib colormap to use for shading/
-            coloring the edges of the connectivity graph. Note that
-            the ``theme``, if specified, will override this.
-        show_points: bool (optional False)
-            Whether to display the points over top of the edge
-            connectivity. Further options allow for coloring/
-            shading the points accordingly.
-        labels: array, shape (n_samples,) (optional, default None)
-            An array of labels (assumed integer or categorical),
-            one for each data sample.
-            This will be used for coloring the points in
-            the plot according to their label. Note that
-            this option is mutually exclusive to the ``values``
-            option.
-        values: array, shape (n_samples,) (optional, default None)
-            An array of values (assumed float or continuous),
-            one for each sample.
-            This will be used for coloring the points in
-            the plot according to a colorscale associated
-            to the total range of values. Note that this
-            option is mutually exclusive to the ``labels``
-            option.
-        theme: string (optional, default None)
-            A color theme to use for plotting. A small set of
-            predefined themes are provided which have relatively
+    sort: Literal["raw", "abs"] = "raw",
+    save_show_or_return: Literal["save", "show", "return"] = "return",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[Axes]:
+    """Plot connectivity relationships of the underlying UMAP simplicial set data structure.
+
+    Internally UMAP will make use of what can be viewed as a weighted graph. This graph can be plotted using the layout
+    provided by UMAP as a potential diagnostic view of the embedding. Currently this only works for 2D embeddings. While
+    there are many optional parameters to further control and tailor the plotting, you only need pass in the trained/fit
+    umap model to get results. This plot utility will attempt to do the hard work of avoiding over-plotting issues and
+    provide options for plotting the points as well as using edge bundling for graph visualization.
+
+    Args:
+        x: the first component of the embedding.
+        y: the second component of the embedding.
+        edge_df: the dataframe denotes the graph edge pairs. The three columns include 'source', 'target' and 'weight'.
+        highlights: the list that cells will be restricted to. Defaults to None.
+        edge_bundling: the edge bundling method to use. Currently supported are None or 'hammer'. See the datashader
+            docs on graph visualization for more details. Defaults to None.
+        edge_cmap: the name of a matplotlib colormap to use for shading/coloring the edges of the connectivity graph.
+            Note that the `theme`, if specified, will override this. Defaults to "gray_r".
+        show_points: whether to display the points over top of the edge connectivity. Further options allow for
+            coloring/shading the points accordingly. Defaults to True.
+        labels: An array of labels (assumed integer or categorical), one for each data sample. This will be used for
+            coloring the points in the plot according to their label. Note that this option is mutually exclusive to the
+            `values` option. Defaults to None.
+        values: an array of values (assumed float or continuous), one for each sample. This will be used for coloring
+            the points in the plot according to a colorscale associated to the total range of values. Note that this
+            option is mutually exclusive to the `labels` option. Defaults to None.
+        theme: a color theme to use for plotting. A small set of predefined themes are provided which have relatively
             good aesthetics. Available themes are:
                * 'blue'
                * 'red'
                * 'green'
                * 'inferno'
                * 'fire'
                * 'viridis'
                * 'darkblue'
                * 'darkred'
-               * 'darkgreen'
-        cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring
-            or shading points. If no labels or values are passed
-            this will be used for shading points according to
-            density (largely only of relevance for very large
-            datasets). If values are passed this will be used for
-            shading according the value. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        color_key: dict or array, shape (n_categories) (optional, default None)
-            A way to assign colors to categoricals. This can either be
-            an explicit dict mapping labels to colors (as strings of form
-            '#RRGGBB'), or an array like object providing one color for
-            each distinct category being provided in ``labels``. Either
-            way this mapping will be used to color points according to
-            the label. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        color_key_cmap: string (optional, default 'Spectral')
-            The name of a matplotlib colormap to use for categorical coloring.
-            If an explicit ``color_key`` is not given a color mapping for
-            categories can be generated from the label list and selecting
-            a matching list of colors from the given colormap. Note
-            that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        background: string (optional, default 'white)
-            The color of the background. Usually this will be either
-            'white' or 'black', but any color name will work. Ideally
-            one wants to match this appropriately to the colors being
-            used for points etc. This is one of the things that themes
-            handle for you. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        width: int (optional, default 800)
-            The desired width of the plot in pixels.
-        height: int (optional, default 800)
-            The desired height of the plot in pixels
-        sort: `str` (optional, default `raw`)
-            The method to reorder data so that high values points will be on top of background points. Can be one of
-            {'raw', 'abs'}, i.e. sorted by raw data or sort by absolute values.
-        save_show_or_return: {'show', 'save', 'return'} (default: `return`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'connectivity_base', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-
-    Returns
-    -------
-    result:
-        Either return None or a matplotlib axis with the relevant plot displayed based on arguments.
-        If you are using a notbooks and have ``%matplotlib inline`` set
-        then this will simply display inline.
+               * 'darkgreen'.
+            Defaults to None.
+        cmap: the name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to "Blues".
+        color_key: a way to assign colors to categorical. This can either be an explicit dict mapping labels to colors
+            (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct category being
+            provided in `labels`. Either way this mapping will be used to color points according to the label. Note that
+            if theme is passed then this value will be overridden by the corresponding option of the theme. Defaults to
+            None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to "Spectral".
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to "black".
+        figsize: the desired size of the figure. Defaults to (7, 5).
+        ax: the axis on which the subplot would be shown. If set to be `None`, a new axis would be created. Defaults to
+            None.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs'}, i.e. sorted by raw data or sort by absolute values. Defaults to "raw".
+        save_show_or_return: whether to save, show or return the figure. Defaults to "return".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the
+                {
+                    "path": None,
+                    "prefix": 'connectivity_base',
+                    "dpi": None,
+                    "ext": 'pdf',
+                    "transparent": True,
+                    "close": True,
+                    "verbose": True
+                }
+            as its parameters. Otherwise, you can provide a dictionary that properly modify those keys according to your
+            needs. Defaults to {}.
+
+    Raises:
+        ImportError: `datashader` is not installed.
+        NotImplementedError: invalid `theme`.
+        ValueError: invalid `edge_bundling`.
+        NotImplementedError: invalid `save_show_or_return`.
+
+    Returns:
+        The matplotlib axis with the relevant plot displayed by default. If `save_show_or_return` is set to be `"show"`
+        or `"save"`, nothing would be returned.
     """
 
-    import datashader as ds
-    import datashader.bundling as bd
-    import datashader.transfer_functions as tf
-    import matplotlib.pyplot as plt
+    try:
+        import datashader as ds
+        import datashader.bundling as bd
+        import datashader.transfer_functions as tf
+    except ImportError as e:
+        logging.critical('"datashader" package is required for this function', exc_info=True)
+        raise e
 
     dpi = plt.rcParams["figure.dpi"]
 
-    if theme is not None:
-        cmap = _themes[theme]["cmap"]
-        color_key_cmap = _themes[theme]["color_key_cmap"]
-        edge_cmap = _themes[theme]["edge_cmap"]
-        background = _themes[theme]["background"]
+    available_themes = [
+        "blue",
+        "red",
+        "green",
+        "inferno",
+        "fire",
+        "viridis",
+        "darkblue",
+        "darkgreen",
+        "darkred",
+    ]
+    if theme is None:
+        pass
+    else:
+        if theme in available_themes:
+            cmap = _themes[theme]["cmap"]
+            color_key_cmap = _themes[theme]["color_key_cmap"]
+            edge_cmap = _themes[theme]["edge_cmap"]
+            background = _themes[theme]["background"]
+        else:
+            raise NotImplementedError('Invalid value for "theme".')
 
     points = np.array([x, y]).T
     point_df = pd.DataFrame(points, columns=("x", "y"))
 
     point_size = 500.0 / np.sqrt(points.shape[0])
     if point_size > 1:
         px_size = int(np.round(point_size))
@@ -307,99 +305,162 @@
         fig = plt.figure(figsize=figsize)
         ax = fig.add_subplot(111)
 
     _embed_datashader_in_an_axis(result, ax)
 
     ax.set(xticks=[], yticks=[])
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "connectivity_base",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 docstrings.delete_params("con_base.parameters", "edge_df", "save_show_or_return", "save_kwargs")
 
 
 @docstrings.with_indent(4)
 def nneighbors(
     adata: AnnData,
     x: int = 0,
     y: int = 1,
-    color: str = "ntr",
-    basis: str = "umap",
-    layer: str = "X",
-    highlights: Union[list] = None,
+    color: List[str] = ["ntr"],
+    basis: List[str] = ["umap"],
+    layer: List[str] = ["X"],
+    highlights: Optional[list] = None,
     ncols: int = 1,
-    edge_bundling: Optional[str] = None,
+    edge_bundling: Optional[Literal["hammer"]] = None,
     edge_cmap: str = "gray_r",
     show_points: bool = True,
     labels: Optional[list] = None,
     values: Optional[list] = None,
-    theme: Optional[str] = None,
+    theme: Optional[
+        Literal[
+            "blue",
+            "red",
+            "green",
+            "inferno",
+            "fire",
+            "viridis",
+            "darkblue",
+            "darkgreen",
+            "darkred",
+        ]
+    ] = None,
     cmap: str = "Blues",
-    color_key: Optional[Union[dict, list]] = None,
+    color_key: Union[dict, list, None] = None,
     color_key_cmap: str = "Spectral",
     background: str = "black",
     figsize: tuple = (6, 4),
     ax: Optional[Axes] = None,
-    save_show_or_return: str = "return",
+    save_show_or_return: Literal["save", "show", "return"] = "return",
     save_kwargs: dict = {},
-) -> Union[None, Figure]:
+) -> Optional[Figure]:
     """Plot nearest neighbor graph of cells used to embed data into low dimension space.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object that include the umap embedding and simplicial graph.
-        x: `int`
-            The first component of the embedding.
-        y: `int`
-            The second component of the embedding.
-        color: `str` or list of `str` or None (default: 'ntr')
-            Gene name(s) or cell annotation column(s)
-        basis: `str` or list of `str` (default: `X`)
-            Which low dimensional embedding will be used to visualize the cell.
-        layer: `str` or list of `str` (default: `X`)
-            The layers of data to represent the gene expression level.
-        highlights: `list`, `list of list` or None (default: `None`)
-            The list that cells will be restricted to.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'nneighbors', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        %(con_base.parameters.no_edge_df|save_show_or_return|save_kwargs)s
-
-    Returns
-    -------
-    Nothing but plot the nearest neighbor graph.
+    Args:
+        adata: an Annodata object that include the umap embedding and simplicial graph.
+        x: the first component of the embedding. Defaults to 0.
+        y: the second component of the embedding. Defaults to 1.
+        color: gene name(s) or cell annotation column(s) used for coloring the graph. Defaults to ["ntr"].
+        basis: the low dimensional embedding to be used to visualize the cell. Defaults to ["umap"].
+        layer: the layers of data representing the gene expression level. Defaults to ["X"].
+        highlights: the list that cells will be restricted to. Defaults to None.
+        ncols: the number of columns to be plotted. Defaults to 1.
+        edge_bundling: the edge bundling method to use. Currently supported are None or 'hammer'. See the datashader
+            docs on graph visualization for more details. Defaults to None.
+        edge_cmap: the name of a matplotlib colormap to use for shading/coloring the edges of the connectivity graph.
+            Note that the `theme`, if specified, will override this. Defaults to "gray_r".
+        show_points: whether to display the points over top of the edge connectivity. Further options allow for
+            coloring/shading the points accordingly. Defaults to True.
+        labels: an array of labels (assumed integer or categorical), one for each data sample. This will be used for
+            coloring the points in the plot according to their label. Note that this option is mutually exclusive to the
+            `values` option. Defaults to None.
+        values: an array of values (assumed float or continuous), one for each sample. This will be used for coloring
+            the points in the plot according to a colorscale associated to the total range of values. Note that this
+            option is mutually exclusive to the `labels` option. Defaults to None.
+        theme: a color theme to use for plotting. A small set of predefined themes are provided which have relatively
+            good aesthetics. Available themes are:
+               * 'blue'
+               * 'red'
+               * 'green'
+               * 'inferno'
+               * 'fire'
+               * 'viridis'
+               * 'darkblue'
+               * 'darkred'
+               * 'darkgreen'.
+            Defaults to None.
+        cmap: the name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to "Blues".
+        color_key: a way to assign colors to categoricals. This can either be an explicit dict mapping labels to colors
+            (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct category being
+            provided in `labels`. Either way this mapping will be used to color points according to the label. Note that
+            if theme is passed then this value will be overridden by the corresponding option of the theme. Defaults to
+            None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to "Spectral".
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to "black".
+        figsize: the desired size of the figure. Defaults to (6, 4).
+        ax: the axis on which the subplot would be shown. If set to be `None`, a new axis would be created. Defaults to
+            None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "return".
+        save_kwargs: a dictionary that will passed to the save_fig function. By default it is an empty dictionary and
+            the save_fig function will use the
+                {
+                    "path": None,
+                    "prefix": 'connectivity_base',
+                    "dpi": None,
+                    "ext": 'pdf',
+                    "transparent": True,
+                    "close": True,
+                    "verbose": True
+                }
+            as its parameters. Otherwise you can provide a dictionary that properly modify those keys according to your
+            needs. Defaults to {}.
+
+    Raises:
+        TypeError: wrong type of `x` and `y`.
+        NotImplementedError: invalid `save_show_or_return`.
+
+    Returns:
+        The matplotlib axis with the plotted knn graph by default. If `save_show_or_return` is set to be `"show"`
+        or `"save"`, nothing would be returned.
     """
 
     import matplotlib.pyplot as plt
     import seaborn as sns
 
     if type(x) is not int or type(y) is not int:
-        raise Exception(
+        raise TypeError(
             "x, y have to be integers (components in the a particular embedding {}) for nneighbor "
             "function".format(basis)
         )
 
     n_c, n_l, n_b = (
         0 if color is None else len(color),
         0 if layer is None else len(layer),
@@ -515,32 +576,37 @@
 
                 ax.set_xlabel(
                     cur_b + "_1",
                 )
                 ax.set_ylabel(cur_b + "_2")
                 ax.set_title(cur_c)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "nneighbors",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
+    else:
+        raise NotImplementedError('Invalid "save_show_or_return".')
 
 
 def pgraph():
     """Plot principal graph of cells that learnt from graph embedding algorithms.
 
     return:
     """
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/dynamics.py` & `dynamo-release-1.3.0/dynamo/plot/dynamics.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,19 @@
 import sys
 import warnings
-from typing import Optional, Union
+from typing import List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import pandas as pd
 from anndata import AnnData
+from matplotlib.figure import Figure
 
 from ..configuration import _themes
 from ..dynamo_logger import main_warning
 from ..estimation.csc.velocity import sol_s, sol_u, solve_first_order_deg
 from ..estimation.tsc.utils_moments import moments
 from ..tools.utils import get_mapper, get_valid_bools, index_gene, log1p_, update_dict
 from .scatters import scatters
@@ -24,167 +30,163 @@
     save_fig,
 )
 from .utils_dynamics import *
 
 
 def phase_portraits(
     adata: AnnData,
-    genes: list,
+    genes: List[str],
     x: int = 0,
     y: int = 1,
     pointsize: Optional[float] = None,
     vkey: Optional[str] = None,
     ekey: Optional[str] = None,
     basis: str = "umap",
     log1p: bool = True,
     color: str = "cell_cycle_phase",
     use_smoothed: bool = True,
     highlights: Optional[list] = None,
     discrete_continous_div_themes: Optional[list] = None,
     discrete_continous_div_cmap: Optional[list] = None,
-    discrete_continous_div_color_key: Optional[list] = [None, None, None],
+    discrete_continous_div_color_key: list = [None, None, None],
     discrete_continous_div_color_key_cmap: Optional[list] = None,
-    figsize: tuple = (6, 4),
+    figsize: Tuple[float, float] = (6, 4),
     ncols: Optional[int] = None,
     legend: str = "upper left",
     background: Optional[str] = None,
     show_quiver: bool = False,
     quiver_size: Optional[float] = None,
     quiver_length: Optional[float] = None,
     no_vel_u: bool = True,
     frontier: bool = True,
     q_kwargs_dict: dict = {},
     show_arrowed_spines: Optional[bool] = None,
-    save_show_or_return: str = "show",
+    save_show_or_return: Literal["save", "show", "return"] = "show",
     save_kwargs: dict = {},
     **kwargs,
-):
-    """Draw the phase portrait, expression values , velocity on the low dimensional embedding.
-    Note that this function allows to manually set the theme, cmap, color_key and color_key_cmap
-    for the phase portrait, expression and velocity subplots. When the background is 'black',
-    the default themes for each of those subplots are  ["glasbey_dark", "inferno", "div_blue_black_red"],
-    respectively. When the background is 'black', the default themes are  "glasbey_white", "viridis",
-    "div_blue_red".
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        genes: `list`
-            A list of gene names that are going to be visualized.
-        x: `int` (default: `0`)
-                The column index of the low dimensional embedding for the x-axis
-        y: `int` (default: `1`)
-                The column index of the low dimensional embedding for the y-axis
-        pointsize: `None` or `float` (default: None)
-                The scale of the point size. Actual point cell size is calculated as `2500.0 / np.sqrt(adata.shape[0]) *
-                pointsize`
-        vkey: `string` or None (default: None)
-            Which velocity key used for visualizing the magnitude of velocity. Can be either velocity in the layers slot
-            or the keys in the obsm slot.
-        ekey: `str` or None (default: `None`)
-            The layer of data to represent the gene expression level.
-        basis: `string` (default: umap)
-            Which low dimensional embedding will be used to visualize the cell.
-        log1p: `bool` (default: `True`)
-            Whether to log1p transform the expression so that visualization can be robust to extreme values.
-        color: `string` (default: 'cell_cycle_phase')
-            Which group will be used to color cells, only used for the phase portrait because the other two plots are
-            colored by the velocity magnitude or the gene expression value, respectively.
-        use_smoothed: `bool` (default: `True`)
-            Whether to use smoothed 1/2-nd moments as gene expression for the first and third columns. This is useful
-            for checking the confidence of transition genes. For example, you may see a very nice linear pattern for
-            some genes with the smoothed expression but this could just be an artificially generated when the number of
-            expressed cells is very low. This raises red flags for the quality of the velocity values we learned for
+) -> Optional[Figure]:
+    """Draw the phase portrait, expression values, velocity on the low dimensional embedding.
+
+    Note that this function allows to manually set the theme, cmap, color_key and color_key_cmap for the phase portrait,
+    expression and velocity subplots. When the background is 'black', the default themes for each of those subplots are
+    ["glasbey_dark", "inferno", "div_blue_black_red"], respectively. When the background is 'black', the default themes
+    are "glasbey_white", "viridis", "div_blue_red".
+
+    Args:
+        adata: an AnnData object.
+        genes: a list of gene names that are going to be visualized.
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        pointsize: the scale of the point size. Actual point cell size is calculated as
+            `2500.0 / np.sqrt(adata.shape[0]) * pointsize`. Defaults to None.
+        vkey: the velocity key used for visualizing the magnitude of velocity. Can be either velocity in the layers slot
+            or the keys in the obsm slot. Defaults to None.
+        ekey: the layer of data to represent the gene expression level. Defaults to None.
+        basis: the key of the low dimensional embedding will be used to visualize the cell. Defaults to "umap".
+        log1p: whether to log1p transform the expression so that visualization can be robust to extreme values. Defaults
+            to True.
+        color: the group to be used to color cells. It would only be used for the phase portrait because the other two
+            plots are colored by the velocity magnitude or the gene expression value respectively. Defaults to
+            "cell_cycle_phase".
+        use_smoothed: Whether to use smoothed 1/2-nd moments as gene expression for the first and third columns. This is
+            useful for checking the confidence of transition genes. For example, you may see a very nice linear pattern
+            for some genes with the smoothed expression but this could just be an artificially generated when the number
+            of expressed cells is very low. This raises red flags for the quality of the velocity values we learned for
             those genes. And we recommend to set the higher values (for example, 10% of all cells) for `min_cell_s`,
             `min_cell_u` or `shared_count` of the `fg_kwargs` argument of the dyn.pl.receipe_monocle. Note that this is
             often related to the small single cell datasets (like plate-based scRNA-seq or scSLAM-seq/NASC-seq, etc).
-        highlights: `list` (default: None)
-            Which color group will be highlighted. if highligts is a list of lists - each list is relate to each color
-            element.
-        discrete_continous_div_themes: `list[str, str, str]` (optional, default None)
-            The discrete, continous and divergent color themes to use for plotting. The description for each element in
-            the list is as following.
-            A small set of predefined themes are provided which have relatively good aesthetics. Available themes are:
+            Defaults to True.
+        highlights: the color group to be highlighted. If highligts is a list of lists, each list is relate to each
+            color element. Defaults to None.
+        discrete_continous_div_themes: the discrete, continous and divergent color themes to use for plotting,
+            respectively. The description for each element in the list is as following. A small set of predefined themes
+            are provided which have relatively good aesthetics. Available themes are:
                * 'blue'
                * 'red'
                * 'green'
                * 'inferno'
                * 'fire'
                * 'viridis'
                * 'darkblue'
                * 'darkred'
-               * 'darkgreen'
-        discrete_continous_div_cmap: `list[str, str, str]`  (optional, default 'Blues')
-            The names of  discrete, continous and divergent matplotlib colormap to use for coloring
-            or shading points. The description for each element in the list is as following. If no labels or values are
-            passed this will be used for shading points according to density (largely only of relevance for very large
-            datasets). If values are passed this will be used for shading according the value. Note that if theme is
-            passed then this value will be overridden by the corresponding option of the theme.
-        discrete_continous_div_color_key: `list[dict or array,, dict or array,, dict or array,]` (default [None, None,
-            None]).
-            The description for each element in the list is as following. The shape (n_categories) (optional, default
-            None). A list to assign discrete, continous and divergent colors to categoricals. This can either be an
-            explicit dict mapping labels to colors (as strings of form '#RRGGBB'), or an array like object providing one
-            color for each distinct category being provided in ``labels``. Either way this mapping will be used to color
-            points according to the label. Note that if theme is passed then this value will be overridden by the
-            corresponding option of the theme.
-        discrete_continous_div_color_key_cmap: `list[str, str, str]`, (optional, default 'Spectral')
-            The names of discrete, continous and divergent matplotlib colormap to use for categorical coloring.
-            The description for each element in the list is as following. If an explicit ``color_key`` is not given a
-            color mapping for categories can be generated from the label list and selecting a matching list of colors
-            from the given colormap. Note that if theme is passed then this value will be overridden by the corresponding
-            option of the theme.
-        figsize: `None` or `[float, float]` (default: None)
-                The width and height of each panel in the figure.
-        ncols: `None` or `int` (default: None)
-        ncol: `None` or `int` (default: None)
-                Number of columns in each facet grid.
-        legend: `str` (default: `on data`)
-                Where to put the legend.  Legend is drawn by seaborn with “brief” mode, numeric hue and size variables
-                will be represented with a sample of evenly spaced values. By default legend is drawn on top of cells.
-        background:
-            background color
-        show_quiver: `bool` (default: False)
-            Whether to show the quiver plot. If velocity for x component (corresponds to either spliced, total RNA,
-            protein, etc) or y component (corresponds to either unspliced, new RNA, protein, etc) are both calculated,
-            quiver represents velocity for both components otherwise the uncalculated component (usually y component)
-            will be set to be 0.
-        quiver_size: `float` or None (default: None)
-            The size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is used to
-            calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
+               * 'darkgreen'.
+            Defaults to None.
+        discrete_continous_div_cmap: the names of discrete, continous and divergent matplotlib colormap to use for
+            coloring or shading points. The description for each element in the list is as following. If no labels or
+            values are passed this will be used for shading points according to density (largely only of relevance for
+            very large datasets). If values are passed this will be used for shading according the value. Note that if
+            theme is passed then this value will be overridden by the corresponding option of the theme. Defaults to
+            None.
+        discrete_continous_div_color_key: A list to assign discrete, continous and divergent colors to categoricals.
+            This can either be an explicit dict mapping labels to colors (as strings of form '#RRGGBB'), or an array
+            like object providing one color for each distinct category being provided in `labels`. Either way this
+            mapping will be used to color points according to the label. Note that if theme is passed then this value
+            will be overridden by the corresponding option of the theme. Defaults to [None, None, None].
+        discrete_continous_div_color_key_cmap: the names of discrete, continous and divergent matplotlib colormap to use
+            for categorical coloring. If an explicit `color_key` is not given a color mapping for categories can be
+            generated from the label list and selecting a matching list of colors from the given colormap. Note that if
+            theme is passed then this value will be overridden by the corresponding option of the theme. Defaults to
+            None.
+        figsize: the width and height of each panel in the figure. Defaults to (6, 4).
+        ncols: number of columns in each facet grid. Defaults to None.
+        legend: the position to draw the legend. Legend is drawn by seaborn with “brief” mode, numeric hue and size v
+            ariables will be represented with a sample of evenly spaced values. By default, legend is drawn on top of
+            cells. Defaults to "upper left".
+        background: the background color. If set to None the face color of the figure would be used. Defaults to None.
+        show_quiver: Whether to show the quiver plot. If velocity for x component (corresponds to either spliced, total
+            RNA, protein, etc.) or y component (corresponds to either unspliced, new RNA, protein, etc.) are both
+            calculated, quiver represents velocity for both components otherwise the uncalculated component (usually y
+            component) will be set to be 0. Defaults to False.
+        quiver_size: the size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is
+            used to calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
             quiver_size) of the quiver. This is done via the `default_quiver_args` function which also calculate the
-            scale of the quiver (1 / quiver_length).
-        quiver_length: `float` or None (default: None)
-            The length of quiver. The quiver length which will be used to calculate scale of quiver. Note that befoe
-            applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler function. Scale
-            of quiver indicates the nuumber of data units per arrow length unit, e.g., m/s per plot width; a smaller
-            scale parameter makes the arrow longer.
-        no_vel_u: `bool` (default: True)
-            Wheter to not show velocity U (velocity of unpsliced RNAs).
-        q_kwargs_dict: `dict` (default: {})
-            The dictionary of the quiver arguments. The default setting of quiver argument is identical to that used in
-            the cell_wise_velocity and grid_velocity.
-        show_arrowed_spines: bool or None (optional, default None)
-            Whether to show a pair of arrowed spines represeenting the basis of the scatter is currently using. If None,
-            only the first panel in the expression / velocity plot will have the arrowed spine.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig
-            function will use the {"path": None, "prefix": 'phase_portraits', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly
-            modify those keys according to your needs.
-        **kwargs:
-            Additional parameters that will be passed to plt.scatter function
-
-    Returns
-    -------
-        A matplotlib plot that shows 1) the phase portrait of each category used in velocity embedding, cells' low
-        dimensional embedding, colored either by 2) the gene expression level or 3) the velocity magnitude values.
+            scale of the quiver (1 / quiver_length). Defaults to None.
+        quiver_length: the length of quiver. The quiver length which will be used to calculate scale of quiver. Note
+            that before applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler
+            function. Scale of quiver indicates the number of data units per arrow length unit, e.g., m/s per plot
+            width; a smaller scale parameter makes the arrow longer. Defaults to None.
+        no_vel_u: whether to not show velocity U (velocity of unspliced RNAs). Defaults to True.
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+        q_kwargs_dict: the dictionary of the quiver arguments. The default setting of quiver argument is identical to
+            that used in the cell_wise_velocity and grid_velocity. Defaults to {}.
+        show_arrowed_spines: whether to show a pair of arrowed spines represeenting the basis of the scatter is
+            currently using. If None, only the first panel in the expression / velocity plot will have the arrowed
+            spine. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the
+                {
+                    "path": None,
+                    "prefix": 'phase_portraits',
+                    "dpi": None,
+                    "ext": 'pdf',
+                    "transparent": True,
+                    "close": True,
+                    "verbose": True
+                }
+            as its parameters. Otherwise, you can provide a dictionary that properly modify those keys according to
+            your needs. Defaults to {}.
+        **kwargs: additional parameters that will be passed to `plt.scatter` function.
+
+    Raises:
+        ValueError: missing velocity data in the AnnData object.
+        ValueError: cannot find layer with given `v_key`.
+        ValueError: missing velocity_gamma column in the AnnData object.
+        ValueError: missing velocity_gamma column in the AnnData object.
+        ValueError: corrupted AnnData object missing basic labeled/unlabeled or spliced/unspliced data.
+        NotImplementedError: invalid `save_show_or_return`.
+
+    Returns:
+        None would be returned in default and the plotted figure would be shown directly. The matplotlib plot would show
+        1) the phase portrait of each category used in velocity embedding, cells' low dimensional embedding, colored
+        either by 2) the gene expression level or 3) the velocity magnitude values. If set
+        `save_show_or_return='return'` as a kwarg, the axes of the plot would be returned.
     """
 
     import matplotlib.pyplot as plt
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     # from matplotlib.colors import DivergingNorm  # TwoSlopeNorm
@@ -229,15 +231,15 @@
         k_name = "gamma"
 
     valid_id = np.isfinite(np.array(adata.var.loc[_genes, k_name], dtype="float")).flatten()
     genes = np.array(_genes)[valid_id].tolist()
     # idx = [adata.var.index.to_list().index(i) for i in genes]
 
     if len(genes) == 0:
-        raise Exception(
+        raise ValueError(
             "adata has no genes listed in your input gene vector or "
             "velocity estimation for those genes are not performed. "
             "Please try to run dyn.tl.dynamics(adata, filter_gene_mode='no')"
             "to estimate velocity for all genes: {}".format(_genes)
         )
 
     if not "X_" + basis in adata.obsm.keys():
@@ -322,15 +324,15 @@
             P_vec = index_gene(adata, adata.layers["velocity_P"], genes)
     else:
         if vkey in adata.layers.keys():
             V_vec = index_gene(adata, adata.layers[vkey], genes)
             if "velocity_P" in adata.obsm.keys():
                 P_vec = index_gene(adata, adata.layers["velocity_P"], genes)
         else:
-            raise Exception("adata has no vkey {} in either the layers or the obsm slot".format(vkey))
+            raise ValueError("adata has no vkey {} in either the layers or the obsm slot".format(vkey))
 
     E_vec, V_vec = (
         E_vec.A if issparse(E_vec) else E_vec,
         V_vec.A if issparse(V_vec) else V_vec,
     )
 
     if k_name in adata.var.columns:
@@ -341,15 +343,15 @@
             index_gene(adata, adata.var.gamma_b.values, genes),
         )
         (
             gamma[~np.isfinite(list(gamma))],
             velocity_offset[~np.isfinite(list(velocity_offset))],
         ) = (0, 0)
     else:
-        raise Exception(
+        raise ValueError(
             "adata does not seem to have velocity_gamma column. Velocity estimation is required before "
             "running this function."
         )
 
     if mode == "labeling":
         new_mat, tot_mat = (
             index_gene(adata, adata.layers[mapper["X_new"]], genes),
@@ -435,15 +437,15 @@
                 gamma_P = adata.var.delta[genes].values
                 velocity_offset_P = (
                     [0] * n_cells
                     if (not ("delta_b" in adata.var.columns) or adata.var.delta_b.unique() is None)
                     else adata.var.delta_b[genes].values
                 )
             else:
-                raise Exception(
+                raise ValueError(
                     "adata does not seem to have velocity_gamma column. Velocity estimation is required before "
                     "running this function."
                 )
 
             P = (
                 index_gene(adata, adata.obsm[mapper["X_protein"]], genes)
                 if (["X_protein"] in adata.obsm.keys() or [mapper["X_protein"]] in adata.obsm.keys())
@@ -495,15 +497,15 @@
                     "color": np.repeat(color_vec, n_genes),
                     "vel_u": vel_u.flatten(),
                     "vel_s": vel_s.flatten(),
                 },
                 index=range(n_cells * n_genes),
             )
     else:
-        raise Exception(
+        raise ValueError(
             "Your adata is corrupted. Make sure that your layer has keys new, old for the labelling mode, "
             "spliced, ambiguous, unspliced for the splicing model and uu, ul, su, sl for the full mode"
         )
 
     num_per_gene = 6 if ("protein" in adata.obsm.keys() and mode == "full") else 3
     ncols = min([num_per_gene, ncols]) if ncols is not None else num_per_gene
     nrow, ncol = int(np.ceil(num_per_gene * n_genes / ncols)), ncols
@@ -1058,110 +1060,109 @@
             ax6.set_title(gn + " (protein velocity)")
             if show_arrowed_spines_:
                 ax6 = arrowed_spines(ax6, basis)
             else:
                 despline_all(ax6)
                 deaxis_all(ax6)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "phase_portraits",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
 
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
             plt.tight_layout()
 
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
 
 
 def dynamics(
     adata: AnnData,
-    genes: Union[list],
+    genes: List[str],
     unit: str = "hours",
     log_unnormalized: bool = True,
     y_log_scale: bool = False,
     ci: Optional[str] = None,
     ncols: Optional[int] = None,
-    figsize: Union[list, tuple, None] = None,
+    figsize: Union[List[float], Tuple[float, float], None] = None,
     dpi: Optional[float] = None,
     boxwidth: Optional[float] = None,
     barwidth: Optional[float] = None,
     true_param_prefix: Optional[str] = None,
     show_moms_fit: bool = False,
     show_variance: bool = True,
     show_kin_parameters: bool = True,
-    gene_order: str = "column",
+    gene_order: Literal["column", "row"] = "column",
     font_size_scale: float = 1,
-    save_show_or_return: str = "show",
+    save_show_or_return: Literal["save", "show", "return"] = "show",
     save_kwargs: dict = {},
-):
+) -> Optional[Figure]:
     """Plot the data and fitting of different metabolic labeling experiments.
-    Note that if non-smoothed data was used for kinetic fitting, you often won't see boxplot
-    but only the triangles for the mean values. This is because the raw data has a lot of dropouts,
-    thus the median is outside of the whiskers of the boxplot and the boxplot is then not drawn
-    by default.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        genes: list of `str`
-            key for variable or gene names.
-        unit: `str` (default: `hour`)
-            The unit of the labeling time, for example, `hours` or `minutes`.
-        y_log_scale: `bool` (default: `True`)
-            Whether or not to use log scale for y-axis.
-        ci: `str` (for example: "95%") or None (default: `None`)
-            The confidence interval to be drawed for the parameter fitting. Currently not used.
-        ncols: `int` or None (default: `None`)
-            The number of columns in the plot.
-        figsize: `[float, float]` or `(float, float)` or None
-            The size of the each panel in the figure.
-        dpi: `float` or None
-            Figure resolution.
-        boxwidth: `float`
-            The width of the box of the boxplot.
-        barwidth: `float`
-            The width of the bar of the barplot.
-        true_param_prefix: `str`
-            The prefix for the column names of true parameters in the .var attributes. Useful for the simulation data.
-        show_moms_fit: `bool` (default: `True`)
-            Whether to show fitting curves associated with the stochastic models, only works for non-deterministic models.
-        show_variance: `bool` (default: `True`)
-            Whether to add a boxplot to show the variance at each time point.
-        show_kin_parameters: `bool` (default: `True`)
-            Whether to include the estimated kinetic parameter values on the plot.
-        gene_order: `str` (default: `column`)
-            The order of genes to present on the figure, either row-major or column major.
-        font_size_scale: `float` (default: `1`)
-            A value that will be used for scaling
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'dynamics', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-
-    Returns
-    -------
-        Nothing but plot the figure of the metabolic fitting.
+
+    Note that if non-smoothed data was used for kinetic fitting, you often won't see boxplot but only the triangles for
+    the mean values. This is because the raw data has a lot of dropouts, thus the median is outside of the whiskers of
+    the boxplot and the boxplot is then not drawn by default.
+
+    Args:
+        adata: an Annodata object.
+        genes: the key for variable or gene names.
+        unit: the unit of the labeling time, for example, `hours` or `minutes`. Defaults to "hours".
+        log_unnormalized: whether the data has logged value. Defaults to True.
+        y_log_scale: whether to use log scale for y-axis. Defaults to False.
+        ci: the confidence interval to be drawn for the parameter fitting. Currently not used. Defaults to None.
+        ncols: the number of columns in the plot. Defaults to None.
+        figsize: the size of the each panel in the figure. Defaults to None.
+        dpi: the resolution of the figure. Defaults to None.
+        boxwidth: the width of the box of the boxplot. Defaults to None.
+        barwidth: the width of the bar of the barplot. Defaults to None.
+        true_param_prefix: the prefix for the column names of true parameters in the .var attributes. Useful for the
+            simulation data. Defaults to None.
+        show_moms_fit: whether to show fitting curves associated with the stochastic models, only works for
+            non-deterministic models. Defaults to False.
+        show_variance: whether to add a boxplot to show the variance at each time point. Defaults to True.
+        show_kin_parameters: whether to include the estimated kinetic parameter values on the plot. Defaults to True.
+        gene_order: the order of genes to present on the figure, either row-major or column major. Defaults to "column".
+        font_size_scale: the scale factor of fonts. Defaults to 1.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the
+            {
+                "path": None,
+                "prefix": 'dynamics',
+                "dpi": None,
+                "ext": 'pdf',
+                "transparent": True,
+                "close": True,
+                "verbose": True
+            }
+            as its parameters. Otherwise, you can provide a dictionary that properly modify those keys according to your
+            needs. Defaults to {}.
+
+    Raises:
+        ValueError: the gene specified does not have fitted kinetic parameters.
+
+    Returns:
+        None would be returned in default and the plotted metabolic fitting figure would be shown directly. If set
+        `save_show_or_return='return'` as a kwarg, the axes of the plot would be returned.
     """
 
     import matplotlib
     import matplotlib.pyplot as plt
 
     params = {
         "font.size": 4 * font_size_scale,
@@ -1177,17 +1178,15 @@
         "axes.spines.top": False,
     }
     matplotlib.rcParams.update(params)
 
     show_kin_parameters = True if true_param_prefix else show_kin_parameters
 
     uns_keys = np.array(adata.uns_keys())
-    tmp = np.array([i.split("_dynamics")[0] if i.endswith("_dynamics") else None for i in uns_keys])
-    tmp1 = [False if i is None else True for i in tmp]
-    group = tmp[tmp1][0] if sum(tmp1) > 0 else None
+    group = next((i.split("_dynamics")[0] for i in uns_keys if i.endswith("_dynamics")), None)
 
     if group is not None:
         uns_key = group + "_dynamics"
         _group, grp_len = (
             np.unique(adata.obs[group]),
             len(np.unique(adata.obs[group])),
         )
@@ -2725,30 +2724,34 @@
                         ax.set_xlabel("time (" + unit + ")")
                         ax.set_title(gene_name + ": " + title_[j])
                 elif experiment_type == "multi_time_series":
                     pass  # group by different groups
                 elif experiment_type == "coassay":
                     pass  # show protein velocity (steady state and the Gamma distribution model)
     # g.autofmt_xdate(rotation=-30, ha='right')
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "dynamics",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
+
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
 
 
 def dynamics_(
     adata,
     gene_names,
     color,
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/fate.py` & `dynamo-release-1.3.0/dynamo/plot/fate.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,145 +1,147 @@
-from typing import List, NamedTuple, Optional, Union
+from typing import Any, Dict, Optional, Tuple
 
-import matplotlib
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
+import matplotlib.pyplot as plt
 import pandas as pd
 import seaborn as sns
 from anndata import AnnData
+from matplotlib.axes import Axes
 
 from ..prediction.fate import fate_bias as fate_bias_pd
 from ..tools.utils import update_dict
 from .scatters import save_fig, scatters
 from .utils import map2color
 
 
 def fate_bias(
     adata: AnnData,
     group: str,
-    basis: Union[str, None] = "umap",
-    fate_bias_df: Union[pd.DataFrame, None] = None,
-    figsize: tuple = (6, 4),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    **cluster_maps_kwargs: dict
-):
+    basis: str = "umap",
+    fate_bias_df: Optional[pd.DataFrame] = None,
+    figsize: Tuple[float, float] = (6, 4),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    **cluster_maps_kwargs
+) -> Optional[Axes]:
     """Plot the lineage (fate) bias of cells states whose vector field trajectories are predicted.
 
     This function internally calls `dyn.tl.fate_bias` to calculate fate bias dataframe. You can also visualize the data
     frame via pandas stlying (https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html), for example:
-
         >>> df = dyn.vf.fate_bias(adata)
         >>> df.style.background_gradient(cmap='viridis')
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the predicted fate trajectories in the `uns` attribute.
-        group: `str`
-            The column key that corresponds to the cell type or other group information for quantifying the bias of cell
-            state.
-        basis: `str` or None (default: `None`)
-            The embedding data space that cell fates were predicted and cell fates will be quantified.
-        fate_bias_df: `pandas.DataFrame` or None (default: `None`)
-            The DataFrame that stores the fate bias information, calculated via fate_bias_df = dyn.tl.fate_bias(adata).
-        figsize: `None` or `[float, float]` (default: None)
-            The width and height of a figure.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'fate_bias', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        cluster_maps_kwargs:
-            Additional arguments passed to sns.clustermap.
-
-    Returns
-    -------
-        Nothing but plot a heatmap shows the fate bias of each cell state to each of the cell group.
+    Args:
+        adata: the AnnData object that contains the predicted fate trajectories in the `uns` attribute.
+        group: the column key that corresponds to the cell type or other group information for quantifying the bias of
+            cell state.
+        basis: the embedding data space that cell fates were predicted and cell fates will be quantified. Defaults to
+            "umap".
+        fate_bias_df: the DataFrame that stores the fate bias information. If None, it would be calculated via
+             fate_bias_df = dyn.tl.fate_bias(adata). Defaults to None.
+        figsize: the size of the figure. Defaults to (6, 4).
+        save_show_or_return: whether to save, show or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the
+                {
+                    "path": None,
+                    "prefix": 'phase_portraits',
+                    "dpi": None,
+                    "ext": 'pdf',
+                    "transparent": True,
+                    "close": True,
+                    "verbose": True
+                }
+            as its parameters. Otherwise, you can provide a dictionary that properly modify those keys according to
+            your needs. Defaults to {}.
+        **cluster_map_kwargs: any other kwargs to be passed to `seaborn.clustermap`.
+
+    Returns:
+        None would be returned by default and the heatmap showing the fate bias of each cell state to each of the cell
+            group would be shown. If `save_show_or_return` is set to be `return`, the matplotlib axis of the plot would
+            be returned.
     """
 
-    import matplotlib.pyplot as plt
-
     fate_bias = fate_bias_pd(adata, group=group, basis=basis) if fate_bias_df is None else fate_bias_df
 
     if "confidence" in fate_bias.keys():
         fate_bias.set_index([fate_bias.index, fate_bias.confidence], inplace=True)
 
     ax = sns.clustermap(
         fate_bias, col_cluster=True, row_cluster=True, figsize=figsize, yticklabels=False, **cluster_maps_kwargs
     )
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "fate_bias",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def fate(
     adata: AnnData,
     x: int = 0,
     y: int = 1,
     basis: str = "pca",
     color: str = "ntr",
-    ax: matplotlib.axes.Axes = None,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    **kwargs: dict
-):
+    ax: Optional[Axes] = None,
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    **kwargs
+) -> Optional[Axes]:
     """Draw the predicted integration paths on the low-dimensional embedding.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        basis: `str`
-            The reduced dimension.
-        x: `int` (default: `0`)
-            The column index of the low dimensional embedding for the x-axis.
-        y: `int` (default: `1`)
-            The column index of the low dimensional embedding for the y-axis.
-        color: `string` (default: `ntr`)
-            Any column names or gene expression, etc. that will be used for coloring cells.
-        ax: `matplotlib.Axis` (optional, default `None`)
-            The matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure. If "both", it will save and plot the figure at the same time. If
-            "all", the figure will be saved, displayed and the associated axis and other object will be return.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-        kwargs:
-            Additional arguments passed to pl.scatters or plt.scatters.
-
-    Returns
-    -------
-        result:
-            Either None or a matplotlib axis with the relevant plot displayed.
-            If you are using a notbooks and have ``%matplotlib inline`` set
-            then this will simply display inline.
+    Args:
+        adata: an Annodata object.
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        basis: the basis used for dimension reduction. Defaults to "pca".
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        ax: the matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
+            If None, new axis would be created. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the
+                {
+                    "path": None,
+                    "prefix": 'phase_portraits',
+                    "dpi": None,
+                    "ext": 'pdf',
+                    "transparent": True,
+                    "close": True,
+                    "verbose": True
+                }
+            as its parameters. Otherwise, you can provide a dictionary that properly modify those keys according to
+            your needs. Defaults to {}.
+        **kwargs: any other kwargs to be passed to `dynamo.pl.scatters`.
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be `return`, the matplotlib axis of the
+        plot would be returned.
     """
 
-    import matplotlib.pyplot as plt
-
     ax = scatters(adata, basis=basis, color=color, save_show_or_return="return", ax=ax, **kwargs)
 
     fate_key = "fate" if basis is None else "fate_" + basis
     lap_dict = adata.uns[fate_key]
 
     for i, j in zip(lap_dict["prediction"], lap_dict["t"]):
         ax.scatter(*i[:, [x, y]].T, c=map2color(j))
@@ -151,15 +153,20 @@
             "prefix": "kinetic_curves",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
+
+        # prevent the plot from being closed if the plot need to be shown or returned.
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
         save_fig(**s_kwargs)
-    elif save_show_or_return in ["show", "both", "all"]:
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return in ["return", "all"]:
+    if save_show_or_return in ["return", "all"]:
         return ax
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/heatmaps.py` & `dynamo-release-1.3.0/dynamo/plot/heatmaps.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,15 +1,23 @@
 import math
 import warnings
 from functools import reduce
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import numpy as np
 import pandas as pd
 import scipy.spatial as ss
 import seaborn
+from anndata import AnnData
+from matplotlib.colors import Colormap
 
 from ..dynamo_logger import main_info, main_info_insert_adata, main_warning
 from ..estimation.fit_jacobian import (
     fit_hill_grad,
     hill_act_func,
     hill_act_grad,
     hill_inh_func,
@@ -33,15 +41,16 @@
 )
 
 
 def bandwidth_nrd(x):
     x = pd.Series(x)
     h = (x.quantile([0.75]).values - x.quantile([0.25]).values) / 1.34
 
-    return 4 * 1.06 * min(math.sqrt(np.var(x, ddof=1)), h) * (len(x) ** (-1 / 5))
+    res = 4 * 1.06 * min(math.sqrt(np.var(x, ddof=1)), h) * (len(x) ** (-1 / 5))
+    return np.asscalar(res) if isinstance(res, np.ndarray) else res
 
 
 def rep(x, length):
     len_x = len(x)
     n = int(length / len_x)
     r = length % len_x
     re = []
@@ -64,75 +73,83 @@
     return np.tile(x, length_out // len(x) + 1)[:length_out]
 
 
 def dnorm(x, u=0, sig=1):
     return np.exp(-((x - u) ** 2) / (2 * sig**2)) / (math.sqrt(2 * math.pi) * sig)
 
 
-def kde2d(x, y, h=None, n=25, lims=None):
+def kde2d(
+    x: List[float], y: List[float], h: Optional[List[float]] = None, n: int = 25, lims: Optional[List[float]] = None
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
     """Reproduce kde2d function behavior from MASS package in R.
-    Two-dimensional kernel density estimation with an axis-aligned
-    bivariate normal kernel, evaluated on a square grid.
 
-    Arguments
-    ---------
-        x:  `List`
-            x coordinate of data
-        y:  `List`
-            y coordinate of data
-        h:  `List` (Default: None)
-            vector of bandwidths for :math:`x` and :math:`y` directions.  Defaults to normal reference bandwidth
-            (see `bandwidth.nrd`). A scalar value will be taken to apply to both directions.
-        n: `int` (Default: 25)
-            Number of grid points in each direction.  Can be scalar or a length-2 integer list.
-        lims: `List` (Default: None)
-            The limits of the rectangle covered by the grid as :math:`x_l, x_u, y_l, y_u`.
-
-    Returns
-    -------
-        A list of three components
-        gx, gy: `List`
-            The x and y coordinates of the grid points, lists of length `n`.
-        z:  `List`
-            An :math:`n[1]` by :math:`n[2]` matrix of the estimated density: rows correspond to the value of :math:`x`,
-            columns to the value of :math:`y`.
+    Two-dimensional kernel density estimation with an axis-aligned bivariate normal kernel, evaluated on a square grid.
+
+    Args:
+        x: x coordinate of the data.
+        y: y coordinate of the data.
+        h: vector of bandwidths for `x` and `y` directions. if None, it would use normal reference bandwidth
+            (see `bandwidth.nrd`). A scalar value will be taken to apply to both directions. Defaults to None.
+        n: number of grid points in each direction. Can be scalar or a length-2 integer list. Defaults to 25.
+        lims: the limits of the rectangle covered by the grid as `x_l, x_u, y_l, y_u`. Defaults to None.
+
+    Raises:
+        ValueError: `x` and `y` have different sizes.
+        ValueError: `x` or `y` has non-valid values.
+        ValueError: `lims` has non-valid values.
+        ValueError: `h` has non-positive values.
+
+    Returns:
+        A tuple (gx, gy, z) where `gx` and `gy` are x and y coordinates of grid points, respectively; `z` is a `n[1]`
+        by `n[2]` matrix of estimated density: its rows correspond to the value of `x` and columns to the value of `y`.
     """
+
     nx = len(x)
     if not lims:
         lims = [min(x), max(x), min(y), max(y)]
     if len(y) != nx:
-        raise Exception("data vectors must be the same length")
-    elif (False in np.isfinite(x)) or (False in np.isfinite(y)):
-        raise Exception("missing or infinite values in the data are not allowed")
-    elif False in np.isfinite(lims):
-        raise Exception("only finite values are allowed in 'lims'")
+        raise ValueError("data vectors must be the same length")
+    elif not np.all(np.isfinite(x)) or not np.all(np.isfinite(y)):
+        raise ValueError("missing or infinite values in the data are not allowed")
+    elif not np.all(np.isfinite(lims)):
+        raise ValueError("only finite values are allowed in 'lims'")
     else:
         n = rep(n, length=2) if isinstance(n, list) else rep([n], length=2)
         gx = np.linspace(lims[0], lims[1], n[0])
         gy = np.linspace(lims[2], lims[3], n[1])
         if h is None:
             h = [bandwidth_nrd(x), bandwidth_nrd(y)]
         else:
             h = np.array(rep(h, length=2))
 
-        if h[0] <= 0 or h[1] <= 0:
-            raise Exception("bandwidths must be strictly positive")
+        if np.any(h <= 0):
+            raise ValueError("bandwidths must be strictly positive")
         else:
             h /= 4
-            ax = pd.DataFrame()
-            ay = pd.DataFrame()
-            for i in range(len(x)):
-                ax[i] = (gx - x[i]) / h[0]
-            for i in range(len(y)):
-                ay[i] = (gy - y[i]) / h[1]
+            ax = pd.DataFrame((gx - x[:, np.newaxis]) / h[0]).T
+            ay = pd.DataFrame((gy - y[:, np.newaxis]) / h[1]).T
             z = (np.matrix(dnorm(ax)) * np.matrix(dnorm(ay).T)) / (nx * h[0] * h[1])
     return gx, gy, z
 
 
-def kde2d_to_mean_and_sigma(gx, gy, dens):
+def kde2d_to_mean_and_sigma(
+    gx: np.ndarray, gy: np.ndarray, dens: np.ndarray
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """Calculate the mean and sigma of y grids corresponding to x grids from kde2d.
+
+    Args:
+        gx: x coordinates of grid points.
+        gy: y coordinates of grid points.
+        dens: estimated kernel density.
+
+    Returns:
+        A tuple (x_grid, y_mean, y_sigm) where x_grid is unique values of `gx`, `y_mean` and `y_sigm` are corresponding
+        mean and sigma of y grids.
+    """
+
     x_grid = np.unique(gx)
     y_mean = np.zeros(len(x_grid))
     y_sigm = np.zeros(len(x_grid))
     for i, x in enumerate(x_grid):
         mask = gx == x
         den = dens[mask]
         Y_ = gy[mask]
@@ -140,130 +157,149 @@
         sigm = np.sqrt(np.average((Y_ - mean) ** 2, weights=den))
         y_mean[i] = mean
         y_sigm[i] = sigm
     return x_grid, y_mean, y_sigm
 
 
 def response(
-    adata,
-    pairs_mat,
-    xkey=None,
-    ykey=None,
-    log=True,
-    drop_zero_cells=True,
-    delay=0,
-    grid_num=25,
-    n_row=1,
-    n_col=None,
-    cmap=None,
-    show_ridge=False,
-    show_rug=True,
-    zero_indicator=False,
-    zero_line_style="w--",
-    zero_line_width=2.5,
-    mean_style="c*",
-    fit_curve=False,
-    fit_mode="hill",
-    curve_style="c-",
-    curve_lw=2.5,
-    no_degradation=True,
-    show_extent=False,
-    ext_format=None,
-    stacked_fraction=False,
-    figsize=(6, 4),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    return_data=False,
-):
+    adata: AnnData,
+    pairs_mat: np.ndarray,
+    xkey: Optional[str] = None,
+    ykey: Optional[str] = None,
+    log: bool = True,
+    drop_zero_cells: bool = True,
+    delay: int = 0,
+    grid_num: int = 25,
+    n_row: int = 1,
+    n_col: Optional[int] = None,
+    cmap: Union[str, Colormap, None] = None,
+    show_ridge: bool = False,
+    show_rug: bool = True,
+    zero_indicator: bool = False,
+    zero_line_style: str = "w--",
+    zero_line_width: float = 2.5,
+    mean_style: str = "c*",
+    fit_curve: bool = False,
+    fit_mode: Literal["hill"] = "hill",
+    curve_style: str = "c-",
+    curve_lw: float = 2.5,
+    no_degradation: bool = True,
+    show_extent: bool = False,
+    ext_format: Optional[List[str]] = None,
+    stacked_fraction: bool = False,
+    figsize: Tuple[float, float] = (6, 4),
+    save_show_or_return: Literal["save", "show", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    return_data: bool = False,
+) -> Union[
+    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame],
+    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict[str, Dict[str, Any]]],
+    None,
+]:
     """Plot the lagged DREVI plot pairs of genes across pseudotime.
+
     This plotting function builds on the original idea of DREVI plot but is extended in the context for causal network.
-    It considers the time delay between the hypothetical regulators to the target genes which is parametered by :math:`d`.
-    Lagged DREVI plot first estimates the joint density (:math:`P(x_{t - d}, y_t)`) for variables :math:`x_{t - d} and y_t`, then it
-    divides the joint density by the marginal density :math:`P(x_{t - d})` to get the conditional density estimate
-    (:math:`P(x_{t - d}, y_t | x_{x - d})`). We then calculate the z-score normalizing each column of conditional density. Note
-    that this plot tries to demonstrate the potential influence between two variables instead of the factual influence.
-    A red line corresponding to the point with maximal density on each :math:`x` value is plot which indicates the maximal possible
-    point for :math:`y_t` give the value of :math:`x_{t - d}`. The 2-d density is estimated through the kde2d function.
-
-    Arguments
-    ---------
-        adata: `Anndata`
-            Annotated Data Frame, an Anndata object.
-        pairs_mat: 'np.ndarray'
-            A matrix where each row is the gene pair and the first column is the hypothetical source or regulator while
-            the second column represents the hypothetical target. The name in this matrix should match the name in the
-            gene_short_name column of the adata object.
-        log: `bool` (Default: True)
-            A logic argument used to determine whether or not you should perform log transformation (using :math:`log(expression + 1)`)
-            before calculating density estimates, default to be TRUE.
-        drop_zero_cells: `bool` (Default: True)
-            Whether to drop cells that with zero expression for either the potential regulator or potential target. This
-            can signify the relationship between potential regulators and targets, speed up the calculation, but at the risk
-            of ignoring strong inhibition effects from certain regulators to targets.
-        delay: `int` (Default: 0)
-            The time delay between the source and target gene. Always zero because we don't have real time-series.
-        k: `int` (Default: 5)
-            Number of k-nearest neighbors used in calculating 2-D kernel density
-        grid_num: `int` (Default: 25)
-            The number of grid when creating the lagged DREVI plot.
-        n_row: `int` (Default: None)
-            number of columns used to layout the faceted cluster panels.
-        n_col: `int` (Default: 1)
-            number of columns used to layout the faceted cluster panels.
-        ext_format: None or `str` or List[str]
-            The string/list of strings (the first is for x and second for y labels) that will be used to format the ticks
-            on x or y-axis. If it is None or one of the element in the list is None, the default setting will be used.
-        stacked_fraction: bool (default: False)
-            If True the jacobian will be represented as a stacked fraction in the title, otherwise a linear fraction
-            style is used.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure. If "both", it will save and plot the figure at the same time. If
+    It considers the time delay between the hypothetical regulators to the target genes which is parametered by `d`.
+    Lagged DREVI plot first estimates the joint density (`P(x_{t - d}, y_t)`) for variables `x_{t - d} and y_t`, then it
+    divides the joint density by the marginal density `P(x_{t - d})` to get the conditional density estimate
+    (`P(x_{t - d}, y_t | x_{x - d})`). We then calculate the z-score normalizing each column of conditional density.
+    Note that this plot tries to demonstrate the potential influence between two variables instead of the factual
+    influence. A red line corresponding to the point with maximal density on each `x` value is plot which indicates the
+    maximal possible point for `y_t` give the value of `x_{t - d}`. The 2-d density is estimated through the kde2d
+    function.
+
+    Args:
+        adata: an AnnData object.
+        pairs_mat: a matrix where each row is the gene pair and the first column is the hypothetical source or regulator
+            while the second column represents the hypothetical target. The name in this matrix should match the name in
+            the gene_short_name column of the adata object.
+        xkey: the key of the layer storing `x` data. Defaults to None.
+        ykey: the key of the layer storing `y` data. Defaults to None.
+        log: whether to perform log transformation (using `log(expression + 1)`) before calculating density estimates.
+            Defaults to True.
+        drop_zero_cells: whether to drop cells that with zero expression for either the potential regulator or potential
+            target. This can signify the relationship between potential regulators and targets, speed up the
+            calculation, but at the risk of ignoring strong inhibition effects from certain regulators to targets.
+            Defaults to True.
+        delay: the time delay between the source and target gene. Always zero because we don't have real time-series.
+            Defaults to 0.
+        grid_num: the number of grid when creating the lagged DREVI plot. Defaults to 25.
+        n_row: number of rows used to layout the faceted cluster panels. Defaults to 1.
+        n_col: number of columns used to layout the faceted cluster panels. Defaults to None.
+        cmap: the color map used to plot the heatmap. Could be the name of the color map or a matplotlib color map
+            object. If None, the color map would be generated automatically. Defaults to None.
+        show_ridge: whether to show the ridge curve. Defaults to False.
+        show_rug: whether to plot marginal distributions by drawing ticks along the x and y axes. Defaults to True.
+        zero_indicator: whether to plot the zero line in the graph. Defaults to False.
+        zero_line_style: line style of the zero line. Defaults to "w--".
+        zero_line_width: line width of the zero line. Defaults to 2.5.
+        mean_style: the line style for plotting the y mean data. Defaults to "c*".
+        fit_curve: whether to fit the curve between `x` and `y`. Defaults to False.
+        fit_mode: the fitting mode for fitting the curve between `x` and `y`. Currently, the relationship can only be
+            fit into Hill function. Defaults to "hill".
+        curve_style: the line style of fitted curve. Defaults to "c-".
+        curve_lw: the line width of the fitted curve. Defaults to 2.5.
+        no_degradation: whether to consider degradation when fitting Hill equations. Defaults to True.
+        show_extent: whether to extend the figure. If False, `show_ridge` and `show_rug` would be set to False
+            automatically. Defaults to False.
+        ext_format: the string/list of strings (the first is for x and second for y labels) that will be used to format
+            the ticks on x or y-axis. If it is None or one of the element in the list is None, the default setting will
+            be used. Defaults to None.
+        stacked_fraction: If True the jacobian will be represe nted as a stacked fraction in the title, otherwise a
+            linear fraction tyle is used. Defaults to False.
+        figsize: size of the figure. Defaults to (6, 4).
+        save_show_or_return: whether to save or show the figure. If "both", it will save and plot the figure at the same time. If
             "all", the figure will be saved, displayed and the associated axis and other object will be return.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-
-    Returns
-    -------
-        In addition to figure created by matplotlib, it also returns:
-        flat_res: 'pd.core.frame.DataFrame'
-            a pandas data frame used to create the heatmap with four columns (`x`: x-coordinate; `y`: y-coordinate; `den`:
-            estimated density at x/y coordinate; `type`: the corresponding gene pair).
-        flat_res_subset: 'pd.core.frame.DataFrame'
-            a pandas data frame used to create the heatmap for the last gene pair (if multiple gene-pairs are inputted) with
-            four columns (`x`: x-coordinate; `y`: y-coordinate; `den`: estimated density at x/y coordinate; `type`: the
-            corresponding gene pair).
-        ridge_curve_subset: 'pd.core.frame.DataFrame'
-            a pandas data frame used to create the read ridge line for the last gene pair (if multiple gene-pairs are inputted) with
-            four columns (`x`: x-coordinate; `y`: y-coordinate; `type`: the corresponding gene pair).
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        return_data: whether to return the data used to generate the heatmap. Defaults to False.
+
+    Raises:
+        ValueError: no preprocessing data in adata.uns
+        ValueError: No layers named as `xkey` or `ykey`.
+        ValueError: adata does not contain gene data specified in pairs_mat
+        ValueError: `n_col * n_row` is less than number of gene pairs
+        NotImplementedError: invalid `fit_mode`
+
+    Returns:
+        None would be returned in default. If `return_data` is set to be True, a tuple (flat_res, flat_res_subset,
+        ridge_curve_subset) would be returned, where flat_res is a pandas data frame used to create the
+        heatmap with four columns (`x`: x-coordinate; `y`: y-coordinate; `den`: estimated density at x/y coordinate;
+        `type`: the corresponding gene pair), flat_res_subset is a pandas data frame used to create the heatmap for the
+        last gene pair (if multiple gene-pairs are inputted) with four columns (`x`: x-coordinate; `y`: y-coordinate;
+        `den`: estimated density at x/y coordinate; `type`: the corresponding gene pair), and ridge_curve_subset is a
+        pandas data frame used to create the read ridge line for the last gene pair (if multiple gene-pairs are
+        inputted) with four columns (`x`: x-coordinate; `y`: y-coordinate; `type`: the corresponding gene pair). If
+        `fit_curve` is True, the hill function fitting result would also be returned at the 4th position.
     """
+
     import matplotlib
     import matplotlib.pyplot as plt
     from matplotlib.colors import ListedColormap
     from matplotlib.ticker import MaxNLocator
     from mpl_toolkits.axes_grid1.inset_locator import inset_axes
 
     if show_extent is False:
         show_ridge = False
         show_rug = False
 
     all_genes_in_pair = np.unique(pairs_mat)
 
     if "pp" not in adata.uns_keys():
-        raise Exception("You must first run dyn.pp.recipe_monocle and dyn.tl.moments before running this function.")
+        raise ValueError("You must first run dyn.pp.recipe_monocle and dyn.tl.moments before running this function.")
 
     if xkey is None:
         xkey = "M_t" if adata.uns["pp"]["has_labeling"] else "M_s"
     if ykey is None:
         ykey = "M_n" if adata.uns["pp"]["has_labeling"] else "M_u"
 
     if not set([xkey, ykey]) <= set(adata.layers.keys()).union(set(["jacobian"])):
-        raise Exception(
+        raise ValueError(
             f"adata.layers doesn't have {xkey, ykey} layers. Please specify the correct layers or "
             "perform relevant preprocessing and vector field analyses first."
         )
     if cmap is None:
         cmap = matplotlib.colors.LinearSegmentedColormap.from_list(
             "response", ["#000000", "#000000", "#000000", "#800080", "#FF0000", "#FFFF00"]
         )
@@ -272,15 +308,15 @@
         "height": "50%",  # height : 50%
         "loc": "lower left",
         "bbox_to_anchor": (1.0125, 0.0, 1, 1),
         "borderpad": 0,
     }
 
     if not (set(all_genes_in_pair) <= set(adata.var_names)):
-        raise Exception(
+        raise ValueError(
             "adata doesn't include all genes in gene_pairs_mat. Make sure all genes are included in adata.var_names."
         )
 
     flat_res = pd.DataFrame(columns=["x", "y", "den", "type"])
     ridge_curve = pd.DataFrame(columns=["x", "y", "type"])
     xy = pd.DataFrame()
 
@@ -387,15 +423,15 @@
         id = id + 1
 
     gene_pairs_num = len(flat_res.type.unique())
 
     n_col = gene_pairs_num if n_col is None else n_col
 
     if n_row * n_col < gene_pairs_num:
-        raise Exception("The number of row or column specified is less than the gene pairs")
+        raise ValueError("The number of row or column specified is less than the gene pairs")
     figsize = (figsize[0] * n_col, figsize[1] * n_row) if figsize is not None else (4 * n_col, 4 * n_row)
     fig, axes = plt.subplots(n_row, n_col, figsize=figsize, sharex=False, sharey=False, squeeze=False)
 
     fit_dict = None
     if fit_curve:
         fit_dict = {}
 
@@ -523,14 +559,16 @@
                         xplot,
                         scale_func(func(xs, pdict["A"], pdict["K"], pdict["n"], pdict["g"]), ylabels, grid_num),
                         curve_style,
                         linewidth=curve_lw,
                     )
                 else:
                     raise NotImplementedError("The hill function can be applied to the Jacobian response heatmap only.")
+            else:
+                raise NotImplementedError("Currently, the fit_mode can only be served by the hill function.")
 
         # set the x/y ticks
         inds = np.linspace(0, grid_num - 1, 5, endpoint=True)
         axes[i, j].set_xticks(inds)
         axes[i, j].set_yticks(inds)
 
         if ext_format is None:
@@ -567,99 +605,137 @@
             axes[i, j].set_xticklabels(xlabels, rotation=30, ha="right")
         else:
             axes[i, j].set_xticklabels(xlabels)
 
         axes[i, j].set_yticklabels(ylabels)
 
     plt.subplots_adjust(left=0.1, right=1, top=0.80, bottom=0.1, wspace=0.1)
-    if save_show_or_return in ["save", "both", "all"]:
+    if save_show_or_return in ["save", "both"]:
         s_kwargs = {
             "path": None,
             "prefix": "scatters",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        # prevent the plot from being closed if the plot need to be shown or returned.
+        if save_show_or_return == "both":
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return in ["show", "both", "all"]:
+    if save_show_or_return in ["show", "both"]:
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
             plt.tight_layout()
 
         plt.show()
 
+    list_for_return = []
+
     if return_data:
         if fit_dict is None:
-            return (flat_res, flat_res_subset, ridge_curve_subset)
+            list_for_return += [flat_res, flat_res_subset, ridge_curve_subset]
         else:
-            return (flat_res, flat_res_subset, ridge_curve_subset, fit_dict)
+            list_for_return += [flat_res, flat_res_subset, ridge_curve_subset, fit_dict]
     else:
         adata.uns["response"] = {
             "flat_res": flat_res,
             "flat_res_subset": flat_res_subset,
             "ridge_curve_subset": ridge_curve_subset,
         }
         if fit_dict is not None:
             adata.uns["response"]["fit_curve"] = fit_dict
 
+    if list_for_return:
+        return tuple(list_for_return)
+
 
 def plot_hill_function(
-    adata,
-    pairs_mat=None,
-    normalize=True,
-    n_row=1,
-    n_col=None,
-    figsize=(6, 4),
-    linewidth=2,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
+    adata: AnnData,
+    pairs_mat: Optional[np.ndarray] = None,
+    normalize: bool = True,
+    n_row: int = 1,
+    n_col: Optional[int] = None,
+    figsize: Tuple[float, float] = (6, 4),
+    linewidth: float = 2,
+    save_show_or_return: Literal["save", "show", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **plot_kwargs,
-):
+) -> None:
+    """Plot the hill function curve generated by `dynamo.pl.response`.
 
+    Args:
+        adata: an AnnData object.
+        pairs_mat: a matrix where each row is the gene pair and the first column is the hypothetical source or regulator
+            while the second column represents the hypothetical target. The name in this matrix should match the name in
+            the gene_short_name column of the adata object. If None, all gene pairs in
+            `adata.uns["response"]["fit_curve"]` would be used. Defaults to None.
+        normalize: whether to normalize the curve. Defaults to True.
+        n_row: the number of subplot rows. Defaults to 1.
+        n_col: the number of subplot cols. If None, it would be calculated to show all subplots. Defaults to None.
+        figsize: the size of the figure. Defaults to (6, 4).
+        linewidth: the line width of the curve. Defaults to 2.
+        save_show_or_return: whether to save or show the figure. Could be one of "save", "show", "both", or "all".
+            "both" and "all" have the same effect. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        **plot_kwargs: any other kwargs passed to `pyplot.plot`.
+
+    Raises:
+        ValueError: no `"response"` data found in `adata.uns`.
+        ValueError: no `"fit_curve"` data found in `adata.uns["response"]`.
+        ValueError: adata does not contain gene data specified in pairs_mat
+        ValueError: `n_col * n_row` is less than the number of gene pairs.
+        ValueError: the gene specified in `pairs_mat` is not found in `adata.uns["response"]["fit_curve"]`.
+        NotImplementedError: invalid `fit_type` in `adata.uns["response"]["fit_curve"]`.
+        NotImplementedError: invalid `mode` in `adata.uns["response"]["fit_curve"]`.
+    """
     import matplotlib.pyplot as plt
 
     if "response" not in adata.uns.keys():
-        raise Exception("`response` is not found in `.uns`. Run `pl.response` first.")
+        raise ValueError("`response` is not found in `.uns`. Run `pl.response` first.")
     if "fit_curve" not in adata.uns["response"].keys():
-        raise Exception("`fit_curve` is not found. Run `pl.response` with `fit_curve=True` first.")
+        raise ValueError("`fit_curve` is not found. Run `pl.response` with `fit_curve=True` first.")
 
     fit_dict = adata.uns["response"]["fit_curve"]
     if pairs_mat is None:
         pairs_mat = []
         for pairs in fit_dict.keys():
             genes = pairs.split("_")
             pairs_mat.append([genes[0], genes[1]])
 
     all_genes_in_pair = np.unique(pairs_mat)
 
     if not (set(all_genes_in_pair) <= set(adata.var_names)):
-        raise Exception(
+        raise ValueError(
             "adata doesn't include all genes in gene_pairs_mat. Make sure all genes are included in adata.var_names."
         )
 
     gene_pairs_num = len(pairs_mat)
     n_col = gene_pairs_num if n_col is None else n_col
 
     if n_row * n_col < gene_pairs_num:
-        raise Exception("The number of row or column specified is less than the gene pairs")
+        raise ValueError("The number of row or column specified is less than the gene pairs")
     figsize = (figsize[0] * n_col, figsize[1] * n_row) if figsize is not None else (4 * n_col, 4 * n_row)
     fig, axes = plt.subplots(n_row, n_col, figsize=figsize, sharex=False, sharey=False, squeeze=False)
 
     for pair_i, gene_pairs in enumerate(pairs_mat):
         i, j = pair_i % n_row, pair_i // n_row  # %: remainder; //: integer division
 
         gene_pair_name = gene_pairs[0] + "->" + gene_pairs[1]
 
         key = f"{gene_pairs[0]}_{gene_pairs[1]}"
         if key not in fit_dict.keys():
-            raise Exception(f"The gene pair {key} is not found in the dictionary.")
+            raise ValueError(f"The gene pair {key} is not found in the dictionary.")
 
         mode = fit_dict[key]["mode"]
         x_grid = fit_dict[key]["x_grid"]
         fit_type = fit_dict[key]["type"]
         A, K, n, g = (
             fit_dict[key]["param"]["A"],
             fit_dict[key]["param"]["K"],
@@ -682,131 +758,137 @@
             axes[i, j].set_xlabel(gene_pairs[0])
             axes[i, j].set_ylabel(r"$f_{%s}$" % gene_pairs[1])
             axes[i, j].set_title(gene_pair_name)
         else:
             raise NotImplementedError(f"The fit mode `{mode}` is not supported.")
 
     plt.subplots_adjust(left=0.1, right=1, top=0.80, bottom=0.1, wspace=0.1)
-    if save_show_or_return in ["save", "both", "all"]:
+    if save_show_or_return in ["save", "both"]:
         s_kwargs = {
             "path": None,
             "prefix": "scatters",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        # prevent the plot from being closed if the plot need to be shown or returned.
+        if save_show_or_return == "both":
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return in ["show", "both", "all"]:
+    if save_show_or_return in ["show", "both"]:
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
             plt.tight_layout()
 
         plt.show()
 
 
 def causality(
-    adata,
-    pairs_mat,
-    hessian_matrix=False,
-    xkey=None,
-    ykey=None,
-    zkey=None,
-    log=True,
-    drop_zero_cells=False,
-    delay=0,
-    k=30,
-    normalize=True,
-    grid_num=25,
-    n_row=1,
-    n_col=None,
-    cmap="viridis",
-    show_rug=True,
-    show_extent=False,
-    ext_format=None,
-    stacked_fraction=False,
-    figsize=(6, 4),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    return_data=False,
+    adata: AnnData,
+    pairs_mat: np.ndarray,
+    hessian_matrix: bool = False,
+    xkey: Optional[str] = None,
+    ykey: Optional[str] = None,
+    zkey: Optional[str] = None,
+    log: bool = True,
+    drop_zero_cells: bool = False,
+    delay: int = 0,
+    k: int = 30,
+    normalize: bool = True,
+    grid_num: int = 25,
+    n_row: int = 1,
+    n_col: Optional[int] = None,
+    cmap: Union[str, Colormap, None] = "viridis",
+    show_rug: bool = True,
+    show_extent: bool = False,
+    ext_format: Optional[List[str]] = None,
+    stacked_fraction: bool = False,
+    figsize: Tuple[float, float] = (6, 4),
+    save_show_or_return: Literal["save", "show", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    return_data: bool = False,
     **kwargs,
-):
-    """Plot the heatmap for the expected value :math:`y(t)` given :math:`x(t - d)` and :math:`y(t - 1)`.
-    This plotting function tries to intuitively visualize the informatioin transfer from :math:`x(t - d)` to :math:`y(t)`
-    given :math:`y(t)`'s previous state :math:`y(t - 1)`. Firstly, we divide the expression space for :math:`x(t - d)` to
-    :math:`y(t - 1)` based on grid_num and then we estimate the k-nearest neighbor for each of the grid. We then use a
-    Gaussian kernel to estimate the expected value for :math:`y(t)`. It is then displayed in two dimension with :math:`x(t - d)`
-    and :math:`y(t - 1)` as two axis and the color represents the expected value of :math:`y(t)` give :math:`x(t - d)` and
-    :math:`y(t - 1)`. This function accepts a matrix where each row is the gene pair and the first column is the hypothetical
-    source or regulator while the second column represents the hypothetical target. The name in this matrix should match
-    the name in the gene_short_name column of the cds_subset object.
-
-    Arguments
-    ---------
-        adata: `Anndata`
-            Annotated Data Frame, an Anndata object.
-        hessian_matrix: `bool`
-            Whether to visualize the Hessian matrix results (row, column are the regulatory/co-regulator gene expression
-            while the color is the results of the Hessian to the effector).
-        pairs_mat: 'np.ndarray'
-            A matrix where each row is the gene pair and the first column is the hypothetical source or regulator while
-            the second column represents the hypothetical target. The name in this matrix should match the name in the
-            gene_short_name column of the adata object.
-        log: `bool` (Default: True)
-            A logic argument used to determine whether or not you should perform log transformation (using log(expression + 1))
-            before calculating density estimates, default to be TRUE.
-        drop_zero_cells: `bool` (Default: True)
-            Whether to drop cells that with zero expression for either the potential regulator or potential target. This
-            can signify the relationship between potential regulators and targets, speed up the calculation, but at the risk
-            of ignoring strong inhibition effects from certain regulators to targets.
-        delay: `int` (Default: 1)
-            The time delay between the source and target gene. Always zero because we don't have real time-series.
-        k: `int` (Default: 5)
-            Number of k-nearest neighbors used in calculating 2-D kernel density
-        grid_num: `int` (Default: 25)
-            The number of grid when creating the lagged DREVI plot.
-        n_row: `int` (Default: None)
-            number of columns used to layout the faceted cluster panels.
-        n_col: `int` (Default: 1)
-            number of columns used to layout the faceted cluster panels.
-        ext_format: None or `str` or List[str]
-            The string/list of strings (the first is for x and second for y labels) that will be used to format the ticks
-            on x or y-axis. If it is None or one of the element in the list is None, the default setting will be used.
-        stacked_fraction: bool (default: False)
-            If True the jacobian will be represented as a stacked fraction in the title, otherwise a linear fraction
-            style is used.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure. If "both", it will save and plot the figure at the same time. If
+) -> Optional[pd.DataFrame]:
+    """Plot the heatmap for the expected value `z(t)` given `x` and `y` data.
+
+    Args:
+        adata: an AnnData object.
+        pairs_mat: a matrix where each row is the gene pair and the first column is the hypothetical source or regulator
+            while the second column represents the hypothetical target. The name in this matrix should match the name in
+            the gene_short_name column of the adata object.
+        hessian_matrix: whether to visualize the Hessian matrix results (row, column are the regulatory/co-regulator
+            gene expression while the color is the results of the Hessian to the effector). Defaults to False.
+        xkey: the layer key of `x` data (regulator gene 1). If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        ykey: the layer key of `y` data (regulator gene 2).  If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        zkey: the layer key of `z` data (the hypothetical target).  If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        log: whether to perform log transformation (using log(expression + 1)) before calculating density estimates.
+            Defaults to True.
+        drop_zero_cells: whether to drop cells that with zero expression for either the potential regulator or potential
+            target. This can signify the relationship between potential regulators and targets, speed up the
+            calculation, but at the risk of ignoring strong inhibition effects from certain regulators to targets.
+            Defaults to False.
+        delay: the time delay between the source and target gene. Always zero because we don't have real time-series.
+            Defaults to 0.
+        k: number of k-nearest neighbors used in calculating 2-D kernel density. Defaults to 30.
+        normalize: whether to row-scale the data. Defaults to True.
+        grid_num: the number of grid when creating the lagged DREVI plot. Defaults to 25.
+        n_row: the number of rows used to layout the faceted cluster panels. Defaults to 1.
+        n_col: the number of columns used to layout the faceted cluster panels. Defaults to None.
+        cmap: the color map used to plot the heatmap. Could be the name of the color map or a matplotlib color map
+            object. If None, the color map would be generated automatically. Defaults to "viridis".
+        show_rug: whether to plot marginal distributions by drawing ticks along the x and y axes. Defaults to True.
+        show_extent: whether to extend the figure. If False, `show_ridge` and `show_rug` would be set to False
+            automatically. Defaults to False.
+        ext_format: the string/list of strings (the first is for x and second for y labels) that will be used to format
+            the ticks on x or y-axis. If it is None or one of the element in the list is None, the default setting will
+            be used. Defaults to None.
+        stacked_fraction: if True the jacobian will be represented as a stacked fraction in the title, otherwise a
+            linear fraction style is used. Defaults to False.
+        figsize: the size of the figure. Defaults to (6, 4).
+        save_show_or_return: whether to save or show the figure. If "both", it will save and plot the figure at the same time. If
             "all", the figure will be saved, displayed and the associated axis and other object will be return.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-
-    Returns
-    -------
-        A figure created by matplotlib.
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}. Defaults to {}.
+        return_data: whether to return the calculated causality data. Defaults to False.
+
+    Raises:
+        ValueError: no preprocessing data in adata.uns
+        ValueError: No layers named as `xkey`, `ykey`, or `zkey`.
+        ValueError: adata does not contain gene data specified in pairs_mat
+        ValueError: `n_col * n_row` is less than number of gene pairs
+
+    Returns:
+        None would be returned by default. If `return_data` is set to be True, the causality data, a DataFrame with
+        columns ("x": x coordination, "y": y coordination, "expected_z": expected z coordination, "pair": gene pairs) would
+        be returned.
     """
+
     import matplotlib
     import matplotlib.pyplot as plt
     from matplotlib.colors import ListedColormap
     from matplotlib.ticker import MaxNLocator
     from mpl_toolkits.axes_grid1.inset_locator import inset_axes
 
     if show_extent is False:
         show_rug = False
 
     all_genes_in_pair = np.unique(pairs_mat)
 
     if "pp" not in adata.uns_keys():
-        raise Exception("You must first run dyn.pp.recipe_monocle and dyn.tl.moments before running this function.")
+        raise ValueError("You must first run dyn.pp.recipe_monocle and dyn.tl.moments before running this function.")
 
     if xkey is None:
         xkey = "M_t" if adata.uns["pp"]["has_labeling"] else "M_s"
     if ykey is None:
         ykey = "M_t" if adata.uns["pp"]["has_labeling"] else "M_s"
     if zkey is None:
         if hessian_matrix:
@@ -815,28 +897,28 @@
             zkey = "M_n" if adata.uns["pp"]["has_labeling"] else "M_u"
     if cmap is None:
         cmap = matplotlib.colors.LinearSegmentedColormap.from_list(
             "causality", ["#008000", "#ADFF2F", "#FFFF00", "#FFA500", "#FFC0CB", "#FFFFFE"]
         )
 
     if not set([xkey, ykey, zkey]) <= set(adata.layers.keys()).union(set(["jacobian", "hessian_pca"])):
-        raise Exception(
+        raise ValueError(
             f"adata.layers doesn't have {xkey, ykey, zkey} layers. Please specify the correct layers or "
             "perform relevant preprocessing and vector field analyses first."
         )
 
     inset_dict = {
         "width": "5%",  # width = 5% of parent_bbox width
         "height": "50%",  # height : 50%
         "loc": "lower left",
         "bbox_to_anchor": (1.0125, 0.0, 1, 1),
         "borderpad": 0,
     }
     if not (set(all_genes_in_pair) <= set(adata.var_names)):
-        raise Exception(
+        raise ValueError(
             "adata doesn't include all genes in gene_pairs_mat. Make sure all genes are included in adata.var_names."
         )
 
     flat_res = pd.DataFrame(columns=["x", "y", "expected_z", "pair"])
     xy = pd.DataFrame()
 
     id = 0
@@ -969,15 +1051,15 @@
         id = id + 1
 
     gene_pairs_num = len(flat_res.pair.unique())
 
     n_col = gene_pairs_num if n_col is None else n_col
 
     if n_row * n_col < gene_pairs_num:
-        raise Exception("The number of row or column specified is less than the gene pairs")
+        raise ValueError("The number of row or column specified is less than the gene pairs")
 
     figsize = (figsize[0] * n_col, figsize[1] * n_row) if figsize is not None else (4 * n_col, 4 * n_row)
     fig, axes = plt.subplots(n_row, n_col, figsize=figsize, sharex=False, sharey=False, squeeze=False)
 
     for x, flat_res_type in enumerate(flat_res.pair.unique()):
         gene_pairs = flat_res_type.split("->")
 
@@ -1104,132 +1186,147 @@
             axes[i, j].set_xticklabels(xlabels)
 
         axes[i, j].set_yticklabels(ylabels)
 
     # plt.ticklabel_format(axis="both", style="sci", scilimits=(0, 0))
 
     plt.subplots_adjust(left=0.1, right=1, top=0.80, bottom=0.1, wspace=0.1)
-    if save_show_or_return in ["save", "both", "all"]:
+    if save_show_or_return in ["save", "both"]:
         s_kwargs = {
             "path": None,
             "prefix": "scatters",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        # prevent the plot from being closed if the plot need to be shown or returned.
+        if save_show_or_return == "both":
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return in ["show", "both", "all"]:
+    if save_show_or_return in ["show", "both"]:
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
             plt.tight_layout()
 
         plt.show()
 
     if return_data:
         return flat_res
     else:
         adata.uns[kwargs.pop("save_key", "causality")] = flat_res
 
 
 def comb_logic(
-    adata,
-    pairs_mat,
-    xkey=None,
-    ykey=None,
-    zkey=None,
-    log=True,
-    drop_zero_cells=False,
-    delay=0,
-    grid_num=25,
-    n_row=1,
-    n_col=None,
-    cmap="bwr",
-    normalize=True,
-    k=30,
-    show_rug=True,
-    show_extent=False,
-    ext_format=None,
-    stacked_fraction=False,
-    figsize=(6, 4),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    return_data=False,
-):
-    """Plot the combinatorial influence of two genes :math:`x`, :math:`y` to the target :math:`z`.
-    This plotting function tries to intuitively visualize the influence from genes :math:`x` and :math:`y` to the target :math:`z`.
-    Firstly, we divide the expression space for :math:`x` and :math:`y` based on grid_num and then we estimate the k-nearest neighbor for each of the
-    grid. We then use a Gaussian kernel to estimate the expected value for :math:`z`. It is then displayed in two dimension with :math:`x` and :math:`y`
-    as two axis and the color represents the value of the expected of :math:`z`. This function accepts a matrix where each row is the gene pair
-    and the target genes for this pair. The first column is the first hypothetical source or regulator, the second column represents
-    the second hypothetical target while the third column represents the hypothetical target gene. The name in this matrix should match
-    the name in the gene_short_name column of the cds_subset object.
-
-    Arguments
-    ---------
-        adata: `Anndata`
-            Annotated Data Frame, an Anndata object.
-        pairs_mat: 'np.ndarray'
-            A matrix where each row is the gene pair and the first and second columns are the hypothetical source or regulator while
-            the third column represents the hypothetical target. The name in this matrix should match the name in the
-            gene_short_name column of the adata object.
-        log: `bool` (Default: True)
-            A logic argument used to determine whether or not you should perform log transformation (using log(expression + 1))
-            before calculating density estimates, default to be TRUE.
-        drop_zero_cells: `bool` (Default: True)
-            Whether to drop cells that with zero expression for either the potential regulator or potential target. This
-            can signify the relationship between potential regulators and targets, speed up the calculation, but at the risk
-            of ignoring strong inhibition effects from certain regulators to targets.
-        delay: `int` (Default: 1)
-            The time delay between the source and target gene. Always zero because we don't have real time-series.
-        grid_num: `int` (Default: 25)
-            The number of grid when creating the lagged DREVI plot.
-        n_row: `int` (Default: None)
-            number of columns used to layout the faceted cluster panels.
-        normalize: `bool` (Default: True)
-            Whether to row-scale the data
-        n_col: `int` (Default: 1)
-            number of columns used to layout the faceted cluster panels.
-        ext_format: None or `str` or List[str]
-            The string/list of strings (the first is for x and second for y labels) that will be used to format the ticks
-            on x or y-axis. If it is None or one of the element in the list is None, the default setting will be used.
-        stacked_fraction: bool (default: False)
-            If True the jacobian will be represented as a stacked fraction in the title, otherwise a linear fraction
-            style is used.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure. If "both", it will save and plot the figure at the same time. If
+    adata: AnnData,
+    pairs_mat: np.ndarray,
+    xkey: Optional[str] = None,
+    ykey: Optional[str] = None,
+    zkey: Optional[str] = None,
+    log: bool = True,
+    drop_zero_cells: bool = False,
+    delay: int = 0,
+    grid_num: int = 25,
+    n_row: int = 1,
+    n_col: Optional[int] = None,
+    cmap: Union[str, Colormap, None] = "bwr",
+    normalize: bool = True,
+    k: int = 30,
+    show_rug: bool = True,
+    show_extent: bool = False,
+    ext_format: Optional[List[str]] = None,
+    stacked_fraction: bool = False,
+    figsize: Tuple[float, float] = (6, 4),
+    save_show_or_return: Literal["save", "show", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    return_data: bool = False,
+) -> Optional[pd.DataFrame]:
+    """Plot the combinatorial influence of two genes `x`, `y` to the target `z`.
+
+    This plotting function tries to intuitively visualize the influence from genes `x` and `y` to the target `z`.
+    Firstly, we divide the expression space for `x` and `y` based on grid_num and then we estimate the k-nearest
+    neighbor for each of the grid. We then use a Gaussian kernel to estimate the expected value for `z`. It is then
+    displayed in two dimension with `x` and `y` as two axis and the color represents the value of the expected of `z`.
+    This function accepts a matrix where each row is the gene pair and the target genes for this pair. The first column
+    is the first hypothetical source or regulator, the second column represents the second hypothetical target while the
+    third column represents the hypothetical target gene. The name in this matrix should match the name in the
+    gene_short_name column of the cds_subset object.
+
+    Args:
+        adata: an AnnData object.
+        pairs_mat: a matrix where each row is the gene pair and the first column is the hypothetical source or regulator
+            while the second column represents the hypothetical target. The name in this matrix should match the name in
+            the gene_short_name column of the adata object.
+        xkey: the layer key of `x` data (regulator gene 1). If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        ykey: the layer key of `y` data (regulator gene 2).  If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        zkey: the layer key of `z` data (the hypothetical target).  If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        log: whether to perform log transformation (using log(expression + 1)) before calculating density estimates.
+            Defaults to True.
+        drop_zero_cells: whether to drop cells that with zero expression for either the potential regulator or potential
+            target. This can signify the relationship between potential regulators and targets, speed up the
+            calculation, but at the risk of ignoring strong inhibition effects from certain regulators to targets.
+            Defaults to False.
+        delay: the time delay between the source and target gene. Always zero because we don't have real time-series.
+            Defaults to 0.
+        grid_num: the number of grid when creating the lagged DREVI plot. Defaults to 25.
+        n_row: the number of rows used to layout the faceted cluster panels. Defaults to 1.
+        n_col: the number of columns used to layout the faceted cluster panels. Defaults to None.
+        cmap: the color map used to plot the heatmap. Could be the name of the color map or a matplotlib color map
+            object. If None, the color map would be generated automatically. Defaults to "viridis".
+        normalize: whether to row-scale the data. Defaults to True.
+        k: number of k-nearest neighbors used in calculating 2-D kernel density. Defaults to 30.
+        show_rug: whether to plot marginal distributions by drawing ticks along the x and y axes. Defaults to True.
+        show_extent: whether to extend the figure. If False, `show_ridge` and `show_rug` would be set to False
+            automatically. Defaults to False.
+        ext_format: the string/list of strings (the first is for x and second for y labels) that will be used to format
+            the ticks on x or y-axis. If it is None or one of the element in the list is None, the default setting will
+            be used. Defaults to None.
+        stacked_fraction: if True the jacobian will be represented as a stacked fraction in the title, otherwise a
+            linear fraction style is used. Defaults to False.
+        figsize: the size of the figure. Defaults to (6, 4).
+        save_show_or_return: whether to save or show the figure. If "both", it will save and plot the figure at the same time. If
             "all", the figure will be saved, displayed and the associated axis and other object will be return.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-
-    Returns
-    -------
-        A figure created by matplotlib.
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}. Defaults to {}.
+        return_data: whether to return the calculated causality data. Defaults to False.
+
+    Raises:
+        ValueError: no preprocessing data in adata.uns
+        ValueError: No layers named as `xkey`, `ykey`, or `zkey`.
+
+    Returns:
+        None would be returned by default. If `return_data` is set to be True, the causality data, a DataFrame with
+        columns ("x": x coordination, "y": y coordination, "expected_z": expected z coordination, "pair": gene pairs) would
+        be returned.
     """
+
     import matplotlib
     from matplotlib.colors import ListedColormap
 
     if "pp" not in adata.uns_keys():
-        raise Exception("You must first run dyn.pp.recipe_monocle and dyn.tl.moments before running this function.")
+        raise ValueError("You must first run dyn.pp.recipe_monocle and dyn.tl.moments before running this function.")
 
     if xkey is None:
         xkey = "M_t" if adata.uns["pp"]["has_labeling"] else "M_s"
     if ykey is None:
         ykey = "M_t" if adata.uns["pp"]["has_labeling"] else "M_s"
     if zkey is None:
         zkey = "velocity_T" if adata.uns["pp"]["has_labeling"] else "velocity_S"
 
     if not set([xkey, ykey, zkey]) <= set(adata.layers.keys()).union(set(["jacobian"])):
-        raise Exception(
+        raise ValueError(
             f"adata.layers doesn't have {xkey, ykey, zkey} layers. Please specify the correct layers or "
             "perform relevant preprocessing and vector field analyses first."
         )
 
     if cmap is None:
         cmap = matplotlib.colors.LinearSegmentedColormap.from_list("comb_logic", ["#00CF8D", "#FFFF99", "#FF0000"])
 
@@ -1281,90 +1378,97 @@
             save_kwargs=save_kwargs,
             return_data=return_data,
             save_key="comb_logic",
         )
 
 
 def hessian(
-    adata,
-    pairs_mat,
-    xkey=None,
-    ykey=None,
-    zkey=None,
-    log=True,
-    drop_zero_cells=False,
-    delay=0,
-    grid_num=25,
-    n_row=1,
-    n_col=None,
-    cmap="bwr",
-    normalize=True,
-    k=30,
-    show_rug=True,
-    show_extent=False,
-    ext_format=None,
-    stacked_fraction=False,
-    figsize=(6, 4),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    return_data=False,
-):
-    """Plot the combinatorial influence of two genes :math:`x`, :math:`y` to the target :math:`z`.
-    This plotting function tries to intuitively visualize the influence from genes :math:`x` and :math:`y` to the target :math:`z`.
-    Firstly, we divide the expression space for :math:`x` and :math:`y` based on grid_num and then we estimate the k-nearest neighbor for each of the
-    grid. We then use a Gaussian kernel to estimate the expected value for :math:`z`. It is then displayed in two dimension with :math:`x` and :math:`y`
-    as two axis and the color represents the value of the expected of :math:`z`. This function accepts a matrix where each row is the gene pair
-    and the target genes for this pair. The first column is the first hypothetical source or regulator, the second column represents
-    the second hypothetical target while the third column represents the hypothetical target gene. The name in this matrix should match
-    the name in the gene_short_name column of the cds_subset object.
-
-    Arguments
-    ---------
-        adata: `Anndata`
-            Annotated Data Frame, an Anndata object.
-        pairs_mat: 'np.ndarray'
-            A matrix where each row is the gene pair and the first and second columns are the hypothetical source or regulator while
-            the third column represents the hypothetical target. The name in this matrix should match the name in the
-            gene_short_name column of the adata object.
-        log: `bool` (Default: True)
-            A logic argument used to determine whether or not you should perform log transformation (using log(expression + 1))
-            before calculating density estimates, default to be TRUE.
-        drop_zero_cells: `bool` (Default: True)
-            Whether to drop cells that with zero expression for either the potential regulator or potential target. This
-            can signify the relationship between potential regulators and targets, speed up the calculation, but at the risk
-            of ignoring strong inhibition effects from certain regulators to targets.
-        delay: `int` (Default: 1)
-            The time delay between the source and target gene. Always zero because we don't have real time-series.
-        grid_num: `int` (Default: 25)
-            The number of grid when creating the lagged DREVI plot.
-        n_row: `int` (Default: None)
-            number of columns used to layout the faceted cluster panels.
-        normalize: `bool` (Default: True)
-            Whether to row-scale the data
-        n_col: `int` (Default: 1)
-            number of columns used to layout the faceted cluster panels.
-        ext_format: None or `str` or List[str]
-            The string/list of strings (the first is for x and second for y labels) that will be used to format the ticks
-            on x or y-axis. If it is None or one of the element in the list is None, the default setting will be used.
-        stacked_fraction: bool (default: False)
-            If True the jacobian will be represented as a stacked fraction in the title, otherwise a linear fraction
-            style is used.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure. If "both", it will save and plot the figure at the same time. If
+    adata: AnnData,
+    pairs_mat: np.ndarray,
+    xkey: Optional[str] = None,
+    ykey: Optional[str] = None,
+    zkey: Optional[str] = None,
+    log: bool = True,
+    drop_zero_cells: bool = False,
+    delay: int = 0,
+    grid_num: int = 25,
+    n_row: int = 1,
+    n_col: Optional[int] = None,
+    cmap: Union[str, Colormap, None] = "bwr",
+    normalize: bool = True,
+    k: int = 30,
+    show_rug: bool = True,
+    show_extent: bool = False,
+    ext_format: Optional[List[str]] = None,
+    stacked_fraction: bool = False,
+    figsize: Tuple[float, float] = (6, 4),
+    save_show_or_return: Literal["save", "show", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    return_data: bool = False,
+) -> Optional[pd.DataFrame]:
+    """Plot the combinatorial influence of two genes `x`, `y` to the target `z`.
+
+    This plotting function tries to intuitively visualize the influence from genes `x` and `y` to the target `z`.
+    Firstly, we divide the expression space for `x` and `y` based on grid_num and then we estimate the k-nearest
+    neighbor for each of the grid. We then use a Gaussian kernel to estimate the expected value for `z`. It is then
+    displayed in two dimension with `x` and `y` as two axis and the color represents the value of the expected of `z`.
+    This function accepts a matrix where each row is the gene pair and the target genes for this pair. The first column
+    is the first hypothetical source or regulator, the second column represents the second hypothetical target while the
+    third column represents the hypothetical target gene. The name in this matrix should match the name in the
+    gene_short_name column of the cds_subset object.
+
+    Args:
+        adata: an AnnData object.
+        pairs_mat: a matrix where each row is the gene pair and the first column is the hypothetical source or regulator
+            while the second column represents the hypothetical target. The name in this matrix should match the name in
+            the gene_short_name column of the adata object.
+        xkey: the layer key of `x` data (regulator gene 1). If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        ykey: the layer key of `y` data (regulator gene 2).  If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        zkey: the layer key of `z` data (the hypothetical target).  If None, dynamo's default naming rule would be used.
+            Defaults to None.
+        log: whether to perform log transformation (using log(expression + 1)) before calculating density estimates.
+            Defaults to True.
+        drop_zero_cells: whether to drop cells that with zero expression for either the potential regulator or potential
+            target. This can signify the relationship between potential regulators and targets, speed up the
+            calculation, but at the risk of ignoring strong inhibition effects from certain regulators to targets.
+            Defaults to False.
+        delay: the time delay between the source and target gene. Always zero because we don't have real time-series.
+            Defaults to 0.
+        grid_num: the number of grid when creating the lagged DREVI plot. Defaults to 25.
+        n_row: the number of rows used to layout the faceted cluster panels. Defaults to 1.
+        n_col: the number of columns used to layout the faceted cluster panels. Defaults to None.
+        cmap: the color map used to plot the heatmap. Could be the name of the color map or a matplotlib color map
+            object. If None, the color map would be generated automatically. Defaults to "viridis".
+        normalize: whether to row-scale the data. Defaults to True.
+        k: number of k-nearest neighbors used in calculating 2-D kernel density. Defaults to 30.
+        show_rug: whether to plot marginal distributions by drawing ticks along the x and y axes. Defaults to True.
+        show_extent: whether to extend the figure. If False, `show_ridge` and `show_rug` would be set to False
+            automatically. Defaults to False.
+        ext_format: the string/list of strings (the first is for x and second for y labels) that will be used to format
+            the ticks on x or y-axis. If it is None or one of the element in the list is None, the default setting will
+            be used. Defaults to None.
+        stacked_fraction: if True the jacobian will be represented as a stacked fraction in the title, otherwise a
+            linear fraction style is used. Defaults to False.
+        figsize: the size of the figure. Defaults to (6, 4).
+        save_show_or_return: whether to save or show the figure. If "both", it will save and plot the figure at the same time. If
             "all", the figure will be saved, displayed and the associated axis and other object will be return.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-
-    Returns
-    -------
-        A figure created by matplotlib.
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+         and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}. Defaults to {}.
+        return_data: whether to return the calculated causality data. Defaults to False.
+
+    Returns:
+        None would be returned by default. If `return_data` is set to be True, the causality data, a DataFrame with
+        columns ("x": x coordination, "y": y coordination, "expected_z": expected z coordination, "pair": gene pairs)
+        would be returned.
     """
+
     import matplotlib
     from matplotlib.colors import ListedColormap
 
     if cmap is None:
         cmap = matplotlib.colors.LinearSegmentedColormap.from_list("comb_logic", ["#00CF8D", "#FFFF99", "#FF0000"])
 
     if return_data:
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/markers.py` & `dynamo-release-1.3.0/dynamo/plot/markers.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,158 +1,137 @@
 import warnings
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import numpy as np
+import numpy.typing as npt
 import pandas as pd
+from anndata import AnnData
+from matplotlib.axes import Axes
+from matplotlib.figure import Figure
 from scipy.sparse import issparse
 
 from ..configuration import _themes, reset_rcParams, set_figure_params
 from ..tools.utils import get_mapper, update_dict
 from .utils import save_fig
 
 
 def bubble(
-    adata,
-    genes,
-    group,
-    gene_order=None,
-    group_order=None,
-    layer=None,
-    theme=None,
-    cmap=None,
-    color_key=None,
-    color_key_cmap="Spectral",
-    background="white",
-    pointsize=None,
-    vmin=0,
-    vmax=100,
-    sym_c=False,
-    alpha=0.8,
-    edgecolor=None,
-    linewidth=0,
-    type="violin",
-    sort="diagnoal",
-    transpose=False,
-    rotate_xlabel="horizontal",
-    rotate_ylabel="horizontal",
-    figsize=None,
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    genes: List[str],
+    group: str,
+    gene_order: Optional[List[str]] = None,
+    group_order: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    theme: Optional[
+        Literal["blue", "red", "green", "inferno", "fire", "viridis", "darkblue", "darkred", "darkgreen"]
+    ] = None,
+    cmap: Optional[str] = None,
+    color_key: Union[dict, npt.ArrayLike] = None,
+    color_key_cmap: Optional[str] = "Spectral",
+    background: Optional[str] = "white",
+    pointsize: Optional[float] = None,
+    vmin: float = 0,
+    vmax: float = 100,
+    sym_c: bool = False,
+    alpha: float = 0.8,
+    edgecolor: Optional[str] = None,
+    linewidth: float = 0,
+    type: Literal["violin", "dot"] = "violin",
+    sort: str = "diagnoal",
+    transpose: bool = False,
+    rotate_xlabel: Union[float, Literal["vertical", "horizontal"]] = "horizontal",
+    rotate_ylabel: Union[float, Literal["vertical", "horizontal"]] = "horizontal",
+    figsize: Optional[Tuple[float, float]] = None,
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
+) -> Optional[Tuple[Figure, List[Axes]]]:
     """Bubble plots generalized to velocity, acceleration, curvature.
+
     It supports either the `dot` or `violin` plot mode. This function is loosely based on
     https://github.com/QuKunLab/COVID-19/blob/master/step3_plot_umap_and_marker_gene_expression.ipynb
 
-    # add sorting
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        genes: `list`
-            The gene list, i.e. marker gene or top acceleration, curvature genes, etc.
-        group: `str`
-            The column key in `adata.obs` that will be used to group cells.
-        gene_order: `None` or `list` (default: `None`)
-            The gene groups order that will show up in the resulting bubble plot.
-        group_order: `None` or `list` (default: `None`)
-            The cells groups order that will show up in the resulting bubble plot.
-        layer: `None` or `str` (default: `None`)
-            The layer of data to use for the bubble plot.
-        theme: string (optional, default None)
-            A color theme to use for plotting. A small set of
-            predefined themes are provided which have relatively
+    Args:
+        adata: an AnnData object.
+        genes: the gene list, i.e. marker gene or top acceleration, curvature genes, etc.
+        group: the column key in `adata.obs` that will be used to group cells.
+        gene_order: the gene groups order that will show up in the resulting bubble plot. If None, the order of `genes`
+            would be used. Defaults to None.
+        group_order: the cells groups order that will show up in the resulting bubble plot. If None,
+            `adata.obs['group']` would be used. Defaults to None.
+        layer: the layer of data to use for the bubble plot. Defaults to None.
+        theme: a color theme to use for plotting. A small set of predefined themes are provided which have relatively
             good aesthetics. Available themes are:
                * 'blue'
                * 'red'
                * 'green'
                * 'inferno'
                * 'fire'
                * 'viridis'
                * 'darkblue'
                * 'darkred'
-               * 'darkgreen'
-        cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring
-            or shading points. If no labels or values are passed
-            this will be used for shading points according to
-            density (largely only of relevance for very large
-            datasets). If values are passed this will be used for
-            shading according the value. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        color_key: dict or array, shape (n_categories) (optional, default None)
-            A way to assign colors to categoricals. This can either be
-            an explicit dict mapping labels to colors (as strings of form
-            '#RRGGBB'), or an array like object providing one color for
-            each distinct category being provided in ``labels``. Either
-            way this mapping will be used to color points according to
-            the label. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        color_key_cmap: string (optional, default 'Spectral')
-            The name of a matplotlib colormap to use for categorical coloring.
-            If an explicit ``color_key`` is not given a color mapping for
-            categories can be generated from the label list and selecting
-            a matching list of colors from the given colormap. Note
-            that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        background: string or None (optional, default 'None`)
-            The color of the background. Usually this will be either
-            'white' or 'black', but any color name will work. Ideally
-            one wants to match this appropriately to the colors being
-            used for points etc. This is one of the things that themes
-            handle for you. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        pointsize: `None` or `float` (default: None)
-            The scale of the point size. Actual point cell size is calculated as `500.0 / np.sqrt(adata.shape[0]) *
-            pointsize`
-        vmin: `float` (default: `0`)
-            The percentage of minimal value to consider.
-        vmax: `float` (default: `100`)
-            The percentage of maximal value to consider.
-        sym_c: `bool` (default: `False`)
-            Whether do you want to make the limits of continuous color to be symmetric, normally this should be used for
-            plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative values.
-        alpha: `float` (default: `0.8`)
-            alpha value of the plot
-        edgecolor: `str` or `None` (default: `None`)
-            The color of the edge of the dots when type is to be `dot`.
-        linewidth: `str` or `None` (default: `None`)
-            The width of the edge of the dots when type is to be `dot`.
-        type: `str` (default: `violin`)
-            The type of the bubble plot, one of `{'violin', 'dot'}`.
-        figsize: `None` or `[float, float]` (default: None)
-            The width and height of a figure.
-        sort: `str` (default: `diagnol`)
-            The method for sorting genes. Not implemented. Need to implement in 2021.
-        transpose: `bool` (default: `False`)
-            Whether to transpose the row/column of the resulting bubble plot. Gene and cell types are on x/y-axis by
-            default.
-        rotate_xlabel: `float` (default: `horizontal`)
-            The angel to rotate the x-label.
-        rotate_ylabel: `float` (default: `horizontal`)
-            The angel to rotate the y-label.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        kwargs:
-            Additional arguments passed to plt.scatters or sns.violinplot.
-
-
-    Returns
-    -------
-        Nothing but plot the bubble plots.
-
+               * 'darkgreen'.
+            Defaults to None.
+        cmap: the name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to None.
+        color_key: a way to assign colors to categoricals. This can either be an explicit dict mapping labels to colors
+            (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct category being
+            provided in `labels`. Either way this mapping will be used to color points according to the label. Note that
+            if theme is passed then this value will be overridden by the corresponding option of the theme. Defaults to
+            None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to "Spectral".
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to "white".
+        pointsize: the scale of the point size. Actual point cell size is calculated as
+            `500.0 / np.sqrt(adata.shape[0]) * pointsize`. Defaults to None.
+        vmin: the percentage of minimal value to consider. Defaults to 0.
+        vmax: the percentage of maximal value to consider. Defaults to 100.
+        sym_c: whether do you want to make the limits of continuous color to be symmetric, normally this should be used
+            for plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative
+            values. Defaults to False.
+        alpha: alpha value of the plot. Defaults to 0.8.
+        edgecolor: the color of the edge of the dots when type is to be `dot`. Defaults to None.
+        linewidth: the width of the edge of the dots when type is to be `dot`. Defaults to 0.
+        type: the type of the bubble plot, one of "violin" or "dot". Defaults to "violin".
+        sort: the method for sorting genes. Not implemented. Defaults to "diagnoal".
+        transpose: whether to transpose the row/column of the resulting bubble plot. Gene and cell types are on x/y-axis
+            by default. Defaults to False.
+        rotate_xlabel: the angel to rotate the x-label or "horizontal" or "vertical". Defaults to "horizontal".
+        rotate_ylabel: the angel to rotate the y-label or "horizontal" or "vertical".. Defaults to "horizontal".
+        figsize: the size of the figure. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Can be one of "save", "show", or "return".
+            Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide
+            a dictionary that properly modify those keys according to your needs.. Defaults to {}.
+
+    Raises:
+        ValueError: `group` is not a column name of `adata.obs`
+        ValueError: gene name in `genes` is not found in `adata.vars`.
+        ValueError: `group_order` is not a subset of `adata.obs[group]`.
+        ValueError: `gene_order` is not a subset of `adata.var_names.intersection(set(genes)).to_list()`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be `return`, the matplotlib figure and
+        axes would be returned.
     """
+
     import matplotlib
     import matplotlib.pyplot as plt
     import seaborn as sns
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     if background is None:
@@ -333,35 +312,38 @@
                     list(map(str, np.array(clusters))),
                     rotation=rotate_ylabel,
                     ha="right",
                     va="center",
                 )
         axes[igene].set_xlabel("") if transpose else axes[igene].set_ylabel("")
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "violin",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
         if background is not None:
             reset_rcParams()
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
             plt.tight_layout()
 
         plt.show()
         if background is not None:
             reset_rcParams()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         if background is not None:
             reset_rcParams()
 
         return fig, axes
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/networks.py` & `dynamo-release-1.3.0/dynamo/plot/networks.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,71 +1,68 @@
+from typing import Any, Dict, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
 import networkx as nx
 import numpy as np
-import nxviz as nv
-import nxviz.annotate
 import pandas as pd
+from anndata import AnnData
 from matplotlib.axes import Axes
 
 from ..tools.utils import flatten, index_gene, update_dict
 from .utils import save_fig, set_colorbar
 from .utils_graph import ArcPlot
 
 
 def nxvizPlot(
-    adata,
-    cluster,
-    cluster_name,
-    edges_list,
-    plot="arcplot",
-    network=None,
-    weight_scale=5e3,
-    weight_threshold=1e-4,
-    figsize=(6, 6),
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    cluster: str,
+    cluster_name: str,
+    edges_list: Dict[str, pd.DataFrame],
+    plot: Literal["arcplot", "circosplot"] = "arcplot",
+    network: Optional[nx.classes.digraph.DiGraph] = None,
+    weight_scale: float = 5e3,
+    weight_threshold: float = 1e-4,
+    figsize: Tuple[float, float] = (6, 6),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
+) -> Optional[Any]:
     """Arc or circos plot of gene regulatory network for a particular cell cluster.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`.
-            AnnData object.
-        cluster: `str`
-            The group key that points to the columns of `adata.obs`.
-        cluster_name: `str` (default: `None`)
-            The group whose network and arcplot will be constructed and created.
-        edges_list: `dict` of `pandas.DataFrame`
-            A dictionary of dataframe of interactions between input genes for each group of cells based on ranking
-            information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight` three
-            columns.
-        plot: `str` (default: `arcplot`)
-            Which nxviz plot to use, one of {'arcplot', 'circosplot'}.
-        network: class:`~networkx.classes.digraph.DiGraph`
-            A direct network for this cluster constructed based on Jacobian analysis.
-        weight_scale: `float` (default: `1e3`)
-            Because values in Jacobian matrix is often small, the value will be multiplied by the weight_scale so that
-            the edge will have proper width in display.
-        weight_threshold: `float` (default: `weight_threshold`)
-            The threshold of weight that will be used to trim the edges for network reconstruction.
-        figsize: `None` or `[float, float]` (default: (6, 6)
-            The width and height of each panel in the figure.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'arcplot', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-        **kwargs:
-            Additional parameters that will pass to ArcPlot or CircosPlot
-
-    Returns
-    -------
-        Nothing but plot an ArcPlot of the input direct network.
+    Args:
+        adata: an AnnData object.
+        cluster: the group key that points to the column of `adata.obs`
+        cluster_name: the group whose network and arcplot would be constructed and created.
+        edges_list: a dictionary of dataframe of interactions between input genes for each group of cells based on
+            ranking information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight`
+            three columns.
+        plot: which nxviz plot to use, one of 'arcplot' or 'circosplot'. Defaults to "arcplot".
+        network: a direct network for this cluster constructed based on Jacobian analysis. Defaults to None.
+        weight_scale: the factor by which the Jacobian matrix values are multiplied so that the edge could be displayer
+            with proper width. Defaults to 5e3.
+        weight_threshold: the threshold of weight that will be used to trim the edges for network reconstruction. Defaults to 1e-4.
+        figsize: the size of each panel of the figure. Defaults to (6, 6).
+        save_show_or_return: whether to save, show, or return the plotted figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'arcplot', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide
+            a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        **kwargs: any other kwargs that would be passed to Arcplot or CircosPlot.
+
+    Raises:
+        ImportError: nxviz or networkx is not installed.
+        ValueError: `weight_threshold` too high that no edge pass it.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the generated `nxviz` plot
+        object would be returned.
     """
 
     _, has_labeling = (
         adata.uns["pp"].get("has_splicing"),
         adata.uns["pp"].get("has_labeling"),
     )
     layer = "M_s" if not has_labeling else "M_t"
@@ -171,110 +168,94 @@
             figsize=figsize,
         )
 
     # recover network edge weights
     for e in network.edges():
         network.edges[e]["weight"] /= weight_scale
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         # Draw a to the screen
         nv_ax.draw()
         plt.autoscale()
         s_kwargs = {
             "path": None,
             "prefix": prefix,
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         # Draw a to the screen
         nv_ax.draw()
         plt.autoscale()
         # Display the plot
         plt.show()
         # plt.savefig('./unknown_arcplot.pdf', dpi=300)
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return nv_ax
 
 
 def arcPlot(
-    adata,
-    cluster,
-    cluster_name,
-    edges_list=None,
-    network=None,
-    color=None,
-    cmap="viridis",
-    node_size=100,
-    cbar=True,
-    cbar_title=None,
-    figsize=(6, 6),
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    cluster: str,
+    cluster_name: str,
+    edges_list: Optional[Dict[str, pd.DataFrame]] = None,
+    network: Optional[nx.classes.DiGraph] = None,
+    color: Optional[str] = None,
+    cmap: "str" = "viridis",
+    node_size: int = 100,
+    cbar: bool = True,
+    cbar_title: Optional[str] = None,
+    figsize: Tuple[str, str] = (6, 6),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
+) -> Optional[Any]:
     """Arc plot of gene regulatory network for a particular cell cluster.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`.
-            AnnData object.
-        cluster: `str`
-            The group key that points to the columns of `adata.obs`.
-        cluster_name: `str` (default: `None`)
-            The group whose network and arcplot will be constructed and created.
-        edges_list: `dict` of `pandas.DataFrame`
-            A dictionary of dataframe of interactions between input genes for each group of cells based on ranking
-            information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight` three
-            columns.
-        network: class:`~networkx.classes.digraph.DiGraph`
-            A direct network for this cluster constructed based on Jacobian analysis.
-        color: `str` or None (default: `None`)
-            The layer key that will be used to retrieve average expression to color the node of each gene.
-        node_size: `float` (default: `100`)
-            The size of the node, a constant.
-        cbar: `bool` (default: `True`)
-            Whether or not to display colorbar when `color` is not None.
-        cbar_title: `float` (default: `weight_threshold`)
-            The title of the color bar when displayed.
-        figsize: `None` or `[float, float]` (default: (6, 6)
-            The width and height of each panel in the figure.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'arcplot', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-        **kwargs:
-            Additional parameters that will eventually pass to ArcPlot.
-
-    Returns
-    -------
-        Nothing but plot an ArcPlot of the input direct network.
+    Args:
+        adata: an AnnData object.
+        cluster: the group key that points to the column of `adata.obs`.
+        cluster_name: the group whose network and arcplot will be constructed and created.
+        edges_list: a dictionary of dataframe of interactions between input genes for each group of cells based on
+            ranking information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight`
+            three columns. If None, the network should be provided directly. Defaults to None.
+        network: a direct network for this cluster constructed based on Jacobian analysis. If None, `edges_list` must be
+            provided to construct the network. Defaults to None.
+        color: the layer key that will be used to retrieve average expression to color the node of each gene. If None,
+            the nodes would not be colored. Defaults to None.
+        cmap: the color map used for the ArcPlot. Defaults to "viridis".
+        node_size: the size of the node, a constant. Defaults to 100.
+        cbar: whether to display the colorbar when `color` is not None. Defaults to True.
+        cbar_title: the titke of the colorbar when displayed. Defaults to None.
+        figsize: the size of each panel of the figure. Defaults to (6, 6).
+        save_show_or_return: whether to save, show, or return the plotted ArcPlot. Could be one of 'save', 'show', or
+            'return'. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'arcplot', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        **kwargs: any other kwargs that would be passed to ArcPlot.
+
+    Raises:
+        ImportError: `networkx` not installed.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to 'return', the generated ArcPlot object
+        would be return.
     """
 
-    """nxvizPlot(adata,
-            cluster,
-            cluster_name,
-            edges_list,
-            plot='arcplot',
-            network=network,
-            weight_scale=weight_scale,
-            figsize=figsize,
-            save_show_or_return=save_show_or_return,
-            save_kwargs=save_kwargs,
-            **kwargs,
-            )"""
     import matplotlib
     import matplotlib.pyplot as plt
     from matplotlib.ticker import MaxNLocator
 
     try:
         import networkx as nx
     except ImportError:
@@ -329,142 +310,117 @@
             cb.ax.set_title(cbar_title)
 
         cb.set_alpha(1)
         cb.draw_all()
         cb.locator = MaxNLocator(nbins=3, integer=True)
         cb.update_ticks()
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         # Draw a to the screen
         plt.autoscale()
         s_kwargs = {
             "path": None,
             "prefix": "arcPlot",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         # Draw a to the screen
         plt.autoscale()
         # Display the plot
         plt.show()
         # plt.savefig('./unknown_arcplot.pdf', dpi=300)
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ap
 
 
 def circosPlot(
     network: nx.Graph,
-    node_label_key: str = None,
+    node_label_key: Optional[str] = None,
     circos_label_layout: str = "rotate",
-    node_color_key: str = None,
-    show_colorbar=True,
+    node_color_key: Optional[str] = None,
+    show_colorbar: bool = True,
     edge_lw_scale: float = 0.5,
     edge_alpha_scale: float = 0.5,
 ) -> Axes:
     """wrapper for drawing circos plot via nxviz >= 0.7.3
 
-    Parameters
-    ----------
-    network : nx.Graph
-        a network graph instance
-    node_label_key : str, optional
-        node label (attribute) in network for grouping nodes, by default None
-    circos_label_layout : str, optional
-        layout of circos plot (see nxviz docs for details), by default "rotate"
-    node_color_key : str, optional
-        node attribute in network, corresponding to color values of nodes, by default None
-    show_colorbar : bool, optional
-        whether to show colorbar, by default True
-    edge_lw_scale : float
-        the line width scale of edges drawn in in plot
-    edge_alpha_scale : float
-        the alpha (opacity, transparency) scale of edges, the value shoud be in [0, 1.0]
+    Args:
+        network : a network graph instance
+        node_label_key : node label (attribute) in network for grouping nodes, by default None
+        circos_label_layout : layout of circos plot (see nxviz docs for details), by default "rotate"
+        node_color_key : node attribute in network, corresponding to color values of nodes, by default None
+        show_colorbar : whether to show colorbar, by default True
+        edge_lw_scale : the line width scale of edges drawn in plot
+        edge_alpha_scale : the alpha (opacity, transparency) scale of edges, the value should be in [0, 1.0]
+
+    Returns:
+        the Matplotlib Axes on which the Circos plot is drawn.
     """
+    try:
+        import nxviz as nv
+        from nxviz import annotate
+    except ImportError:
+        raise ImportError("install nxviz via `pip install nxviz`.")
+
     ax = nv.circos(
         network,
         group_by=node_label_key,
         node_color_by=node_color_key,
         edge_lw_by="weight",
         edge_alpha_by="weight",
         edge_enc_kwargs={
             "lw_scale": edge_lw_scale,
             "alpha_scale": edge_alpha_scale,
         },
     )
 
-    nv.annotate.circos_labels(network, group_by=node_label_key, layout=circos_label_layout)
+    annotate.circos_labels(network, group_by=node_label_key, layout=circos_label_layout)
     if node_color_key and show_colorbar:
-        nv.annotate.node_colormapping(
+        annotate.node_colormapping(
             network,
             color_by=node_color_key,
             legend_kwargs={"loc": "upper right", "bbox_to_anchor": (0.0, 1.0)},
             ax=None,
         )
     return ax
 
 
 def circosPlotDeprecated(
-    adata,
-    cluster,
-    cluster_name,
-    edges_list,
-    network=None,
-    weight_scale=5e3,
-    weight_threshold=1e-4,
-    figsize=(12, 6),
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    cluster: str,
+    cluster_name: str,
+    edges_list: Dict[str, pd.DataFrame],
+    network: nx.classes.digraph.DiGraph = None,
+    weight_scale: float = 5e3,
+    weight_threshold: float = 1e-4,
+    figsize: Tuple[float, float] = (12, 6),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
-    """Note: this function is written with nxviz old version (<=0.3.x, or higher) API
-    for the latest nxviz version compatibility, please refer to `dyn.pl.circos_plot`.
-    Circos plot of gene regulatory network for a particular cell cluster.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`.
-            AnnData object.
-        cluster: `str`
-            The group key that points to the columns of `adata.obs`.
-        cluster_name: `str` (default: `None`)
-            The group whose network and arcplot will be constructed and created.
-        edges_list: `dict` of `pandas.DataFrame`
-            A dictionary of dataframe of interactions between input genes for each group of cells based on ranking
-            information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight` three
-            columns.
-        network: class:`~networkx.classes.digraph.DiGraph`
-            A direct network for this cluster constructed based on Jacobian analysis.
-        weight_scale: `float` (default: `1e3`)
-            Because values in Jacobian matrix is often small, the value will be multiplied by the weight_scale so that
-            the edge will have proper width in display.
-        weight_threshold: `float` (default: `weight_threshold`)
-            The threshold of weight that will be used to trim the edges for network reconstruction.
-        figsize: `None` or `[float, float]` (default: (12, 6)
-            The width and height of each panel in the figure.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'arcplot', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-        **kwargs:
-            Additional parameters that will eventually pass to CircosPlot.
-
-    Returns
-    -------
-        Nothing but plot an CircosPlot of the input direct network.
+) -> Optional[Any]:
+
+    """Deprecated.
+
+    A wrapper of `dynamo.pl.networks.nxvizPlot` to plot Circos graph. See the `nxvizPlot` for more information.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the generated `nxviz` plot
+        object would be returned.
     """
+
     nxvizPlot(
         adata,
         cluster,
         cluster_name,
         edges_list,
         plot="circosplot",
         network=network,
@@ -474,52 +430,51 @@
         save_show_or_return=save_show_or_return,
         save_kwargs=save_kwargs,
         **kwargs,
     )
 
 
 def hivePlot(
-    adata,
-    edges_list,
-    cluster,
-    cluster_names=None,
-    weight_threshold=1e-4,
-    figsize=(6, 6),
-    save_show_or_return="show",
-    save_kwargs={},
-):
+    adata: AnnData,
+    edges_list: Dict[str, pd.DataFrame],
+    cluster: str,
+    cluster_names: Optional[str] = None,
+    weight_threshold: float = 1e-4,
+    figsize: Tuple[float, float] = (6, 6),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Axes:
     """Hive plot of cell cluster specific gene regulatory networks.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`.
-            AnnData object.
-        edges_list: `dict` of `pandas.DataFrame`
-            A dictionary of dataframe of interactions between input genes for each group of cells based on ranking
-            information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight` three
-            columns.
-        cluster: `str`
-            The group key that points to the columns of `adata.obs`.
-        cluster_names: `str` (default: `None`)
-            The group whose network and arcplot will be constructed and created.
-        weight_threshold: `float` (default: `weight_threshold`)
-            The threshold of weight that will be used to trim the edges for network reconstruction.
-        figsize: `None` or `[float, float]` (default: (6, 6)
-            The width and height of each panel in the figure.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'hiveplot', "dpi": None, "ext": 'pdf',
-            "transparent": True,  "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-
-    Returns
-    -------
-        Nothing but plot a hive plot of the input cell cluster specific direct network.
+    Args:
+        adata: an AnnData object.
+        edges_list: a dictionary of dataframe of interactions between input genes for each group of cells based on
+            ranking information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight`
+            three columns.
+        cluster: the group key that points to the columns of `adata.obs`.
+        cluster_names: the group whose network and arcplot will be constructed and created. Defaults to None.
+        weight_threshold: the threshold of weight that will be used to trim the edges for network reconstruction.
+            Defaults to 1e-4.
+        figsize: the size of each panel of the figure. Defaults to (6, 6).
+        save_show_or_return: whether to save, show, or return the figure. Could be one of "save", "show", or "return".
+            Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'hiveplot', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        ImportError: `networkx` or `hiveplotlib` not installed
+        ValueError: invalid `edge_keys`
+        ValueError: invalid `cluster_names`
+        ValueError: `weight_threshold` too high to have any edge to pass it.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to `return`, the matplotlib axes of the
+        figure would be returned.
     """
 
     # _, has_labeling = (
     #     adata.uns["pp"].get("has_splicing"),
     #     adata.uns["pp"].get("has_labeling"),
     # )
     # layer = "M_s" if not has_labeling else "M_t"
@@ -624,25 +579,28 @@
     edge_viz_mpl(hive_plot=hp, fig=fig, ax=ax, alpha=0.7, zorder=-1)
 
     # ax.set_title("Hive Plot", fontsize=20, y=0.9)
     # custom_lines = [Line2D([0], [0], color=f'C{i}', lw=3, linestyle='-') for i in range(len(reg_groups))]
     # ax.legend(custom_lines, reg_groups, loc='upper left', bbox_to_anchor=(0.37, 0.35),
     #           title="Regulatory network based on Jacobian analysis")
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "hiveplot",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/preprocess.py` & `dynamo-release-1.3.0/dynamo/plot/preprocess.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-from typing import Optional, Sequence, Union
+from typing import Any, Dict, List, Optional, Tuple
 
-import matplotlib
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
+import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
+import seaborn as sns
 from anndata import AnnData
 from matplotlib.axes import Axes
+from matplotlib.collections import PathCollection
 from scipy.sparse import csr_matrix, issparse
 
 from ..configuration import DynamoAdataKeyManager
 from ..dynamo_logger import main_warning
-from ..preprocessing import preprocess as pp
-from ..preprocessing.preprocess_monocle_utils import top_table
+from ..preprocessing import gene_selection
+from ..preprocessing.gene_selection import get_prediction_by_svr
 from ..preprocessing.utils import detect_experiment_datatype
 from ..tools.utils import get_mapper, update_dict
 from .utils import save_fig
 
 
 def basic_stats(
     adata: AnnData,
     group: Optional[str] = None,
-    figsize: tuple = (4, 3),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-):
+    figsize: Tuple[float, float] = (4, 3),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[sns.FacetGrid]:
     """Plot the basic statics (nGenes, nCounts and pMito) of each category of adata.
 
-    Parameters
-    ----------
-    adata: :class:`~anndata.AnnData`
-        an Annodata object
-    group: `string` (default: None)
-        Which group to facets the data into subplots. Default is None, or no faceting will be used.
-    figsize:
-        Figure size of each facet.
-    save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-        Whether to save, show or return the figure.
-    save_kwargs: `dict` (default: `{}`)
-        A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig
-        function will use the {"path": None, "prefix": 'basic_stats', "dpi": None, "ext": 'pdf', "transparent": True,
-        "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify
-        those keys according to your needs.
-
-    Returns
-    -------
-        A violin plot that shows the fraction of each category, produced by seaborn.
+    Args:
+        adata: an AnnData object.
+        group: the column key of `adata.obs` to facet the data into subplots. If None, no faceting will be used.
+            Defaults to None.
+        figsize: the size of each panel in the figure. Defaults to (4, 3).
+        save_show_or_return: whether to save, show, or return the plots. Could be one of 'save', 'show', or 'return'.
+            Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'basic_stats', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to 'return', the generated figure
+        (seaborn.FacetGrid) would be returned.
     """
 
-    import matplotlib.pyplot as plt
-    import seaborn as sns
-
     if len(adata.obs.columns.intersection(["nGenes", "nCounts", "pMito"])) != 3:
-        from ..preprocessing.utils import basic_stats
+        from ..preprocessing.QC import basic_stats
 
         basic_stats(adata)
 
     df = pd.DataFrame(
         {
             "nGenes": adata.obs["nGenes"],
             "nCounts": adata.obs["nCounts"],
@@ -99,87 +99,83 @@
     # important to add this before setting titles
     g.set_titles(row_template="{row_name}", col_template="{col_name}")
 
     g.set_xlabels("")
     g.set_ylabels("")
     g.set(ylim=(0, None))
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "basic_stats",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
-        import matplotlib.pyplot as plt
-
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
 
 
 def show_fraction(
     adata: AnnData,
-    genes: Optional[list] = None,
+    genes: Optional[List[str]] = None,
     group: Optional[str] = None,
-    figsize: tuple = (4, 3),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-):
+    figsize: Tuple[float, float] = (4, 3),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[sns.FacetGrid]:
     """Plot the fraction of each category of data used in the velocity estimation.
 
-    Parameters
-    ----------
-    adata: :class:`~anndata.AnnData`
-        an Annodata object
-    genes: `list` like:
-        The list of gene names from which the fraction will be calculated.
-    group: `string` (default: None)
-        Which group to facets the data into subplots. Default is None, or no faceting will be used.
-    figsize: `string` (default: (4, 3))
-        Figure size of each facet.
-    save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-        Whether to save, show or return the figure.
-    save_kwargs: `dict` (default: `{}`)
-        A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig
-        function will use the {"path": None, "prefix": 'show_fraction', "dpi": None, "ext": 'pdf', "transparent": True,
-        "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify
-        those keys according to your needs.
-
-    Returns
-    -------
-        A violin plot that shows the fraction of each category, produced by seaborn.
+    Args:
+        adata: an AnnData object.
+        genes: the list of gene names from which the fraction will be calculated. Defaults to None.
+        group: the column key of `adata.obs` to facet the data into subplots. If None, no faceting will be used.
+            Defaults to None.
+        figsize: the size of each panel in the figure. Defaults to (4, 3).
+        save_show_or_return: whether to save, show, or return the plots. Could be one of 'save', 'show', or 'return'.
+            Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'show_fraction', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        ValueError: `genes` does not contain any genes from the adata object.
+        ValueError: `adata` does not have proper splicing or labeling data.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to 'return', the generated figure
+        (seaborn.FacetGrid) would be returned.
     """
 
-    import matplotlib.pyplot as plt
-    import seaborn as sns
-
     if genes is not None:
         genes = list(adata.var_names.intersection(genes))
 
         if len(genes) == 0:
-            raise Exception("The gene list you provided doesn't much any genes from the adata object.")
+            raise ValueError("The gene list you provided doesn't much any genes from the adata object.")
 
     mode = None
     if pd.Series(["spliced", "unspliced"]).isin(adata.layers.keys()).all():
         mode = "splicing"
     elif pd.Series(["new", "total"]).isin(adata.layers.keys()).all():
         mode = "labelling"
     elif pd.Series(["uu", "ul", "su", "sl"]).isin(adata.layers.keys()).all():
         mode = "full"
 
     if not (mode in ["labelling", "splicing", "full"]):
-        raise Exception("your data doesn't seem to have either splicing or labeling or both information")
+        raise ValueError("your data doesn't seem to have either splicing or labeling or both information")
 
     if mode == "labelling":
         new_mat, total_mat = (
             (adata.layers["new"], adata.layers["total"])
             if genes is None
             else (
                 adata[:, genes].layers["new"],
@@ -339,165 +335,157 @@
     # important to add this before setting titles
     g.set_titles(row_template="{row_name}", col_template="{col_name}")
 
     g.set_xlabels("")
     g.set_ylabels("Fraction")
     g.set(ylim=(0, None))
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "show_fraction",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
 
 
 def variance_explained(
     adata: AnnData,
     threshold: float = 0.002,
     n_pcs: Optional[int] = None,
-    figsize: tuple = (4, 3),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-):
+    figsize: Tuple[float, float] = (4, 3),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Axes:
     """Plot the accumulative variance explained by the principal components.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-        threshold: `float` (default: `0.002`)
-            The threshold for the second derivative of the cumulative sum of the variance for each principal component.
-            This threshold is used to determine the number of principal component used for downstream non-linear
-            dimension reduction.
-        n_pcs: `int` (default: `None`)
-            Number of principal components.
-        figsize: `string` (default: (4, 3))
-            Figure size of each facet.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'variance_explained', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-
-    Returns
-    -------
-        Nothing but make a matplotlib based plot for showing the cumulative variance explained by each PC.
+    Args:
+        adata: an AnnDate object.
+        threshold: the threshold for the second derivative of the cumulative sum of the variance for each principal
+            component. This threshold is used to determine the number of principle components used for downstream non-
+            linear dimension reduction. Defaults to 0.002.
+        n_pcs: the number of principal components. If None, the number of components would be inferred automatically.
+            Defaults to None.
+        figsize: the size of each panel of the figure. Defaults to (4, 3).
+        save_show_or_return: whether to save, show, or return the generated figure. Can be one of 'save', 'show', or
+            'return'. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'variance_explained', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib Axes of the
+        figure would be returned.
     """
 
-    import matplotlib.pyplot as plt
-
     var_ = adata.uns["explained_variance_ratio_"]
     _, ax = plt.subplots(figsize=figsize)
     ax.plot(var_, c="r")
     tmp = np.diff(np.diff(np.cumsum(var_)) > threshold)
     n_comps = n_pcs if n_pcs is not None else np.where(tmp)[0][0] if np.any(tmp) else 20
     ax.axvline(n_comps, c="r")
     ax.set_xlabel("PCs")
     ax.set_ylabel("Variance explained")
     ax.set_xticks(list(ax.get_xticks()) + [n_comps])
     ax.set_xlim(0, len(var_))
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "variance_explained",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def biplot(
     adata: AnnData,
-    pca_components: Sequence[int] = [0, 1],
+    pca_components: Tuple[int, int] = [0, 1],
     pca_key: str = "X_pca",
     loading_key: str = "PCs",
-    figsize: tuple = (6, 4),
+    figsize: Tuple[float, float] = (6, 4),
     scale_pca_embedding: bool = False,
     draw_pca_embedding: bool = False,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    ax: Union[matplotlib.axes._subplots.SubplotBase, None] = None,
-):
-    """A biplot overlays a score plot and a loadings plot in a single graph. In such a plot, points are the projected
-    observations; vectors are the projected variables. If the data are well-approximated by the first two principal
-    components, a biplot enables you to visualize high-dimensional data by using a two-dimensional graph. See more at:
-    https://blogs.sas.com/content/iml/2019/11/06/what-are-biplots.html
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    ax: Optional[Axes] = None,
+) -> Axes:
+    """A biplot overlays a score plot and a loadings plot in a single graph.
+
+    In such a plot, points are the projected observations; vectors are the projected variables. If the data are well-
+    approximated by the first two principal components, a biplot enables you to visualize high-dimensional data by using
+    a two-dimensional graph. See more at: https://blogs.sas.com/content/iml/2019/11/06/what-are-biplots.html
 
     In general, the score plot and the loadings plot will have different scales. Consequently, you need to rescale the
     vectors or observations (or both) when you overlay the score and loadings plots. There are four common choices of
     scaling. Each scaling emphasizes certain geometric relationships between pairs of observations (such as distances),
     between pairs of variables (such as angles), or between observations and variables. This article discusses the
     geometry behind two-dimensional biplots and shows how biplots enable you to understand relationships in multivariate
     data.
 
-    Parameters
-    ----------
-        adata:
-            An Annodata object that has pca and loading information prepared.
-        pca_components:
-            The pca components that will be used to draw the biplot.
-        pca_key:
-            A key to the pca embedding matrix, in `.obsm`.
-        loading_key:
-            A key to the pca loading matrix, in either `.uns` or `.obsm`.
-        figsize:
-            The figure size.
-        scale_pca_embedding:
-            Whether to scale the pca embedding.
-        draw_pca_embedding:
-            Whether to draw the pca embedding.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'biplot', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        ax
-            An ax where the biplot will be appended to.
-
-    Returns
-    -------
-        If save_show_or_return is not `return`, return nothing but plot or save the biplot; otherwise return an axes
-        with the biplot in it.
+    Args:
+        adata: an AnnData object that has pca and loading information prepared.
+        pca_components: the index of the pca components in loading matrix. Defaults to [0, 1].
+        pca_key: the key to the pca embedding matrix in `adata.obsm`. Defaults to "X_pca".
+        loading_key: the key to the pca loading matrix in either `adata.uns` or `adata.varm`. Defaults to "PCs".
+        figsize: the size of each subplot. Defaults to (6, 4).
+        scale_pca_embedding: whether to scale the pca embedding. Defaults to False.
+        draw_pca_embedding: whether to draw the pca embedding. Defaults to False.
+        save_show_or_return: whether to save, show, or return the generated figure. Can be one of 'save', 'show', or
+            'return'. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'variance_explained', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+        ax: the axes object on which the graph would be plotted. If None, a new axis would be created. Defaults to None.
+
+    Raises:
+        ValueError: invalid `loading_key`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib Axes of the
+        figure would be returned.
     """
 
-    import matplotlib.pyplot as plt
-
     if loading_key in adata.uns.keys():
         PCs = adata.uns[loading_key]
     elif loading_key in adata.varm.keys():
         PCs = adata.varm[loading_key]
     else:
-        raise Exception(f"No PC matrix {loading_key} found in neither .uns nor .varm.")
+        raise ValueError(f"No PC matrix {loading_key} found in neither .uns nor .varm.")
 
     # rotation matrix
     xvector = PCs[:, pca_components[0]]
     yvector = PCs[:, pca_components[1]]
 
     # pca components
     xs = adata.obsm[pca_key][:, pca_components[0]]
@@ -523,84 +511,79 @@
     ax.set_ylabel("PC" + str(pca_components[1]))
     if draw_pca_embedding:
         for i in range(len(xs)):
             # circles project cells
             ax.plot(xs[i] * scalex, ys[i] * scaley, "b", alpha=0.1)
             ax.text(xs[i] * scalex * 1.01, ys[i] * scaley * 1.01, list(adata.obs.cluster)[i], color="b", alpha=0.1)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "biplot",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    else:
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def loading(
     adata: AnnData,
     n_pcs: int = 10,
     loading_key: str = "PCs",
     n_top_genes: int = 10,
     ncol: int = 5,
-    figsize: tuple = (6, 4),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-):
+    figsize: Tuple[float] = (6, 4),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> List[List[Axes]]:
     """Plot the top absolute pca loading genes.
 
     Red text are positive loading genes while black negative loading genes.
 
-    Parameters
-    ----------
-        adata:
-            An Annodata object that has pca and loading information prepared.
-        n_pcs:
-            Number of pca.
-        loading_key:
-            A key to the pca loading matrix, in either `.uns` or `.obsm`.
-        n_top_genes:
-            Number of top genes with highest absolute loading score.
-        ncol:
-            Number of panels on the resultant figure.
-        figsize:
-            Figure size.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'biplot', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-
-    Returns
-    -------
-        If save_show_or_return is not `return`, return nothing but plot or save the biplot; otherwise return an axes
-        with the loading plot in it.
+    Args:
+        adata: an AnnData object that has pca and loading information prepared.
+        n_pcs: the number of pca components. Defaults to 10.
+        loading_key: the key to the pca loading matrix. Defaults to "PCs".
+        n_top_genes: the number of top genes with the highest absolute loading score. Defaults to 10.
+        ncol: the number of columns of the subplots. Defaults to 5.
+        figsize: the size of each panel of the figure. Defaults to (6, 4).
+        save_show_or_return: whether to save, show, or return the generated figure. Can be one of 'save', 'show', or
+            'return'. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'biplot', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        ValueError: invalid `loading_key`
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib Axes of the
+        figure would be returned.
     """
 
-    import matplotlib.pyplot as plt
-
     if loading_key in adata.uns.keys():
         PCs = adata.uns[loading_key]
     elif loading_key in adata.varm.keys():
         PCs = adata.varm[loading_key]
     else:
-        raise Exception(f"No PC matrix {loading_key} found in neither .uns nor .varm.")
+        raise ValueError(f"No PC matrix {loading_key} found in neither .uns nor .varm.")
 
     if n_pcs is None:
         n_pcs = PCs.shape[1]
 
     x = np.arange(n_top_genes)
     genes = adata.var_names[adata.var.use_for_pca]
 
@@ -618,238 +601,203 @@
         for j in x:
             axes[cur_row, cur_col].text(
                 x[j], sort_val[j] * 1.01, genes[sort_ind[j]], color="r" if cur_sign[sort_ind[j]] > 0 else "k"
             )
 
         axes[cur_row, cur_col].set_title("PC " + str(i))
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "loading",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    else:
+    if save_show_or_return in ["return", "all"]:
         return axes
 
 
 def feature_genes(
     adata: AnnData,
     layer: str = "X",
-    mode: Union[None, str] = None,
+    mode: Optional[Literal["dispersion", "gini", "SVR"]] = None,
     figsize: tuple = (4, 3),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-):
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[PathCollection]:
     """Plot selected feature genes on top of the mean vs. dispersion scatterplot.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object
-        layer: `str` (default: `X`)
-            The data from a particular layer (include X) used for making the feature gene plot.
-        mode: None or `str` (default: `None`)
-            The method to select the feature genes (can be either `dispersion`, `gini` or `SVR`).
-        figsize: `string` (default: (4, 3))
-            Figure size of each facet.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'feature_genes', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-
-    Returns
-    -------
-        Nothing but plots the selected feature genes via the mean, CV plot.
+    Args:
+        adata: an AnnData object.
+        layer: the data from a particular layer (include X) used for making the feature gene plot. Defaults to "X".
+        mode: the method to select the feature genes (can be either `dispersion`, `gini` or `SVR`). Defaults to None.
+        figsize: the size of each panel of the figure. Defaults to (4, 3).
+        save_show_or_return: whether to save, show, or return the generated figure. Can be one of 'save', 'show', or
+            'return'. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'feature_genes', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        ValueError: vector machine regression result not available in the AnnData object.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the `PathCollection`
+        generated with `pyplot.scatter` would be returned.
     """
 
-    import matplotlib.pyplot as plt
-
     mode = adata.uns["feature_selection"] if mode is None else mode
-
     layer = DynamoAdataKeyManager.get_available_layer_keys(adata, layer, include_protein=False)[0]
-
     uns_store_key = None
-    if mode == "dispersion":
-        uns_store_key = "dispFitInfo" if layer in ["raw", "X"] else layer + "_dispFitInfo"
 
-        table = top_table(adata, layer)
-        x_min, x_max = (
-            np.nanmin(table["mean_expression"]),
-            np.nanmax(table["mean_expression"]),
-        )
-    elif mode == "SVR":
+    if "_dispersion" in mode:  # "cv_dispersion", "fano_dispersion"
         prefix = "" if layer == "X" else layer + "_"
         uns_store_key = "velocyto_SVR" if layer == "raw" or layer == "X" else layer + "_velocyto_SVR"
 
         if not np.all(pd.Series([prefix + "log_m", prefix + "score"]).isin(adata.var.columns)):
-            raise Exception("Looks like you have not run support vector machine regression yet, try run SVRs first.")
+            raise ValueError("Looks like you have not run support vector machine regression yet, try run SVRs first.")
         else:
             table = adata.var.loc[:, [prefix + "log_m", prefix + "log_cv", prefix + "score"]]
             table = table.loc[
                 np.isfinite(table[prefix + "log_m"]) & np.isfinite(table[prefix + "log_cv"]),
                 :,
             ]
             x_min, x_max = (
                 np.nanmin(table[prefix + "log_m"]),
                 np.nanmax(table[prefix + "log_m"]),
             )
 
     ordering_genes = adata.var["use_for_pca"] if "use_for_pca" in adata.var.columns else None
 
     mu_linspace = np.linspace(x_min, x_max, num=1000)
-    fit = (
-        adata.uns[uns_store_key]["disp_func"](mu_linspace)
-        if mode == "dispersion"
-        else adata.uns[uns_store_key]["SVR"](mu_linspace.reshape(-1, 1))
-    )
+    if "_dispersion" in mode:
+        mean = adata.uns[uns_store_key]["mean"]
+        cv = adata.uns[uns_store_key]["cv"]
+        svr_gamma = adata.uns[uns_store_key]["svr_gamma"]
+        fit, _ = get_prediction_by_svr(mean, cv, svr_gamma)
+        fit = fit(mu_linspace.reshape(-1, 1))
 
     plt.figure(figsize=figsize)
     plt.plot(mu_linspace, fit, alpha=0.4, color="r")
     valid_ind = (
         table.index.isin(ordering_genes.index[ordering_genes])
         if ordering_genes is not None
         else np.ones(table.shape[0], dtype=bool)
     )
 
     valid_disp_table = table.iloc[valid_ind, :]
-    if mode == "dispersion":
-        ax = plt.scatter(
-            valid_disp_table["mean_expression"],
-            valid_disp_table["dispersion_empirical"],
-            s=3,
-            alpha=1,
-            color="xkcd:red",
-        )
-    elif mode == "SVR":
+    if "_dispersion" in mode:
         ax = plt.scatter(
             valid_disp_table[prefix + "log_m"],
             valid_disp_table[prefix + "log_cv"],
             s=3,
             alpha=1,
             color="xkcd:red",
         )
 
     neg_disp_table = table.iloc[~valid_ind, :]
 
-    if mode == "dispersion":
-        ax = plt.scatter(
-            neg_disp_table["mean_expression"],
-            neg_disp_table["dispersion_empirical"],
-            s=3,
-            alpha=0.5,
-            color="xkcd:grey",
-        )
-    elif mode == "SVR":
+    if "_dispersion" in mode:
         ax = plt.scatter(
             neg_disp_table[prefix + "log_m"],
             neg_disp_table[prefix + "log_cv"],
             s=3,
             alpha=0.5,
             color="xkcd:grey",
         )
 
-    # plt.xlim((0, 100))
-    if mode == "dispersion":
-        plt.xscale("log")
     plt.yscale("log")
     plt.xlabel("Mean (log)")
     plt.ylabel("Dispersion (log)") if mode == "dispersion" else plt.ylabel("CV (log)")
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "feature_genes",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def exp_by_groups(
     adata: AnnData,
-    genes: list,
+    genes: List[str],
     layer: Optional[str] = None,
     group: Optional[str] = None,
     use_ratio: bool = False,
     use_smoothed: bool = True,
     log: bool = True,
     angle: int = 0,
     re_order: bool = True,
-    figsize: tuple = (4, 3),
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-):
+    figsize: Tuple[float] = (4, 3),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[sns.FacetGrid]:
     """Plot the (labeled) expression values of genes across different groups (time points).
 
-    This function can be used as a sanity check about the labeled species to see whether they increase or decrease
-    across time for a kinetic or degradation experiment, etc.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        genes: `list`
-            The list of genes that you want to plot the gene expression.
-        group: `string` (default: None)
-            Which group information to plot aganist (as elements on x-axis). Default is None, or no groups will be used.
-            Normally you should supply the column that indicates the time related to the labeling experiment. For
+    Args:
+        adata: an AnnData object,
+        genes: the list of genes that you want to plot the gene expression.
+        layer: the layer key containing the expression data. If None, the layer used would be inferred automatically.
+            Defaults to None.
+        group: the key of group information in `adata.obs` that will be plotted against to. If None, no groups will be
+            used. Normally you should supply the column that indicates the time related to the labeling experiment. For
             example, it can be either the labeling time for a kinetic experiment or the chase time for a degradation
-            experiment.
-        use_ratio: `bool` (default: False)
-            Whether to plot the fraction of expression (for example NTR, new to total ratio) over groups.
-        use_smoothed: `bool` (default: 'True')
-            Whether to use the smoothed data as gene expression.
-        log: `bool` (default: `True`)
-            Whether to log1p transform the expression data.
-        angle: `float` (default: `0`)
-            The angle to rotate the xtick labels for the purpose of avoiding overlapping between text.
-        re_order: `bool` (default: `True`)
-            Whether to reorder categories before drawing groups on the x-axis.
-        figsize: `string` (default: (4, 3))
-            Figure size of each facet.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'exp_by_groups', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-
-    Returns
-    -------
-        A violin plot that shows each gene's expression (row) across different groups (time), produced by seaborn.
+            experiment. Defaults to None.
+        use_ratio: whether to plot the fraction of expression (for example NTR, new to total ratio) over groups.
+            Defaults to False.
+        use_smoothed: whether to use the smoothed data as gene expression. Defaults to True.
+        log: whether to log1p transform the expression data. Defaults to True.
+        angle: the angle to rotate the xtick labels for the purpose of avoiding overlapping between text. Defaults to 0.
+        re_order: whether to reorder categories before drawing groups on the x-axis. Defaults to True.
+        figsize: the size of each panel of the figure. Defaults to (4, 3).
+        save_show_or_return: whether to save, show, or return the generated figure. Can be one of 'save', 'show', or
+            'return'. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'exp_by_groups', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        ValueError: invalid `genes`.
+        ValueError: invalid `group`.
+        ValueError: invalid `layer`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the `FacetGrid` with violin
+        plot generated with seaborn would be returned.
     """
 
-    import matplotlib.pyplot as plt
-    import seaborn as sns
-
     valid_genes = adata.var_names.intersection(genes)
     if len(valid_genes) == 0:
         raise ValueError("The adata object doesn't include any gene from the list you provided!")
     if group is not None and group not in adata.obs.keys():
         raise ValueError(f"The group {group} is not existed in your adata object!")
 
     (
@@ -954,110 +902,109 @@
         g.set_ylabels("log(Expression + 1)")
     else:
         g.set_ylabels("Expression")
 
     g.set_xlabels("")
     g.set(ylim=(0, None))
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "exp_by_groups",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
 
 
 def highest_frac_genes(
     adata: AnnData,
     n_top: int = 30,
-    gene_prefix_list: list = None,
+    gene_prefix_list: Optional[List[str]] = None,
     gene_prefix_only: bool = True,
-    show: Optional[bool] = True,
+    show: bool = True,
     save_path: str = None,
     ax: Optional[Axes] = None,
-    gene_annotations: Optional[list] = None,
+    gene_annotations: Optional[List[str]] = None,
     gene_annotation_key: str = "use_for_pca",
     log: bool = False,
     store_key: str = "highest_frac_genes",
-    orient: str = "v",
-    figsize: Union[list, None] = None,
-    layer: Union[str, None] = None,
-    title: Union[str, None] = None,
+    orient: Literal["v", "h"] = "v",
+    figsize: Optional[Tuple[float]] = None,
+    layer: Optional[str] = None,
+    title: Optional[str] = None,
     v_rotation: float = 35,
     **kwargs,
-):
-    """Plot top genes
+) -> Axes:
+    """Plot the top genes.
+
+    Args:
+        adata: an AnnData object.
+        n_top: the number of top genes to show. Defaults to 30.
+        gene_prefix_list: a list of gene name prefix. Defaults to None.
+        gene_prefix_only: whether to show prefix of genes only. It only takes effect if gene prefix is provided.
+            Defaults to True.
+        show: whether to show the plots. Defaults to True.
+        save_path: the path to save the figure. Defaults to None.
+        ax: the axis on which the graph will be plotted. If None, a new axis would be created. Defaults to None.
+        gene_annotations: annotations for genes, or annotations for gene prefix subsets. Defaults to None.
+        gene_annotation_key:  gene annotations key in adata.var. Defaults to "use_for_pca".
+        log: whether to use log scale. Defaults to False.
+        store_key: the key for storing expression percent results. Defaults to "highest_frac_genes".
+        orient: the orientation of the graph. Can be one of 'v' or 'h'. 'v' for genes in x-axis and 'h' for genes on
+            y-axis. Defaults to "v".
+        figsize: the size of each panel of the figure. Defaults to None.
+        layer: layer on which the gene percents will be computed. Defaults to None.
+        title: the title of the figure. Defaults to None.
+        v_rotation: rotation of text sticks when the direction is vertica. Defaults to 35.
+
+    Raises:
+        ValueError: invalid AnnData object.
+        NotImplementedError: invalid `orient`.
 
-    Parameters
-    ----------
-    adata:
-        adata input
-    n_top : int, optional
-        #top genes to show, by default 30
-    gene_prefix_list : list, optional
-        A list of gene name prefix, by default None
-    gene_prefix_only: bool, optional
-        whether to show prefix of genes only. It only takes effect if gene prefix list is provided, by default True
-    show :
-        whether to show the results, by default True
-    save_path : str, optional
-        path to save the figure, by default None
-    ax :
-        use an existing ax, by default None
-    gene_annotations : Optional[list], optional
-        Annotations for genes, or annotations for gene prefix subsets, by default None
-    gene_annotation_key : str, optional
-        gene annotations key in adata.var, by default "use_for_pca".
-        This option is not available for gene_prefix_list and thus users should
-        pass gene_annotations argument for the prefix list.
-    log : bool, optional
-        whether to use log scale, by default False
-    store_key : str, optional
-        key for storing expression percent results, by default "highest_frac_genes"
-    v_rotation:
-        rotation of text sticks when the direction is vertical
+    Returns:
+        The matplotlib Axes of the figure.
     """
-    import matplotlib.pyplot as plt
-    import seaborn as sns
 
     if ax is None:
         length = n_top * 0.4
         if figsize is None:
             if orient == "v":
                 fig, ax = plt.subplots(figsize=(length, 5))
             else:
                 fig, ax = plt.subplots(figsize=(7, length))
         else:
             fig, ax = plt.subplots(figsize=figsize)
     if log:
         ax.set_xscale("log")
 
-    adata = pp.highest_frac_genes(
+    adata = gene_selection.highest_frac_genes(
         adata,
         store_key=store_key,
         n_top=n_top,
         layer=layer,
         gene_prefix_list=gene_prefix_list,
         gene_prefix_only=gene_prefix_only,
     )
     if adata is None:
         # something wrong with user input or compute_top_genes_df
-        return
+        raise ValueError("Invalid adata. ")
     top_genes_df, selected_indices = (
         adata.uns[store_key]["top_genes_df"],
         adata.uns[store_key]["selected_indices"],
     )
 
     # TODO use top genes_df dataframe; however this logic currently
     # does not fit subset logics and may fail tests.
@@ -1077,15 +1024,15 @@
 
     if gene_annotations is None:
         if gene_annotation_key in adata.var:
             gene_annotations = adata.var[gene_annotation_key][selected_indices]
 
         else:
             main_warning(
-                "%s not in adata.var, ignoring the gene annotation key when plotting",
+                "%s not in adata.var, ignoring the gene annotation key when plotting" % gene_annotation_key,
                 indent_level=2,
             )
 
     if orient == "v":
         ax.set_xticklabels(ax.get_xticklabels(), rotation=v_rotation, ha="right")
         ax.set_xlabel("genes")
         ax.set_ylabel("fractions of total counts")
@@ -1104,15 +1051,15 @@
             ax2 = ax.twinx()
             ax2.set_ylim(ax.get_ylim())
             ax2.set_yticks(ax.get_yticks())
             ax2.set_yticks(list(range(len(gene_annotations))))
             ax2.set_yticklabels(gene_annotations)
             ax2.set_ylabel(gene_annotation_key)
     else:
-        raise NotImplementedError()
+        raise NotImplementedError("Invalid orient option")
 
     if title is None:
         if layer is None:
             ax.set_title("Rank by gene expression fraction")
         else:
             ax.set_title("Rank by %s fraction" % layer)
     if show:
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/pseudotime.py` & `dynamo-release-1.3.0/dynamo/plot/pseudotime.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,20 +1,30 @@
+from typing import Any, Dict, Tuple
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
 import numpy as np
+from anndata import AnnData
 
 from ..tools.utils import update_dict
 from .utils import save_fig
 
 
 def plot_direct_graph(
-    adata,
-    layout=None,
-    figsize=[6, 4],
-    save_show_or_return="show",
-    save_kwargs={},
-):
+    adata: AnnData,
+    layout: None = None,
+    figsize: Tuple[float, float] = (6, 4),
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> None:
+    """Not implemented."""
+
     df_mat = adata.uns["df_mat"]
 
     import matplotlib.pyplot as plt
     import networkx as nx
 
     edge_color = "gray"
 
@@ -54,25 +64,28 @@
             width=W / np.max(W) * 5,
             edge_cmap=plt.cm.Blues,
             options=options,
         )
     else:
         raise Exception("layout", layout, " is not supported.")
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_direct_graph",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/scPotential.py` & `dynamo-release-1.3.0/dynamo/plot/scPotential.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,46 +1,49 @@
+from typing import Any, Dict, Optional
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
+import numpy as np
+from anndata import AnnData
+from matplotlib.axes import Axes
+
 from ..tools.utils import update_dict
 from .utils import save_fig
 
 
 def show_landscape(
-    adata,
-    Xgrid,
-    Ygrid,
-    Zgrid,
-    basis="umap",
-    save_show_or_return="show",
-    save_kwargs={},
-):
+    adata: AnnData,
+    Xgrid: np.ndarray,
+    Ygrid: np.ndarray,
+    Zgrid: np.ndarray,
+    basis: str = "umap",
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[Axes]:
     """Plot the quasi-potential landscape.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains Xgrid, Ygrid and Zgrid data for visualizing potential landscape.
-        Xgrid: `numpy.ndarray`
-            x-coordinates of the Grid produced from the meshgrid function.
-        Ygrid: `numpy.ndarray`
-                y-coordinates of the Grid produced from the meshgrid function.
-        Zgrid: `numpy.ndarray`
-                z-coordinates or potential at each of the x/y coordinate.
-        basis: `str` (default: umap)
-            The method of dimension reduction. By default it is trimap. Currently it is not checked with Xgrid and Ygrid.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'show_landscape', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-
-    Returns
-    -------
-        A 3D plot showing the quasi-potential of each cell state.
-
+    Args:
+        adata: an AnnData object that contains Xgrid, Ygrid and Zgrid data for visualizing potential landscape.
+        Xgrid: x-coordinates of the Grid produced from the meshgrid function.
+        Ygrid: y-coordinates of the Grid produced from the meshgrid function.
+        Zgrid: z-coordinates or potential at each of the x/y coordinate.
+        basis: the method of dimension reduction. By default, it is trimap. Currently, it is not checked with Xgrid and
+            Ygrid. Defaults to "umap".
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'show_landscape', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your need. Defaults to {}.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib axes of the
+        figure would be returned.
     """
 
     if "grid_Pot_" + basis in adata.uns.keys():
         Xgrid_, Ygrid_, Zgrid_ = (
             adata.uns["grid_Pot_" + basis]["Xgrid"],
             adata.uns["grid_Pot_" + basis]["Ygrid"],
             adata.uns["grid_Pot_" + basis]["Zgrid"],
@@ -80,31 +83,34 @@
 
     # Add a color bar which maps values to colors.
     # fig.colorbar(surf, shrink=0.5, aspect=5)
     ax.set_xlabel(basis + "_1")
     ax.set_ylabel(basis + "_2")
     ax.set_zlabel("U")
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "show_landscape",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 # show_pseudopot(Xgrid, Ygrid, Zgrid)
 
 # % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 # % % -- Plot selected paths on pot. surface --
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/scVectorField.py` & `dynamo-release-1.3.0/dynamo/plot/scVectorField.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,14 @@
-from typing import List, Optional, Union
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
-import matplotlib
 import numpy as np
 import pandas as pd
 from anndata import AnnData
 
 # from scipy.sparse import issparse
 from matplotlib import cm
 from matplotlib.axes import Axes
@@ -47,91 +51,98 @@
 
 def cell_wise_vectors_3d(
     adata: AnnData,
     basis: str = "umap",
     x: int = 0,
     y: int = 1,
     z: int = 2,
-    ekey: str = None,
+    ekey: Optional[str] = None,
     vkey: str = "velocity_S",
-    X: Union[np.array, spmatrix] = None,
-    V: Union[np.array, spmatrix] = None,
+    X: Union[np.ndarray, spmatrix] = None,
+    V: Union[np.ndarray, spmatrix] = None,
     color: Union[str, List[str]] = None,
     layer: str = "X",
     background: Optional[str] = "white",
     ncols: int = 4,
-    figsize: tuple = (6, 4),
+    figsize: Tuple[float] = (6, 4),
     ax: Optional[Axes] = None,
-    inverse: True = False,
+    inverse: bool = False,
     cell_inds: str = "all",
     vector: str = "velocity",
     save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    quiver_3d_kwargs: dict = {
+    save_kwargs: Dict[str, Any] = {},
+    quiver_3d_kwargs: Dict[str, Any] = {
         "zorder": 3,
         "length": 2,
         "linewidth": 5,
         "arrow_length_ratio": 5,
         "norm": cm.colors.Normalize(),
         "cmap": cm.PRGn,
     },
     grid_color: Optional[str] = None,
     axis_label_prefix: Optional[str] = None,
-    axis_labels: Optional[list] = None,
-    elev: float = None,
-    azim: float = None,
+    axis_labels: Optional[List[str]] = None,
+    elev: Optional[float] = None,
+    azim: Optional[float] = None,
     alpha: Optional[float] = None,
-    show_magnitude=False,
-    titles: list = None,
+    show_magnitude: bool = False,
+    titles: Optional[List[str]] = None,
     **cell_wise_kwargs,
-):
+) -> np.ndarray:
     """Plot the velocity or acceleration vector of each cell.
 
-    Parameters
-    ----------
-        %(scatters.parameters.no_show_legend|kwargs|save_kwargs)s
-        ekey: `str` (default: "M_s")
-            The expression key
-        vkey: `str` (default: "velocity_S")
-            The velocity key
-        inverse: `bool` (default: False)
-            Whether to inverse the direction of the velocity vectors.
-        cell_inds: `str` or `list` (default: all)
-            the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell indices) or str
-            (Cell names).
-        quiver_size: `float` or None (default: None)
-            The size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is used to
-            calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
-            quiver_size) of the quiver. This is done via the `default_quiver_args` function which also calculate the
-            scale of the quiver (1 / quiver_length).
-        quiver_length: `float` or None (default: None)
-            The length of quiver. The quiver length which will be used to calculate scale of quiver. Note that befoe
-            applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler function. Scale
-            of quiver indicates the nuumber of data units per arrow length unit, e.g., m/s per plot width; a smaller
-            scale parameter makes the arrow longer.
-        vector: `str` (default: `velocity`)
-            Which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity field or
-            acceleration field will be plotted.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show
-            area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips &
-            tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq
-            paper: https://science.sciencemag.org/content/367/6482/1151.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'cell_wise_velocity', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        s_kwargs_dict: `dict` (default: {})
-            The dictionary of the scatter arguments.
-        cell_wise_kwargs:
-            Additional parameters that will be passed to plt.quiver function
-    Returns
-    -------
-        Nothing but a cell wise quiver plot.
+    Args:
+        adata: an AnnData object.
+        basis: the reduced dimension stored in adata.obsm. The specific basis key will be constructed in the following
+            priority if exits: 1) specific layer input +  basis 2) X_ + basis 3) basis. E.g. if basis is PCA, `scatters`
+            is going to look for 1) if specific layer is spliced, `spliced_pca` 2) `X_pca` (dynamo convention) 3) `pca`.
+            Defaults to "umap".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        z: the column index of the low dimensional embedding for the z-axis. Defaults to 2.
+        ekey: the expression key. Defaults to None.
+        vkey: the velocity key. Defaults to "velocity_S".
+        X: the expression array. If None, the array would be determined by `ekey` provided. Defaults to None.
+        V: the velocity array. If None, the array would be determined by `vkey` provided. Defaults to None.
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        layer: the layer of data to use for the scatter plot. Defaults to "X".
+        background: the background color of the figure. Defaults to "white".
+        ncols: the number of sub-plot columns. Defaults to 4.
+        figsize: the size of each sub-plot panel. Defaults to (6, 4).
+        ax: the axes to plot on. Only work when there is one graph to plot. If None, new axes would be created. Defaults
+            to None.
+        inverse: whether to inverse the direction of the velocity vectors. Defaults to False.
+        cell_inds: the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell indices)
+            or str (Cell names). Defaults to "all".
+        vector: which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity
+            field or acceleration field will be plotted. Defaults to "velocity".
+        save_show_or_return: whether to save, show or return the generated figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            an the save_fig function will use the {"path": None, "prefix": 'cell_wise_velocity', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        quiver_3d_kwargs: any other kwargs to be passed to `pyplot.quiver`. Defaults to { "zorder": 3, "length": 2,
+            "linewidth": 5, "arrow_length_ratio": 5, "norm": cm.colors.Normalize(), "cmap": cm.PRGn, }.
+        grid_color: the color of the grid lines. Defaults to None.
+        axis_label_prefix: the prefix of the axis labels. Defaults to None.
+        axis_labels: the axis labels. Defaults to None.
+        elev: the elevation angle in degrees rotates the camera above the plane pierced by the vertical axis, with a
+            positive angle corresponding to a location above that plane. Defaults to None.
+        azim: the azimuthal angle in degrees rotates the camera about the vertical axis, with a positive angle
+            corresponding to a right-handed rotation. Defaults to None.
+        alpha: the transparency of the colors. Defaults to None.
+        show_magnitude: whether to show original values or normalize the data. Defaults to False.
+        titles: the titles of the subplots. Defaults to None.
+
+    Raises:
+        ValueError: invalid `x`, `y`, or `z`.
+
+    Returns:
+        None will be returned by default. If `save_show_or_return` is set to 'return', an array of axes of the subplots
+        would be returned.
     """
 
     import matplotlib.pyplot as plt
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     def add_axis_label(ax, labels):
@@ -272,29 +283,33 @@
             # facecolors=color_vec,
             **quiver_3d_kwargs,
         )
         ax.set_title(titles[i])
         ax.set_facecolor(background)
         add_axis_label(ax, axis_labels)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "cell_wise_vectors_3d",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
+
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return axes
 
 
 def grid_vectors_3d():
     pass
 
 
@@ -348,82 +363,71 @@
 
 
 def line_integral_conv(
     adata: AnnData,
     basis: str = "umap",
     U_grid: Optional[np.ndarray] = None,
     V_grid: Optional[np.ndarray] = None,
-    xy_grid_nums: Union[tuple, list] = [50, 50],
-    method: str = "yt",
+    xy_grid_nums: Union[Tuple[int], List[int]] = [50, 50],
+    method: Literal["yt", "lic"] = "yt",
     cmap: str = "viridis",
     normalize: bool = False,
     density: float = 1,
-    lim=(0, 1),
+    lim: Tuple[float, float] = (0, 1),
     const_alpha: bool = False,
     kernellen: float = 100,
     V_threshold: Optional[float] = None,
     vector: str = "velocity",
-    file=None,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    g_kwargs_dict: dict = {},
+    file: Optional[str] = None,
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    g_kwargs_dict: Dict[str, Any] = {},
 ):
     """Visualize vector field with quiver, streamline and line integral convolution (LIC), using velocity estimates on a
      grid from the associated data. A white noise background will be used for texture as default. Adjust the bounds of
      lim in the range of [0, 1] which applies upper and lower bounds to the values of line integral convolution and
      enhance the visibility of plots. When const_alpha=False, alpha will be weighted spatially by the values of line
      integral convolution; otherwise a constant value of the given alpha is used.
 
-    Arguments
-    ---------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains U_grid and V_grid data
-        basis: `str` (default: trimap)
-            The dimension reduction method to use.
-        U_grid: 'np.ndarray' (default: None)
-            Original velocity on the first dimension of a 2 d grid.
-        V_grid: 'np.ndarray' (default: None)
-            Original velocity on the second dimension of a 2 d grid.
-        xy_grid_nums: `tuple` (default: (50, 50))
-            the number of grids in either x or y axis. The number of grids has to be the same on both dimensions.
-        method: 'float'
-            sigma2 is defined as sum(sum((Y - V)**2)) / (N * D)
-        cmap: 'float'
-            Percentage of inliers in the samples. This is an inital value for EM iteration, and it is not important.
-        normalize: 'float'
-            Paramerter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is a.
-        density: 'float'
-            Paramerter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is a.
-        lim: 'float'
-            Paramerter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is a.
-        const_alpha: 'float'
-            Paramerter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is a.
-        kernellen: 'float'
-            Paramerter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is a.
-        V_threshold: `float` or `None` (default: None)
-            The threshold of velocity value for visualization
-        vector: `str` (default: `velocity`)
-            Which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity field or
-            acceleration field will be plotted.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'line_integral_conv', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-
-    Returns
-    -------
-        Nothing, but plot the vector field with quiver, streamline and line integral convolution (LIC).
+    Args:
+        adata: an AnnData object that contains U_grid and V_grid data.
+        basis: the dimension reduction method to use. Defaults to "umap".
+        U_grid: original velocity on the first dimension of a 2 d grid. Defaults to None.
+        V_grid: original velocity on the second dimension of a 2 d grid. Defaults to None.
+        xy_grid_nums: the number of grids in either x or y axis. The number of grids has to be the same on both
+            dimensions. Defaults to [50, 50].
+        method: the method to visualize the data. Defaults to "yt".
+        cmap: the colormap used to plot the figure. Defaults to "viridis".
+        normalize: whether to normalize the original data. Defaults to False.
+        density: density of the streamlines. Defaults to 1.
+        lim: the value of line integral convolution will be clipped to the range of lim, which applies upper and lower
+            bounds to the values of line integral convolution and enhance the visibility of plots. Each element should
+            be in the range of [0,1].. Defaults to (0, 1).
+        const_alpha: whether to prevent the alpha from being weighted spatially by the values of line integral
+            convolution; otherwise a constant value of the given alpha is used. Defaults to False.
+        kernellen: the lens of kernel for convolution, which is the length over which the convolution will be performed.
+            For longer kernellen, longer streamline structure will appear. Defaults to 100.
+        V_threshold: the threshold of velocity value for visualization. Defaults to None.
+        vector: which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity
+            field or acceleration field will be plotted. Defaults to "velocity".
+        file: the path to save the slice figure. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'line_integral_conv', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        g_kwargs_dict: any other kwargs that would be passed to `dynamo.tl.grid_velocity_filter`. Defaults to {}.
+
+    Raises:
+        Exception: _description_
+        Exception: _description_
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be True, the generated `yt.SlicePlot` will
+        be returned.
     """
 
     import matplotlib.pyplot as plt
 
     X = adata.obsm["X_" + basis][:, :2] if "X_" + basis in adata.obsm.keys() else None
     V = adata.obsm[vector + "_" + basis][:, :2] if vector + "_" + basis in adata.obsm.keys() else None
 
@@ -530,31 +534,34 @@
             # plt.rc('axes', labelsize=8)
             slc.save(file, mpl_kwargs={"figsize": [2, 2]})
     elif method == "lic":
         # velocyto_tex = runlic(V_grid, V_grid, 100)
         # plot_LIC_gray(velocyto_tex)
         pass
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "line_integral_conv",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return slc
 
 
 @docstrings.with_indent(4)
 def cell_wise_vectors(
     adata: AnnData,
     basis: str = "umap",
@@ -564,351 +571,142 @@
     ekey: str = "M_s",
     vkey: str = "velocity_S",
     color: Union[str, List[str]] = "ntr",
     layer: str = "X",
     highlights: Optional[list] = None,
     labels: Optional[list] = None,
     values: Optional[list] = None,
-    theme: Optional[str] = None,
+    theme: Optional[
+        Literal[
+            "blue",
+            "red",
+            "green",
+            "inferno",
+            "fire",
+            "viridis",
+            "darkblue",
+            "darkred",
+            "darkgreen",
+        ]
+    ] = None,
     cmap: Optional[str] = None,
-    color_key: Union[dict, list] = None,
+    color_key: Union[Dict[str, str], List[str], None] = None,
     color_key_cmap: Optional[str] = None,
     background: Optional[str] = "white",
     ncols: int = 4,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
-    show_legend="on data",
+    pointsize: Optional[float] = None,
+    figsize: Tuple[float, float] = (6, 4),
+    show_legend: str = "on data",
     use_smoothed: bool = True,
     ax: Optional[Axes] = None,
-    sort: str = "raw",
+    sort: Literal["raw", "abs", "neg"] = "raw",
     aggregate: Optional[str] = None,
     show_arrowed_spines: bool = False,
-    inverse: True = False,
-    cell_inds: str = "all",
-    quiver_size: Optional[float] = 1,
-    quiver_length: Optional[float] = None,
-    vector: str = "velocity",
-    frontier: bool = False,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    s_kwargs_dict: dict = {},
-    projection: str = "2d",
-    **cell_wise_kwargs,
-):
-    """Plot the velocity or acceleration vector of each cell.
-    Parameters
-    ----------
-        %(scatters.parameters.no_show_legend|kwargs|save_kwargs)s
-        ekey: `str` (default: "M_s")
-            The expression key
-        vkey: `str` (default: "velocity_S")
-            The velocity key
-        inverse: `bool` (default: False)
-            Whether to inverse the direction of the velocity vectors.
-        cell_inds: `str` or `list` (default: all)
-            the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell indices) or str
-            (Cell names).
-        quiver_size: `float` or None (default: None)
-            The size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is used to
-            calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
-            quiver_size) of the quiver. This is done via the `default_quiver_args` function which also calculate the
-            scale of the quiver (1 / quiver_length).
-        quiver_length: `float` or None (default: None)
-            The length of quiver. The quiver length which will be used to calculate scale of quiver. Note that befoe
-            applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler function. Scale
-            of quiver indicates the nuumber of data units per arrow length unit, e.g., m/s per plot width; a smaller
-            scale parameter makes the arrow longer.
-        vector: `str` (default: `velocity`)
-            Which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity field or
-            acceleration field will be plotted.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show
-            area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips &
-            tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq
-            paper: https://science.sciencemag.org/content/367/6482/1151.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'cell_wise_velocity', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        s_kwargs_dict: `dict` (default: {})
-            The dictionary of the scatter arguments.
-        cell_wise_kwargs:
-            Additional parameters that will be passed to plt.quiver function
-    Returns
-    -------
-        Nothing but a cell wise quiver plot.
-    """
-
-    import matplotlib.pyplot as plt
-    from matplotlib import rcParams
-    from matplotlib.colors import to_hex
-
-    if projection == "2d":
-        projection_dim_indexer = [x, y]
-    elif projection == "3d":
-        projection_dim_indexer = [x, y, z]
-    if type(x) == str and type(y) == str:
-        if len(adata.var_names[adata.var.use_for_dynamics].intersection([x, y])) != 2:
-            raise ValueError(
-                "If you want to plot the vector flow of two genes, please make sure those two genes "
-                "belongs to dynamics genes or .var.use_for_dynamics is True."
-            )
-        X = adata[:, projection_dim_indexer].layers[ekey].A
-        V = adata[:, projection_dim_indexer].layers[vkey].A
-        layer = ekey
-    else:
-        if ("X_" + basis in adata.obsm.keys()) and (vector + "_" + basis in adata.obsm.keys()):
-            X = adata.obsm["X_" + basis][:, projection_dim_indexer]
-            V = adata.obsm[vector + "_" + basis][:, projection_dim_indexer]
-        else:
-            if "X_" + basis not in adata.obsm.keys():
-                layer, basis = basis.split("_")
-                reduceDimension(adata, layer=layer, reduction_method=basis)
-            if "kmc" not in adata.uns_keys():
-                cell_velocities(adata, vkey="velocity_S", basis=basis)
-                X = adata.obsm["X_" + basis][:, projection_dim_indexer]
-                V = adata.obsm[vector + "_" + basis][:, projection_dim_indexer]
-            else:
-                kmc = adata.uns["kmc"]
-                X = adata.obsm["X_" + basis][:, projection_dim_indexer]
-                V = kmc.compute_density_corrected_drift(X, kmc.Idx, normalize_vector=True)
-                adata.obsm[vector + "_" + basis] = V
-
-    X, V = X.copy(), V.copy()
-
-    V /= 3 * quiver_autoscaler(X, V)
-    if inverse:
-        V = -V
-    df = None
-    main_info("X shape: " + str(X.shape) + " V shape: " + str(V.shape))
-    if projection == "2d":
-        df = pd.DataFrame({"x": X[:, 0], "y": X[:, 1], "u": V[:, 0], "v": V[:, 1]})
-    elif projection == "3d":
-        df = pd.DataFrame({"x": X[:, 0], "y": X[:, 1], "z": X[:, 2], "u": V[:, 0], "v": V[:, 1], "w": V[:, 2]})
-    else:
-        raise NotImplementedError
-
-    if cell_inds == "all":
-        ix_choice = np.arange(adata.shape[0])
-    elif cell_inds == "random":
-        ix_choice = np.random.choice(np.range(adata.shape[0]), size=1000, replace=False)
-    elif type(cell_inds) is int:
-        ix_choice = np.random.choice(np.range(adata.shape[0]), size=cell_inds, replace=False)
-    elif type(cell_inds) is list:
-        if type(cell_inds[0]) is str:
-            cell_inds = [adata.obs_names.to_list().index(i) for i in cell_inds]
-        ix_choice = cell_inds
-
-    df = df.iloc[ix_choice, :]
-
-    if background is None:
-        _background = rcParams.get("figure.facecolor")
-        background = to_hex(_background) if type(_background) is tuple else _background
-
-    if quiver_size is None:
-        quiver_size = 1
-    if background == "black":
-        edgecolors = "white"
-    else:
-        edgecolors = "black"
-
-    head_w, head_l, ax_l, scale = default_quiver_args(quiver_size, quiver_length)  #
-    quiver_kwargs = {
-        "angles": "xy",
-        "scale": scale,
-        "scale_units": "xy",
-        "width": 0.0005,
-        "headwidth": head_w,
-        "headlength": head_l,
-        "headaxislength": ax_l,
-        "minshaft": 1,
-        "minlength": 1,
-        "pivot": "tail",
-        "linewidth": 0.1,
-        "edgecolors": edgecolors,
-        "alpha": 1,
-        "zorder": 10,
-    }
-    quiver_kwargs = update_dict(quiver_kwargs, cell_wise_kwargs)
-    quiver_3d_kwargs = {"arrow_length_ratio": scale}
-
-    axes_list, color_list, _ = scatters(
-        adata=adata,
-        basis=basis,
-        x=x,
-        y=y,
-        z=z,
-        color=color,
-        layer=layer,
-        highlights=highlights,
-        labels=labels,
-        values=values,
-        theme=theme,
-        cmap=cmap,
-        color_key=color_key,
-        color_key_cmap=color_key_cmap,
-        background=background,
-        ncols=ncols,
-        pointsize=pointsize,
-        figsize=figsize,
-        show_legend=show_legend,
-        use_smoothed=use_smoothed,
-        aggregate=aggregate,
-        show_arrowed_spines=show_arrowed_spines,
-        ax=ax,
-        sort=sort,
-        save_show_or_return="return",
-        frontier=frontier,
-        projection=projection,
-        **s_kwargs_dict,
-        return_all=True,
-    )
-
-    # single axis output
-    if type(axes_list) != list:
-        axes_list = [axes_list]
-    x0, x1 = df.iloc[:, 0], df.iloc[:, 1]
-    v0, v1 = df.iloc[:, 2], df.iloc[:, 3]
-
-    if projection == "3d":
-        x0, x1, x2 = df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2]
-        v0, v1, v2 = df.iloc[:, 3], df.iloc[:, 4], df.iloc[:, 5]
-
-    for i in range(len(axes_list)):
-        ax = axes_list[i]
-        if projection == "2d":
-            ax.quiver(
-                x0,
-                x1,
-                v0,
-                v1,
-                color=color_list[i],
-                facecolors=color_list[i],
-                **quiver_kwargs,
-            )
-        elif projection == "3d":
-            ax.quiver(
-                x0,
-                x1,
-                x2,
-                v0,
-                v1,
-                v2,
-                # color=color_list[i],
-                # facecolors=color_list[i],
-                **quiver_3d_kwargs,
-            )
-        ax.set_facecolor(background)
-
-    if save_show_or_return == "save":
-        s_kwargs = {
-            "path": None,
-            "prefix": "cell_wise_vector",
-            "dpi": None,
-            "ext": "pdf",
-            "transparent": True,
-            "close": True,
-            "verbose": True,
-        }
-        s_kwargs = update_dict(s_kwargs, save_kwargs)
-
-        save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
-        if projection != "3d":
-            plt.tight_layout()
-        plt.show()
-    elif save_show_or_return == "return":
-        return axes_list
-
-
-@docstrings.with_indent(4)
-def cell_wise_vectors(
-    adata: AnnData,
-    basis: str = "umap",
-    x: int = 0,
-    y: int = 1,
-    z: int = 2,
-    ekey: str = "M_s",
-    vkey: str = "velocity_S",
-    color: Union[str, List[str]] = "ntr",
-    layer: str = "X",
-    highlights: Optional[list] = None,
-    labels: Optional[list] = None,
-    values: Optional[list] = None,
-    theme: Optional[str] = None,
-    cmap: Optional[str] = None,
-    color_key: Union[dict, list] = None,
-    color_key_cmap: Optional[str] = None,
-    background: Optional[str] = "white",
-    ncols: int = 4,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
-    show_legend="on data",
-    use_smoothed: bool = True,
-    ax: Optional[Axes] = None,
-    sort: str = "raw",
-    aggregate: Optional[str] = None,
-    show_arrowed_spines: bool = False,
-    inverse: True = False,
+    inverse: bool = False,
     cell_inds: str = "all",
     quiver_size: Optional[float] = 1,
     quiver_length: Optional[float] = None,
     vector: str = "velocity",
     frontier: bool = False,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    s_kwargs_dict: dict = {},
-    projection: str = "2d",
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    s_kwargs_dict: Dict[str, Any] = {},
+    projection: Literal["2d", "3d"] = "2d",
     **cell_wise_kwargs,
-):
+) -> Optional[List[Axes]]:
     """Plot the velocity or acceleration vector of each cell.
 
-    Parameters
-    ----------
-        %(scatters.parameters.no_show_legend|kwargs|save_kwargs)s
-        ekey: `str` (default: "M_s")
-            The expression key
-        vkey: `str` (default: "velocity_S")
-            The velocity key
-        inverse: `bool` (default: False)
-            Whether to inverse the direction of the velocity vectors.
-        cell_inds: `str` or `list` (default: all)
-            the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell indices) or str
-            (Cell names).
-        quiver_size: `float` or None (default: None)
-            The size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is used to
-            calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
+    Args:
+        adata: an AnnData object.
+        basis: the reduced dimension stored in adata.obsm. The specific basis key will be constructed in the following
+            priority if exits: 1) specific layer input +  basis 2) X_ + basis 3) basis. E.g. if basis is PCA, `scatters`
+            is going to look for 1) if specific layer is spliced, `spliced_pca` 2) `X_pca` (dynamo convention) 3) `pca`.
+            Defaults to "umap".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        z: the column index of the low dimensional embedding for the z-axis. Defaults to 2.
+        ekey: the expression key. Defaults to "M_s".
+        vkey: the velocity key. Defaults to "velocity_S".
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        layer: the layer of data to use for the scatter plot. Defaults to "X".
+        highlights: the color group that will be highlighted. If highligts is a list of lists, each list is relate to
+            each color element. Defaults to None.
+        labels: an array of labels (assumed integer or categorical), one for each data sample. This will be used for
+            coloring the points in the plot according to their label. Note that this option is mutually exclusive to the
+            `values` option. Defaults to None.
+        values: an array of values (assumed float or continuous), one for each sample. This will be used for coloring
+            the points in the plot according to a colorscale associated to the total range of values. Note that this
+            option is mutually exclusive to the `labels` option. Defaults to None.
+        theme: A color theme to use for plotting. A small set of predefined themes are provided which have relatively
+            good aesthetics. Available themes are: {'blue', 'red', 'green', 'inferno', 'fire', 'viridis', 'darkblue',
+            'darkred', 'darkgreen'}. Defaults to None.
+        cmap: The name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to None.
+        color_key: the method to assign colors to categoricals. This can either be an explicit dict mapping labels to
+            colors (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct
+            category being provided in `labels`. Either way this mapping will be used to color points according to the
+            label. Note that if theme is passed then this value will be overridden by the corresponding option of the
+            theme. Defaults to None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to None.
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to None.
+        ncols: the number of columns for the figure. Defaults to 4.
+        pointsize: the scale of the point size. Actual point cell size is calculated as
+            `500.0 / np.sqrt(adata.shape[0]) * pointsize`. Defaults to None.
+        figsize: the width and height of a figure. Defaults to (6, 4).
+        show_legend: whether to display a legend of the labels. Defaults to "on data".
+        use_smoothed: whether to use smoothed values (i.e. M_s / M_u instead of spliced / unspliced, etc.). Defaults to
+            True.
+        ax: the matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
+            Defaults to None.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values. Defaults
+            to "raw".
+        aggregate: the column in adata.obs that will be used to aggregate data points. Defaults to None.
+        show_arrowed_spines: whether to show a pair of arrowed spines representing the basis of the scatter is currently
+            using. Defaults to False.
+        inverse: whether to inverse the direction of the velocity vectors. Defaults to False.
+        cell_inds: the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell indices)
+            or str (Cell names). Defaults to "all".
+        quiver_size: the size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is
+            used to calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
             quiver_size) of the quiver. This is done via the `default_quiver_args` function which also calculate the
-            scale of the quiver (1 / quiver_length).
-        quiver_length: `float` or None (default: None)
-            The length of quiver. The quiver length which will be used to calculate scale of quiver. Note that befoe
-            applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler function. Scale
-            of quiver indicates the nuumber of data units per arrow length unit, e.g., m/s per plot width; a smaller
-            scale parameter makes the arrow longer.
-        vector: `str` (default: `velocity`)
-            Which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity field or
-            acceleration field will be plotted.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show
-            area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips &
-            tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq
-            paper: https://science.sciencemag.org/content/367/6482/1151.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'cell_wise_velocity', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        s_kwargs_dict: `dict` (default: {})
-            The dictionary of the scatter arguments.
-        cell_wise_kwargs:
-            Additional parameters that will be passed to plt.quiver function
-    Returns
-    -------
-        Nothing but a cell wise quiver plot.
+            scale of the quiver (1 / quiver_length). Defaults to 1.
+        quiver_length: the length of quiver. The quiver length which will be used to calculate scale of quiver. Note
+            that befoe applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler
+            function. Scale of quiver indicates the nuumber of data units per arrow length unit, e.g., m/s per plot
+            width; a smaller scale parameter makes the arrow longer. Defaults to None.
+        vector: which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity
+            field or acceleration field will be plotted. Defaults to "velocity".
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to False.
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "show".
+        save_kwargs: A dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'cell_wise_velocity', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        s_kwargs_dict: any other kwargs that will be passed to `dynamo.pl.scatters`. Defaults to {}.
+        projection: the projection property of the matplotlib.Axes. Defaults to "2d".
+
+    Raises:
+        ValueError: invalid `x` or `y`.
+        NotImplementedError: Invalid `projection`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be `return`, the matplotlib axes of the
+        generated subplots would be returned.
     """
 
     import matplotlib.pyplot as plt
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     if projection == "2d":
@@ -953,15 +751,15 @@
     df = None
     main_info("X shape: " + str(X.shape) + " V shape: " + str(V.shape))
     if projection == "2d":
         df = pd.DataFrame({"x": X[:, 0], "y": X[:, 1], "u": V[:, 0], "v": V[:, 1]})
     elif projection == "3d":
         df = pd.DataFrame({"x": X[:, 0], "y": X[:, 1], "z": X[:, 2], "u": V[:, 0], "v": V[:, 1], "w": V[:, 2]})
     else:
-        raise NotImplementedError
+        raise NotImplementedError("Projection method %s is not implemented" % projection)
 
     if cell_inds == "all":
         ix_choice = np.arange(adata.shape[0])
     elif cell_inds == "random":
         ix_choice = np.random.choice(np.range(adata.shape[0]), size=1000, replace=False)
     elif type(cell_inds) is int:
         ix_choice = np.random.choice(np.range(adata.shape[0]), size=cell_inds, replace=False)
@@ -1067,32 +865,35 @@
                 v2,
                 # color=color_list[i],
                 # facecolors=color_list[i],
                 **quiver_3d_kwargs,
             )
         ax.set_facecolor(background)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "cell_wise_vector",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         if projection != "3d":
             plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return axes_list
 
 
 @docstrings.with_indent(4)
 def grid_vectors(
     adata: AnnData,
     basis: str = "umap",
@@ -1101,98 +902,150 @@
     ekey: str = "M_s",
     vkey: str = "velocity_S",
     color: Union[str, List[str]] = "ntr",
     layer: str = "X",
     highlights: Optional[list] = None,
     labels: Optional[list] = None,
     values: Optional[list] = None,
-    theme: Optional[str] = None,
+    theme: Optional[
+        Literal[
+            "blue",
+            "red",
+            "green",
+            "inferno",
+            "fire",
+            "viridis",
+            "darkblue",
+            "darkred",
+            "darkgreen",
+        ]
+    ] = None,
     cmap: Optional[str] = None,
-    color_key: Union[dict, list] = None,
+    color_key: Union[Dict[str, str], List[str], None] = None,
     color_key_cmap: Optional[str] = None,
     background: Optional[str] = "white",
     ncols: int = 4,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
-    show_legend="on data",
+    pointsize: Optional[float] = None,
+    figsize: Tuple[float] = (6, 4),
+    show_legend: str = "on data",
     use_smoothed: bool = True,
     ax: Optional[Axes] = None,
-    sort: str = "raw",
+    sort: Literal["raw", "abs", "neg"] = "raw",
     aggregate: Optional[str] = None,
     show_arrowed_spines: bool = False,
     inverse: bool = False,
     cell_inds: Union[str, list] = "all",
-    method: str = "gaussian",
-    xy_grid_nums: list = [50, 50],
+    method: Literal["SparseVFC", "gaussian"] = "gaussian",
+    xy_grid_nums: Tuple[int, int] = (50, 50),
     cut_off_velocity: bool = True,
     quiver_size: Optional[float] = None,
     quiver_length: Optional[float] = None,
     vector: str = "velocity",
     frontier: bool = False,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    s_kwargs_dict: dict = {},
-    q_kwargs_dict: dict = {},
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    s_kwargs_dict: Dict[str, Any] = {},
+    q_kwargs_dict: Dict[str, Any] = {},
     **grid_kwargs,
-):
+) -> Union[List[Axes], Axes, None]:
     """Plot the velocity or acceleration vector of each cell on a grid.
 
-    Parameters
-    ----------
-        %(scatters.parameters.no_show_legend|kwargs|save_kwargs)s
-        ekey: `str` (default: "M_s")
-            The expression key
-        vkey: `str` (default: "velocity_S")
-            The velocity key
-        inverse: `bool` (default: False)
-            Whether to inverse the direction of the velocity vectors.
-        cell_inds: `str` or `list` (default: all)
-            the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell integer
-            indices)  or str (Cell names).
-        method: `str` (default: `SparseVFC`)
-            Method to reconstruct the vector field. Currently it supports either SparseVFC (default) or the empirical
-            method Gaussian kernel method from RNA velocity (Gaussian).
-        xy_grid_nums: `tuple` (default: (50, 50))
-            the number of grids in either x or y axis.
-        cut_off_velocity: `bool` (default: True)
-            Whether to remove small velocity vectors from the recovered the vector field grid, either through the simple
-            Gaussian kernel (applicable to 2D) or the powerful sparseVFC approach.
-        quiver_size: `float` or None (default: None)
-            The size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is used to
-            calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
+    Args:
+        adata: an AnnData object.
+        basis: the reduced dimension stored in adata.obsm. The specific basis key will be constructed in the following
+            priority if exits: 1) specific layer input +  basis 2) X_ + basis 3) basis. E.g. if basis is PCA, `scatters`
+            is going to look for 1) if specific layer is spliced, `spliced_pca` 2) `X_pca` (dynamo convention) 3) `pca`.
+            Defaults to "umap".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        ekey: the expression key. Defaults to "M_s".
+        vkey: the velocity key. Defaults to "velocity_S".
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        layer: the layer of data to use for the scatter plot. Defaults to "X".
+        highlights: the color group that will be highlighted. If highligts is a list of lists, each list is relate to
+            each color element. Defaults to None.
+        labels: an array of labels (assumed integer or categorical), one for each data sample. This will be used for
+            coloring the points in the plot according to their label. Note that this option is mutually exclusive to the
+            `values` option. Defaults to None.
+        values: an array of values (assumed float or continuous), one for each sample. This will be used for coloring
+            the points in the plot according to a colorscale associated to the total range of values. Note that this
+            option is mutually exclusive to the `labels` option. Defaults to None.
+        theme: A color theme to use for plotting. A small set of predefined themes are provided which have relatively
+            good aesthetics. Available themes are: {'blue', 'red', 'green', 'inferno', 'fire', 'viridis', 'darkblue',
+            'darkred', 'darkgreen'}. Defaults to None.
+        cmap: The name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to None.
+        color_key: the method to assign colors to categoricals. This can either be an explicit dict mapping labels to
+            colors (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct
+            category being provided in `labels`. Either way this mapping will be used to color points according to the
+            label. Note that if theme is passed then this value will be overridden by the corresponding option of the
+            theme. Defaults to None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to None.
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to None.
+        ncols: the number of columns for the figure. Defaults to 4.
+        pointsize: the scale of the point size. Actual point cell size is calculated as
+            `500.0 / np.sqrt(adata.shape[0]) * pointsize`. Defaults to None.
+        figsize: the width and height of a figure. Defaults to (6, 4).
+        show_legend: whether to display a legend of the labels. Defaults to "on data".
+        use_smoothed: whether to use smoothed values (i.e. M_s / M_u instead of spliced / unspliced, etc.). Defaults to
+            True.
+        ax: the matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
+            Defaults to None.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values. Defaults
+            to "raw".
+        aggregate: the column in adata.obs that will be used to aggregate data points. Defaults to None.
+        show_arrowed_spines: whether to show a pair of arrowed spines representing the basis of the scatter is currently
+            using. Defaults to False.
+        inverse: whether to inverse the direction of the velocity vectors. Defaults to False.
+        cell_inds: the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell integer
+            indices) or str (Cell names). Defaults to "all".
+        method: method to reconstruct the vector field. Currently it supports either SparseVFC (default) or the
+            empirical method Gaussian kernel method from RNA velocity (Gaussian). Defaults to "gaussian".
+        xy_grid_nums: the number of grids in either x or y axis. Defaults to (50, 50).
+        cut_off_velocity: whether to remove small velocity vectors from the recovered the vector field grid, either
+            through the simple Gaussian kernel (applicable to 2D) or the powerful sparseVFC approach. Defaults to True.
+        quiver_size: the size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is
+            used to calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x
             quiver_size) of the quiver. This is done via the `default_quiver_args` function which also calculate the
-            scale of the quiver (1 / quiver_length).
-        quiver_length: `float` or None (default: None)
-            The length of quiver. The quiver length which will be used to calculate scale of quiver. Note that befoe
-            applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler function. Scale
-            of quiver indicates the nuumber of data units per arrow length unit, e.g., m/s per plot width; a smaller
-            scale parameter makes the arrow longer.
-        vector: `str` (default: `velocity`)
-            Which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity field or
-            acceleration field will be plotted.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show
-            area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips &
-            tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq
-            paper: https://science.sciencemag.org/content/367/6482/1151.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'grid_velocity', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        s_kwargs_dict: `dict` (default: {})
-            The dictionary of the scatter arguments.
-        q_kwargs_dict: `dict` (default: {})
-            The dictionary of the quiver arguments.
-        grid_kwargs:
-            Additional parameters that will be passed to velocity_on_grid function.
-
-    Returns
-    -------
-        Nothing but a quiver plot on the grid.
+            scale of the quiver (1 / quiver_length). Defaults to None.
+        quiver_length: the length of quiver. The quiver length which will be used to calculate scale of quiver. Note
+            that befoe applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler
+            function. Scale of quiver indicates the nuumber of data units per arrow length unit, e.g., m/s per plot
+            width; a smaller scale parameter makes the arrow longer. Defaults to None.
+        vector: which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity
+            field or acceleration field will be plotted. Defaults to "velocity".
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to False.
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'grid_velocity', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs.. Defaults to {}.
+        s_kwargs_dict: any other kwargs that would be passed to `dynamo.pl.scatters`. Defaults to {}.
+        q_kwargs_dict: any other kwargs that would be passed to `pyplot.quiver`. Defaults to {}.
+        **grid_kwargs: any other kwargs that would be passed to `dynamo.tl.grid_velocity_filter`.
+
+    Raises:
+        ValueError: invalid `x` or `y`.
+        NotImplementedError: invalid `method`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be `return`, the matplotlib axes of the
+        generated subplots would be returned.
     """
 
     import matplotlib.pyplot as plt
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     if type(x) == str and type(y) == str:
@@ -1296,15 +1149,15 @@
     elif "grid_velocity_" + basis in adata.uns.keys():
         X_grid, V_grid, _ = (
             adata.uns["grid_velocity_" + basis]["VecFld"]["X_grid"],
             adata.uns["grid_velocity_" + basis]["VecFld"]["V_grid"],
             adata.uns["grid_velocity_" + basis]["VecFld"]["D"],
         )
     else:
-        raise Exception(
+        raise NotImplementedError(
             "Vector field learning method {} is not supported or the grid velocity is collected for "
             "the current adata object.".format(method)
         )
 
     V_grid /= 3 * quiver_autoscaler(X_grid, V_grid)
     if inverse:
         V_grid = -V_grid
@@ -1377,31 +1230,34 @@
         for i in range(len(axes_list)):
             axes_list[i].quiver(X_grid[0], X_grid[1], V_grid[0], V_grid[1], **quiver_kwargs)
             axes_list[i].set_facecolor(background)
     else:
         axes_list.quiver(X_grid[0], X_grid[1], V_grid[0], V_grid[1], **quiver_kwargs)
         axes_list.set_facecolor(background)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "grid_velocity",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return axes_list
 
 
 @docstrings.with_indent(4)
 def streamline_plot(
     adata: AnnData,
     basis: str = "umap",
@@ -1410,91 +1266,144 @@
     ekey: str = "M_s",
     vkey: str = "velocity_S",
     color: Union[str, List[str]] = "ntr",
     layer: str = "X",
     highlights: Optional[list] = None,
     labels: Optional[list] = None,
     values: Optional[list] = None,
-    theme: Optional[str] = None,
+    theme: Optional[
+        Literal[
+            "blue",
+            "red",
+            "green",
+            "inferno",
+            "fire",
+            "viridis",
+            "darkblue",
+            "darkred",
+            "darkgreen",
+        ]
+    ] = None,
     cmap: Optional[str] = None,
-    color_key: Union[dict, list] = None,
+    color_key: Union[Dict[str, str], List[str], None] = None,
     color_key_cmap: Optional[str] = None,
     background: Optional[str] = "white",
     ncols: int = 4,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
-    show_legend="on data",
+    pointsize: Optional[float] = None,
+    figsize: Tuple[float, float] = (6, 4),
+    show_legend: str = "on data",
     use_smoothed: bool = True,
     ax: Optional[Axes] = None,
-    sort: str = "raw",
+    sort: Literal["raw", "abs", "neg"] = "raw",
     aggregate: Optional[str] = None,
     show_arrowed_spines: bool = False,
     inverse: bool = False,
     cell_inds: Union[str, list] = "all",
-    method: str = "gaussian",
-    xy_grid_nums: list = [50, 50],
+    method: Literal["gaussian", "SparseVFC"] = "gaussian",
+    xy_grid_nums: Tuple[int, int] = (50, 50),
     cut_off_velocity: bool = True,
     density: float = 1,
     linewidth: float = 1,
     streamline_alpha: float = 1,
     vector: str = "velocity",
     frontier: bool = False,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    s_kwargs_dict: dict = {},
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    s_kwargs_dict: Dict[str, Any] = {},
     **streamline_kwargs,
-):
+) -> List[Axes]:
     """Plot the velocity vector of each cell.
 
-    Parameters
-    ----------
-        %(scatters.parameters.no_show_legend|kwargs|save_kwargs)s
-        ekey: `str` (default: "M_s")
-            The expression key
-        vkey: `str` (default: "velocity_S")
-            The velocity key
-        inverse: `bool` (default: False)
-            Whether to inverse the direction of the velocity vectors.
-        cell_inds: `str` or `list` (default: all)
-            the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell integer
-            indices) or str (Cell names).
-        method: `str` (default: `SparseVFC`)
-            Method to reconstruct the vector field. Currently it supports either SparseVFC (default) or the empirical
-            method Gaussian kernel method from RNA velocity (Gaussian).
-        xy_grid_nums: `tuple` (default: (50, 50))
-            the number of grids in either x or y axis.
-        cut_off_velocity: `bool` (default: True)
-            Whether to remove small velocity vectors from the recovered the vector field grid, either through the simple
-            Gaussian kernel (applicable only to 2D) or the powerful sparseVFC approach.
-        density: `float` or None (default: 1)
-            density of the plt.streamplot function.
-        linewidth: `float` or None (default: 1)
-            multiplier of automatically calculated linewidth passed to the plt.streamplot function.
-        streamline_alpha: `float` or None (default: 1)
-            The alpha value applied to the vector field stream lines.
-        vector: `str` (default: `velocity`)
-            Which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity field or
-            acceleration field will be plotted.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show
-            area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips &
-            tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq
-            paper: https://science.sciencemag.org/content/367/6482/1151.
-       save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'streamline_plot', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        s_kwargs_dict: `dict` (default: {})
-            The dictionary of the scatter arguments.
-        streamline_kwargs:
-            Additional parameters that will be passed to plt.streamplot function
-    Returns
-    -------
-        Nothing but a streamline plot that integrates paths in the vector field.
+    Args:
+        adata: an AnnData object.
+        basis: the reduced dimension stored in adata.obsm. The specific basis key will be constructed in the following
+            priority if exits: 1) specific layer input +  basis 2) X_ + basis 3) basis. E.g. if basis is PCA, `scatters`
+            is going to look for 1) if specific layer is spliced, `spliced_pca` 2) `X_pca` (dynamo convention) 3) `pca`.
+            Defaults to "umap".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        ekey: the expression key. Defaults to "M_s".
+        vkey: the velocity key. Defaults to "velocity_S".
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        layer: the layer of data to use for the scatter plot. Defaults to "X".
+        highlights: the color group that will be highlighted. If highligts is a list of lists, each list is relate to
+            each color element. Defaults to None.
+        labels: an array of labels (assumed integer or categorical), one for each data sample. This will be used for
+            coloring the points in the plot according to their label. Note that this option is mutually exclusive to the
+            `values` option. Defaults to None.
+        values: an array of values (assumed float or continuous), one for each sample. This will be used for coloring
+            the points in the plot according to a colorscale associated to the total range of values. Note that this
+            option is mutually exclusive to the `labels` option. Defaults to None.
+        theme: A color theme to use for plotting. A small set of predefined themes are provided which have relatively
+            good aesthetics. Available themes are: {'blue', 'red', 'green', 'inferno', 'fire', 'viridis', 'darkblue',
+            'darkred', 'darkgreen'}. Defaults to None.
+        cmap: The name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to None.
+        color_key: the method to assign colors to categoricals. This can either be an explicit dict mapping labels to
+            colors (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct
+            category being provided in `labels`. Either way this mapping will be used to color points according to the
+            label. Note that if theme is passed then this value will be overridden by the corresponding option of the
+            theme. Defaults to None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to None.
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to None.
+        ncols: the number of columns for the figure. Defaults to 4.
+        pointsize: the scale of the point size. Actual point cell size is calculated as
+            `500.0 / np.sqrt(adata.shape[0]) * pointsize`. Defaults to None.
+        figsize: the width and height of a figure. Defaults to (6, 4).
+        show_legend: whether to display a legend of the labels. Defaults to "on data".
+        use_smoothed: whether to use smoothed values (i.e. M_s / M_u instead of spliced / unspliced, etc.). Defaults to
+            True.
+        ax: the matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
+            Defaults to None.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values. Defaults
+            to "raw".
+        aggregate: the column in adata.obs that will be used to aggregate data points. Defaults to None.
+        show_arrowed_spines: whether to show a pair of arrowed spines representing the basis of the scatter is currently
+            using. Defaults to False.
+        inverse: whether to inverse the direction of the velocity vectors. Defaults to False.
+        cell_inds: the cell index that will be chosen to draw velocity vectors. Can be a list of integers (cell integer
+            indices) or str (Cell names). Defaults to "all".
+        method: the method to reconstruct the vector field. Currently, it supports either SparseVFC (default) or the
+            empirical method Gaussian kernel method from RNA velocity (Gaussian). Defaults to "gaussian".
+        xy_grid_nums: the number of grids in either x or y axis. Defaults to (50, 50).
+        cut_off_velocity: whether to remove small velocity vectors from the recovered the vector field grid, either
+            through the simple Gaussian kernel (applicable only to 2D) or the powerful sparseVFC approach. Defaults to True.
+        density: density of the `plt.streamplot` function. Defaults to 1.
+        linewidth: multiplier of automatically calculated linewidth passed to the `plt.streamplot function`. Defaults
+            to 1.
+        streamline_alpha: the alpha value applied to the vector field streamlines. Defaults to 1.
+        vector: which vector type will be used for plotting, one of {'velocity', 'acceleration'} or either velocity
+            field or acceleration field will be plotted. Defaults to "velocity".
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to False.
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'streamline_plot', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs.. Defaults to {}.
+        s_kwargs_dict: any other kwargs that would be passed to `dynamo.pl.scatters`. Defaults to {}.
+
+    Raises:
+        ValueError: invalid `x` or `y`.
+        NotImplementedError: invalid `method`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to 'return', the matplotlib Axes objects of
+        the subplots would be returned.
     """
 
     import matplotlib.pyplot as plt
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     if type(x) == str and type(y) == str:
@@ -1598,15 +1507,15 @@
     elif "grid_velocity_" + basis in adata.uns.keys():
         X_grid, V_grid, _ = (
             adata.uns["grid_velocity_" + basis]["VecFld"]["X_grid"],
             adata.uns["grid_velocity_" + basis]["VecFld"]["V_grid"],
             adata.uns["grid_velocity_" + basis]["VecFld"]["D"],
         )
     else:
-        raise Exception(
+        raise NotImplementedError(
             "Vector field learning method {} is not supported or the grid velocity is collected for "
             "the current adata object.".format(method)
         )
 
     if inverse:
         V_grid = -V_grid
     streamplot_kwargs = {
@@ -1687,75 +1596,73 @@
         set_stream_line_alpha(s, streamline_alpha)
 
     if type(axes_list) == list:
         for i in range(len(axes_list)):
             ax = axes_list[i]
             streamplot_2d(ax)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "streamline_plot",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
-        # TODO: fix bug: the following line causing plotting issue
-        # plt.tight_layout()
+    if save_show_or_return in ["show", "both", "all"]:
+        plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return axes_list
 
 
 # refactor line_conv_integration
 
 
 def plot_energy(
     adata: AnnData,
     basis: Optional[str] = None,
     vecfld_dict: Optional[dict] = None,
-    figsize: Optional[tuple] = None,
+    figsize: Optional[Tuple[float, float]] = None,
     fig: Optional[Figure] = None,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-):
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+) -> Optional[Figure]:
     """Plot the energy and energy change rate over each optimization iteration.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with vector field function reconstructed.
-        basis: `str` or None (default: `None`)
-            The reduced dimension embedding (pca or umap, for example) of cells from which vector field function was
+    Args:
+        adata: an Annodata object with vector field function reconstructed.
+        basis: the reduced dimension embedding (pca or umap, for example) of cells from which vector field function was
             reconstructed. When basis is None, the velocity vector field function building from the full gene expression
-            space is used.
-        vecfld_dict: `str` or None (default: `None`)
-            The dictionary storing the information for the reconstructed velocity vector field function. If None, the
-            corresponding dictionary stored in the adata object will be used.
-        figsize: `[float, float]` or `None` (default: None)
-            The width and height of the resulting figure when fig is set to be None.
-        fig: `matplotlib.figure.Figure` or None
-            The figure object where panels of the energy or energy change rate over iteration plots will be appended to.
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'energy', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-
-    Returns
-    -------
-        Nothing, but plot the  energy or energy change rate each optimization iteration.
+            space is used. Defaults to None.
+        vecfld_dict: the dictionary storing the information for the reconstructed velocity vector field function. If
+            None, the corresponding dictionary stored in the adata object will be used. Defaults to None.
+        figsize: the width and height of the resulting figure when fig is set to be None. Defaults to None.
+        fig: the figure object where panels of the energy or energy change rate over iteration plots will be appended
+            to. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'energy', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs.. Defaults to {}.
+
+    Raises:
+        ValueError: invalid `basis`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to 'return', the matplotlib Figure object
+        of the graph would be returned.
     """
 
     import matplotlib.pyplot as plt
 
     if vecfld_dict is None:
         vf_key = "VecFld" if basis is None else "VecFld_" + basis
         if vf_key not in adata.uns.keys():
@@ -1784,25 +1691,28 @@
         ax = fig.add_subplot(1, 2, 2)
         ax.plot(Iterations, tecr, "k")
         ax.plot(tecr, "r.")
         ax.set_yscale("log")
         plt.xlabel("Iteration")
         plt.ylabel("Energy change rate")
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "energy",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return fig
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/scatters.py` & `dynamo-release-1.3.0/dynamo/plot/scatters.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,16 @@
 # code adapted from https://github.com/lmcinnes/umap/blob/7e051d8f3c4adca90ca81eb45f6a9d1372c076cf/umap/plot.py
 import warnings
 from numbers import Number
-from typing import List, Optional, Union
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import anndata
 import matplotlib.cm
 import numpy as np
 import pandas as pd
 from anndata import AnnData
 from matplotlib import patches
@@ -45,263 +50,221 @@
     y: int = 1,
     z: int = 2,
     color: str = "ntr",
     layer: str = "X",
     highlights: Optional[list] = None,
     labels: Optional[list] = None,
     values: Optional[list] = None,
-    theme: Optional[str] = None,
+    theme: Optional[
+        Literal[
+            "blue",
+            "red",
+            "green",
+            "inferno",
+            "fire",
+            "viridis",
+            "darkblue",
+            "darkred",
+            "darkgreen",
+        ]
+    ] = None,
     cmap: Optional[str] = None,
-    color_key: Union[dict, list] = None,
+    color_key: Union[Dict[str, str], List[str], None] = None,
     color_key_cmap: Optional[str] = None,
     background: Optional[str] = None,
     ncols: int = 4,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
-    show_legend="on data",
+    pointsize: Optional[float] = None,
+    figsize: Tuple[float, float] = (6, 4),
+    show_legend: str = "on data",
     use_smoothed: bool = True,
     aggregate: Optional[str] = None,
     show_arrowed_spines: bool = False,
-    ax: Optional[matplotlib.axes.Axes] = None,
-    sort: str = "raw",
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
+    ax: Optional[Axes] = None,
+    sort: Literal["raw", "abs", "neg"] = "raw",
+    save_show_or_return: Literal["save", "show", "return", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     return_all: bool = False,
     add_gamma_fit: bool = False,
     frontier: bool = False,
     contour: bool = False,
     ccmap: Optional[str] = None,
     alpha: float = 0.1,
     calpha: float = 0.4,
     sym_c: bool = False,
     smooth: bool = False,
     dpi: int = 100,
-    inset_dict: dict = {},
-    marker: str = None,
-    group: str = None,
-    add_group_gamma_fit=False,
-    affine_transform_degree: int = None,
-    affine_transform_A=None,
-    affine_transform_b=None,
-    stack_colors=False,
-    stack_colors_threshold=0.001,
-    stack_colors_title="stacked colors",
-    stack_colors_legend_size=2,
-    stack_colors_cmaps=None,
+    inset_dict: Dict[str, Any] = {},
+    marker: Optional[str] = None,
+    group: Optional[str] = None,
+    add_group_gamma_fit: bool = False,
+    affine_transform_degree: Optional[int] = None,
+    affine_transform_A: Optional[float] = None,
+    affine_transform_b: Optional[float] = None,
+    stack_colors: bool = False,
+    stack_colors_threshold: float = 0.001,
+    stack_colors_title: str = "stacked colors",
+    stack_colors_legend_size: float = 2,
+    stack_colors_cmaps: Optional[List[str]] = None,
     despline: bool = True,
     deaxis: bool = True,
-    despline_sides: Union[None, List[str]] = None,
-    projection="2d",
+    despline_sides: Optional[List[str]] = None,
+    projection: str = "2d",
     **kwargs,
-) -> Union[None, Axes]:
-    """Plot an embedding as points. Currently this only works
-    for 2D embeddings. While there are many optional parameters
-    to further control and tailor the plotting, you need only
-    pass in the trained/fit umap model to get results. This plot
-    utility will attempt to do the hard work of avoiding
-    overplotting issues, and make it easy to automatically
-    colour points by a categorical labelling or numeric values.
-    This method is intended to be used within a Jupyter
-    notebook with ``%matplotlib inline``.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        basis: `str`
-            The reduced dimension stored in adata.obsm. The specific basis key will be constructed in the following priority if exits: 1) specific layer input +  basis 2) X_ + basis 3) basis. E.g. if basis is PCA, `scatters` is going to look for 1) if specific layer is spliced, `spliced_pca` 2) `X_pca` (dynamo convention) 3) `pca`
-        x: `int` (default: `0`)
-            The column index of the low dimensional embedding for the x-axis.
-        y: `int` (default: `1`)
-            The column index of the low dimensional embedding for the y-axis.
-        color: `string` (default: `ntr`)
-            Any column names or gene expression, etc. that will be used for coloring cells.
-        layer: `str` (default: `X`)
-            The layer of data to use for the scatter plot.
-        highlights: `list` (default: None)
-            Which color group will be highlighted. if highligts is a list of lists - each list is relate to each color
-            element.
-        labels: array, shape (n_samples,) (optional, default None)
-            An array of labels (assumed integer or categorical),
-            one for each data sample.
-            This will be used for coloring the points in
-            the plot according to their label. Note that
-            this option is mutually exclusive to the ``values``
-            option.
-        values: array, shape (n_samples,) (optional, default None)
-            An array of values (assumed float or continuous),
-            one for each sample.
-            This will be used for coloring the points in
-            the plot according to a colorscale associated
-            to the total range of values. Note that this
-            option is mutually exclusive to the ``labels``
-            option.
-        theme: string (optional, default None)
-            A color theme to use for plotting. A small set of
-            predefined themes are provided which have relatively
-            good aesthetics. Available themes are:
-               * 'blue'
-               * 'red'
-               * 'green'
-               * 'inferno'
-               * 'fire'
-               * 'viridis'
-               * 'darkblue'
-               * 'darkred'
-               * 'darkgreen'
-        cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring
-            or shading points. If no labels or values are passed
-            this will be used for shading points according to
-            density (largely only of relevance for very large
-            datasets). If values are passed this will be used for
-            shading according the value. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        color_key: dict or array, shape (n_categories) (optional, default None)
-            A way to assign colors to categoricals. This can either be
-            an explicit dict mapping labels to colors (as strings of form
-            '#RRGGBB'), or an array like object providing one color for
-            each distinct category being provided in ``labels``. Either
-            way this mapping will be used to color points according to
-            the label. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        color_key_cmap:
-            The name of a matplotlib colormap to use for categorical coloring.
-            If an explicit ``color_key`` is not given a color mapping for
-            categories can be generated from the label list and selecting
-            a matching list of colors from the given colormap. Note
-            that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        background: string or None (optional, default 'None`)
-            The color of the background. Usually this will be either
-            'white' or 'black', but any color name will work. Ideally
-            one wants to match this appropriately to the colors being
-            used for points etc. This is one of the things that themes
-            handle for you. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        ncols: int (optional, default `4`)
-            Number of columns for the figure.
-        pointsize: `None` or `float` (default: None)
-            The scale of the point size. Actual point cell size is calculated as `500.0 / np.sqrt(adata.shape[0]) *
-            pointsize`
-        figsize: `None` or `[float, float]` (default: None)
-            The width and height of a figure.
-        show_legend: bool (optional, default True)
-            Whether to display a legend of the labels
-        use_smoothed: bool (optional, default True)
-            Whether to use smoothed values (i.e. M_s / M_u instead of spliced / unspliced, etc.).
-        aggregate: `str` or `None` (default: `None`)
-            The column in adata.obs that will be used to aggregate data points.
-        show_arrowed_spines: bool (optional, default False)
-            Whether to show a pair of arrowed spines representing the basis of the scatter is currently using.
-        ax: `matplotlib.Axis` (optional, default `None`)
-            The matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
-        sort: `str` (optional, default `raw`)
-            The method to reorder data so that high values points will be on top of background points. Can be one of
-            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure. If "both", it will save and plot the figure at the same time. If
-            "all", the figure will be saved, displayed and the associated axis and other object will be return.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent":
-            True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that
-            properly modify those keys according to your needs.
-        return_all: `bool` (default: `False`)
-            Whether to return all the scatter related variables. Default is False.
-        add_gamma_fit: `bool` (default: `False`)
-            Whether to add the line of the gamma fitting. This will automatically turn on if `basis` points to gene
-            names and those genes have went through gamma fitting.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show
-            area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips &
-            tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq
-            paper: https://science.sciencemag.org/content/367/6482/1151. If `contour` is set  to be True, `frontier`
-            will be ignored as `contour` also add an outlier for data points.
-        contour: `bool` (default: `False`)
-            Whether to add an countor on top of scatter plots. We use tricontourf to plot contour for non-gridded data.
-            The shapely package was used to create a polygon of the concave hull of the scatters. With the polygon we
-            then check if the mean of the triangulated points are within the polygon and use this as our condition to
+) -> Union[
+    Axes,
+    List[Axes],
+    Tuple[Axes, List[str], Literal["white", "black"]],
+    Tuple[List[Axes], List[str], Literal["white", "black"]],
+    None,
+]:
+    """Plot an embedding as points. Currently this only works for 2D embeddings. While there are many optional
+    parameters to further control and tailor the plotting, you need only pass in the trained/fit umap model to get
+    results. This plot utility will attempt to do the hard work of avoiding overplotting issues, and make it easy to
+    automatically color points by a categorical labelling or numeric values. This method is intended to be used within a
+    Jupyter notebook with `%matplotlib inline`.
+
+    Args:
+        adata: an AnnData object.
+        basis: the reduced dimension stored in adata.obsm. The specific basis key will be constructed in the following
+            priority if exits: 1) specific layer input +  basis 2) X_ + basis 3) basis. E.g. if basis is PCA, `scatters`
+            is going to look for 1) if specific layer is spliced, `spliced_pca` 2) `X_pca` (dynamo convention) 3) `pca`.
+            Defaults to "umap".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        z: the column index of the low dimensional embedding for the z-axis. Defaults to 2.
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        layer: the layer of data to use for the scatter plot. Defaults to "X".
+        highlights: the color group that will be highlighted. If highligts is a list of lists, each list is relate to
+            each color element. Defaults to None.
+        labels: an array of labels (assumed integer or categorical), one for each data sample. This will be used for
+            coloring the points in the plot according to their label. Note that this option is mutually exclusive to the
+            `values` option. Defaults to None.
+        values: an array of values (assumed float or continuous), one for each sample. This will be used for coloring
+            the points in the plot according to a colorscale associated to the total range of values. Note that this
+            option is mutually exclusive to the `labels` option. Defaults to None.
+        theme: A color theme to use for plotting. A small set of predefined themes are provided which have relatively
+            good aesthetics. Available themes are: {'blue', 'red', 'green', 'inferno', 'fire', 'viridis', 'darkblue',
+            'darkred', 'darkgreen'}. Defaults to None.
+        cmap: The name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to None.
+        color_key: the method to assign colors to categoricals. This can either be an explicit dict mapping labels to
+            colors (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct
+            category being provided in `labels`. Either way this mapping will be used to color points according to the
+            label. Note that if theme is passed then this value will be overridden by the corresponding option of the
+            theme. Defaults to None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to None.
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to None.
+        ncols: the number of columns for the figure. Defaults to 4.
+        pointsize: the scale of the point size. Actual point cell size is calculated as
+            `500.0 / np.sqrt(adata.shape[0]) * pointsize`. Defaults to None.
+        figsize: the width and height of a figure. Defaults to (6, 4).
+        show_legend: whether to display a legend of the labels. Defaults to "on data".
+        use_smoothed: whether to use smoothed values (i.e. M_s / M_u instead of spliced / unspliced, etc.). Defaults to
+            True.
+        aggregate: the column in adata.obs that will be used to aggregate data points. Defaults to None.
+        show_arrowed_spines: whether to show a pair of arrowed spines representing the basis of the scatter is currently
+            using. Defaults to False.
+        ax: the matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
+            Defaults to None.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values. Defaults
+            to "raw".
+        save_show_or_return: whether to save, show or return the figure. If "both", it will save and plot the figure at
+            the same time. If "all", the figure will be saved, displayed and the associated axis and other object will
+            be return. Defaults to "show".
+        save_kwargs: A dictionary that will passed to the save_fig function. By default it is an empty dictionary and
+            the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        return_all: whether to return all the scatter related variables. Defaults to False.
+        add_gamma_fit: whether to add the line of the gamma fitting. This will automatically turn on if `basis` points
+            to gene names and those genes have went through gamma fitting. Defaults to False.
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. If `contour` is set  to be True,
+            `frontier` will be ignored as `contour` also add an outlier for data points. Defaults to False.
+        contour: whether to add an countor on top of scatter plots. We use tricontourf to plot contour for non-gridded
+            data. The shapely package was used to create a polygon of the concave hull of the scatters. With the polygon
+            we then check if the mean of the triangulated points are within the polygon and use this as our condition to
             form the mask to create the contour. We also add the polygon shape as a frontier of the data point (similar
             to when setting `frontier = True`). When the color of the data points is continuous, we will use the same
             cmap as for the scatter points by default, when color is categorical, no contour will be drawn but just the
             polygon. cmap can be set with `ccmap` argument. See below. This has recently changed to use seaborn's
-            kdeplot.
-        ccmap: `str` or `None` (default: `None`)
-            The name of a matplotlib colormap to use for coloring or shading points the contour. See above.
-        calpha: `float` (default: `0.4`)
-            Contour alpha value passed into sns.kdeplot. The value should be inbetween [0, 1]
-        sym_c: `bool` (default: `False`)
-            Whether do you want to make the limits of continuous color to be symmetric, normally this should be used for
-            plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative values.
-        smooth: `bool` or `int` (default: `False`)
-            Whether do you want to further smooth data and how much smoothing do you want. If it is `False`, no
+            kdeplot. Defaults to False.
+        ccmap: the name of a matplotlib colormap to use for coloring or shading points the contour. See above.
+            Defaults to None.
+        alpha: the point's alpha (transparency) value. Defaults to 0.1.
+        calpha: contour alpha value passed into sns.kdeplot. The value should be inbetween [0, 1]. Defaults to 0.4.
+        sym_c: whether do you want to make the limits of continuous color to be symmetric, normally this should be used
+            for plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative
+            values. Defaults to False.
+        smooth: whether do you want to further smooth data and how much smoothing do you want. If it is `False`, no
             smoothing will be applied. If `True`, smoothing based on one step diffusion of connectivity matrix
-            (`.uns['moment_cnn'] will be applied. If a number larger than 1, smoothing will based on `smooth` steps of
+            (`.uns['moment_cnn']`) will be applied. If a number larger than 1, smoothing will based on `smooth` steps of
             diffusion.
-        dpi: `float`, (default: 100.0)
-            The resolution of the figure in dots-per-inch. Dots per inches (dpi) determines how many pixels the figure
+        dpi: the resolution of the figure in dots-per-inch. Dots per inches (dpi) determines how many pixels the figure
             comprises. dpi is different from ppi or points per inches. Note that most elements like lines, markers,
             texts have a size given in points so you can convert the points to inches. Matplotlib figures use Points per
             inch (ppi) of 72. A line with thickness 1 point will be 1./72. inch wide. A text with fontsize 12 points
             will be 12./72. inch heigh. Of course if you change the figure size in inches, points will not change, so a
             larger figure in inches still has the same size of the elements.Changing the figure size is thus like taking
             a piece of paper of a different size. Doing so, would of course not change the width of the line drawn with
             the same pen. On the other hand, changing the dpi scales those elements. At 72 dpi, a line of 1 point size
             is one pixel strong. At 144 dpi, this line is 2 pixels strong. A larger dpi will therefore act like a
             magnifying glass. All elements are scaled by the magnifying power of the lens. see more details at answer 2
             by @ImportanceOfBeingErnest:
-            https://stackoverflow.com/questions/47633546/relationship-between-dpi-and-figure-size
-        inset_dict: `dict` (default: {})
-            A dictionary of parameters in inset_ax. Example, something like {"width": "5%", "height": "50%", "loc":
+            https://stackoverflow.com/questions/47633546/relationship-between-dpi-and-figure-size. Defaults to 100.
+        inset_dict: a  dictionary of parameters in inset_ax. Example, something like {"width": "5%", "height": "50%", "loc":
             'lower left', "bbox_to_anchor": (0.85, 0.90, 0.145, 0.145), "bbox_transform": ax.transAxes, "borderpad": 0}
             See more details at https://matplotlib.org/api/_as_gen/mpl_toolkits.axes_grid1.inset_locator.inset_axes.html
-            or https://stackoverflow.com/questions/39803385/what-does-a-4-element-tuple-argument-for-bbox-to-anchor-mean
-            -in-matplotlib
-        marker: `str` (default: None)
-            The marker style. marker can be either an instance of the class or the text shorthand for a particular
-            marker. See matplotlib.markers for more information about marker styles.
-        affine_transform_degree:
-            Transform coordinates of points according to some degree.
-        affine_transform_A:
-            Coefficients in affine transformation Ax + b. 2D for now.
-        affine_transform_b:
-            Bias in affine transformation Ax + b.
-        stack_colors:
-            Whether to stack all color on the same ax passed above.
-            Currently only support 18 sequential matplotlib default cmaps assigning to different color groups.
-            (#colors should be smaller than 18, reuse if #colors > 18. TODO generate cmaps according to #colors)
-        stack_colors_threshold:
-            A threshold for filtering out points values < threshold when drawing each color.
-            E.g. if you do not want points with values < 1 showing up on axis, set threshold to be 1
-        stack_colors_title:
-            The title for the stack_color plot.
-        stack_colors_legend_size:
-            Control the legend size in stack color plot.
-        stack_colors_cmaps:
-            a list of cmaps that will be used to map values to color when stacking colors on the same subplot. The order corresponds to the order of color.
-        despline:
-            Whether to remove splines of the figure.
-        despline_sides:
-            Which side of splines should be removed. Can be any combination of `["bottom", "right", "top", "left"]`.
-        deaxis:
-            Whether to remove axis ticks of the figure.
-        kwargs:
-            Additional arguments passed to plt.scatters.
-
-    Returns
-    -------
-        result:
-            Either None or a matplotlib axis with the relevant plot displayed.
-            If you are using a notbooks and have ``%matplotlib inline`` set
-            then this will simply display inline.
+            or https://stackoverflow.com/questions/39803385/what-does-a-4-element-tuple-argument-for-bbox-to-anchor-mean-in-matplotlib.
+            Defaults to {}.
+        marker: the marker style. marker can be either an instance of the class or the text shorthand for a particular
+            marker. See matplotlib.markers for more information about marker styles. Defaults to None.
+        group: the key in `adata.obs` corresponding to the cell group data. Defaults to None.
+        add_group_gamma_fit: whether to plot the cell group's gamma fit results. Defaults to False.
+        affine_transform_degree: transform coordinates of points according to some degree. Defaults to None.
+        affine_transform_A: coefficients in affine transformation Ax + b. 2D for now. Defaults to None.
+        affine_transform_b: bias in affine transformation Ax + b. Defaults to None.
+        stack_colors: whether to stack all color on the same ax passed above. Currently only support 18 sequential
+            matplotlib default cmaps assigning to different color groups. (#colors should be smaller than 18, reuse if
+            #colors > 18. TODO generate cmaps according to #colors). Defaults to False.
+        stack_colors_threshold: a threshold for filtering out points values < threshold when drawing each color. E.g. if
+            you do not want points with values < 1 showing up on axis, set threshold to be 1. Defaults to 0.001.
+        stack_colors_title: the title for the stack_color plot. Defaults to "stacked colors".
+        stack_colors_legend_size: the legend size in stack color plot. Defaults to 2.
+        stack_colors_cmaps: a list of cmaps that will be used to map values to color when stacking colors on the same
+            subplot. The order corresponds to the order of color. Defaults to None.
+        despline: whether to remove splines of the figure. Defaults to True.
+        deaxis: whether to remove axis ticks of the figure. Defaults to True.
+        despline_sides: which side of splines should be removed. Can be any combination of `["bottom", "right", "top", "left"]`. Defaults to None.
+        projection: the projection property of the matplotlib.Axes. Defaults to "2d".
+        **kwargs: any other kwargs that would be passed to `pyplot.scatters`.
+
+    Raises:
+        ValueError: invalid adata object: lacking of required layers.
+        ValueError: `basis` not found in `adata.obsm`.
+        ValueError: invalid `x` or `y`.
+        ValueError: `labels` and `values` conflicted.
+        ValueError: invalid velocity estimation in `adata`.
+        ValueError: invalid velocity estimation in `adata`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return' or 'all', the matplotlib axes
+        object of the generated plots would be returned. If `return_all` is set to be true, the list of colors used and
+        the font color would also be returned.
     """
 
     import matplotlib.pyplot as plt
     from matplotlib import rcParams
     from matplotlib.colors import rgb2hex, to_hex
 
     # 2d is not a projection in matplotlib, default is None (rectilinear)
@@ -380,15 +343,15 @@
                 x, y = ["X_spliced"], ["X_unspliced"]
             elif "spliced" in adata.layers.keys() and "unspliced" in adata.layers.keys():
                 x, y = ["spliced"], ["unspliced"]
             elif "total" in adata.layers.keys() and "new" in adata.layers.keys():
                 x, y = ["total"], ["new"]
             else:
                 raise ValueError(
-                    "your adata oject is corrupted. Please make sure it has at least one of the following "
+                    "your adata object is corrupted. Please make sure it has at least one of the following "
                     "pair of layers:"
                     "'M_s', 'X_spliced', 'M_t', 'X_total', 'spliced', 'total' and "
                     "'M_u', 'X_unspliced', 'M_n', 'X_new', 'unspliced', 'new'. "
                 )
 
     if use_smoothed:
         mapper = get_mapper()
@@ -485,15 +448,15 @@
             prefix = cur_l + "_"
         elif ("X_" + cur_b) in adata.obsm.keys():
             prefix = "X_"
         elif cur_b in adata.obsm.keys():
             # special case for spatial for compatibility with other packages
             prefix = ""
         else:
-            raise Exception("Please check if basis=%s exists in adata.obsm" % basis)
+            raise ValueError("Please check if basis=%s exists in adata.obsm" % basis)
 
         basis_key = prefix + cur_b
         main_info("plotting with basis key=%s" % basis_key, indent_level=2)
 
         # if basis_key in adata.obsm.keys():
         #     if type(x) != str and type(y) != str:
         #         x_, y_ = (
@@ -654,15 +617,15 @@
                     anndata._core.views.ArrayView,
                     np.ndarray,
                 ]:
                     points = pd.DataFrame({"x": flatten(cur_x), "y": flatten(cur_y)})
                     points.columns = ["x", "y"]
                     cur_title = cur_b
                 else:
-                    raise Exception("Make sure your `x` and `y` are integers, gene names, column names in .obs, etc.")
+                    raise ValueError("Make sure your `x` and `y` are integers, gene names, column names in .obs, etc.")
 
                 if aggregate is not None:
                     groups, uniq_grp = (
                         _adata.obs[aggregate],
                         list(_adata.obs[aggregate].unique()),
                     )
                     group_color, group_median = (
@@ -852,15 +815,15 @@
                             xnew,
                             xnew * _adata[:, cur_b].var.loc[:, k_name].unique()
                             + _adata[:, cur_b].var.loc[:, "gamma_b"].unique(),
                             dashes=[6, 2],
                             c=font_color,
                         )
                     else:
-                        raise Exception(
+                        raise ValueError(
                             "_adata does not seem to have %s column. Velocity estimation is required "
                             "before running this function." % k_name
                         )
                 if group is not None and add_group_gamma_fit and cur_b in _adata.var_names[_adata.var.use_for_dynamics]:
                     cell_groups = _adata.obs[group]
                     unique_groups = np.unique(cell_groups)
                     k_suffix = "gamma_k" if _adata.uns["dynamics"]["experiment_type"] == "one-shot" else "gamma"
@@ -912,32 +875,36 @@
             "prefix": "scatters",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
+
+        # prevent the plot from being closed if the plot need to be shown or returned.
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
         save_fig(**s_kwargs)
         if background is not None:
             reset_rcParams()
-    elif save_show_or_return in ["show", "both", "all"]:
+    if save_show_or_return in ["show", "both", "all"]:
         if show_legend:
             plt.subplots_adjust(right=0.85)
 
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
-            # TODO: tight_layout causing "AttributeError: 'AnchoredSizeLocator' object has no attribute 'get_subplotspec'"
-            # plt.tight_layout()
+            plt.tight_layout()
 
         plt.show()
         if background is not None:
             reset_rcParams()
-    elif save_show_or_return in ["return", "all"]:
+    if save_show_or_return in ["return", "all"]:
         if background is not None:
             reset_rcParams()
 
         if return_all:
             return (axes_list, color_list, font_color) if total_panels > 1 else (ax, color_out, font_color)
         else:
             return axes_list if total_panels > 1 else ax
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/space.py` & `dynamo-release-1.3.0/dynamo/plot/space.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-from typing import Union
+from typing import List, Optional, Tuple, Union
 
 import anndata
 import numpy as np
+from matplotlib.axes import Axes
 
 from ..dynamo_logger import (
     main_critical,
     main_finish_progress,
     main_info,
     main_log_time,
     main_warning,
@@ -15,83 +16,72 @@
 
 docstrings.delete_params("scatters.parameters", "adata", "basis", "figsize")
 
 
 @docstrings.with_indent(4)
 def space(
     adata: anndata.AnnData,
-    color: Union[list, str, None] = None,
-    genes: Union[list, None] = [],
-    gene_cmaps=None,
+    color: Union[List[str], str, None] = None,
+    genes: Optional[List[str]] = [],
+    gene_cmaps: Optional[List[str]] = None,
     space_key: str = "spatial",
     width: float = 6,
     marker: str = ".",
-    pointsize: Union[float, None] = None,
+    pointsize: Optional[float] = None,
     dpi: int = 100,
     ps_sample_num: int = 1000,
     alpha: float = 0.8,
     stack_genes: bool = False,
     stack_genes_threshold: float = 0.01,
     stack_colors_legend_size: int = 10,
-    figsize=None,
+    figsize: Tuple[float, float] = None,
     *args,
     **kwargs
-):
-    """\
-    Scatter plot for physical coordinates of each cell.
-
-    Parameters
-    ----------
-        adata:
-            an Annodata object that contain the physical coordinates for each bin/cell, etc.
-        genes:
-            The gene list that will be used to plot the gene expression on the same scatter plot. Each gene will have a
-            different color. Can be a single gene name string and we will convert it to a list.
-        gene_cmaps:
-            A list of cmaps for mapping each gene's values according to a type of cmap when stacking gene colors on the same subplot. The order of each gene's cmap corresponds to the order in genes.
-        color: `string` (default: `ntr`)
-            Any or any list of column names or gene names, etc. that will be used for coloring cells. If `color` is not None, stack_genes will be disabled automatically because `color` can contain non numerical values.
-        space_key: `str`
-            The key to space coordinates.
-        stack_genes:
-            whether to show all gene plots on the same plot
-        stack_genes_threshold:
-            lower bound of gene values that will be drawn on the plot.
-        stack_colors_legend_size:
-            control the size of legend when stacking genes
-        alpha: `float`
-            The alpha value of the scatter points.
-        width: `int`
-        marker:
-            a string representing some marker from matplotlib
-            https://matplotlib.org/stable/api/markers_api.html#module-matplotlib.markers
-        pointsize: `float`
-            The size of the points on the scatter plot.
-        dpi: `float`, (default: 100.0)
-            The resolution of the figure in dots-per-inch. Dots per inches (dpi) determines how many pixels the figure
+) -> Union[Axes, List[Axes]]:
+    """Scatter plot for physical coordinates of each cell.
+
+    Args:
+        adata: an Annodata object that contain the physical coordinates for each bin/cell, etc.
+        color: any or any list of column names or gene names, etc. that will be used for coloring cells. If `color` is
+            not None, stack_genes will be disabled automatically because `color` can contain non numerical values.
+            Defaults to None.
+        genes: the gene list that will be used to plot the gene expression on the same scatter plot. Each gene will have
+            a different color. Can be a single gene name string and we will convert it to a list. Defaults to [].
+        gene_cmaps: a list of cmaps for mapping each gene's values according to a type of cmap when stacking gene colors
+            on the same subplot. The order of each gene's cmap corresponds to the order in genes. Defaults to None.
+        space_key: the key to space coordinates. Defaults to "spatial".
+        width: the width of the figure. Would be used when `figsize` is not specified. Defaults to 6.
+        marker: a string representing some marker from matplotlib
+            https://matplotlib.org/stable/api/markers_api.html#module-matplotlib.markers. Defaults to ".".
+        pointsize: the size of the points on the scatter plot. Defaults to None.
+        dpi: the resolution of the figure in dots-per-inch. Dots per inches (dpi) determines how many pixels the figure
             comprises. dpi is different from ppi or points per inches. Note that most elements like lines, markers,
             texts have a size given in points so you can convert the points to inches. Matplotlib figures use Points per
             inch (ppi) of 72. A line with thickness 1 point will be 1./72. inch wide. A text with fontsize 12 points
             will be 12./72. inch heigh. Of course if you change the figure size in inches, points will not change, so a
             larger figure in inches still has the same size of the elements.Changing the figure size is thus like taking
             a piece of paper of a different size. Doing so, would of course not change the width of the line drawn with
             the same pen. On the other hand, changing the dpi scales those elements. At 72 dpi, a line of 1 point size
             is one pixel strong. At 144 dpi, this line is 2 pixels strong. A larger dpi will therefore act like a
             magnifying glass. All elements are scaled by the magnifying power of the lens. see more details at answer 2
             by @ImportanceOfBeingErnest:
-            https://stackoverflow.com/questions/47633546/relationship-between-dpi-and-figure-size
-        ps_sample_num: `int`
-            The number of bins / cells that will be sampled to estimate the distance between different bin / cells.
-
-        %(scatters.parameters.no_adata|basis|figsize)s
-
-    Returns
-    -------
-        plots gene or cell feature of the adata object on the physical spatial coordinates.
+            https://stackoverflow.com/questions/47633546/relationship-between-dpi-and-figure-size. Defaults to 100.
+        ps_sample_num: the number of bins / cells that will be sampled to estimate the distance between different
+            bin / cells. Defaults to 1000.
+        alpha: the point's alpha (transparency) value. Defaults to 0.8.
+        stack_genes: whether to stack all genes on the same ax passed above. Defaults to False.
+        stack_genes_threshold: a threshold for filtering out points values < threshold when drawing each gene. Defaults
+            to 0.01.
+        stack_colors_legend_size: the legend size in stack gene plot. Defaults to 10.
+        figsize: the size of each subplot. Defaults to None.
+
+    Returns:
+        The matplotlib axes of the generated subplots.
     """
+
     main_info("Plotting spatial info on adata")
     main_log_time()
     if color is not None and stack_genes:
         main_warning(
             "Set `stack_genes` to False because `color` argument cannot be used with stack_genes. If you would like to stack genes (or other numeical values), please pass gene expression like column names into `gene` argument."
         )
         stack_genes = False
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/state_graph.py` & `dynamo-release-1.3.0/dynamo/plot/least_action_path.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,275 +1,210 @@
-from typing import Optional, Union
+from typing import Any, Dict, Optional, Tuple
 
 import numpy as np
-import pandas as pd
 from anndata import AnnData
 from matplotlib.axes import Axes
 
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
+from ..prediction.utils import (
+    interp_curvature,
+    interp_second_derivative,
+    kneedle_difference,
+)
 from ..tools.utils import update_dict
-from .scatters import docstrings, scatters
-from .utils import save_fig
+from ..utils import denormalize, normalize
+from .ezplots import plot_X, zscatter
+from .scatters import save_fig, scatters
+from .utils import map2color
 
-docstrings.delete_params("scatters.parameters", "aggregate", "kwargs", "save_kwargs")
 
-
-def create_edge_patch(posA, posB, width=1, node_rad=0, connectionstyle="arc3, rad=0.25", facecolor="k", **kwargs):
-    import matplotlib.patches as pat
-
-    style = "simple,head_length=%d,head_width=%d,tail_width=%d" % (
-        10,
-        10,
-        3 * width,
-    )
-    return pat.FancyArrowPatch(
-        posA=posA,
-        posB=posB,
-        arrowstyle=style,
-        connectionstyle=connectionstyle,
-        facecolor=facecolor,
-        shrinkA=node_rad,
-        shrinkB=node_rad,
-        **kwargs,
-    )
-
-
-def create_edge_patches_from_markov_chain(
-    P,
-    X,
-    width=3,
-    node_rad=0,
-    tol=1e-7,
-    connectionstyle="arc3, rad=0.25",
-    facecolor="k",
-    edgecolor="k",
-    alpha=0.8,
-    **kwargs
-):
-    """
-    create edge patches from a markov chain transition matrix. If P[i, j] > tol, an arrow is created from
-    node i to j.
-    """
-    arrows = []
-    for i in range(P.shape[0]):
-        for j in range(P.shape[0]):
-            if P[i, j] > tol:
-                if type(facecolor) == str:
-                    fc = facecolor
-                else:
-                    if type(facecolor) == pd.DataFrame:
-                        fc = facecolor.iloc[i, j]
-                    else:
-                        fc = facecolor[i, j]
-
-                if type(edgecolor) == str:
-                    ec = edgecolor
-                else:
-                    if type(edgecolor) == pd.DataFrame:
-                        ec = edgecolor.iloc[i, j]
-                    else:
-                        ec = edgecolor[i, j]
-
-                if type(alpha) == float:
-                    ac = alpha * min(2 * P[i, j], 1)
-                else:
-                    if type(alpha) == pd.DataFrame:
-                        ac = alpha.iloc[i, j]
-                    else:
-                        ac = alpha[i, j]
-
-                arrows.append(
-                    create_edge_patch(
-                        X[i],
-                        X[j],
-                        width=P[i, j] * width,
-                        node_rad=node_rad,
-                        connectionstyle=connectionstyle,
-                        facecolor=fc,
-                        edgecolor=ec,
-                        alpha=ac,
-                        **kwargs,
-                    )
-                )
-    return arrows
-
-
-@docstrings.with_indent(4)
-def state_graph(
+def least_action(
     adata: AnnData,
-    group: Optional[str] = None,
-    transition_threshold: float = 0.001,
-    keep_only_one_direction: bool = True,
-    edge_scale: float = 1,
-    state_graph: Union[None, np.ndarray] = None,
-    edgecolor: Union[None, np.ndarray, pd.DataFrame] = None,
-    facecolor: Union[None, np.ndarray, pd.DataFrame] = None,
-    graph_alpha: Union[None, np.ndarray, pd.DataFrame] = None,
-    basis: str = "umap",
     x: int = 0,
     y: int = 1,
+    basis: str = "pca",
     color: str = "ntr",
-    layer: str = "X",
-    highlights: Optional[list] = None,
-    labels: Optional[list] = None,
-    values: Optional[list] = None,
-    theme: Optional[str] = None,
-    cmap: Optional[str] = None,
-    color_key: Union[dict, list] = None,
-    color_key_cmap: Optional[str] = None,
-    background: Optional[str] = None,
-    ncols: int = 4,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
-    show_legend: bool = True,
-    use_smoothed: bool = True,
-    show_arrowed_spines: bool = False,
     ax: Optional[Axes] = None,
-    sort: str = "raw",
-    frontier: bool = False,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
-    s_kwargs_dict: dict = {"alpha": 1},
-    **kwargs
-):
-    """Plot a summarized cell type (state) transition graph. This function tries to create a model that summarizes
-    the possible cell type transitions based on the reconstructed vector field function.
-
-    Parameters
-    ----------
-        group: `str` or `None` (default: `None`)
-            The column in adata.obs that will be used to aggregate data points for the purpose of creating a cell type
-            transition model.
-        transition_threshold: `float` (default: 0.001)
-            The threshold of cell fate transition. Transition will be ignored if below this threshold.
-        keep_only_one_direction: `bool` (default: True)
-            Whether to only keep the higher transition between two cell type. That is if the transition rate from A to B
-            is higher than B to A, only edge from A to B will be plotted.
-        edge_scale: `float` (default: 1)
-            The scaler that can be used to scale the edge width of drawn transition graph.
-        state_graph: `np.ndarray`, `pd.DataFrame` or `None` (default: None)
-            The lumped transition graph between cell states (e.g. cell clusters or types).
-        edgecolor: `np.ndarray`, `pd.DataFrame` or `None` (default: None)
-            The edge color of the arcs that corresponds to the lumped transition graph between cell states.
-        facecolor: `np.ndarray`, `pd.DataFrame` or `None` (default: None)
-            The edge color of the arcs that corresponds to the lumped transition graph between cell states.
-        graph_alpha: `np.ndarray`, `pd.DataFrame` or `None` (default: None)
-            The alpha of the arcs that corresponds to the lumped transition graph between cell states.
-        %(scatters.parameters.no_aggregate|kwargs|save_kwargs)s
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'state_graph', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        s_kwargs_dict: `dict` (default: {"alpha": 1})
-            The dictionary of the scatter arguments.
-    Returns
-    -------
-        Plot the a model of cell fate transition that summarizes the possible lineage commitments between different cell
-        types.
+    save_show_or_return: Literal["save", "show", "return", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    **kwargs,
+) -> Optional[Axes]:
+    """Draw the least action paths on the low-dimensional embedding.
+
+    Args:
+        adata: an AnnData object.
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        basis: the basis used for dimension reduction. Defaults to "pca".
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        ax: the matplotlib axes object where new plots will be added to. Only applicable to drawing a single component.
+            If None, new axis would be created. Defaults to None.
+        save_show_or_return: whether the figure should be saved, show, or return. Can be one of "save", "show",
+            "return", "both", "all". "both" means that the figure would be shown and saved but not returned. Defaults to
+            "show".
+        save_kwargs:a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary and
+            the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be `"return"` or `"all"`, the matplotlib
+        axis of the generated figure would be returned.
     """
 
     import matplotlib.pyplot as plt
-    from matplotlib import rcParams
-    from matplotlib.colors import to_hex
 
-    aggregate = group
+    ax = scatters(adata, basis=basis, color=color, save_show_or_return="return", ax=ax, **kwargs)
 
-    points = adata.obsm["X_" + basis][:, [x, y]]
-    unique_group_obs = adata.obs[group].unique()
-    if type(unique_group_obs) is np.ndarray:
-        groups, uniq_grp = adata.obs[group], unique_group_obs.tolist()
-    elif type(unique_group_obs) is pd.Series:
-        groups, uniq_grp = adata.obs[group], unique_group_obs.to_list()
-    else:
-        groups, uniq_grp = adata.obs[group], list(unique_group_obs)
-    group_median = np.zeros((len(uniq_grp), 2))
-    # grp_size = adata.obs[group].value_counts()[uniq_grp].values
-    # s_kwargs_dict.update({"s": grp_size})
-
-    if state_graph is None:
-        Pl = adata.uns[group + "_graph"]["group_graph"]
-        if keep_only_one_direction:
-            Pl[Pl - Pl.T < 0] = 0
-        if transition_threshold is not None:
-            Pl[Pl < transition_threshold] = 0
-
-        Pl /= Pl.sum(1)[:, None] * edge_scale
-    else:
-        Pl = state_graph
-
-    for i, cur_grp in enumerate(uniq_grp):
-        group_median[i, :] = np.nanmedian(points[np.where(groups == cur_grp)[0], :2], 0)
-
-    if background is None:
-        _background = rcParams.get("figure.facecolor")
-        background = to_hex(_background) if type(_background) is tuple else _background
-
-    plt.figure(facecolor=_background)
-    axes_list, color_list, font_color = scatters(
-        adata=adata,
-        basis=basis,
-        x=x,
-        y=y,
-        color=color,
-        layer=layer,
-        highlights=highlights,
-        labels=labels,
-        values=values,
-        theme=theme,
-        cmap=cmap,
-        color_key=color_key,
-        color_key_cmap=color_key_cmap,
-        background=background,
-        ncols=ncols,
-        pointsize=pointsize,
-        figsize=figsize,
-        show_legend=show_legend,
-        use_smoothed=use_smoothed,
-        aggregate=aggregate,
-        show_arrowed_spines=show_arrowed_spines,
-        ax=ax,
-        sort=sort,
-        save_show_or_return="return",
-        frontier=frontier,
-        **s_kwargs_dict,
-        return_all=True,
-    )
-
-    edgecolor = "k" if edgecolor is None else edgecolor
-    facecolor = "k" if facecolor is None else facecolor
-    graph_alpha = 0.8 if graph_alpha is None else graph_alpha
-
-    arrows = create_edge_patches_from_markov_chain(
-        Pl, group_median, edgecolor=edgecolor, facecolor=facecolor, alpha=graph_alpha, tol=0.01, node_rad=15
-    )
-    if type(axes_list) == list:
-        for i in range(len(axes_list)):
-            for arrow in arrows:
-                axes_list[i].add_patch(arrow)
-                axes_list[i].set_facecolor(background)
-    else:
-        for arrow in arrows:
-            axes_list.add_patch(arrow)
-            axes_list.set_facecolor(background)
+    LAP_key = "LAP" if basis is None else "LAP_" + basis
+    lap_dict = adata.uns[LAP_key]
 
-    plt.axis("off")
+    for i, j in zip(lap_dict["prediction"], lap_dict["action"]):
+        ax.scatter(*i[:, [x, y]].T, c=map2color(j))
+        ax.plot(*i[:, [x, y]].T, c="k")
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
-            "prefix": "state_graph",
+            "prefix": "kinetic_curves",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        # prevent the plot from being closed if the plot need to be shown or returned.
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
-        if show_legend:
-            plt.subplots_adjust(right=0.85)
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
-        return axes_list, color_list, font_color
+    if save_show_or_return in ["return", "all"]:
+        return ax
+
+
+def lap_min_time(
+    adata: AnnData,
+    basis: str = "pca",
+    show_paths: bool = False,
+    show_elbow: bool = True,
+    show_elbow_func: bool = False,
+    color: str = "ntr",
+    figsize: Tuple[float, float] = (6, 4),
+    n_col: int = 3,
+    save_show_or_return: Literal["save", "show", "both", "all"] = "show",
+    save_kwargs: Dict[str, Any] = {},
+    **kwargs,
+) -> None:
+    """Plot minimum time of the least action paths.
+
+    Args:
+        adata: an AnnData object.
+        basis: the basis used for dimension reduction. Defaults to "pca".
+        show_paths: whether to plot the path together with the time. Defaults to False.
+        show_elbow: whether to mark the elbow point on time-action curve. Defaults to True.
+        show_elbow_func: whether to show the time-action curve that elbow is on. Defaults to False.
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        figsize: the size of the figure. Defaults to (6, 4).
+        n_col: the number of subplot columns. Defaults to 3.
+        save_show_or_return: whether to save or show the figure. Can be one of "save", "show", "both" or "all". "both"
+            and "all" have the same effect. The axis of the plot cannot be returned here. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide
+            a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        **kwargs: not used here.
+
+    Raises:
+        NotImplementedError: unsupported method to find the elbow.
+    """
+
+    import matplotlib.pyplot as plt
+
+    LAP_key = "LAP" if basis is None else "LAP_" + basis
+    min_t_dict = adata.uns[LAP_key]["min_t"]
+    method = min_t_dict["method"]
+
+    for k in range(len(min_t_dict["A"])):
+        A = min_t_dict["A"][k]
+        T = min_t_dict["T"][k]
+        i_elbow = min_t_dict["i_elbow"][k]
+        paths = min_t_dict["paths"][k]
+
+        num_t = len(A)
+        if method == "hessian":
+            T_, A_ = normalize(T), normalize(A)
+            T_, der = interp_second_derivative(T_, A_)
+            T_ = denormalize(T_, np.min(T), np.max(T))
+        elif method == "curvature":
+            T_, A_ = normalize(T), normalize(A)
+            T_, der = interp_curvature(T_, A_)
+            T_ = denormalize(T_, np.min(T), np.max(T))
+        elif method == "kneedle":
+            der = kneedle_difference(T, A)
+            T_ = T
+        else:
+            raise NotImplementedError(f"Unsupported method {method}.")
+
+        if show_paths:
+            n_row = int(np.ceil((num_t + 1) / n_col))
+        else:
+            n_row = 1
+            n_col = 1
+
+        figsize = (figsize[0] * n_col, figsize[1] * n_row) if figsize is not None else (4 * n_col, 4 * n_row)
+        fig, axes = plt.subplots(n_row, n_col, figsize=figsize, sharex=False, sharey=False, squeeze=False)
+
+        for c in range(1 + num_t):
+            i, j = c % n_row, c // n_row
+
+            if c == 0:
+                axes[i, j].plot(T, A)
+                if show_elbow:
+                    axes[i, j].plot([T[i_elbow], T[i_elbow]], [np.max(A), np.min(A)], "--")
+                axes[i, j].set_xlabel("LAP time")
+                axes[i, j].set_ylabel("action")
+                # axes[i, j].set_title(f'pair {i}')
+
+                if show_elbow_func:
+                    ax2 = axes[i, j].twinx()
+                    ax2.plot(T_, der, c="r")
+                    ax2.tick_params(axis="y", labelcolor="r")
+                    ax2.set_ylabel(method)
+
+            elif show_paths:
+                plt.sca(axes[i, j])
+                zscatter(adata, basis=basis, color=color)
+                plot_X(paths[c - 1], c="k")
+                plt.title(f"path {c-1}")
+                # scatters(adata, basis=basis, color=color, ax=axes[i, j], **kwargs)
+                # axes[i, j].scatter(*i[:, [x, y]].T, c=map2color(j))
+
+        if save_show_or_return in ["save", "both", "all"]:
+            s_kwargs = {
+                "path": None,
+                "prefix": "kinetic_curves",
+                "dpi": None,
+                "ext": "pdf",
+                "transparent": True,
+                "close": True,
+                "verbose": True,
+            }
+
+            # prevent the plot from being closed if the plot need to be shown or returned.
+            if save_show_or_return == "both":
+                s_kwargs["close"] = False
+
+            s_kwargs = update_dict(s_kwargs, save_kwargs)
+
+            save_fig(**s_kwargs)
+        if save_show_or_return in ["show", "both"]:
+            plt.tight_layout()
+            plt.show()
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/streamtube.py` & `dynamo-release-1.3.0/dynamo/plot/streamtube.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,70 +1,78 @@
 from numbers import Number
+from typing import Any, Dict, List, Optional, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import numpy as np
 import pandas as pd
+from anndata import AnnData
 from pandas.api.types import is_categorical_dtype
 
 from ..configuration import _themes
 from ..tools.Markov import prepare_velocity_grid_data
 from .utils import _to_hex, is_cell_anno_column, is_gene_name
 
 
 def plot_3d_streamtube(
-    adata,
-    color,
-    layer,
-    group,
-    init_group,
-    basis="umap",
-    dims=[0, 1, 2],
-    theme=None,
-    background=None,
-    cmap=None,
-    color_key=None,
-    color_key_cmap=None,
-    html_fname=None,
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    color: str,
+    layer: str,
+    group: str,
+    init_group: str,
+    basis: str = "umap",
+    dims: List[int] = [0, 1, 2],
+    theme: Optional[str] = None,
+    background: Optional[str] = None,
+    cmap: Optional[str] = None,
+    color_key: Union[Dict[str, str], List[str], None] = None,
+    color_key_cmap: Optional[str] = None,
+    html_fname: Optional[str] = None,
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
 ):
     """Plot a interative 3d streamtube plot via plotly.
 
     A streamtube is a tubular region surrounded by streamlines that form a closed loop. It's a continuous version of a
     streamtube plot (3D quiver plot) and can provide insight into flow data from natural systems. The color of tubes is
     determined by their local norm, and the diameter of the field by the local divergence of the vector field.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            An Annodata object, must have vector field reconstructed for the input `basis` whose dimension should at
+    Args:
+        adata: an Annodata object, must have vector field reconstructed for the input `basis` whose dimension should at
             least 3D.
-        color: `string` (default: `ntr`)
-            Any column names or gene expression, etc. that will be used for coloring cells.
-        group: `str`
-            The column names of adata.obs that will be used to search for cells, together with `init_group` to set the
-            initial state of the streamtube.
-        init_group: `str`
-            The group name among all names in `group` that will be used to set the initial states of the stream tube.
-        basis: `str`
-            The reduced dimension.
-        html_fname: `str` or None
-            html file name that will be use to save the streamtube interactive plot.
-        dims: `list` (default: `[0, 1, 2]`)
-            The number of dimensions that will be used to construct the vector field for streamtube plot.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-
-    Returns
-    -------
-        Nothing but render an interactive streamtube plot. If html_fname is not None, the plot will save to a html file.
+        color: any column names or gene expression, etc. that will be used for coloring cells.
+        layer: the layer key of the expression data.
+        group: the column names of adata.obs that will be used to search for cells, together with `init_group` to set
+            the initial state of the streamtube.
+        init_group: the group name among all names in `group` that will be used to set the initial states of the stream
+            tube.
+        basis: the reduced dimension. Defaults to "umap".
+        dims: the number of dimensions that will be used to construct the vector field for streamtube plot. Defaults to
+            [0, 1, 2].
+        theme: the theme of the plot. Defaults to None.
+        background: the background color of the plot. Defaults to None.
+        cmap: The name of a matplotlib colormap to use for coloring the plots. Defaults to None.
+        color_key: the method to assign colors to categoricals. Defaults to None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. Defaults to None.
+        html_fname: html file name that will be used to save the streamtube interactive plot. Defaults to None.
+        save_show_or_return: whether to save, show, or return the figures. Defaults to "show".
+        save_kwargs:  A dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        ImportError: plotly is not installed.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the generated plotly figure
+        would be returned.
     """
 
     try:
         # 3D streamtube:
         import plotly.graph_objects as go
     except ImportError:
         raise ImportError("You need to install the package `plotly`. Install hiveplotlib via `pip install plotly`")
@@ -140,15 +148,15 @@
         X,
         [60, 60, 60],
         density=grid_kwargs_dict["density"],
         smooth=grid_kwargs_dict["smooth"],
         n_neighbors=grid_kwargs_dict["n_neighbors"],
     )
 
-    from .vectorfield.utils import vecfld_from_adata
+    from ..vectorfield.utils import vecfld_from_adata
 
     VecFld, func = vecfld_from_adata(adata, basis="umap")
 
     velocity_grid = func(X_grid)
 
     fig = go.Figure(
         data=go.Streamtube(
@@ -184,16 +192,16 @@
         x=X[:, 0],
         y=X[:, 1],
         z=X[:, 2],
         mode="markers",
         marker=dict(size=2, color=colors.values),
     )
 
-    if save_show_or_return == "save" or html_fname is not None:
+    if (save_show_or_return in ["save", "both", "all"]) or html_fname is not None:
         html_fname = "streamtube_" + color + "_" + group + "_" + init_group if html_fname is None else html_fname
         save_kwargs_ = {"file": html_fname, "auto_open": True}
         save_kwargs_.update(save_kwargs)
         fig.write_html(**save_kwargs_)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         fig.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return fig
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/theme.py` & `dynamo-release-1.3.0/dynamo/plot/theme.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/plot/time_series.py` & `dynamo-release-1.3.0/dynamo/plot/time_series.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,104 +1,98 @@
 # include pseudotime and predict cell trajectory
-from typing import Optional, Union
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import numpy as np
 from anndata import AnnData
 from scipy.interpolate import interp1d
 from scipy.sparse import issparse
+from seaborn import FacetGrid
+from seaborn.matrix import ClusterGrid
 
 from ..docrep import DocstringProcessor
 from ..external.hodge import ddhodge
 from ..prediction.utils import fetch_exprs
 from ..tools.utils import update_dict
 from .utils import _to_hex, save_fig
 
 docstrings = DocstringProcessor()
 
 
 @docstrings.get_sectionsf("kin_curves")
 def kinetic_curves(
     adata: AnnData,
-    genes: list,
+    genes: List[str],
     mode: str = "vector_field",
     basis: Optional[str] = None,
     layer: str = "X",
     project_back_to_high_dim: bool = True,
     tkey: str = "potential",
     dist_threshold: float = 1e-10,
     ncol: int = 4,
     color: Union[list, None] = "ntr",
     c_palette: str = "Set2",
     standard_scale: int = 0,
     traj_ind: int = 0,
     log: bool = True,
     save_show_or_return: str = "show",
     save_kwargs: dict = {},
-):
+) -> Optional[FacetGrid]:
     """Plot the gene expression dynamics over time (pseudotime or inferred real time) as kinetic curves.
 
     Note that by default `potential` estimated with the diffusion graph built from reconstructed vector field will be
     used as the measure of pseudotime.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        genes: `list`
-            The gene names whose gene expression will be faceted.
-        mode: `str` (default: `vector_field`)
-            Which data mode will be used, either vector_field, lap or pseudotime. if mode is vector_field, the
+    Args:
+        adata: an Annodata object.
+        genes: the gene names whose gene expression will be faceted.
+        mode: which data mode will be used, either vector_field, lap or pseudotime. if mode is vector_field, the
             trajectory predicted by vector field function will be used; if mode is lap, the trajectory predicted by
             least action path will be used otherwise pseudotime trajectory (defined by time argument) will be used.
-            By default `potential` estimated with the diffusion graph built from reconstructed vector field will be used
-            as pseudotime when mode is pseudotime.
-        basis: `str` or None (default: `None`)
-            The embedding data used for drawing the kinetic gene expression curves, only used when mode is
-            `vector_field`.
-        layer: `str` (default: X)
-            Which layer of expression value will be used. Not used if mode is `vector_field`.
-        project_back_to_high_dim: `bool` (default: `False`)
-            Whether to map the coordinates in low dimension back to high dimension to visualize the gene expression
-            curves, only used when mode is `vector_field` and basis is not `X`. Currently only works when basis is 'pca'
-            and 'umap'.
-        color: `list` or None (default: 'ntr')
-            A list of attributes of cells (column names in the adata.obs) will be used to color cells.
-        tkey: `str` (default: `potential`)
-            The .obs column that will be used for timing each cell, only used when mode is not `vector_field`.
-        dist_threshold: `float` (default: 1e-10)
-            The threshold for the distance between two points in the gene expression state, i.e, x(t), x(t+1). If below
-            this threshold, we assume steady state is achieved and those data points will not be considered. This
-            argument is ignored when mode is `pseudotime`.
-        ncol: `int` (default: 4)
-            Number of columns in each facet grid.
-        c_palette: Name of color_palette supported in seaborn color_palette function (default: None)
-            The color map function to use.
-        standard_scale: `int` (default: 1)
-            Either 0 (rows) or 1 (columns). Whether or not to standardize that dimension, meaning for each row or
-            column, subtract the minimum and divide each by its maximum.
-        traj_ind: `int` (default: 0)
-            If the element from the dictionary is a list (obtained from a list of trajectories), the index of trajectory
-            that will be selected for visualization.
-        log: `bool` (default: True)
-            Whether to log1p transform your data before data visualization. If expression data is from adata object,
+            Defaults to "vector_field".
+        basis: the embedding data used for drawing the kinetic gene expression curves, only used when mode is
+            `vector_field`. Defaults to None.
+        layer: the key to the layer of expression value will be used. Not used if mode is `vector_field`. Defaults
+            to "X".
+        project_back_to_high_dim: whether to map the coordinates in low dimension back to high dimension to visualize
+            the gene expression curves, only used when mode is `vector_field` and basis is not `X`. Currently only works
+            when basis is 'pca' and 'umap'. Defaults to True.
+        tkey: the .obs column that will be used for timing each cell, only used when mode is not `vector_field`.
+            Defaults to "potential".
+        dist_threshold: the threshold for the distance between two points in the gene expression state, i.e, x(t),
+            x(t+1). If below this threshold, we assume steady state is achieved and those data points will not be
+            considered. This argument is ignored when mode is `pseudotime`. Defaults to 1e-10.
+        ncol: the number of columns in each facet grid. Defaults to 4.
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        c_palette: the color map function to use. Defaults to "Set2".
+        standard_scale: either 0 (rows) or 1 (columns). Whether to standardize that dimension, meaning for each
+            row or column, subtract the minimum and divide each by its maximum. Defaults to 0.
+        traj_ind: if the element from the dictionary is a list (obtained from a list of trajectories), the index of
+            trajectory that will be selected for visualization. Defaults to 0.
+        log: whether to log1p transform your data before data visualization. If expression data is from adata object,
             it is generally already log1p transformed. When the data is from predicted either from traj simulation or
             LAP, the data is generally in the original gene expression space and needs to be log1p transformed. Note:
             when predicted data is not inverse transformed back to original expression space, no transformation will be
-            applied.
-        save_show_or_return: {'show', 'save_fig', 'return'} (default: `show`)
-            Whether to save_fig, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'kinetic_curves', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-
-    Returns
-    -------
-        Nothing but plots the kinetic curves that shows the gene expression dynamics over time.
+            applied. Defaults to True.
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "show".
+        save_kwargs: A dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'kinetic_curves', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+
+    Raises:
+        ValueError: invalid `genes`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the generated
+        `seaborn.FacetGrid` would be returned.
     """
 
     import matplotlib.pyplot as plt
     import pandas as pd
     import seaborn as sns
 
     if mode == "pseudotime" and tkey == "potential" and "potential" not in adata.obs_keys():
@@ -142,15 +136,15 @@
             "Time": np.repeat(time, len(valid_genes)),
             "Expression": exprs.flatten(),
             "Gene": np.tile(valid_genes, len(time)),
         }
     )
 
     if exprs_df.shape[0] == 0:
-        raise Exception(
+        raise ValueError(
             "No genes you provided are detected. Please make sure the genes provided are from the genes "
             "used for vector field reconstructed when layer is set."
         )
 
     # https://stackoverflow.com/questions/43920341/python-seaborn-facetgrid-change-titles
     if len(Color) > 0:
         exprs_df["Color"] = np.repeat(Color, len(valid_genes))
@@ -172,144 +166,147 @@
             data=exprs_df,
             col="Gene",
             col_wrap=ncol,
             kind="line",
             facet_kws={"sharex": True, "sharey": False},
         )
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "kinetic_curves",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return g
 
 
 docstrings.delete_params("kin_curves.parameters", "ncol", "color", "c_palette")
 
 
 @docstrings.with_indent(4)
 def kinetic_heatmap(
     adata: AnnData,
-    genes: list,
+    genes: List[str],
     mode: str = "vector_field",
     basis: Optional[str] = None,
     layer: str = "X",
     project_back_to_high_dim: bool = True,
     tkey: str = "potential",
     dist_threshold: float = 1e-10,
     color_map: int = "BrBG",
-    gene_order_method: str = "maximum",
+    gene_order_method: Literal["maximum", "half_max_ordering", "raw"] = "maximum",
     show_colorbar: bool = False,
-    cluster_row_col: list = [False, False],
-    figsize: tuple = (11.5, 6),
+    cluster_row_col: Tuple[bool, bool] = (False, False),
+    figsize: Tuple[float, float] = (11.5, 6),
     standard_scale: int = 1,
     n_convolve: int = 30,
     spaced_num: int = 100,
     traj_ind: int = 0,
     log: bool = True,
-    gene_group: Union[None, list] = None,
-    gene_group_cmap: Union[None, list] = None,
-    cell_group: Union[None, list] = None,
-    cell_group_cmap: Union[None, list] = None,
+    gene_group: Optional[List[str]] = None,
+    gene_group_cmap: Optional[List[str]] = None,
+    cell_group: Optional[List[str]] = None,
+    cell_group_cmap: Optional[List[str]] = None,
     enforce: bool = False,
-    hline_rows: Union[None, list] = None,
-    hlines_kwargs: dict = {},
-    vline_cols: Union[None, list] = None,
-    vlines_kwargs: dict = {},
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
+    hline_rows: Optional[List[int]] = None,
+    hlines_kwargs: Dict[str, Any] = {},
+    vline_cols: Optional[List[int]] = None,
+    vlines_kwargs: Dict[str, Any] = {},
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     transpose: bool = False,
     **kwargs,
-):
+) -> Optional[ClusterGrid]:
     """Plot the gene expression dynamics over time (pseudotime or inferred real time) in a heatmap.
 
     Note that by default `potential` estimated with the diffusion graph built from reconstructed vector field will be
     used as the measure of pseudotime.
 
-    Parameters
-    ----------
-        %(kin_curves.parameters.no_ncol|color|c_palette)s
-        color_map: `str` (default: `BrBG`)
-            Color map that will be used to color the gene expression. If `half_max_ordering` is True, the
-            color map need to be divergent, good examples, include `BrBG`, `RdBu_r` or `coolwarm`, etc.
-        gene_order_method: `str` (default: `half_max_ordering`) [`half_max_ordering`, `maximum`, `raw`]
-            Supports three different methods for ordering genes when plotting the heatmap: either `half_max_ordering`,
-            `maximum` or `raw`. For `half_max_ordering`, it will order genes into up, down and transit groups by the
-            half max ordering algorithm (HA Pliner, et. al, Molecular cell 71 (5), 858-871. e8). While for `maximum`,
-            it will order by the position of the highest gene expression. `raw` means just use the original order from
-            the input gene list.
-        show_colorbar: `bool` (default: `False`)
-            Whether to show the color bar.
-        cluster_row_col: `[bool, bool]` (default: `[False, False]`)
-            Whether to cluster the row or columns.
-        figsize: `str` (default: `(11.5, 6)`
-            Size of figure
-        standard_scale: `int` (default: 1)
-            Either 0 (rows, cells) or 1 (columns, genes). Whether or not to standardize that dimension, meaning for each
-            row or column, subtract the minimum and divide each by its maximum.
-        n_convolve: `int` (default: 30)
-            Number of cells for convolution.
-        traj_ind: `int` (default: 0)
-            If the element from the dictionary is a list (obtained from a list of trajectories), the index of trajectory
-            that will be selected for visualization.
-        log: `bool` (default: True)
-            Whether to log1p transform your data before data visualization. If expression data is from adata object,
+    Args:
+        adata: an Annodata object.
+        genes: the gene names whose gene expression will be faceted.
+        mode: which data mode will be used, either vector_field, lap or pseudotime. if mode is vector_field, the
+            trajectory predicted by vector field function will be used; if mode is lap, the trajectory predicted by
+            least action path will be used otherwise pseudotime trajectory (defined by time argument) will be used.
+            Defaults to "vector_field".
+        basis: the embedding data used for drawing the kinetic gene expression curves, only used when mode is
+            `vector_field`. Defaults to None.
+        layer: the key to the layer of expression value will be used. Not used if mode is `vector_field`. Defaults
+            to "X".
+        project_back_to_high_dim: whether to map the coordinates in low dimension back to high dimension to visualize
+            the gene expression curves, only used when mode is `vector_field` and basis is not `X`. Currently only works
+            when basis is 'pca' and 'umap'. Defaults to True.
+        tkey: the .obs column that will be used for timing each cell, only used when mode is not `vector_field`.
+            Defaults to "potential".
+        dist_threshold: the threshold for the distance between two points in the gene expression state, i.e, x(t),
+            x(t+1). If below this threshold, we assume steady state is achieved and those data points will not be
+            considered. This argument is ignored when mode is `pseudotime`. Defaults to 1e-10.
+        color_map: the color map that will be used to color the gene expression. If `half_max_ordering` is True, the
+            color map need to be divergent, good examples, include `BrBG`, `RdBu_r` or `coolwarm`, etc. Defaults to
+            "BrBG".
+        gene_order_method: supports three different methods for ordering genes when plotting the heatmap: either
+            `half_max_ordering`, `maximum` or `raw`. For `half_max_ordering`, it will order genes into up, down and
+            transit groups by the half max ordering algorithm (HA Pliner, et al., Molecular cell 71 (5), 858-871. e8).
+            While for `maximum`, it will order by the position of the highest gene expression. `raw` means just use the
+            original order from the input gene list. Defaults to "maximum".
+        show_colorbar: whether to show the color bar. Defaults to False.
+        cluster_row_col: whether to cluster the row or columns. Defaults to (False, False).
+        figsize: size of figure. Defaults to (11.5, 6).
+        standard_scale: either 0 (rows, cells) or 1 (columns, genes). Whether to standardize that dimension,
+            meaning for each row or column, subtract the minimum and divide each by its maximum. Defaults to 1.
+        n_convolve: the number of cells for convolution. Defaults to 30.
+        spaced_num: the number of points on the loess fitting curve. Defaults to 100.
+        traj_ind: if the element from the dictionary is a list (obtained from a list of trajectories), the index of
+            trajectory that will be selected for visualization.. Defaults to 0.
+        log: whether to log1p transform your data before data visualization. If expression data is from adata object,
             it is generally already log1p transformed. When the data is from predicted either from traj simulation or
             LAP, the data is generally in the original gene expression space and needs to be log1p transformed. Note:
             when predicted data is not inverse transformed back to original expression space, no transformation will be
-            applied.
-        gene_group:
-            The key of the gene groups in .var.
-        gene_group_cmap:
-            The str of the colormap for gene groups.
-        cell_group:
-             The key of the cell groups in .obs.
-        cell_group_cmap:
-            The str of the colormap for cell groups.
-        enforce:
-            Whether to recalculate the dataframe that will be used to create the kinetic heatmap. If this is set to be
-            False and the the .uns['kinetic_heatmap'] is in the adata object, we will use data from
-            `.uns['kinetic_heatmap']` directly.
-        hline_rows:
-            The indices of rows that we can place a line on the heatmap.
-        hlines_kwargs:
-            The dictionary of arguments that will be passed into sns_heatmap.ax_heatmap.hlines.
-        vline_cols:
-            The indices of column that we can place a line on the heatmap.
-        vlines_kwargs:
-            The dictionary of arguments that will be passed into sns_heatmap.ax_heatmap.vlines.
-        save_show_or_return: {'show', 'save_fig', 'return'} (default: `show`)
-            Whether to save_fig, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'kinetic_heatmap', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        transpose:
-            Whether to transpose the dataframe and swap X-Y in heatmap. In single cell case, `transpose=True` results in gene on the x-axis.
-        kwargs:
-            All other keyword arguments are passed to heatmap(). Currently `xticklabels=False, yticklabels='auto'` is
-            passed to heatmap() by default.
-
-    Returns
-    -------
-        Nothing but plots a heatmap that shows the gene expression dynamics over time.
+            applied. Defaults to True.
+        gene_group: the key of the gene groups in .var. Defaults to None.
+        gene_group_cmap: the str of the colormap for gene groups. Defaults to None.
+        cell_group: the key of the cell groups in .obs. Defaults to None.
+        cell_group_cmap: the str of the colormap for cell groups. Defaults to None.
+        enforce: whether to recalculate the dataframe that will be used to create the kinetic heatmap. If this is set to
+            be False and the .uns['kinetic_heatmap'] is in the adata object, we will use data from
+            `.uns['kinetic_heatmap']` directly.. Defaults to False.
+        hline_rows: the indices of rows that we can place a line on the heatmap. Defaults to None.
+        hlines_kwargs: a dictionary of arguments that will be passed into sns_heatmap.ax_heatmap.hlines. Defaults to {}.
+        vline_cols: the indices of column that we can place a line on the heatmap. Defaults to None.
+        vlines_kwargs: a dictionary of arguments that will be passed into sns_heatmap.ax_heatmap.vlines. Defaults to {}.
+        save_show_or_return: whether to save, show, or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'kinetic_heatmap', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        transpose: whether to transpose the dataframe and swap X-Y in heatmap. In single cell case, `transpose=True`
+            results in gene on the x-axis. Defaults to False.
+        **kwargs: any other keyword arguments are passed to heatmap(). Currently `xticklabels=False, yticklabels='auto'`
+            is passed to heatmap() by default.
+
+    Raises:
+        NotImplementedError: invalid `order_method`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the generated seaborn
+        ClusterGrid would be returned.
     """
 
     import matplotlib.pyplot as plt
     import pandas as pd
     import seaborn as sns
 
     if enforce or "kinetic_heatmap" not in adata.uns_keys():
@@ -373,15 +370,15 @@
                     exprs[max_sort, :],
                     index=np.array(valid_genes)[max_sort],
                     columns=adata.obs_names,
                 )
             else:
                 df = pd.DataFrame(exprs[max_sort, :], index=np.array(valid_genes)[max_sort])
         else:
-            raise Exception("gene order_method can only be either half_max_ordering or maximum")
+            raise NotImplementedError("gene order_method can only be either raw, half_max_ordering or maximum")
 
         adata.uns["kinetics_heatmap"] = df
     else:
         df = adata.uns["kinetics_heatmap"]
 
     row_colors, col_colors = None, None
     if gene_group is not None:
@@ -465,33 +462,36 @@
     if hline_rows is not None:
         hl_kwargs = update_dict({"linestyles": "dashdot"}, hlines_kwargs)
         sns_heatmap.ax_heatmap.hlines(hline_rows, *sns_heatmap.ax_heatmap.get_xlim(), **hl_kwargs)
     if vline_cols is not None:
         vline_kwargs = update_dict({"linestyles": "dashdot"}, vlines_kwargs)
         sns_heatmap.ax_heatmap.vlines(vline_cols, *sns_heatmap.ax_heatmap.get_ylim(), **vline_kwargs)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "kinetic_heatmap",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         if show_colorbar:
             plt.subplots_adjust(right=0.85)
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return sns_heatmap
 
 
 def _half_max_ordering(exprs, time, mode, interpolate=False, spaced_num=100):
     """Implement the half-max ordering algorithm from HA Pliner, Molecular Cell, 2018.
 
     Parameters
@@ -648,96 +648,86 @@
     return res
 
 
 @docstrings.with_indent(4)
 def jacobian_kinetics(
     adata: AnnData,
     basis: str = "umap",
-    regulators: Optional[list] = None,
-    effectors: Optional[list] = None,
+    regulators: Optional[List[str]] = None,
+    effectors: Optional[List[str]] = None,
     mode: str = "pseudotime",
     tkey: str = "potential",
     color_map: str = "bwr",
-    gene_order_method: str = "raw",
+    gene_order_method: Literal["raw", "half_max_ordering", "maximum"] = "raw",
     show_colorbar: bool = False,
-    cluster_row_col: list = [False, True],
-    figsize: tuple = (11.5, 6),
+    cluster_row_col: Tuple[bool, bool] = [False, True],
+    figsize: Tuple[float, float] = (11.5, 6),
     standard_scale: int = 1,
     n_convolve: int = 30,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
+) -> Optional[ClusterGrid]:
     """Plot the Jacobian dynamics over time (pseudotime or inferred real time) in a heatmap.
 
     Note that by default `potential` estimated with the diffusion graph built from reconstructed vector field will be
     used as the measure of pseudotime.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        basis: `str`
-            The reduced dimension basis.
-        regulators: `list` or `None` (default: `None`)
-            The list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        effectors: `List` or `None` (default: `None`)
-            The list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        mode: `str` (default: `vector_field`)
-            Which data mode will be used, either vector_field or pseudotime. if mode is vector_field, the trajectory
+    Args:
+        adata: an Annodata object.
+        basis: the reduced dimension basis. Defaults to "umap".
+        regulators: the list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        effectors: the list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        mode: which data mode will be used, either vector_field or pseudotime. if mode is vector_field, the trajectory
             predicted by vector field function will be used, otherwise pseudotime trajectory (defined by time argument)
-            will be used. By default `potential` estimated with the diffusion graph built reconstructed vector field
-            will be used as pseudotime.
-        tkey: `str` (default: `potential`)
-            The .obs column that will be used for timing each cell, only used when mode is not `vector_field`.
-        color_map: `str` (default: `BrBG`)
-            Color map that will be used to color the gene expression. If `half_max_ordering` is True, the
-            color map need to be divergent, good examples, include `BrBG`, `RdBu_r` or `coolwarm`, etc.
-        gene_order_method: `str` (default: `half_max_ordering`) [`half_max_ordering`, `maximum`]
-            Supports two different methods for ordering genes when plotting the heatmap: either `half_max_ordering`,
-            or `maximum`. For `half_max_ordering`, it will order genes into up, down and transit groups by the half
-            max ordering algorithm (HA Pliner, et. al, Molecular cell 71 (5), 858-871. e8). While for `maximum`,
-            it will order by the position of the highest gene expression.
-        show_colorbar: `bool` (default: `False`)
-            Whether to show the color bar.
-        cluster_row_col: `[bool, bool]` (default: `[False, False]`)
-            Whether to cluster the row or columns.
-        figsize: `str` (default: `(11.5, 6)`
-            Size of figure
-        standard_scale: `int` (default: 1)
-            Either 0 (rows, cells) or 1 (columns, genes). Whether or not to standardize that dimension, meaning for each
-            row or column, subtract the minimum and divide each by its maximum.
-        n_convolve: `int` (default: 30)
-            Number of cells for convolution.
-        save_show_or_return: {'show', 'save_fig', 'return'} (default: `show`)
-            Whether to save_fig, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'kinetic_curves', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        kwargs:
-            All other keyword arguments are passed to heatmap(). Currently `xticklabels=False, yticklabels='auto'` is
-            passed to heatmap() by default.
-    Returns
-    -------
-        Nothing but plots a heatmap that shows the element of Jacobian matrix dynamics over time (potential decreasing).
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> adata = dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
-    >>> dyn.vf.jacobian(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
-    >>> dyn.pl.jacobian_kinetics(adata)
+            will be used. By default, `potential` estimated with the diffusion graph built reconstructed vector field
+            will be used as pseudotime. Defaults to "pseudotime".
+        tkey: the .obs column that will be used for timing each cell, only used when mode is not `vector_field`.
+            Defaults to "potential".
+        color_map: color map that will be used to color the gene expression. If `half_max_ordering` is True, the
+            color map need to be divergent, good examples, include `BrBG`, `RdBu_r` or `coolwarm`, etc. Defaults to
+            "bwr".
+        gene_order_method: supports two different methods for ordering genes when plotting the heatmap: either
+            `half_max_ordering`, or `maximum`. For `half_max_ordering`, it will order genes into up, down and transit
+            groups by the half max ordering algorithm (HA Pliner, et al., Molecular cell 71 (5), 858-871. e8). While for
+            `maximum`, it will order by the position of the highest gene expression. Or, use `raw` to prevent any
+            ordering. Defaults to "raw".
+        show_colorbar: whether to show the color bar. Defaults to False.
+        cluster_row_col: whether to cluster the row or columns. Defaults to [False, True].
+        figsize: the size of the figures. Defaults to (11.5, 6).
+        standard_scale: can be either 0 (rows, cells) or 1 (columns, genes). Whether to standardize that
+            dimension, meaning for each row or column, subtract the minimum and divide each by its maximum. Defaults
+            to 1.
+        n_convolve: the number of cells for convolution. Defaults to 30.
+        save_show_or_return: whether to save, show, or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'kinetic_curves', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs.. Defaults to {}.
+        **kwargs: any other kwargs that would be passed to `seaborn.clustermap`.
+
+    Raises:
+        ValueError: invalid `regulators` or `effectors`.
+        NotImplementedError: invalid `gene_order_method`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the generated seaborn
+        ClusterGrid would be returned.
+    
+    Examples:
+        >>> import dynamo as dyn
+        >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
+        >>> adata = dyn.pp.recipe_monocle(adata)
+        >>> dyn.tl.dynamics(adata)
+        >>> dyn.vf.VectorField(adata, basis='pca')
+        >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
+        >>> dyn.vf.jacobian(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
+        >>> dyn.pl.jacobian_kinetics(adata)
     """
 
     import matplotlib.pyplot as plt
     import pandas as pd
     import seaborn as sns
 
     Jacobian_ = "jacobian" if basis is None else "jacobian_" + basis
@@ -819,15 +809,15 @@
         jacobian_mat /= np.abs(jacobian_mat).max(1)[:, None]
         df = pd.DataFrame(
             jacobian_mat,
             index=np.array(source_targets_),
             columns=adata.obs_names,
         )
     else:
-        raise Exception("gene order_method can only be either half_max_ordering or maximum")
+        raise NotImplementedError("gene order_method can only be either raw, half_max_ordering or maximum")
 
     heatmap_kwargs = dict(
         xticklabels=False,
         yticklabels=1,
         row_colors=None,
         col_colors=None,
         row_linkage=None,
@@ -848,122 +838,115 @@
         figsize=figsize,
         center=0,
         **heatmap_kwargs,
     )
     if not show_colorbar:
         sns_heatmap.cax.set_visible(False)
 
-    if save_show_or_return == "save_fig":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "jacobian_kinetics",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         if show_colorbar:
             plt.subplots_adjust(right=0.85)
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return sns_heatmap
 
 
 @docstrings.with_indent(4)
 def sensitivity_kinetics(
-    adata,
-    basis="umap",
-    regulators=None,
-    effectors=None,
-    mode="pseudotime",
-    tkey="potential",
-    color_map="bwr",
-    gene_order_method="raw",
-    show_colorbar=False,
-    cluster_row_col=[False, True],
-    figsize=(11.5, 6),
-    standard_scale=1,
-    n_convolve=30,
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    basis: str = "umap",
+    regulators: Optional[List[str]] = None,
+    effectors: Optional[List[str]] = None,
+    mode: Literal["pseudotime", "vector_field"] = "pseudotime",
+    tkey: str = "potential",
+    color_map: str = "bwr",
+    gene_order_method: Literal["raw", "maximum", "half_max_ordering"] = "raw",
+    show_colorbar: bool = False,
+    cluster_row_col: Tuple[bool, bool] = (False, True),
+    figsize: Tuple[float, float] = (11.5, 6),
+    standard_scale: int = 1,
+    n_convolve: int = 30,
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
+) -> Optional[ClusterGrid]:
     """Plot the Sensitivity dynamics over time (pseudotime or inferred real time) in a heatmap.
 
     Note that by default `potential` estimated with the diffusion graph built from reconstructed vector field will be
     used as the measure of pseudotime.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        basis: `str`
-            The reduced dimension basis.
-        regulators: `list` or `None` (default: `None`)
-            The list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        effectors: `List` or `None` (default: `None`)
-            The list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        mode: `str` (default: `vector_field`)
-            Which data mode will be used, either vector_field or pseudotime. if mode is vector_field, the trajectory
-            predicted byvector field function will be used, otherwise pseudotime trajectory (defined by time argument)
-            will be used. By default `potential` estimated with the diffusion graph built reconstructed vector field
-            will be used as pseudotime.
-        tkey: `str` (default: `potential`)
-            The .obs column that will be used for timing each cell, only used when mode is not `vector_field`.
-        color_map: `str` (default: `BrBG`)
-            Color map that will be used to color the gene expression. If `half_max_ordering` is True, the
-            color map need to be divergent, good examples, include `BrBG`, `RdBu_r` or `coolwarm`, etc.
-        gene_order_method: `str` (default: `half_max_ordering`) [`half_max_ordering`, `maximum`]
-            Supports two different methods for ordering genes when plotting the heatmap: either `half_max_ordering`,
-            or `maximum`. For `half_max_ordering`, it will order genes into up, down and transit groups by the half
-            max ordering algorithm (HA Pliner, et. al, Molecular cell 71 (5), 858-871. e8). While for `maximum`,
-            it will order by the position of the highest gene expression.
-        show_colorbar: `bool` (default: `False`)
-            Whether to show the color bar.
-        cluster_row_col: `[bool, bool]` (default: `[False, False]`)
-            Whether to cluster the row or columns.
-        figsize: `str` (default: `(11.5, 6)`
-            Size of figure
-        standard_scale: `int` (default: 1)
-            Either 0 (rows, cells) or 1 (columns, genes). Whether or not to standardize that dimension, meaning for each
-            row or column, subtract the minimum and divide each by its maximum.
-        n_convolve: `int` (default: 30)
-            Number of cells for convolution.
-        save_show_or_return: {'show', 'save_fig', 'return'} (default: `show`)
-            Whether to save_fig, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the
-            save_fig function will use the {"path": None, "prefix": 'kinetic_curves', "dpi": None, "ext": 'pdf',
-            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise you can provide a
-            dictionary that properly modify those keys according to your needs.
-        kwargs:
-            All other keyword arguments are passed to heatmap(). Currently `xticklabels=False, yticklabels='auto'` is
-            passed to heatmap() by default.
-    Returns
-    -------
-        Nothing but plots a heatmap that shows the element of Jacobian matrix dynamics over time (potential decreasing).
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> adata = dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
-    >>> dyn.vf.sensitivity(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
-    >>> dyn.pl.sensitivity_kinetics(adata)
+    Args:
+        adata: an AnnData object.
+        basis: the reduced dimension basis. Defaults to "umap".
+        regulators: the list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        effectors: the list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        mode: which data mode will be used, either vector_field or pseudotime. if mode is vector_field, the trajectory
+            predicted by vector field function will be used, otherwise pseudotime trajectory (defined by time argument)
+            will be used. By default, `potential` estimated with the diffusion graph built reconstructed vector field
+            will be used as pseudotime. Defaults to "pseudotime".
+        tkey: the .obs column that will be used for timing each cell, only used when mode is not `vector_field`.
+            Defaults to "potential".
+        color_map: color map that will be used to color the gene expression. If `half_max_ordering` is True, the
+            color map need to be divergent, good examples, include `BrBG`, `RdBu_r` or `coolwarm`, etc. Defaults to
+            "bwr".
+        gene_order_method: supports two different methods for ordering genes when plotting the heatmap: either
+            `half_max_ordering`, or `maximum`. For `half_max_ordering`, it will order genes into up, down and transit
+            groups by the half max ordering algorithm (HA Pliner, et al., Molecular cell 71 (5), 858-871. e8). While for
+            `maximum`, it will order by the position of the highest gene expression. Or, use `raw` to prevent any
+            ordering. Defaults to "raw".
+        show_colorbar: whether to show the color bar. Defaults to False.
+        cluster_row_col: whether to cluster the row or columns. Defaults to (False, True).
+        figsize: the size of the figure. Defaults to (11.5, 6).
+        standard_scale: either 0 (rows, cells) or 1 (columns, genes). Whether to standardize that dimension,
+            meaning for each row or column, subtract the minimum and divide each by its maximum. Defaults to 1.
+        n_convolve: the number of cells for convolution. Defaults to 30.
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'kinetic_curves', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        **kwargs: any other kwargs that would be passed to `heatmap(). Currently `xticklabels=False, yticklabels='auto'`
+            is passed to heatmap() by default.`
+
+    Raises:
+        ValueError: invalid `regulators` or `effectors`.
+        NotImplementedError: invalid `gene_order_method`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the generated seaborn
+        ClusterGrid would be returned.
+        
+    Examples:
+        >>> import dynamo as dyn
+        >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
+        >>> adata = dyn.pp.recipe_monocle(adata)
+        >>> dyn.tl.dynamics(adata)
+        >>> dyn.vf.VectorField(adata, basis='pca')
+        >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
+        >>> dyn.vf.sensitivity(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
+        >>> dyn.pl.sensitivity_kinetics(adata)
     """
 
     import matplotlib.pyplot as plt
     import pandas as pd
     import seaborn as sns
 
     Sensitivity_ = "sensitivity" if basis is None else "sensitivity_" + basis
@@ -1045,15 +1028,15 @@
         sensitivity_mat /= np.abs(sensitivity_mat).max(1)[:, None]
         df = pd.DataFrame(
             sensitivity_mat,
             index=np.array(source_targets_),
             columns=adata.obs_names,
         )
     else:
-        raise Exception("gene order_method can only be either half_max_ordering or maximum")
+        raise NotImplementedError("gene order_method can only be either half_max_ordering or maximum")
 
     heatmap_kwargs = dict(
         xticklabels=False,
         yticklabels=1,
         row_colors=None,
         col_colors=None,
         row_linkage=None,
@@ -1074,27 +1057,30 @@
         figsize=figsize,
         center=0,
         **heatmap_kwargs,
     )
     if not show_colorbar:
         sns_heatmap.cax.set_visible(False)
 
-    if save_show_or_return == "save_fig":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "sensitivity_kinetics",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         if show_colorbar:
             plt.subplots_adjust(right=0.85)
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return sns_heatmap
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/topography.py` & `dynamo-release-1.3.0/dynamo/plot/topography.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,18 @@
 import warnings
-from typing import Callable, List, Optional, Union
+from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import matplotlib.pyplot as plt
 import numpy as np
+import numpy.typing as npt
 import pandas as pd
 import scipy
 from anndata import AnnData
 from matplotlib.axes import Axes
 
 from ..configuration import _themes
 from ..dynamo_logger import LoggerManager
@@ -30,74 +36,59 @@
     set_arrow_alpha,
     set_stream_line_alpha,
 )
 
 
 def plot_flow_field(
     vecfld: VectorField2D,
-    x_range: List,
-    y_range: List,
+    x_range: npt.ArrayLike,
+    y_range: npt.ArrayLike,
     n_grid: int = 100,
-    start_points: np.ndarray = None,
-    integration_direction: str = "both",
-    background: str = None,
+    start_points: Optional[np.ndarray] = None,
+    integration_direction: Literal["forward", "backward", "both"] = "both",
+    background: Optional[str] = None,
     density: float = 1,
     linewidth: float = 1,
     streamline_color: Optional[str] = None,
     streamline_alpha: float = 0.4,
     color_start_points: Optional[float] = None,
-    save_show_or_return: str = "return",
-    save_kwargs: dict = {},
-    ax: Axes = None,
+    save_show_or_return: Literal["save", "show", "return"] = "return",
+    save_kwargs: Dict[str, Any] = {},
+    ax: Optional[Axes] = None,
     **streamline_kwargs,
-):
+) -> Optional[Axes]:
     """Plots the flow field with line thickness proportional to speed.
-    code adapted from: http://be150.caltech.edu/2017/handouts/dynamical_systems_approaches.html
 
-    Parameters
-    ----------
-    vecfld: :class:`~vector_field`
-        An instance of the vector_field class.
-    x_range: array_like, shape (2,)
-        Range of values for x-axis.
-    y_range: array_like, shape (2,)
-        Range of values for y-axis.
-    n_grid : int, default 100
-        Number of grid points to use in computing
-        derivatives on phase portrait.
-    start_points: np.ndarray (default: None)
-        The initial points from which the streamline will be draw.
-    integration_direction:  {'forward', 'backward', 'both'} (default: `both`)
-        Integrate the streamline in forward, backward or both directions. default is 'both'.
-    background: `str` or None (default: None)
-        The background color of the plot.
-    density: `float` (default: 1)
-        density of the plt.streamplot function.
-    linewidth: `float` or None (default: 1)
-        multiplier of automatically calculated linewidth passed to the plt.streamplot function.
-    streamline_color: `str` or None (default: None)
-        The color of the vector field stream lines.
-    streamline_alpha: `float` or None (default: 0.4)
-        The alpha value applied to the vector field stream lines.
-    color_start_points: `float` or None (default: `None`)
-        The color of the starting point that will be used to predict cell fates.
-    save_show_or_return: {'show', 'save', 'return'} (default: `return`)
-        Whether to save, show or return the figure.
-    save_kwargs: `dict` (default: `{}`)
-        A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-        will use the {"path": None, "prefix": 'plot_flow_field', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-        True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-        according to your needs.
-    ax : Matplotlib Axis instance
-        Axis on which to make the plot
-
-    Returns
-    -------
-    output : Matplotlib Axis instance
-        Axis with streamplot included.
+    Code adapted from: http://be150.caltech.edu/2017/handouts/dynamical_systems_approaches.html
+
+    Args:
+        vecfld: an instance of the vector_field class.
+        x_range: the range of values for x-axis.
+        y_range: the range of values for y-axis.
+        n_grid: the number of grid points to use in computing derivatives on phase portrait. Defaults to 100.
+        start_points: the initial points from which the streamline will be drawn. Defaults to None.
+        integration_direction: integrate the streamline in forward, backward or both directions. default is 'both'.
+            Defaults to "both".
+        background: the background color of the plot. Defaults to None.
+        density: the density of the plt.streamplot function. Defaults to 1.
+        linewidth: the multiplier of automatically calculated linewidth passed to the plt.streamplot function. Defaults
+            to 1.
+        streamline_color: the color of the vector field streamlines. Defaults to None.
+        streamline_alpha: the alpha value applied to the vector field streamlines. Defaults to 0.4.
+        color_start_points: the color of the starting point that will be used to predict cell fates. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "return".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'plot_flow_field', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        ax: the Axis on which to make the plot. Defaults to None.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the Axes of the generated
+        figure would be returned.
     """
 
     from matplotlib import patches, rcParams
     from matplotlib.colors import to_hex
 
     if background is None:
         _background = rcParams.get("figure.facecolor")
@@ -180,63 +171,65 @@
             integration_direction=integration_direction,
             color=color_start_points,
             **streamplot_kwargs,
         )
         set_arrow_alpha(ax, streamline_alpha)
         set_stream_line_alpha(s, streamline_alpha)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_flow_field",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def plot_nullclines(
     vecfld: VectorField2D,
-    vecfld_dict: dict = None,
+    vecfld_dict: Dict[str, Any] = None,
     lw: float = 3,
     background: Optional[float] = None,
-    save_show_or_return: str = "return",
-    save_kwargs: dict = {},
-    ax: Axes = None,
-):
+    save_show_or_return: Literal["save", "show", "return"] = "return",
+    save_kwargs: Dict[str, Any] = {},
+    ax: Optional[Axes] = None,
+) -> Optional[Axes]:
     """Plot nullclines stored in the VectorField2D class.
 
-    Arguments
-    ---------
-        vecfld: :class:`~VectorField2D`
-            An instance of the VectorField2D class which presumably has fixed points computed and stored.
-        lw: `float` (default: 3)
-            The linewidth of the nullcline.
-        background: `str` or None (default: None)
-            The background color of the plot.
-        save_show_or_return: {'show', 'save', 'return'} (default: `return`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'plot_nullclines', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        ax: :class:`~matplotlib.axes.Axes`
-            The matplotlib axes used for plotting. Default is to use the current axis.
+    Args:
+        vecfld: an instance of the VectorField2D class which presumably has fixed points computed and stored.
+        vecfld_dict: a dict with entries to create a `VectorField2D` instance. Defaults to None.
+        lw: the linewidth of the nullcline. Defaults to 3.
+        background: the background color of the plot. Defaults to None.
+        save_show_or_return: whether to save, show, or return the figure. Defaults to "return".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'plot_nullclines', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        ax: the matplotlib axes used for plotting. Default is to use the current axis. Defaults to None.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the Axes of the generated
+        figure would be returned.
     """
+
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     if background is None:
         _background = rcParams.get("figure.facecolor")
         _background = to_hex(_background) if type(_background) is tuple else _background
     else:
@@ -279,73 +272,72 @@
 
     if NCx is not None and NCy is not None:
         for ncx in NCx:
             ax.plot(*ncx.T, c=colors[0], lw=lw)
         for ncy in NCy:
             ax.plot(*ncy.T, c=colors[1], lw=lw)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_nullclines",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def plot_fixed_points_2d(
-    vecfld,
-    marker="o",
-    markersize=200,
-    cmap=None,
-    filltype=["full", "top", "none"],
-    background=None,
-    save_show_or_return="return",
-    save_kwargs={},
-    ax=None,
-):
+    vecfld: VectorField2D,
+    marker: str = "o",
+    markersize: float = 200,
+    cmap: Optional[str] = None,
+    filltype: List[str] = ["full", "top", "none"],
+    background: Optional[str] = None,
+    save_show_or_return: Literal["save", "show", "return"] = "return",
+    save_kwargs: Dict[str, Any] = {},
+    ax: Optional[Axes] = None,
+) -> Optional[Axes]:
     """Plot fixed points stored in the VectorField2D class.
 
-    Arguments
-    ---------
-        vecfld: :class:`~VectorField2D`
-            An instance of the VectorField2D class which presumably has fixed points computed and stored.
-        marker: `str` (default: `o`)
-            The marker type. Any string supported by matplotlib.markers.
-        markersize: `float` (default: 200)
-            The size of the marker.
-        cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring or shading the confidence of fixed points. If None, the
-            default color map will set to be viridis (inferno) when the background is white (black).
-        filltype: list
-            The fill type used for stable, saddle, and unstable fixed points. Default is 'full', 'top' and 'none',
-            respectively.
-        background: `str` or None (default: None)
-            The background color of the plot.
-        save_show_or_return: {'show', 'save', 'return'} (default: `return`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'plot_fixed_points', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        ax: :class:`~matplotlib.axes.Axes`
-            The matplotlib axes used for plotting. Default is to use the current axis.
+    Args:
+        vecfld: an instance of the VectorField2D class which presumably has fixed points computed and stored.
+        marker: the marker type. Any string supported by matplotlib.markers. Defaults to "o".
+        markersize: the size of the marker. Defaults to 200.
+        cmap: the name of a matplotlib colormap to use for coloring or shading the confidence of fixed points. If None,
+            the default color map will set to be viridis (inferno) when the background is white (black). Defaults to
+            None.
+        filltype: the fill type used for stable, saddle, and unstable fixed points. Default is 'full', 'top' and 'none',
+            respectively. Defaults to ["full", "top", "none"].
+        background: the background color of the plot. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "return".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'plot_fixed_points', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        ax: the matplotlib axes used for plotting. Default is to use the current axis. Defaults to None.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the Axes of the generated
+        figure would be returned.
     """
+
     import matplotlib
     import matplotlib.patheffects as PathEffects
     from matplotlib import markers, rcParams
     from matplotlib.colors import to_hex
 
     if background is None:
         _background = rcParams.get("figure.facecolor")
@@ -392,80 +384,75 @@
         txt.set_path_effects(
             [
                 PathEffects.Stroke(linewidth=1.5, foreground=_background, alpha=0.8),
                 PathEffects.Normal(),
             ]
         )
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_fixed_points",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def plot_fixed_points(
     vecfld: VectorField2D,
-    vecfld_dict: dict = None,
+    vecfld_dict: Dict[str, Any] = None,
     marker: str = "o",
     markersize: int = 200,
     c: str = "w",
     cmap: Optional[str] = None,
-    filltype: list = ["full", "top", "none"],
+    filltype: List[str] = ["full", "top", "none"],
     background: Optional[str] = None,
-    save_show_or_return: str = "return",
-    save_kwargs: dict = {},
-    ax: Axes = None,
+    save_show_or_return: Literal["save", "show", "return"] = "return",
+    save_kwargs: Dict[str, Any] = {},
+    ax: Optional[Axes] = None,
     **kwargs,
-):
+) -> Optional[Axes]:
     """Plot fixed points stored in the VectorField2D class.
 
-    Arguments
-    ---------
-        vecfld: :class:`~vector_field`
-            An instance of the vector_field class.
-        basis: `str` (default: 'umap')
-            The basis on which the fixed points are ploted.
-        marker: `str` (default: `o`)
-            The marker type. Any string supported by matplotlib.markers.
-        markersize: `float` (default: 200)
-            The size of the marker.
-        cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring or shading the confidence of fixed points. If None, the
-            default color map will set to be viridis (inferno) when the background is white (black).
-        filltype: list
-            The fill type used for stable, saddle, and unstable fixed points. Default is 'full', 'top' and 'none',
-            respectively.
-        background: `str` or None (default: None)
-            The background color of the plot.
-        save_show_or_return: {'show', 'save', 'return'} (default: `return`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'plot_fixed_points', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        ax: :class:`~matplotlib.axes.Axes`
-            The matplotlib axes used for plotting. Default is to use the current axis.
-        kwargs:
-            Key word arguments passed to the find_fixed_point function of the vector field class for high dimension fixed
-            point identification.
+    Args:
+        vecfld: an instance of the vector_field class.
+        vecfld_dict: a dict with entries to create a `VectorField2D` instance. Defaults to None.
+        marker: the marker type. Any string supported by matplotlib.markers. Defaults to "o".
+        markersize: the size of the marker. Defaults to 200.
+        c: the marker colors. Defaults to "w".
+        cmap: the name of a matplotlib colormap to use for coloring or shading the confidence of fixed points. If None,
+            the default color map will set to be viridis (inferno) when the background is white (black). Defaults to
+            None.
+        filltype: the fill type used for stable, saddle, and unstable fixed points. Default is 'full', 'top' and 'none',
+            respectively. Defaults to ["full", "top", "none"].
+        background: the background color of the plot. Defaults to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "return".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'plot_fixed_points', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        ax: the matplotlib axes used for plotting. Default is to use the current axis. Defaults to None.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the Axes of the generated
+        figure would be returned.
     """
 
     import matplotlib
     import matplotlib.patheffects as PathEffects
     from matplotlib import markers, rcParams
     from matplotlib.colors import to_hex
 
@@ -553,81 +540,75 @@
         txt.set_path_effects(
             [
                 PathEffects.Stroke(linewidth=1.5, foreground=_background, alpha=0.8),
                 PathEffects.Normal(),
             ]
         )
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_fixed_points",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def plot_traj(
     f: Callable,
-    y0: List,
-    t: List,
-    args: tuple = (),
+    y0: npt.ArrayLike,
+    t: npt.ArrayLike,
+    args: Sequence[Any] = (),
     lw: float = 2,
     background: Optional[str] = None,
-    integration_direction: str = "both",
-    save_show_or_return: str = "return",
-    save_kwargs: dict = {},
-    ax: Axes = None,
-):
+    integration_direction: Literal["forward", "backward", "both"] = "both",
+    save_show_or_return: Literal["save", "show", "return"] = "return",
+    save_kwargs: Dict[str, Any] = {},
+    ax: Optional[Axes] = None,
+) -> Optional[Axes]:
     """Plots a trajectory on a phase portrait.
-    code adapted from: http://be150.caltech.edu/2017/handouts/dynamical_systems_approaches.html
+    
+    Code adapted from: http://be150.caltech.edu/2017/handouts/dynamical_systems_approaches.html
 
-    Parameters
-    ----------
-    f : function for form f(y, t, *args)
-        The right-hand-side of the dynamical system.
-        Must return a 2-array.
-    y0 : array_like, shape (2,)
-        Initial condition.
-    t : array_like
-        Time points for trajectory.
-    args : tuple, default ()
-        Additional arguments to be passed to f
-    lw : `float`, (default: 2)
-        The line width of the trajectory.
-    background: `str` or None (default: None)
-        The background color of the plot.
-    integration_direction: `str` (default: `forward`)
-        Integrate the trajectory in forward, backward or both directions. default is 'both'.
-    save_show_or_return: {'show', 'save', 'return'} (default: `return`)
-        Whether to save, show or return the figure.
-    save_kwargs: `dict` (default: `{}`)
-        A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-        will use the {"path": None, "prefix": 'plot_traj', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-        True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-        according to your needs.
-    ax : Matplotlib Axis instance
-        Axis on which to make the plot
-
-    Returns
-    -------
-    output : Matplotlib Axis instance
-        Axis with streamplot included.
+    Args:
+        f: the function for form f(y, t, *args). It would work as the right-hand-side of the dynamical system. Must
+            return a 2-array.
+        y0: the initial condition.
+        t: the time points for trajectory.
+        args: additional arguments to be passed to f. Defaults to ().
+        lw: the line width of the trajectory. Defaults to 2.
+        background: the background color of the plot. Defaults to None.
+        integration_direction: Determines whether to integrate the trajectory in the forward, backward, or both
+            direction. Default to "both".
+        save_show_or_return: whether to save, show or return the figure. Defaults to "return".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'plot_traj', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        ax: the axis on which to make the plot. If None, new axis would be created. Defaults to None.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the Axes of the generated
+        figure would be returned.
     """
+
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     if background is None:
         _background = rcParams.get("figure.facecolor")
         _background = to_hex(_background) if type(_background) is tuple else _background
     else:
@@ -641,77 +622,73 @@
     if len(y0.shape) == 1:
         ax = _plot_traj(y0, t, args, integration_direction, ax, color, lw, f)
     else:
         for i in range(y0.shape[0]):
             cur_y0 = y0[i, None]  # don't drop dimension
             ax = _plot_traj(cur_y0, t, args, integration_direction, ax, color, lw, f)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_traj",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 def plot_separatrix(
     vecfld: VectorField2D,
-    x_range: List,
-    y_range: List,
-    t: List,
+    x_range: npt.ArrayLike,
+    y_range: npt.ArrayLike,
+    t: npt.ArrayLike,
     noise: float = 1e-6,
     lw: float = 3,
-    vecfld_dict: dict = None,
+    vecfld_dict: Dict[str, Any] = None,
     background: Optional[str] = None,
-    save_show_or_return: str = "return",
-    save_kwargs: dict = {},
-    ax: Axes = None,
-):
+    save_show_or_return: Literal["save", "show", "return"] = "return",
+    save_kwargs: Dict[str, Any] = {},
+    ax: Optional[Axes] = None,
+) -> Optional[Axes]:
     """Plot separatrix on phase portrait.
 
-    Parameters
-    ----------
-        vecfld: :class:`~VectorField2D`
-            An instance of the VectorField2D class which presumably has fixed points computed and stored.
-        x_range: array_like, shape (2,)
-            Range of values for x-axis.
-        y_range: array_like, shape (2,)
-        t : array_like
-            Time points for trajectory.
-        noise : float
-            A small noise added to steady states for drawing the separatrix.
-        lw : `float`, (default: 2)
-            The line width of the trajectory.
-        background: `str` or None (default: None)
-            The background color of the plot.
-        save_show_or_return: {'show', 'save', 'return'} (default: `return`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'plot_separatrix', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        ax : Matplotlib Axis instance
-            Axis on which to make the plot
-
-        code adapted from: http://be150.caltech.edu/2017/handouts/dynamical_systems_approaches.html
-
+    Args:
+        vecfld: an instance of the VectorField2D class which presumably has fixed points computed and stored.
+        x_range: the range of values for x-axis.
+        y_range: the range of values for y-axis.
+        t: the time points for trajectory.
+        noise: a small noise added to steady states for drawing the separatrix. Defaults to 1e-6.
+        lw: the line width of the trajectory. Defaults to 3.
+        vecfld_dict: a dict with entries to create a `VectorField2D` instance. Defaults to None.
+        background: the background color of the plot. Defaults to None.
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "return".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'plot_separatrix', "dpi": None,
+            "ext": 'pdf', "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can
+            provide a dictionary that properly modify those keys according to your needs. Defaults to {}.
+        ax: the axis on which to make the plot. Defaults to None.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the Axes of the generated
+        figure would get returned.
     """
+
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
 
     if background is None:
         _background = rcParams.get("figure.facecolor")
         _background = to_hex(_background) if type(_background) is tuple else _background
     else:
@@ -777,31 +754,34 @@
 
                     # Plot
                     ax.plot(sep_a, sep_b, "-", color=color, lw=lw)
 
                     all_sep_a = sep_a if all_sep_a is None else np.concatenate((all_sep_a, sep_a))
                     all_sep_b = sep_b if all_sep_b is None else np.concatenate((all_sep_b, sep_b))
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "plot_separatrix",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return ax
 
 
 @docstrings.with_indent(4)
 def topography(
     adata: AnnData,
     basis: str = "umap",
@@ -809,183 +789,221 @@
     x: int = 0,
     y: int = 1,
     color: str = "ntr",
     layer: str = "X",
     highlights: Optional[list] = None,
     labels: Optional[list] = None,
     values: Optional[list] = None,
-    theme: Optional[str] = None,
+    theme: Optional[
+        Literal[
+            "blue",
+            "red",
+            "green",
+            "inferno",
+            "fire",
+            "viridis",
+            "darkblue",
+            "darkred",
+            "darkgreen",
+        ]
+    ] = None,
     cmap: Optional[str] = None,
-    color_key: Union[dict, list] = None,
+    color_key: Union[Dict[str, str], List[str], None] = None,
     color_key_cmap: Optional[str] = None,
     background: Optional[str] = "white",
     ncols: int = 4,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
-    show_legend="on data",
+    pointsize: Optional[float] = None,
+    figsize: Tuple[float, float] = (6, 4),
+    show_legend: str = "on data",
     use_smoothed: bool = True,
     xlim: np.ndarray = None,
     ylim: np.ndarray = None,
-    t: List = None,
-    terms: tuple = ("streamline", "fixed_points"),
-    init_cells: List = None,
-    init_states: List = None,
-    quiver_source: str = "raw",
-    fate: str = "both",
+    t: Optional[npt.ArrayLike] = None,
+    terms: List[str] = ["streamline", "fixed_points"],
+    init_cells: List[int] = None,
+    init_states: np.ndarray = None,
+    quiver_source: Literal["raw", "reconstructed"] = "raw",
+    fate: Literal["history", "future", "both"] = "both",
     approx: bool = False,
     quiver_size: Optional[float] = None,
     quiver_length: Optional[float] = None,
     density: float = 1,
     linewidth: float = 1,
     streamline_color: Optional[str] = None,
     streamline_alpha: float = 0.4,
     color_start_points: Optional[str] = None,
     markersize: float = 200,
     marker_cmap: Optional[str] = None,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     aggregate: Optional[str] = None,
     show_arrowed_spines: bool = False,
-    ax: Axes = None,
-    sort: str = "raw",
+    ax: Optional[Axes] = None,
+    sort: Literal["raw", "abs", "neg"] = "raw",
     frontier: bool = False,
-    s_kwargs_dict: dict = {},
-    q_kwargs_dict: dict = {},
+    s_kwargs_dict: Dict[str, Any] = {},
+    q_kwargs_dict: Dict[str, Any] = {},
+    n: int = 25,
     **streamline_kwargs_dict,
-):
+) -> Union[Axes, List[Axes], None]:
     """Plot the streamline, fixed points (attractor / saddles), nullcline, separatrices of a recovered dynamic system
     for single cells. The plot is created on two dimensional space.
 
-    Topography function plots the full vector field topology including streamline, fixed points, characteristic lines. A key
-    difference between dynamo and Velocyto or scVelo is that we learn a functional form of a vector field which can be
-    used to predict cell fate over arbitrary time and space. On states near observed cells, it retrieves the key kinetics
-    dynamics from the data observed and smoothes them. When time and state is far from your observed single cell RNA-seq
-    datasets, the accuracy of prediction will decay. Vector field can be efficiently reconstructed in high dimension or
-    lower pca/umap space. Since we learn a vector field function, we can plot the full vector via streamline on the entire
-    domain as well as predicts cell fates by providing a set of initial cell states (via `init_cells`, `init_states`). The
-    nullcline and separatrix provide topological information about the reconstructed vector field. By definition, the
-    x/y-nullcline is a set of points in the phase plane so that dx/dt = 0 or dy/dt=0. Geometrically, these are the points
-    where the vectors are either straight up or straight down. Algebraically, we find the x-nullcline by solving
-    f(x,y) = 0. The boundary different different attractor basis is the separatrix because it separates the regions into
-    different subregions with a specific behavior. To find them is a very difficult problem and separatrix calculated by
-    dynamo requres manual inspection.
+    Topography function plots the full vector field topology including streamline, fixed points, characteristic lines. A
+    key difference between dynamo and Velocyto or scVelo is that we learn a functional form of a vector field which can
+    be used to predict cell fate over arbitrary time and space. On states near observed cells, it retrieves the key
+    kinetics dynamics from the data observed and smoothes them. When time and state is far from your observed single
+    cell RNA-seq datasets, the accuracy of prediction will decay. Vector field can be efficiently reconstructed in high
+    dimension or lower pca/umap space. Since we learn a vector field function, we can plot the full vector via
+    streamline on the entire domain as well as predicts cell fates by providing a set of initial cell states (via
+    `init_cells`, `init_states`). The nullcline and separatrix provide topological information about the reconstructed
+    vector field. By definition, the x/y-nullcline is a set of points in the phase plane so that dx/dt = 0 or dy/dt=0.
+    Geometrically, these are the points where the vectors are either straight up or straight down. Algebraically, we
+    find the x-nullcline by solving f(x,y) = 0. The boundary different attractor basis is the separatrix
+    because it separates the regions into different subregions with a specific behavior. To find them is a very
+    difficult problem and separatrix calculated by dynamo requires manual inspection.
 
-    Here is more details on the fixed points drawn on the vector field:  Fixed points are concepts introduced in dynamic
+    Here is more details on the fixed points drawn on the vector field: Fixed points are concepts introduced in dynamic
     systems theory. There are three types of fixed points: 1) repeller: a repelling state that only has outflows, which
     may correspond to a pluripotent cell state (ESC) that tends to differentiate into other cell states automatically or
     under small perturbation; 2) unstable fixed points or saddle points. Those states have attraction on some dimension
-    (genes or reduced dimensions) but diverge in at least one other dimension. Saddle may correspond to progenitors, which
-    are differentiated from ESC/pluripotent cells and relatively stable, but can further differentiate into multiple
-    terminal cell types / states; 3) lastly, stable fixed points / cell type or attractors, which only have inflows and
-    attract all cell states nearby, which may correspond to stable cell types and can only be kicked out of its cell
-    state under extreme perturbation or in very rare situation. Fixed points are numbered with each number color coded.
-    The mapping of the color of the number to the type of fixed point are: red: repellers; blue: saddle points; black:
-    attractors. The scatter point itself also has filled color, which corresponds to confidence of the estimated fixed
-    point. The lighter, the more confident or the fixed points are are closer to the sequenced single cells. Confidence
-    of each fixed points can be used in conjunction with the Jacobian analysis for investigating regulatory network with
-    spatiotemporal resolution.
+    (genes or reduced dimensions) but diverge in at least one other dimension. Saddle may correspond to progenitors,
+    which are differentiated from ESC/pluripotent cells and relatively stable, but can further differentiate into
+    multiple terminal cell types / states; 3) lastly, stable fixed points / cell type or attractors, which only have
+    inflows and attract all cell states nearby, which may correspond to stable cell types and can only be kicked out of
+    its cell state under extreme perturbation or in very rare situation. Fixed points are numbered with each number
+    color coded. The mapping of the color of the number to the type of fixed point are: red: repellers; blue: saddle
+    points; black: attractors. The scatter point itself also has filled color, which corresponds to confidence of the
+    estimated fixed point. The lighter, the more confident or the fixed points are closer to the sequenced single
+    cells. Confidence of each fixed points can be used in conjunction with the Jacobian analysis for investigating
+    regulatory network with spatiotemporal resolution.
 
-    By default, we plot a figure with three subplots , each colors cells either with `potential`, `curl` or `divergence`.
+    By default, we plot a figure with three subplots, each colors cells either with `potential`, `curl` or `divergence`.
     `potential` is related to the intrinsic time, where a small potential is related to smaller intrinsic time and vice
     versa. Divergence can be used to indicate the state of each cell is in. Negative values correspond to potential sink
-    while positive corresponds to potential source. https://en.wikipedia.org/wiki/Divergence. Curl may be related to cell
-    cycle or other cycling cell dynamics. On 2d, negative values correspond to clockwise rotation while positive corresponds
-    to anticlockwise rotation. https://www.khanacademy.org/math/multivariable-calculus/greens-theorem-and-stokes-theorem/formal-definitions-of-divergence-and-curl/a/defining-curl
-    In conjunction with cell cycle score (dyn.pp.cell_cycle_scores), curl can be used to identify cells under active cell
-    cycle progression.
-
-    Parameters
-    ----------
-        %(scatters.parameters.no_show_legend|kwargs|save_kwargs)s
-        fps_basis: `str`
-            The basis that will be used for identifying or retrieving fixed points. Note that if `fps_basis` is
+    while positive corresponds to potential source. https://en.wikipedia.org/wiki/Divergence. Curl may be related to
+    cell cycle or other cycling cell dynamics. On 2d, negative values correspond to clockwise rotation while positive
+    corresponds to anticlockwise rotation.
+    https://www.khanacademy.org/math/multivariable-calculus/greens-theorem-and-stokes-theorem/formal-definitions-of-divergence-and-curl/a/defining-curl
+    In conjunction with cell cycle score (dyn.pp.cell_cycle_scores), curl can be used to identify cells under active
+    cell cycle progression.
+
+    Args:
+        adata: an AnnData object.
+        basis: the reduced dimension stored in adata.obsm. The specific basis key will be constructed in the following
+            priority if exits: 1) specific layer input +  basis 2) X_ + basis 3) basis. E.g. if basis is PCA, `scatters`
+            is going to look for 1) if specific layer is spliced, `spliced_pca` 2) `X_pca` (dynamo convention) 3) `pca`.
+            Defaults to "umap".
+        fps_basis: the basis that will be used for identifying or retrieving fixed points. Note that if `fps_basis` is
             different from `basis`, the nearest cells of the fixed point from the `fps_basis` will be found and used to
-            visualize the position of the fixed point on `basis` embedding.
-        xlim: `numpy.ndarray`
-            The range of x-coordinate
-        ylim: `numpy.ndarray`
-            The range of y-coordinate
-        t:  t_end: `float` (default 1)
-            The length of the time period from which to predict cell state forward or backward over time. This is used
-            by the odeint function.
-        terms: `tuple` (default: ('streamline', 'fixed_points'))
-            A tuple of plotting items to include in the final topography figure.  ('streamline', 'nullcline', 'fixed_points',
-             'separatrix', 'trajectory', 'quiver') are all the items that we can support.
-        init_cells: `list` (default: None)
-            Cell name or indices of the initial cell states for the historical or future cell state prediction with numerical integration.
-            If the names in init_cells are not find in the adata.obs_name, it will be treated as cell indices and must be integers.
-        init_states: `numpy.ndarray` (default: None)
-            Initial cell states for the historical or future cell state prediction with numerical integration. It can be
-            either a one-dimensional array or N x 2 dimension array. The `init_state` will be replaced to that defined by init_cells if
-            init_cells are not None.
-        quiver_source: `numpy.ndarray` {'raw', 'reconstructed'} (default: None)
-            The data source that will be used to draw the quiver plot. If `init_cells` is provided, this will set to be the projected RNA
-            velocity before vector field reconstruction automatically. If `init_cells` is not provided, this will set to be the velocity
-            vectors calculated from the reconstructed vector field function automatically. If quiver_source is `reconstructed`, the velocity
-            vectors calculated from the reconstructed vector field function will be used.
-        fate: `str` {"history", 'future', 'both'} (default: `both`)
-            Predict the historial, future or both cell fates. This corresponds to integrating the trajectory in forward,
-            backward or both directions defined by the reconstructed vector field function. default is 'both'.
-        approx: `bool` (default: False)
-            Whether to use streamplot to draw the integration line from the init_state.
-        quiver_size: `float` or None (default: None)
-            The size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is used to calculate
-            the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength (8 x quiver_size) of the quiver.
-            This is done via the `default_quiver_args` function which also calculate the scale of the quiver (1 / quiver_length).
-        quiver_length: `float` or None (default: None)
-            The length of quiver. The quiver length which will be used to calculate scale of quiver. Note that befoe applying
-            `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler function. Scale of quiver indicates
-            the number of data units per arrow length unit, e.g., m/s per plot width; a smaller scale parameter makes the arrow longer.
-        density: `float` or None (default: 1)
-            density of the plt.streamplot function.
-        linewidth: `float` or None (default: 1)
-            multiplier of automatically calculated linewidth passed to the plt.streamplot function.
-        streamline_color: `str` or None (default: None)
-            The color of the vector field stream lines.
-        streamline_alpha: `float` or None (default: 0.4)
-            The alpha value applied to the vector field stream lines.
-        color_start_points: `float` or None (default: `None`)
-            The color of the starting point that will be used to predict cell fates.
-        markersize: `float` (default: 200)
-            The size of the marker.
-        marker_cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring or shading the confidence of fixed points. If None, the
-            default color map will set to be viridis (inferno) when the background is white (black).
-        save_show_or_return: {'show', 'save', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'topography', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        aggregate: `str` or `None` (default: `None`)
-            The column in adata.obs that will be used to aggregate data points.
-        show_arrowed_spines: bool (optional, default False)
-            Whether to show a pair of arrowed spines representing the basis of the scatter is currently using.
-        ax: Matplotlib Axis instance
-            Axis on which to make the plot
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        s_kwargs_dict: `dict` (default: {})
-            The dictionary of the scatter arguments.
-        q_kwargs_dict:
-            Additional parameters that will be passed to plt.quiver function
-        streamline_kwargs_dict:
-            Additional parameters that will be passed to plt.streamline function
-
-    Returns
-    -------
-        Plot the streamline, fixed points (attractors / saddles), nullcline, separatrices of a recovered dynamic system
-        for single cells or return the corresponding axis, depending on the plot argument.
-
-    See also:: :func:`pp.cell_cycle_scores`
+            visualize the position of the fixed point on `basis` embedding. Defaults to "umap".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        color: any column names or gene expression, etc. that will be used for coloring cells. Defaults to "ntr".
+        layer: the layer of data to use for the scatter plot. Defaults to "X".
+        highlights: the color group that will be highlighted. If highligts is a list of lists, each list is relate to
+            each color element. Defaults to None.
+        labels: an array of labels (assumed integer or categorical), one for each data sample. This will be used for
+            coloring the points in the plot according to their label. Note that this option is mutually exclusive to the
+            `values` option. Defaults to None.
+        values: an array of values (assumed float or continuous), one for each sample. This will be used for coloring
+            the points in the plot according to a colorscale associated to the total range of values. Note that this
+            option is mutually exclusive to the `labels` option. Defaults to None.
+        theme: A color theme to use for plotting. A small set of predefined themes are provided which have relatively
+            good aesthetics. Available themes are: {'blue', 'red', 'green', 'inferno', 'fire', 'viridis', 'darkblue',
+            'darkred', 'darkgreen'}. Defaults to None.
+        cmap: The name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to None.
+        color_key: the method to assign colors to categoricals. This can either be an explicit dict mapping labels to
+            colors (as strings of form '#RRGGBB'), or an array like object providing one color for each distinct
+            category being provided in `labels`. Either way this mapping will be used to color points according to the
+            label. Note that if theme is passed then this value will be overridden by the corresponding option of the
+            theme. Defaults to None.
+        color_key_cmap: the name of a matplotlib colormap to use for categorical coloring. If an explicit `color_key` is
+            not given a color mapping for categories can be generated from the label list and selecting a matching list
+            of colors from the given colormap. Note that if theme is passed then this value will be overridden by the
+            corresponding option of the theme. Defaults to None.
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to None.
+        ncols: the number of columns for the figure. Defaults to 4.
+        pointsize: the scale of the point size. Actual point cell size is calculated as
+            `500.0 / np.sqrt(adata.shape[0]) * pointsize`. Defaults to None.
+        figsize: the width and height of a figure. Defaults to (6, 4).
+        show_legend: whether to display a legend of the labels. Defaults to "on data".
+        use_smoothed: whether to use smoothed values (i.e. M_s / M_u instead of spliced / unspliced, etc.). Defaults to
+            True.
+        xlim: the range of x-coordinate. Defaults to None.
+        ylim: the range of y-coordinate. Defaults to None.
+        t: the length of the time period from which to predict cell state forward or backward over time. This is used by
+            the odeint function. Defaults to None.
+        terms: a tuple of plotting items to include in the final topography figure. ('streamline', 'nullcline',
+            'fixed_points', 'separatrix', 'trajectory', 'quiver') are all the items that we can support. Defaults to
+            ["streamline", "fixed_points"].
+        init_cells: cell name or indices of the initial cell states for the historical or future cell state prediction
+            with numerical integration. If the names in init_cells are not find in the adata.obs_name, it will be
+            treated as cell indices and must be integers. Defaults to None.
+        init_states: the initial cell states for the historical or future cell state prediction with numerical
+            integration. It can be either a one-dimensional array or N x 2 dimension array. The `init_state` will be
+            replaced to that defined by init_cells if init_cells are not None. Defaults to None.
+        quiver_source: the data source that will be used to draw the quiver plot. If `init_cells` is provided, this will
+            set to be the projected RNA velocity before vector field reconstruction automatically. If `init_cells` is
+            not provided, this will set to be the velocity vectors calculated from the reconstructed vector field
+            function automatically. If quiver_source is `reconstructed`, the velocity vectors calculated from the
+            reconstructed vector field function will be used. Defaults to "raw".
+        fate: predict the historial, future or both cell fates. This corresponds to integrating the trajectory in
+            forward, backward or both directions defined by the reconstructed vector field function. Defaults to "both".
+        approx: whether to use streamplot to draw the integration line from the init_state. Defaults to False.
+        quiver_size: the size of quiver. If None, we will use set quiver_size to be 1. Note that quiver quiver_size is
+            used to calculate the head_width (10 x quiver_size), head_length (12 x quiver_size) and headaxislength
+            (8 x quiver_size) of the quiver. This is done via the `default_quiver_args` function which also calculate
+            the scale of the quiver (1 / quiver_length). Defaults to None.
+        quiver_length: the length of quiver. The quiver length which will be used to calculate scale of quiver. Note
+            that befoe applying `default_quiver_args` velocity values are first rescaled via the quiver_autoscaler
+            function. Scale of quiver indicates the number of data units per arrow length unit, e.g., m/s per plot
+            width; a smaller scale parameter makes the arrow longer. Defaults to None.
+        density: the density of the plt.streamplot function. Defaults to 1.
+        linewidth: the multiplier of automatically calculated linewidth passed to the plt.streamplot function. Defaults
+            to 1.
+        streamline_color: the color of the vector field streamlines. Defaults to None.
+        streamline_alpha: the alpha value applied to the vector field streamlines. Defaults to 0.4.
+        color_start_points: the color of the starting point that will be used to predict cell fates. Defaults to None.
+        markersize: the size of the marker. Defaults to 200.
+        marker_cmap: the name of a matplotlib colormap to use for coloring or shading the confidence of fixed points. If
+            None, the default color map will set to be viridis (inferno) when the background is white (black). Defaults
+            to None.
+        save_show_or_return: whether to save, show or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'topography', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        aggregate: the column in adata.obs that will be used to aggregate data points. Defaults to None.
+        show_arrowed_spines: whether to show a pair of arrowed spines representing the basis of the scatter is currently
+            using. Defaults to False.
+        ax: the axis on which to make the plot. Defaults to None.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values. Defaults
+            to "raw". Defaults to "raw".
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to False.
+        s_kwargs_dict: the dictionary of the scatter arguments. Defaults to {}.
+        q_kwargs_dict: additional parameters that will be passed to plt.quiver function. Defaults to {}.
+        n: Number of samples for calculating the fixed points.
+        **streamline_kwargs_dict: any other kwargs that would be passed to `pyplot.streamline`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the Axes of the generated
+        subplots would be returned.
     """
+
     from ..external.hodge import ddhodge
 
     logger = LoggerManager.gen_logger("dynamo-topography-plot")
     logger.log_time()
 
     from matplotlib import rcParams
     from matplotlib.colors import to_hex
@@ -1066,15 +1084,15 @@
             f"Vector field for {fps_basis} is not constructed. " f"Constructing it and mapping its topography now ...",
             indent_level=1,
         )
 
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
 
-            VectorField(adata, fps_basis, map_topography=True)
+            VectorField(adata, fps_basis, map_topography=True, n=n)
     # elif "VecFld2D" not in adata.uns[uns_key].keys():
     #     with warnings.catch_warnings():
     #         warnings.simplefilter("ignore")
     #
     #         _topology(adata, basis, VecFld=None)
     # elif "VecFld2D" in adata.uns[uns_key].keys() and type(adata.uns[uns_key]["VecFld2D"]) == str:
     #     with warnings.catch_warnings():
@@ -1312,29 +1330,32 @@
                 df.iloc[:, 0],
                 df.iloc[:, 1],
                 df.iloc[:, 2],
                 df.iloc[:, 3],
                 **quiver_kwargs,
             )  # color='red',  facecolors='gray'
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": "topography",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
 
             plt.tight_layout()
 
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return axes_list if len(axes_list) > 1 else axes_list[0]
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/utils.py` & `dynamo-release-1.3.0/dynamo/plot/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import copy
 import math
 import os
 
 # import matplotlib.tri as tri
 import warnings
+from typing import Optional
 from warnings import warn
 
 import matplotlib
 import matplotlib.patheffects as PathEffects
 import matplotlib.pyplot as plt
 import numba
 import numpy as np
@@ -1254,26 +1255,22 @@
     return ax
 
 
 # ---------------------------------------------------------------------------------------------------
 # vector field plot related utilities
 
 
-def quiver_autoscaler(X_emb, V_emb):
+def quiver_autoscaler(X_emb: np.ndarray, V_emb: np.ndarray) -> float:
     """Function to automatically calculate the value for the scale parameter of quiver plot, adapted from scVelo
 
-    Parameters
-    ----------
-        X_emb: `np.ndarray`
-            X, Y-axis coordinates
-        V_emb:  `np.ndarray`
-            Velocity (U, V) values on the X, Y-axis
+    Args:
+        X_emb: X, Y-axis coordinates
+        V_emb: Velocity (U, V) values on the X, Y-axis
 
-    Returns
-    -------
+    Returns:
         The scale for quiver plot
     """
 
     import matplotlib.pyplot as plt
 
     fig, ax = plt.subplots()
 
@@ -1357,49 +1354,41 @@
 
 # ---------------------------------------------------------------------------------------------------
 # save_fig figure related
 # ---------------------------------------------------------------------------------------------------
 
 
 def save_fig(
-    path=None,
-    prefix=None,
-    dpi=None,
-    ext="pdf",
-    transparent=True,
-    close=True,
-    verbose=True,
-):
+    path: Optional[str] = None,
+    prefix: Optional[str] = None,
+    dpi: Optional[int] = None,
+    ext: str = "pdf",
+    transparent: bool = True,
+    close: bool = True,
+    verbose: bool = True,
+) -> None:
     """Save a figure from pyplot.
-    code adapated from http://www.jesshamrick.com/2012/09/03/saving-figures-from-pyplot/
 
-    Parameters
-    ----------
-         path: `string`
-            The path (and filename, without the extension) to save_fig the
-            figure to.
-        prefix: `str` or `None`
-            The prefix added to the figure name. This will be automatically set
-            accordingly to the plotting function used.
-        dpi: [ None | scalar > 0 | 'figure' ]
-            The resolution in dots per inch. If None, defaults to rcParams["savefig.dpi"].
-            If 'figure', uses the figure's dpi value.
-        ext: `string` (default='pdf')
-            The file extension. This must be supported by the active
-            matplotlib backend (see matplotlib.backends module).  Most
-            backends support 'png', 'pdf', 'ps', 'eps', and 'svg'.
-        close: `boolean` (default=True)
-            Whether to close the figure after saving.  If you want to save_fig
-            the figure multiple times (e.g., to multiple formats), you
-            should NOT close it in between saves or you will have to
-            re-plot it.
-        verbose: boolean (default=True)
-            Whether to print information about when and where the image
-            has been saved.
+    Code adapated from http://www.jesshamrick.com/2012/09/03/saving-figures-from-pyplot/
+
+    Args:
+        path: the path (and filename, without the extension) to save_fig the figure to. Defaults to None.
+        prefix: the prefix added to the figure name. This will be automatically set accordingly to the plotting function
+            used. Defaults to None.
+        dpi: the resolution in dots per inch. If None, defaults to rcParams["savefig.dpi"]. If 'figure', uses the
+            figure's dpi value. Defaults to None.
+        ext: the file extension. This must be supported by the active matplotlib backend (see matplotlib.backends
+            module).  Most backends support 'png', 'pdf', 'ps', 'eps', and 'svg'. Defaults to "pdf".
+        transparent: whether to save the figure with axes patches and background transparent. Defaults to True.
+        close: whether to close the figure after saving.  If you want to save_fig the figure multiple times (e.g., to
+        multiple formats), you should NOT close it in between saves or you will have to re-plot it. Defaults to True.
+        verbose: whether to print information about when and where the image
+            has been saved. Defaults to True.
     """
+
     import matplotlib.pyplot as plt
 
     prefix = os.path.normpath(prefix)
     if path is None:
         path = os.getcwd() + "/"
 
     # Extract the directory and filename from the given path
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/utils_dynamics.py` & `dynamo-release-1.3.0/dynamo/plot/utils_dynamics.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/plot/utils_graph.py` & `dynamo-release-1.3.0/dynamo/plot/utils_graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -172,15 +172,15 @@
     def set_network(self, network):
         try:
             import networkx as nx
         except ImportError:
             raise ImportError(
                 f"You need to install the packages `networkx`." f"install networkx via `pip install networkx`."
             )
-        self.E = nx.to_numpy_matrix(network)
+        self.E = nx.to_numpy_array(network)
         self.node_names = list(network.nodes)
 
     def compute_node_positions(self, node_order=None):
         if self.E is None:
             raise Exception("The adjacency matrix is not set.")
 
         if node_order is None:
```

### Comparing `dynamo-release-1.2.0/dynamo/plot/vector_calculus.py` & `dynamo-release-1.3.0/dynamo/plot/vector_calculus.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,15 +1,22 @@
 """plotting utilities that are used to visualize the curl, divergence."""
 
-from typing import List, Optional, Union
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import anndata
 import numpy as np
 import pandas as pd
 from anndata import AnnData
+from matplotlib.axes import Axes
+from matplotlib.gridspec import GridSpec
 
 from ..tools.utils import flatten, update_dict
 from ..vectorfield.utils import intersect_sources_targets
 from .scatters import docstrings, scatters
 from .utils import (
     _matplotlib_points,
     arrowed_spines,
@@ -25,129 +32,112 @@
 docstrings.delete_params("scatters.parameters", "adata", "color", "cmap", "frontier")
 
 
 @docstrings.with_indent(4)
 def speed(
     adata: AnnData,
     basis: str = "pca",
-    color: Union[str, list, None] = None,
+    color: Union[str, List[str], None] = None,
     frontier: bool = True,
     *args,
     **kwargs,
-):
-    """\
-    Scatter plot with cells colored by the estimated velocity speed (and other information if provided).
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with speed estimated.
-        basis: `str` or None (default: `pca`)
-            The embedding data in which the vector field was reconstructed and RNA speed was estimated.
-        color: `str`, `list` or None:
-            Any column names or gene names, etc. in addition to the `curl` to be used for coloring cells.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        %(scatters.parameters.no_adata|color|cmap|frontier)s
-
-    Returns
-    -------
-    Nothing but plots scatterplots with cells colored by the estimated speed (and other information if provided).
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> adata = dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.reduceDimension(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> dyn.vf.speed(adata)
-    >>> dyn.pl.speed(adata)
-
-    See also:: :func:`..external.ddhodge.curl` for calculating curl with a diffusion graph built from reconstructed vector
-    field.
+) -> Union[
+    Axes,
+    List[Axes],
+    Tuple[Axes, List[str], Literal["white", "black"]],
+    Tuple[List[Axes], List[str], Literal["white", "black"]],
+    None,
+]:
+    """Scatter plot with cells colored by the estimated velocity speed (and other information if provided).
+
+    Args:
+        adata: an Annodata object with speed estimated.
+        basis: the embedding data in which the vector field was reconstructed and RNA speed was estimated. Defaults to
+            "pca".
+        color: any column names or gene names, etc. in addition to the `curl` to be used for coloring cells. Defaults to
+            None.
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+
+    Raises:
+        Exception: speed information is not found in `adata`.
+
+    Returns:
+        None would be returned by default. If in kwargs `save_show_or_return` is set to be 'return' or 'all', the
+        matplotlib axes object of the generated plots would be returned. If `return_all` is set to be true, the list of
+        colors used and the font color would also be returned. See docs of `dynamo.pl.scatters` for more information.
     """
 
     speed_key = "speed" if basis is None else "speed_" + basis
     color_ = [speed_key]
     if not np.any(adata.obs.columns.isin(color_)):
-        raise Exception(f"{speed_key} is not existed in .obs, try run dyn.tl.speed(adata, basis='{basis}') first.")
+        raise ValueError(f"{speed_key} is not existed in .obs, try run dyn.tl.speed(adata, basis='{basis}') first.")
 
     if color is not None:
         color = [color] if type(color) == str else color
         color_.extend(color)
 
     return scatters(adata, color=color_, frontier=frontier, *args, **kwargs)
 
 
 @docstrings.with_indent(4)
 def curl(
     adata: AnnData,
     basis: str = "umap",
-    color: Union[str, list, None] = None,
+    color: Union[str, List[str], None] = None,
     cmap: str = "bwr",
     frontier: bool = True,
     sym_c: bool = True,
     *args,
     **kwargs,
-):
-    """\
-    Scatter plot with cells colored by the estimated curl (and other information if provided).
+) -> Union[
+    Axes,
+    List[Axes],
+    Tuple[Axes, List[str], Literal["white", "black"]],
+    Tuple[List[Axes], List[str], Literal["white", "black"]],
+    None,
+]:
+    """Scatter plot with cells colored by the estimated curl (and other information if provided).
 
     Cells with negative or positive curl correspond to cells with clock-wise rotation vectors or counter-clock-wise
     ration vectors. Currently only support for 2D vector field. But in principal could be generated to high dimension
     space.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with curl estimated.
-        basis: `str` or None (default: `umap`)
-            The embedding data in which the vector field was reconstructed and RNA curl was estimated.
-        color: `str`, `list` or None:
-            Any column names or gene names, etc. in addition to the `curl` to be used for coloring cells.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        sym_c: `bool` (default: `False`)
-            Whether do you want to make the limits of continuous color to be symmetric, normally this should be used for
-            plotting velocity, curl, divergence or other types of data with both positive or negative values.
-        %(scatters.parameters.no_adata|color|cmap|frontier|sym_c)s
-
-    Returns
-    -------
-    Nothing but plots scatterplots with cells colored by the estimated curl (and other information if provided).
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> adata = dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.reduceDimension(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='umap')
-    >>> dyn.vf.VectorField(adata, basis='umap')
-    >>> dyn.vf.curl(adata, basis='umap')
-    >>> dyn.pl.curl(adata, basis='umap')
-
-    See also:: :func:`..external.ddhodge.curl` for calculating curl with a diffusion graph built from reconstructed vector
-    field.
+    Args:
+        adata: an Annodata object with curl estimated.
+        basis: the embedding data in which the vector field was reconstructed and RNA curl was estimated. Defaults to
+            "umap".
+        color: any column names or gene names, etc. in addition to the `curl` to be used for coloring cells. Defaults to
+            None.
+        cmap: the color map used for the plot. Defaults to "bwr".
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+        sym_c: whether do you want to make the limits of continuous color to be symmetric, normally this should be used
+            for plotting velocity, curl, divergence or other types of data with both positive or negative values.
+            Defaults to True.
+        *args: any other positional arguments to be passed to `dynamo.pl.scatters`.
+        **kwargs: any other kwargs to be passed to `dynamo.pl.scatters`.
+
+    Raises:
+        ValueError: curl information not found in `adata`.
+
+    Returns:
+        None would be returned by default. If in kwargs `save_show_or_return` is set to be 'return' or 'all', the
+        matplotlib axes object of the generated plots would be returned. If `return_all` is set to be true, the list of
+        colors used and the font color would also be returned. See docs of `dynamo.pl.scatters` for more information.
     """
 
     curl_key = "curl" if basis is None else "curl_" + basis
     color_ = [curl_key]
     if not np.any(adata.obs.columns.isin(color_)):
-        raise Exception(f"{curl_key} is not existed in .obs, try run dyn.tl.curl(adata, basis='{basis}') first.")
+        raise ValueError(f"{curl_key} is not existed in .obs, try run dyn.tl.curl(adata, basis='{basis}') first.")
 
     if color is not None:
         color = [color] if type(color) == str else color
         color_.extend(color)
 
     # adata.obs[curl_key] = adata.obs[curl_key].astype('float')
     # adata_ = adata[~ adata.obs[curl_key].isna(), :]
@@ -163,68 +153,54 @@
     )
 
 
 @docstrings.with_indent(4)
 def divergence(
     adata: AnnData,
     basis: str = "pca",
-    color: Union[str, list, None] = None,
+    color: Union[str, List[str], None] = None,
     cmap: str = "bwr",
     frontier: bool = True,
     sym_c: bool = True,
     *args,
     **kwargs,
 ):
-    """\
-    Scatter plot with cells colored by the estimated divergence (and other information if provided).
+    """Scatter plot with cells colored by the estimated divergence (and other information if provided).
 
     Cells with negative or positive divergence correspond to possible sink (stable cell types) or possible source
-    (unstable metastable states or progenitors)
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with divergence estimated.
-        basis: `str` or None (default: `pca`)
-            The embedding data in which the vector field was reconstructed and RNA divergence was estimated.
-        color: `str`, `list` or None:
-            Any column names or gene names, etc. in addition to the `divergence` to be used for coloring cells.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        sym_c: `bool` (default: `False`)
-            Whether do you want to make the limits of continuous color to be symmetric, normally this should be used for
-            plotting velocity, divergence or other types of data with both positive or negative values.
-        %(scatters.parameters.no_adata|color|cmap|frontier|sym_c)s
-
-    Returns
-    -------
-    Nothing but plots scatterplots with cells colored by the estimated divergence (and other information if provided).
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> dyn.vf.divergence(adata)
-    >>> dyn.pl.divergence(adata)
+    (unstable metastable states or progenitors).
 
-    See also:: :func:`..external.ddhodge.divergence` for calculating divergence with a diffusion graph built from reconstructed
-    vector field.
+    Args:
+        adata: an Annodata object with divergence estimated.
+        basis: the embedding data in which the vector field was reconstructed and RNA divergence was estimated. Defaults
+            to "pca".
+        color: any column names or gene names, etc. in addition to the `divergence` to be used for coloring cells.
+            Defaults to None.
+        cmap: The name of a matplotlib colormap to use for coloring or shading points. Defaults to "bwr".
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+        sym_c: whether do you want to make the limits of continuous color to be symmetric, normally this should be used
+            for plotting velocity, curl, divergence or other types of data with both positive or negative values.
+            Defaults to True.
+
+    Raises:
+        ValueError: divergence information not found in `adata`.
+
+    Returns:
+        None would be returned by default. If in kwargs `save_show_or_return` is set to be 'return' or 'all', the
+        matplotlib axes object of the generated plots would be returned. If `return_all` is set to be true, the list of
+        colors used and the font color would also be returned. See docs of `dynamo.pl.scatters` for more information.
     """
 
     div_key = "divergence" if basis is None else "divergence_" + basis
     color_ = [div_key]
     if not np.any(adata.obs.columns.isin(color_)):
-        raise Exception(f"{div_key} is not existed in .obs, try run dyn.tl.divergence(adata, basis='{basis}') first.")
+        raise ValueError(f"{div_key} is not existed in .obs, try run dyn.tl.divergence(adata, basis='{basis}') first.")
 
     # adata.obs[div_key] = adata.obs[div_key].astype('float')
     # adata_ = adata[~ adata.obs[div_key].isna(), :]
 
     if color is not None:
         color = [color] if type(color) == str else color
         color_.extend(color)
@@ -240,57 +216,53 @@
     )
 
 
 @docstrings.with_indent(4)
 def acceleration(
     adata: AnnData,
     basis: str = "pca",
-    color: Union[str, list, None] = None,
+    color: Union[str, List[str], None] = None,
     frontier: bool = True,
     *args,
     **kwargs,
-):
-    """\
-    Scatter plot with cells colored by the estimated acceleration (and other information if provided).
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with curvature estimated.
-        basis: `str` or None (default: `pca`)
-            The embedding data in which the vector field was reconstructed and RNA curvature was estimated.
-        color: `str`, `list` or None:
-            Any column names or gene names, etc. in addition to the `acceleration` to be used for coloring cells.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        %(scatters.parameters.no_adata|color|cmap|frontier)s
-
-    Returns
-    -------
-    Nothing but plots scatterplots with cells colored by the estimated curvature (and other information if provided).
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> dyn.vf.acceleration(adata)
-    >>> dyn.pl.acceleration(adata)
+) -> Union[
+    Axes,
+    List[Axes],
+    Tuple[Axes, List[str], Literal["white", "black"]],
+    Tuple[List[Axes], List[str], Literal["white", "black"]],
+    None,
+]:
+    """Scatter plot with cells colored by the estimated acceleration (and other information if provided).
+
+    Args:
+        adata: an Annodata object with curvature estimated.
+        basis: the embedding data in which the vector field was reconstructed and RNA curvature was estimated. Defaults
+            to "pca".
+        color: any column names or gene names, etc. in addition to the `acceleration` to be used for coloring cells.
+            Defaults to None.
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+        *args: any other positional arguments to be passed to `dynamo.pl.scatters`.
+        **kwargs: any other kwargs to be passed to `dynamo.pl.scatters`.
+
+    Raises:
+        ValueError: acceleration estimation information is not found in `adata`.
+
+    Returns:
+        None would be returned by default. If in kwargs `save_show_or_return` is set to be 'return' or 'all', the
+        matplotlib axes object of the generated plots would be returned. If `return_all` is set to be true, the list of
+        colors used and the font color would also be returned. See docs of `dynamo.pl.scatters` for more information.
     """
 
     acc_key = "acceleration" if basis is None else "acceleration_" + basis
     color_ = [acc_key]
     if not np.any(adata.obs.columns.isin(color_)):
-        raise Exception(
+        raise ValueError(
             f"{acc_key} is not existed in .obs, try run dyn.tl.acceleration(adata, basis='{acc_key}') first."
         )
 
     adata.obs[acc_key] = adata.obs[acc_key].astype("float")
     adata_ = adata[~adata.obs[acc_key].isna(), :]
 
     if color is not None:
@@ -300,57 +272,53 @@
     return scatters(adata_, color=color_, frontier=frontier, *args, **kwargs)
 
 
 @docstrings.with_indent(4)
 def curvature(
     adata: AnnData,
     basis: str = "pca",
-    color: Union[str, list, None] = None,
+    color: Union[str, List[str], None] = None,
     frontier: bool = True,
     *args,
     **kwargs,
-):
-    """\
-    Scatter plot with cells colored by the estimated curvature (and other information if provided).
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with curvature estimated.
-        basis: `str` or None (default: `pca`)
-            The embedding data in which the vector field was reconstructed and RNA curvature was estimated.
-        color: `str`, `list` or None:
-            Any column names or gene names, etc. in addition to the `curvature` to be used for coloring cells.
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        %(scatters.parameters.no_adata|color|cmap|frontier)s
-
-    Returns
-    -------
-    Nothing but plots scatterplots with cells colored by the estimated curvature (and other information if provided).
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> dyn.vf.curvature(adata)
-    >>> dyn.pl.curvature(adata)
+) -> Union[
+    Axes,
+    List[Axes],
+    Tuple[Axes, List[str], Literal["white", "black"]],
+    Tuple[List[Axes], List[str], Literal["white", "black"]],
+    None,
+]:
+    """Scatter plot with cells colored by the estimated curvature (and other information if provided).
+
+    Args:
+        adata: an Annodata object with curvature estimated.
+        basis: the embedding data in which the vector field was reconstructed and RNA curvature was estimated. Defaults
+            to "pca".
+        color: any column names or gene names, etc. in addition to the `curvature` to be used for coloring cells.
+            Defaults to None.
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+        *args: any other positional arguments to be passed to `dynamo.pl.scatters`.
+        **kwargs: any other kwargs to be passed to `dynamo.pl.scatters`.
+
+    Raises:
+        ValueError: curvature information is not found in `adata`.
+
+    Returns:
+        None would be returned by default. If in kwargs `save_show_or_return` is set to be 'return' or 'all', the
+        matplotlib axes object of the generated plots would be returned. If `return_all` is set to be true, the list of
+        colors used and the font color would also be returned. See docs of `dynamo.pl.scatters` for more information.
     """
 
     curv_key = "curvature" if basis is None else "curvature_" + basis
     color_ = [curv_key]
     if not np.any(adata.obs.columns.isin(color_)):
-        raise Exception(
+        raise ValueError(
             f"{curv_key} is not existed in .obs, try run dyn.tl.curvature(adata, basis='{curv_key}') first."
         )
 
     adata.obs[curv_key] = adata.obs[curv_key].astype("float")
     adata_ = adata[~adata.obs[curv_key].isna(), :]
 
     if color is not None:
@@ -359,125 +327,99 @@
 
     return scatters(adata_, color=color_, frontier=frontier, *args, **kwargs)
 
 
 @docstrings.with_indent(4)
 def jacobian(
     adata: AnnData,
-    regulators: Optional[List] = None,
-    effectors: Optional[List] = None,
+    regulators: Optional[List[str]] = None,
+    effectors: Optional[List[str]] = None,
     basis: str = "umap",
     jkey: str = "jacobian",
     j_basis: str = "pca",
     x: int = 0,
     y: int = 1,
     layer: str = "M_s",
-    highlights: list = None,
+    highlights: Optional[list] = None,
     cmap: str = "bwr",
     background: Optional[str] = None,
-    pointsize: Union[None, float] = None,
-    figsize: tuple = (6, 4),
+    pointsize: Optional[float] = None,
+    figsize: Tuple[float, float] = (6, 4),
     show_legend: bool = True,
     frontier: bool = True,
     sym_c: bool = True,
-    sort: str = "abs",
+    sort: Literal["raw", "abs", "neg"] = "abs",
     show_arrowed_spines: bool = False,
     stacked_fraction: bool = False,
-    save_show_or_return: str = "show",
-    save_kwargs: dict = {},
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
-    """\
-    Scatter plot of Jacobian values across cells.
+) -> Optional[GridSpec]:
+    """Scatter plot of Jacobian values across cells.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with Jacobian matrix estimated.
-        regulators: `list` or `None` (default: `None`)
-            The list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        effectors: `List` or `None` (default: `None`)
-            The list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        basis: `str` (default: `umap`)
-            The reduced dimension basis.
-        jkey: `str` (default: `jacobian`)
-            The key to the jacobian dictionary in .uns.
-        j_basis: `str` (default: `pca`)
-            The reduced dimension space that will be used to calculate the jacobian matrix.
-        x: `int` (default: `0`)
-            The column index of the low dimensional embedding for the x-axis.
-        y: `int` (default: `1`)
-            The column index of the low dimensional embedding for the y-axis.
-        highlights: `list` (default: None)
-            Which color group will be highlighted. if highligts is a list of lists - each list is relate to each color element.
-        cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring
-            or shading points. If no labels or values are passed
-            this will be used for shading points according to
-            density (largely only of relevance for very large
-            datasets). If values are passed this will be used for
-            shading according the value. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        background: string or None (optional, default 'None`)
-            The color of the background. Usually this will be either
-            'white' or 'black', but any color name will work. Ideally
-            one wants to match this appropriately to the colors being
-            used for points etc. This is one of the things that themes
-            handle for you. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        figsize: `None` or `[float, float]` (default: (6, 4))
-                The width and height of each panel in the figure.
-        show_legend: bool (optional, default True)
-            Whether to display a legend of the labels
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        sym_c: `bool` (default: `True`)
-            Whether do you want to make the limits of continuous color to be symmetric, normally this should be used for
-            plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative values.
-        sort: `str` (optional, default `abs`)
-            The method to reorder data so that high values points will be on top of background points. Can be one of
-            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values.
-        show_arrowed_spines: bool (optional, default False)
-            Whether to show a pair of arrowed spines representing the basis of the scatter is currently using.
-        stacked_fraction: bool (default: False)
-            If True the jacobian will be represented as a stacked fraction in the title, otherwise a linear fraction
-            style is used.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig
-            function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent": True,
-            "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly
-            modify those keys according to your needs.
-        kwargs:
-            Additional arguments passed to plt._matplotlib_points.
-
-    Returns
-    -------
-    Nothing but plots the n_source x n_targets scatter plots of low dimensional embedding of the adata object, each
-    corresponds to one element in the Jacobian matrix for all sampled cells.
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
-    >>> dyn.vf.jacobian(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
-    >>> dyn.pl.jacobian(adata)
+    Args:
+        adata: an Annodata object with Jacobian matrix estimated.
+        regulators: the list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        effectors: the list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        basis: the reduced dimension basis. Defaults to "umap".
+        jkey: the key to the jacobian dictionary in .uns. Defaults to "jacobian".
+        j_basis: the reduced dimension space that will be used to calculate the jacobian matrix. Defaults to "pca".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        layer: the layer key with jacobian data. Defaults to "M_s".
+        highlights: which color group will be highlighted. if highligts is a list of lists - each list is relate to each
+            color element. Defaults to None.
+        cmap: the name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to "bwr".
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to None.
+        pointsize: the size of plotted points. Defaults to None.
+        figsize: the size of the figure. Defaults to (6, 4).
+        show_legend: whether to display a legend of the labels. Defaults to True.
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+        sym_c: whether do you want to make the limits of continuous color to be symmetric, normally this should be used
+            for plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative
+            values. Defaults to True.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values. Defaults
+            to "abs".
+        show_arrowed_spines: whether to show a pair of arrowed spines representing the basis of the scatter is currently
+            using. Defaults to False.
+        stacked_fraction: whether the jacobian will be represented as a stacked fraction in the title or a linear
+            fraction style will be used. Defaults to False.
+        save_show_or_return: whether to save, show, or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs.. Defaults to {}.
+        **kwargs: any other kwargs that would be passed to `plt._matplotlib_points`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib `GridSpec` of
+        the figure would be returned.
+        
+    Examples:
+        >>> import dynamo as dyn
+        >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
+        >>> dyn.pp.recipe_monocle(adata)
+        >>> dyn.tl.dynamics(adata)
+        >>> dyn.tl.cell_velocities(adata, basis='pca')
+        >>> dyn.vf.VectorField(adata, basis='pca')
+        >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
+        >>> dyn.vf.jacobian(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
+        >>> dyn.pl.jacobian(adata)
     """
 
     regulators, effectors = (
         list(np.unique(regulators)) if regulators is not None else None,
         list(np.unique(effectors)) if effectors is not None else None,
     )
     import matplotlib.pyplot as plt
@@ -633,101 +575,96 @@
                 ax.set_title(r"$\partial f_{%s} / \partial x_{%s}$" % (target, source))
             if i + j == 0 and show_arrowed_spines:
                 arrowed_spines(ax, basis, background)
             else:
                 despline_all(ax)
                 deaxis_all(ax)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": jkey,
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return gs
 
 
 def jacobian_heatmap(
     adata: AnnData,
-    cell_idx: Union[int, List],
+    cell_idx: Union[int, List[int]],
+    average: bool = False,
     jkey: str = "jacobian",
     basis: str = "umap",
-    regulators: Optional[List] = None,
-    effectors: Optional[List] = None,
-    figsize: tuple = (7, 5),
+    regulators: Optional[List[str]] = None,
+    effectors: Optional[List[str]] = None,
+    figsize: Tuple[float, float] = (7, 5),
     ncols: int = 1,
     cmap: str = "bwr",
-    save_show_or_return: str = "show",
+    save_show_or_return: Literal["save", "show", "return"] = "show",
     save_kwargs: dict = {},
     **kwargs,
-):
-    """\
-    Plot the Jacobian matrix for each cell as a heatmap.
+) -> Optional[GridSpec]:
+    """Plot the Jacobian matrix for each cell or the average Jacobian matrix of the cells from input indices as a heatmap.
 
     Note that Jacobian matrix can be understood as a regulatory activity matrix between genes directly computed from the
     reconstructed vector fields.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with Jacobian matrix estimated.
-        cell_idx: `int` or `list`
-            The numeric indices of the cells that you want to draw the jacobian matrix to reveal the regulatory activity.
-        jkey: `str` (default: `jacobian`)
-            The key to the jacobian dictionary in .uns.
-        basis: `str`
-            The reduced dimension basis.
-        regulators: `list` or `None` (default: `None`)
-            The list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        effectors: `List` or `None` (default: `None`)
-            The list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        figsize: `None` or `[float, float]` (default: None)
-                The width and height of each panel in the figure.
-        ncols: `int` (default: `1`)
-            The number of columns for drawing the heatmaps.
-        cmap: `str` (default: `bwr`)
-            The mapping from data values to color space. If not provided, the default will depend on whether center is set.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        kwargs:
-            Additional arguments passed to sns.heatmap.
-
-    Returns
-    -------
-        Nothing but plots the n_cell_idx heatmaps of the corresponding Jacobian matrix for each selected cell.
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
-    >>> dyn.vf.jacobian(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
-    >>> dyn.pl.jacobian_heatmap(adata)
+    Args:
+        adata: an Annodata object with Jacobian matrix estimated.
+        cell_idx: the numeric indices of the cells that you want to draw the jacobian matrix to reveal the regulatory
+            activity.
+        average: whether to average the Jacobian matrix of the cells from the input indices.
+        jkey: the key to the jacobian dictionary in .uns. Defaults to "jacobian".
+        basis: the reduced dimension basis. Defaults to "umap".
+        regulators: the list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        effectors: the list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        figsize: the size of the subplots. Defaults to (7, 5).
+        ncols: the number of columns for drawing the heatmaps. Defaults to 1.
+        cmap: the mapping from data values to color space. If not provided, the default will depend on whether center is
+            set. Defaults to "bwr".
+        save_show_or_return: whether to save, show, or return the generated figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        **kwargs: any other kwargs passed to `sns.heatmap`.
+
+    Raises:
+        ValueError: jacobian information is not found in `adata`
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib `GridSpec` of
+        the figure would be returned.
+
+    Examples:
+        >>> import dynamo as dyn
+        >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
+        >>> dyn.pp.recipe_monocle(adata)
+        >>> dyn.tl.dynamics(adata)
+        >>> dyn.tl.cell_velocities(adata, basis='pca')
+        >>> dyn.vf.VectorField(adata, basis='pca')
+        >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
+        >>> dyn.vf.jacobian(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
+        >>> dyn.pl.jacobian_heatmap(adata)
     """
 
     regulators, effectors = (
         list(np.unique(regulators)) if regulators is not None else None,
         list(np.unique(effectors)) if effectors is not None else None,
     )
     import matplotlib.pyplot as plt
@@ -746,183 +683,175 @@
 
     Der, regulators, effectors = intersect_sources_targets(regulators, regulators_, effectors, effectors_, Der)
 
     adata_ = adata[cell_indx, :]
     valid_cell_idx = list(set(cell_idx).intersection(cell_indx))
     if len(valid_cell_idx) == 0:
         raise ValueError(
-            f"Jacobian matrix was not calculated for the cells you provided {cell_indx}."
+            f"Jacobian matrix was not calculated for the cells you provided {cell_idx}."
             f"Check adata.uns[{Jacobian_}].values() for available cells that have Jacobian matrix calculated."
             f"Note that limiting calculation of Jacobian matrix only for a subset of cells are required for "
             f"speeding up calculations."
         )
     else:
         cell_names = adata.obs_names[valid_cell_idx]
 
-    total_panels, ncols = len(valid_cell_idx), ncols
+    total_panels, ncols = len(valid_cell_idx) if not average else 1, ncols
     nrow, ncol = int(np.ceil(total_panels / ncols)), ncols
 
     if figsize is None:
         g = plt.figure(None, (3 * ncol, 3 * nrow))  # , dpi=160
     else:
         g = plt.figure(None, (figsize[0] * ncol, figsize[1] * nrow))  # , dpi=160
 
     gs = plt.GridSpec(nrow, ncol)
     heatmap_kwargs = dict(xticklabels=1, yticklabels=1)
     heatmap_kwargs = update_dict(heatmap_kwargs, kwargs)
-    for i, name in enumerate(cell_names):
-        ind = np.where(adata_.obs_names == name)[0]
-        J = Der[:, :, ind][:, :, 0].T  # dim 0: target; dim 1: source
+
+    if average:
+        J = np.mean(Der[:, :, valid_cell_idx], axis=2).T
         J = pd.DataFrame(J, index=regulators, columns=effectors)
-        ax = plt.subplot(gs[i])
+        ax = plt.subplot(gs[0, 0])
         sns.heatmap(
             J,
             annot=True,
             ax=ax,
             cmap=cmap,
             cbar=False,
             center=0,
             **heatmap_kwargs,
         )
-        plt.title(name)
+        ax.set_title("Average Jacobian Matrix")
+    else:
+        for i, name in enumerate(cell_names):
+            ind = np.where(adata_.obs_names == name)[0]
+            J = Der[:, :, ind][:, :, 0].T  # dim 0: target; dim 1: source
+            J = pd.DataFrame(J, index=regulators, columns=effectors)
+            ax = plt.subplot(gs[i])
+            sns.heatmap(
+                J,
+                annot=True,
+                ax=ax,
+                cmap=cmap,
+                cbar=False,
+                center=0,
+                **heatmap_kwargs,
+            )
+            ax.title(name)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": jkey + "_heatmap",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return gs
 
 
 @docstrings.with_indent(4)
 def sensitivity(
-    adata,
-    regulators=None,
-    effectors=None,
-    basis="umap",
-    skey="sensitivity",
-    s_basis="pca",
-    x=0,
-    y=1,
-    layer="M_s",
-    highlights=None,
-    cmap="bwr",
-    background=None,
-    pointsize=None,
-    figsize=(6, 4),
-    show_legend=True,
-    frontier=True,
-    sym_c=True,
-    sort="abs",
-    show_arrowed_spines=False,
-    stacked_fraction=False,
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    regulators: Optional[List[str]] = None,
+    effectors: Optional[List[str]] = None,
+    basis: str = "umap",
+    skey: str = "sensitivity",
+    s_basis: str = "pca",
+    x: int = 0,
+    y: int = 1,
+    layer: str = "M_s",
+    highlights: Optional[list] = None,
+    cmap: str = "bwr",
+    background: Optional[str] = None,
+    pointsize: Optional[float] = None,
+    figsize: Tuple[float, float] = (6, 4),
+    show_legend: bool = True,
+    frontier: bool = True,
+    sym_c: bool = True,
+    sort: Literal["abs", "neg", "raw"] = "abs",
+    show_arrowed_spines: bool = False,
+    stacked_fraction: bool = False,
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
-    """\
-    Scatter plot of Sensitivity value across cells.
+) -> Optional[GridSpec]:
+    """Scatter plot of Sensitivity value across cells.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with Jacobian matrix estimated.
-        regulators: `list` or `None` (default: `None`)
-            The list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        effectors: `List` or `None` (default: `None`)
-            The list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        basis: `str` (default: `umap`)
-            The reduced dimension basis.
-        skey: `str` (default: `sensitivity`)
-            The key to the sensitivity dictionary in .uns.
-        s_basis: `str` (default: `pca`)
-            The reduced dimension space that will be used to calculate the jacobian matrix.
-        x: `int` (default: `0`)
-            The column index of the low dimensional embedding for the x-axis.
-        y: `int` (default: `1`)
-            The column index of the low dimensional embedding for the y-axis.
-        highlights: `list` (default: None)
-            Which color group will be highlighted. if highligts is a list of lists - each list is relate to each color element.
-        cmap: string (optional, default 'Blues')
-            The name of a matplotlib colormap to use for coloring
-            or shading points. If no labels or values are passed
-            this will be used for shading points according to
-            density (largely only of relevance for very large
-            datasets). If values are passed this will be used for
-            shading according the value. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        background: string or None (optional, default 'None`)
-            The color of the background. Usually this will be either
-            'white' or 'black', but any color name will work. Ideally
-            one wants to match this appropriately to the colors being
-            used for points etc. This is one of the things that themes
-            handle for you. Note that if theme
-            is passed then this value will be overridden by the
-            corresponding option of the theme.
-        figsize: `None` or `[float, float]` (default: (6, 4))
-                The width and height of each panel in the figure.
-        show_legend: bool (optional, default True)
-            Whether to display a legend of the labels
-        frontier: `bool` (default: `False`)
-            Whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to show area
-            of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib tips & tricks
-            cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from scEU-seq paper:
-            https://science.sciencemag.org/content/367/6482/1151.
-        sym_c: `bool` (default: `True`)
-            Whether do you want to make the limits of continuous color to be symmetric, normally this should be used for
-            plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative values.
-        sort: `str` (optional, default `abs`)
-            The method to reorder data so that high values points will be on top of background points. Can be one of
-            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values.
-        show_arrowed_spines: bool (optional, default False)
-            Whether to show a pair of arrowed spines representing the basis of the scatter is currently using.
-        stacked_fraction: bool (default: False)
-            If True the jacobian will be represented as a stacked fraction in the title, otherwise a linear fraction
-            style is used.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig
-            function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent": True,
-            "close": True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly
-            modify those keys according to your needs.
-        kwargs:
-            Additional arguments passed to plt._matplotlib_points.
-
-    Returns
-    -------
-    Nothing but plots the n_source x n_targets scatter plots of low dimensional embedding of the adata object, each
-    corresponds to one element in the Jacobian matrix for all sampled cells.
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
-    >>> dyn.vf.sensitivity(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
-    >>> dyn.pl.sensitivity(adata)
+    Args:
+        adata: an Annodata object with Jacobian matrix estimated.
+        regulators: the list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        effectors: the list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        basis: the reduced dimension basis. Defaults to "umap".
+        skey: the key to the sensitivity dictionary in .uns. Defaults to "sensitivity".
+        s_basis: the reduced dimension space that will be used to calculate the jacobian matrix. Defaults to "pca".
+        x: the column index of the low dimensional embedding for the x-axis. Defaults to 0.
+        y: the column index of the low dimensional embedding for the y-axis. Defaults to 1.
+        layer: _description_. Defaults to "M_s".
+        highlights: the layer key for the data. Defaults to None.
+        cmap: the name of a matplotlib colormap to use for coloring or shading points. If no labels or values are passed
+            this will be used for shading points according to density (largely only of relevance for very large
+            datasets). If values are passed this will be used for shading according the value. Note that if theme is
+            passed then this value will be overridden by the corresponding option of the theme. Defaults to "bwr".
+        background: the color of the background. Usually this will be either 'white' or 'black', but any color name will
+            work. Ideally one wants to match this appropriately to the colors being used for points etc. This is one of
+            the things that themes handle for you. Note that if theme is passed then this value will be overridden by
+            the corresponding option of the theme. Defaults to None.
+        pointsize: the size of the plotted points. Defaults to None.
+        figsize: the size of each subplot. Defaults to (6, 4).
+        show_legend: whether to display a legend of the labels. Defaults to True.
+        frontier: whether to add the frontier. Scatter plots can be enhanced by using transparency (alpha) in order to
+            show area of high density and multiple scatter plots can be used to delineate a frontier. See matplotlib
+            tips & tricks cheatsheet (https://github.com/matplotlib/cheatsheets). Originally inspired by figures from
+            scEU-seq paper: https://science.sciencemag.org/content/367/6482/1151. Defaults to True.
+        sym_c: whether do you want to make the limits of continuous color to be symmetric, normally this should be used
+            for plotting velocity, jacobian, curl, divergence or other types of data with both positive or negative
+            values. Defaults to True.
+        sort: the method to reorder data so that high values points will be on top of background points. Can be one of
+            {'raw', 'abs', 'neg'}, i.e. sorted by raw data, sort by absolute values or sort by negative values. Defaults
+            to "abs".
+        show_arrowed_spines: whether to show a pair of arrowed spines representing the basis of the scatter is currently
+            using. Defaults to False.
+        stacked_fraction: whether to represent the jacobianas a stacked fraction in the title or a linear fraction style
+            will be used. Defaults to False.
+        save_show_or_return: whether to save, show, or return the fugure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs. Defaults to {}.
+        **kwargs: any other kwargs passed to `plt._matplotlib_points`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib `GridSpec` of
+        the figure would be returned.
+    
+    Examples:
+        >>> import dynamo as dyn
+        >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
+        >>> dyn.pp.recipe_monocle(adata)
+        >>> dyn.tl.dynamics(adata)
+        >>> dyn.tl.cell_velocities(adata, basis='pca')
+        >>> dyn.vf.VectorField(adata, basis='pca')
+        >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
+        >>> dyn.vf.sensitivity(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
+        >>> dyn.pl.sensitivity(adata)
     """
 
     regulators, effectors = (
         list(np.unique(regulators)) if regulators is not None else None,
         list(np.unique(effectors)) if effectors is not None else None,
     )
 
@@ -1075,102 +1004,95 @@
                 ax.set_title(r"$d x_{%s} / d x_{%s}$" % (target, source))
             if i + j == 0 and show_arrowed_spines:
                 arrowed_spines(ax, basis, background)
             else:
                 despline_all(ax)
                 deaxis_all(ax)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": skey,
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return gs
 
 
 def sensitivity_heatmap(
-    adata,
-    cell_idx,
-    skey="sensitivity",
-    basis="pca",
-    regulators=None,
-    effectors=None,
-    figsize=(7, 5),
-    ncols=1,
-    cmap="bwr",
-    save_show_or_return="show",
-    save_kwargs={},
+    adata: AnnData,
+    cell_idx: Union[List[int], int],
+    skey: str = "sensitivity",
+    basis: str = "pca",
+    regulators: Optional[List[str]] = None,
+    effectors: Optional[List[str]] = None,
+    figsize: Tuple[float, float] = (7, 5),
+    ncols: int = 1,
+    cmap: str = "bwr",
+    save_show_or_return: Literal["save", "show", "return"] = "show",
+    save_kwargs: Dict[str, Any] = {},
     **kwargs,
-):
-    """\
-    Plot the Jacobian matrix for each cell as a heatmap.
+) -> Optional[GridSpec]:
+    """Plot the Jacobian matrix for each cell as a heatmap.
 
     Note that Jacobian matrix can be understood as a regulatory activity matrix between genes directly computed from the
     reconstructed vector fields.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with Jacobian matrix estimated.
-        cell_idx: `int` or `list`
-            The numeric indices of the cells that you want to draw the sensitivity matrix to reveal the regulatory activity.
-        skey: `str` (default: `sensitivity`)
-            The key to the sensitivity dictionary in .uns.
-        basis: `str`
-            The reduced dimension basis.
-        regulators: `list` or `None` (default: `None`)
-            The list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        effectors: `List` or `None` (default: `None`)
-            The list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to genes
-            that have already performed Jacobian analysis.
-        figsize: `None` or `[float, float]` (default: None)
-                The width and height of each panel in the figure.
-        ncols: `int` (default: `1`)
-            The number of columns for drawing the heatmaps.
-        cmap: `str` (default: `bwr`)
-            The mapping from data values to color space. If not provided, the default will depend on whether center is set.
-        save_show_or_return: `str` {'save', 'show', 'return'} (default: `show`)
-            Whether to save, show or return the figure.
-        save_kwargs: `dict` (default: `{}`)
-            A dictionary that will passed to the save_fig function. By default it is an empty dictionary and the save_fig function
-            will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf', "transparent": True, "close":
-            True, "verbose": True} as its parameters. Otherwise you can provide a dictionary that properly modify those keys
-            according to your needs.
-        kwargs:
-            Additional arguments passed to sns.heatmap.
-
-    Returns
-    -------
-        Nothing but plots the n_cell_idx heatmaps of the corresponding Jacobian matrix for each selected cell.
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.reduceDimension(adata)
-    >>> dyn.tl.cell_velocities(adata, basis='pca')
-    >>> dyn.vf.VectorField(adata, basis='pca')
-    >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
-    >>> dyn.vf.sensitivity(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
-    >>> dyn.pl.sensitivity_heatmap(adata)
+    Args:
+        adata: an Annodata object with Jacobian matrix estimated.
+        cell_idx: the numeric indices of the cells that you want to draw the sensitivity matrix to reveal the regulatory
+            activity.
+        skey: the key to the sensitivity dictionary in .uns. Defaults to "sensitivity".
+        basis: the reduced dimension basis. Defaults to "pca".
+        regulators: the list of genes that will be used as regulators for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        effectors: the list of genes that will be used as targets for plotting the Jacobian heatmap, only limited to
+            genes that have already performed Jacobian analysis. Defaults to None.
+        figsize: the size of the subplots. Defaults to (7, 5).
+        ncols: the number of columns for drawing the heatmaps. Defaults to 1.
+        cmap: the mapping from data values to color space. If not provided, the default will depend on whether center is
+            set. Defaults to "bwr".
+        save_show_or_return: whether to save, show, or return the figure. Defaults to "show".
+        save_kwargs: a dictionary that will be passed to the save_fig function. By default, it is an empty dictionary
+            and the save_fig function will use the {"path": None, "prefix": 'scatter', "dpi": None, "ext": 'pdf',
+            "transparent": True, "close": True, "verbose": True} as its parameters. Otherwise, you can provide a
+            dictionary that properly modify those keys according to your needs.. Defaults to {}.
+        **kwargs: any other kwargs passed to `sns.heatmap`.
+
+    Raises:
+        ValueError: sensitivity data is not found in `adata`.
+
+    Returns:
+        None would be returned by default. If `save_show_or_return` is set to be 'return', the matplotlib `GridSpec` of
+        the figure would be returned.
+    
+    Examples:
+        >>> import dynamo as dyn
+        >>> adata = dyn.sample_data.hgForebrainGlutamatergic()
+        >>> dyn.pp.recipe_monocle(adata)
+        >>> dyn.tl.dynamics(adata)
+        >>> dyn.tl.reduceDimension(adata)
+        >>> dyn.tl.cell_velocities(adata, basis='pca')
+        >>> dyn.vf.VectorField(adata, basis='pca')
+        >>> valid_gene_list = adata[:, adata.var.use_for_transition].var.index[:2]
+        >>> dyn.vf.sensitivity(adata, regulators=valid_gene_list[0], effectors=valid_gene_list[1])
+        >>> dyn.pl.sensitivity_heatmap(adata)
     """
 
     regulators, effectors = (
         list(np.unique(regulators)) if regulators is not None else None,
         list(np.unique(effectors)) if effectors is not None else None,
     )
 
@@ -1225,25 +1147,28 @@
             cmap=cmap,
             cbar=False,
             center=0,
             **heatmap_kwargs,
         )
         plt.title(name)
 
-    if save_show_or_return == "save":
+    if save_show_or_return in ["save", "both", "all"]:
         s_kwargs = {
             "path": None,
             "prefix": skey + "_heatmap",
             "dpi": None,
             "ext": "pdf",
             "transparent": True,
             "close": True,
             "verbose": True,
         }
         s_kwargs = update_dict(s_kwargs, save_kwargs)
 
+        if save_show_or_return in ["both", "all"]:
+            s_kwargs["close"] = False
+
         save_fig(**s_kwargs)
-    elif save_show_or_return == "show":
+    if save_show_or_return in ["show", "both", "all"]:
         plt.tight_layout()
         plt.show()
-    elif save_show_or_return == "return":
+    if save_show_or_return in ["return", "all"]:
         return gs
```

### Comparing `dynamo-release-1.2.0/dynamo/prediction/__init__.py` & `dynamo-release-1.3.0/dynamo/prediction/__init__.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/prediction/fate.py` & `dynamo-release-1.3.0/dynamo/prediction/fate.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 import itertools
 import warnings
 from multiprocessing.dummy import Pool as ThreadPool
-from typing import Callable, Optional, Union
+from typing import Callable, List, Optional, Tuple, Union
 
 import numpy as np
 import pandas as pd
 from anndata import AnnData
 from sklearn.neighbors import NearestNeighbors
 from tqdm import tqdm
 
+from ..configuration import DKM
 from ..dynamo_logger import (
     LoggerManager,
     main_info,
     main_info_insert_adata,
     main_warning,
 )
 from ..tools.utils import fetch_states, getTseq
@@ -23,97 +24,76 @@
 
 def fate(
     adata: AnnData,
     init_cells: list,
     init_states: Optional[np.ndarray] = None,
     basis: Optional[None] = None,
     layer: str = "X",
-    dims: Union[tuple([list, None] + list(np.ScalarType))] = None,
-    genes: Union[list, None] = None,
+    dims: Optional[Union[int, List[int], Tuple[int], np.ndarray]] = None,
+    genes: Optional[List] = None,
     t_end: Optional[float] = None,
     direction: str = "both",
     interpolation_num: int = 250,
     average: bool = False,
     sampling: str = "arc_length",
     VecFld_true: Callable = None,
     inverse_transform: bool = False,
     Qkey: str = "PCs",
     scale: float = 1,
     cores: int = 1,
     **kwargs: dict,
-):
+) -> AnnData:
     """Predict the historical and future cell transcriptomic states over arbitrary time scales.
 
      This is achieved by integrating the reconstructed vector field function from one or a set of initial cell state(s).
      Note that this function is designed so that there is only one trajectory (based on averaged cell states if multiple
      initial states are provided) will be returned. `dyn.tl._fate` can be used to calculate multiple cell states.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the reconstructed vector field function in the `uns` attribute.
-        init_cells: `list`
-            Cell name or indices of the initial cell states for the historical or future cell state prediction with
+    Args:
+        adata: AnnData object that contains the reconstructed vector field function in the `uns` attribute.
+        init_cells: Cell name or indices of the initial cell states for the historical or future cell state prediction with
             numerical integration. If the names in init_cells not found in the adata.obs_name, it will be treated as
             cell indices and must be integers.
-        init_states: `numpy.ndarray` or None (default: None)
-            Initial cell states for the historical or future cell state prediction with numerical integration.
-        basis: `str` or None (default: `None`)
-            The embedding data to use for predicting cell fate. If `basis` is either `umap` or `pca`, the reconstructed
+        init_states: Initial cell states for the historical or future cell state prediction with numerical integration.
+        basis: The embedding data to use for predicting cell fate. If `basis` is either `umap` or `pca`, the reconstructed
             trajectory will be projected back to high dimensional space via the `inverse_transform` function.
-        layer: `str` or None (default: 'X')
-            Which layer of the data will be used for predicting cell fate with the reconstructed vector field function.
+        layer: Which layer of the data will be used for predicting cell fate with the reconstructed vector field function.
             The layer once provided, will override the `basis` argument and then predicting cell fate in high
             dimensional space.
-        dims: `scalar`, `list` or None (default: `None')
-            The dimensions that will be selected for fate prediction.
-        genes: `list` or None (default: None)
-            The gene names whose gene expression will be used for predicting cell fate. By default (when genes is set to
+        dims: The dimensions that will be selected for fate prediction.
+        genes: The gene names whose gene expression will be used for predicting cell fate. By default (when genes is set to
             None), the genes used for velocity embedding (var.use_for_transition) will be used for vector field
             reconstruction. Note that the genes to be used need to have velocity calculated and corresponds to those
             used in the `dyn.tl.VectorField` function.
-        t_end: `float` (default None)
-            The length of the time period from which to predict cell state forward or backward over time. This is used
+        t_end: The length of the time period from which to predict cell state forward or backward over time. This is used
             by the odeint function.
-        direction: `string` (default: both)
-            The direction to predict the cell fate. One of the `forward`, `backward` or `both` string.
-        interpolation_num: `int` (default: 100)
-            The number of uniformly interpolated time points.
-        average: `str` or `bool` (default: `False`) {'origin', 'trajectory'}
-            The method to calculate the average cell state at each time step, can be one of `origin` or `trajectory`. If
+        direction: The direction to predict the cell fate. One of the `forward`, `backward` or `both` string.
+        interpolation_num: The number of uniformly interpolated time points.
+        average: The method to calculate the average cell state at each time step, can be one of `origin` or `trajectory`. If
             `origin` used, the average expression state from the init_cells will be calculated and the fate prediction
             is based on this state. If `trajectory` used, the average expression states of all cells predicted from the
             vector field function at each time point will be used. If `average` is `False`, no averaging will be
             applied.
-        sampling: `str` (default: `arc_length`)
-            Methods to sample points along the integration path, one of `{'arc_length', 'logspace', 'uniform_indices'}`.
+        sampling: Methods to sample points along the integration path, one of `{'arc_length', 'logspace', 'uniform_indices'}`.
             If `logspace`, we will sample time points linearly on log space. If `uniform_indices`, the sorted unique set
             of all time points from all cell states' fate prediction will be used and then evenly sampled up to
             `interpolation_num` time points. If `arc_length`, we will sample the integration path with uniform arc
             length.
-        VecFld_true: `function`
-            The true ODE function, useful when the data is generated through simulation. Replace VecFld arugment when
+        VecFld_true: The true ODE function, useful when the data is generated through simulation. Replace VecFld argument when
             this has been set.
-        inverse_transform: `bool` (default: `False`)
-            Whether to inverse transform the low dimensional vector field prediction back to high dimensional space.
-        Qkey: str (default: 'PCs')
-            The key of the PCA loading matrix in `.uns`.
-        scale: `float` (default: `1`)
-            The value that will be used to scale the predicted velocity value from the reconstructed vector field
+        inverse_transform: Whether to inverse transform the low dimensional vector field prediction back to high dimensional space.
+        Qkey: The key of the PCA loading matrix in `.uns`.
+        scale: The value that will be used to scale the predicted velocity value from the reconstructed vector field
             function.
-        cores: `int` (default: 1):
-            Number of cores to calculate path integral for predicting cell fate. If cores is set to be > 1,
+        cores: Number of cores to calculate path integral for predicting cell fate. If cores is set to be > 1,
             multiprocessing will be used to parallel the fate prediction.
-        kwargs:
-            Additional parameters that will be passed into the fate function.
+        kwargs: Additional parameters that will be passed into the fate function.
 
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that is updated with the dictionary Fate (includes `t` and `prediction` keys) in uns
+    Returns:
+        adata: AnnData object that is updated with the dictionary Fate (includes `t` and `prediction` keys) in uns
             attribute.
     """
 
     if sampling in ["arc_length", "logspace", "uniform_indices"]:
         if average in ["origin", "trajectory", True]:
             main_warning(
                 f"using {sampling} to sample data points along an integral path at different integration "
@@ -190,15 +170,15 @@
         PCs = adata.uns["PCs"].T
         if PCs.shape[0] == exprs.shape[1]:
             exprs = np.expm1(exprs @ PCs + adata.uns["pca_mean"])
 
         ndim = adata.uns["umap_fit"]["fit"]._raw_data.shape[1]
 
         if "X" in adata.obsm_keys():
-            if ndim == adata.obsm["X"].shape[1]:  # lift the dimension up again
+            if ndim == adata.obsm[DKM.X_PCA].shape[1]:  # lift the dimension up again
                 exprs = adata.uns["pca_fit"].inverse_transform(prediction)
 
         if adata.var.use_for_dynamics.sum() == exprs.shape[1]:
             valid_genes = adata.var_names[adata.var.use_for_dynamics]
         elif adata.var.use_for_transition.sum() == exprs.shape[1]:
             valid_genes = adata.var_names[adata.var.use_for_transition]
         else:
@@ -220,68 +200,51 @@
     if exprs is not None:
         adata.uns[fate_key]["exprs"] = exprs
 
     return adata
 
 
 def _fate(
-    VecFld,
-    init_states,
-    t_end=None,
-    step_size=None,
-    direction="both",
-    interpolation_num=250,
-    average=True,
-    sampling="arc_length",
-    cores=1,
-):
+    VecFld: Callable,
+    init_states: np.ndarray,
+    t_end: Optional[float] = None,
+    step_size: Optional[float] = None,
+    direction: str = "both",
+    interpolation_num: int = 250,
+    average: bool = True,
+    sampling: str = "arc_length",
+    cores: int = 1,
+) -> Tuple[np.ndarray, np.ndarray]:
     """Predict the historical and future cell transcriptomic states over arbitrary time scales by integrating vector
     field functions from one or a set of initial cell state(s).
 
-    Arguments
-    ---------
-        VecFld: `function`
-            Functional form of the vector field reconstructed from sparse single cell samples. It is applicable to the
+    Args:
+        VecFld: Functional form of the vector field reconstructed from sparse single cell samples. It is applicable to the
             entire transcriptomic space.
-        init_states: `numpy.ndarray`
-            Initial cell states for the historical or future cell state prediction with numerical integration.
-        t_end: `float` (default None)
-            The length of the time period from which to predict cell state forward or backward over time. This is used
+        init_states: Initial cell states for the historical or future cell state prediction with numerical integration.
+        t_end: The length of the time period from which to predict cell state forward or backward over time. This is used
             by the odeint function.
-        step_size: `float` or None (default None)
-            Step size for integrating the future or history cell state, used by the odeint function. By default it is
+        step_size: Step size for integrating the future or history cell state, used by the odeint function. By default it is
             None, and the step_size will be automatically calculated to ensure 250 total integration time-steps will be
             used.
-        direction: `string` (default: both)
-            The direction to predict the cell fate. One of the `forward`, `backward`or `both` string.
-        interpolation_num: `int` (default: 100)
-            The number of uniformly interpolated time points.
-        average: `bool` (default: True)
-            A boolean flag to determine whether to smooth the trajectory by calculating the average cell state at each
+        direction: The direction to predict the cell fate. One of the `forward`, `backward`or `both` string.
+        interpolation_num: The number of uniformly interpolated time points.
+        average: A boolean flag to determine whether to smooth the trajectory by calculating the average cell state at each
             time step.
-        sampling: `str` (default: `logspace`)
-            Methods to sample points along the integration path, one of `{'arc_length', 'logspace', 'uniform_indices'}`.
+        sampling: Methods to sample points along the integration path, one of `{'arc_length', 'logspace', 'uniform_indices'}`.
             If `logspace`, we will sample time points linearly on log space. If `uniform_indices`, the sorted unique set
             of all time points from all cell states' fate prediction will be used and then evenly sampled up to
             `interpolation_num` time points. If `arc_length`, we will sample the integration path with uniform arc
             length.
-        cores: `int` (default: 1):
-            Number of cores to calculate path integral for predicting cell fate. If cores is set to be > 1,
+        cores: Number of cores to calculate path integral for predicting cell fate. If cores is set to be > 1,
             multiprocessing will be used to parallel the fate prediction.
 
-    Returns
-    -------
-    t: `numpy.ndarray`
-        The time at which the cell state are predicted.
-    prediction: `numpy.ndarray`
-        Predicted cells states at different time points. Row order corresponds to the element order in t. If init_states
-        corresponds to multiple cells, the expression dynamics over time for each cell is concatenated by rows. That is,
-        the final dimension of prediction is (len(t) * n_cells, n_features). n_cells: number of cells; n_features:
-        number of genes or number of low dimensional embeddings. Of note, if the average is set to be True, the average
-        cell state at each time point is calculated for all cells.
+    Returns:
+        t: The time at which the cell state are predicted.
+        prediction: Predicted cells states at different time points. Row order corresponds to the element order in t. If init_states corresponds to multiple cells, the expression dynamics over time for each cell is concatenated by rows. That is, the final dimension of prediction is (len(t) * n_cells, n_features). n_cells: number of cells; n_features: number of genes or number of low dimensional embeddings. Of note, if the average is set to be True, the average cell state at each time point is calculated for all cells.
     """
 
     t_linspace = getTseq(init_states, t_end, step_size)
 
     if cores == 1:
         t, prediction = integrate_vf_ivp(
             init_states,
@@ -327,28 +290,28 @@
             t = [np.sort(np.unique(t))]
 
     return t, prediction
 
 
 def fate_bias(
     adata: AnnData,
-    group,
+    group: str,
     basis: str = "umap",
     inds: Union[list, None] = None,
     use_sink_percentage: bool = True,
     step_used_percentage: Optional[float] = None,
     speed_percentile: float = 5,
     dist_threshold: Optional[float] = None,
     source_groups: Optional[list] = None,
     metric: str = "euclidean",
     metric_kwds: dict = None,
     cores: int = 1,
     seed: int = 19491001,
     **kwargs,
-):
+) -> pd.DataFrame:
     """Calculate the lineage (fate) bias of states whose trajectory are predicted.
 
     Fate bias is currently calculated as the percentage of points along the predicted cell fate trajectory whose
     distance to their 0-th nearest neighbors on the data are close enough (determined by median 1-st nearest neighbors
     of all observed cells and the dist_threshold) to any cell from each group specified by `group` key. The details is
     described as following:
 
@@ -366,69 +329,52 @@
     `fate_bias` calculate a confidence score for the calculated fate probability with a simple metric, defined as
         :math:`1 - (sum(distances > dist_threshold * median_dist) + walk_back_steps) / (len(indices) + walk_back_steps)`
 
     The `distance` is currently visited integration path’s data points’ 0-th nearest points to the observed cells.
     `median_dist` is median distance of their 1-st nearest cell distance of all observed cells. `walk_back_steps` is the
     steps walked backward along the integration path until all currently visited integration points's 0-th nearest
     points to the observed cells satisfy the distance threshold. `indices` are the time indices of integration points
-    that is regarded as the regions with `small velocity` (note when walking backward, those corresponding points are
+    that is regarded as the regions with `small velocity` (note when walking backward, those corresponding points do
     not necessarily have small velocity anymore).
 
-    Arguments
-    ---------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the predicted fate trajectories in the `uns` attribute.
-        group: `str`
-            The column key that corresponds to the cell type or other group information for quantifying the bias of cell
+    Args:
+        adata: AnnData object that contains the predicted fate trajectories in the `uns` attribute.
+        group: The column key that corresponds to the cell type or other group information for quantifying the bias of cell
             state.
-        basis: `str` or None (default: `None`)
-            The embedding data space where cell fates were predicted and cell fates bias will be quantified.
-        inds:
-            The indices of the time steps that will be used for calculating fate bias.
+        basis: The embedding data space where cell fates were predicted and cell fates bias will be quantified.
+        inds: The indices of the time steps that will be used for calculating fate bias.
             Otherwise inds need to be a list of integers of the time steps.
-        use_sink_percentage:
-            If inds is None and use_sink is True, sink calculation will be applied to calculate
+        use_sink_percentage: If inds is None and use_sink is True, sink calculation will be applied to calculate
             indices used for fate bias calculation
-        step_used_percentage:
-            If inds is None and step_used_percentage is not None,
+        step_used_percentage: If inds is None and step_used_percentage is not None,
             step_used_percentage will be regarded as a percentage,
             and the LAST step_used_percentage of steps will be used for fate bias calculation.
-        speed_percentile: `float` (default: `5`)
-            The percentile of speed that will be used to determine the terminal cells (or sink region on the prediction
+        speed_percentile: The percentile of speed that will be used to determine the terminal cells (or sink region on the prediction
             path where speed is smaller than this speed percentile).
-        dist_threshold: `float` or `None` (default: `None`)
-            A multiplier of the median nearest cell distance on the embedding to determine cells that are outside the
+        dist_threshold: A multiplier of the median nearest cell distance on the embedding to determine cells that are outside the
             sampled domain of cells. If the mean distance of identified "terminal cells" is above this number, we will
             look backward along the trajectory (by minimize all indices by 1) until it finds cells satisfy this
             threshold. By default it is set to be 1 to ensure only considering points that are very close to observed
             data points.
-        source_groups: `list` or `None` (default: `None`)
-            The groups that corresponds to progenitor groups. They has to have at least one intersection with the groups
+        source_groups: The groups that corresponds to progenitor groups. They need to have at least one intersection with the groups
             from the `group` column. If group is not `None`, any identified "source_groups" cells that happen to be in
             those groups will be ignored and the probability of cell fate of those cells will be reassigned to the group
             that has the highest fate probability among other non source_groups group cells.
-        metric: `str` or callable, default='euclidean'
-            The distance metric to use for the tree.  The default metric is , and with p=2 is equivalent to the standard
+        metric: The distance metric to use for the tree.  The default metric with p=2 is equivalent to the standard
             Euclidean metric. See the documentation of :class:`DistanceMetric` for a list of available metrics. If
             metric is "precomputed", X is assumed to be a distance matrix and must be square during fit. X may be a
             :term:`sparse graph`, in which case only "nonzero" elements may be considered neighbors.
-        metric_kwds : dict, default=None
-            Additional keyword arguments for the metric function.
-        cores: `int` (default: 1)
-            The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a
+        metric_kwds : Additional keyword arguments for the metric function.
+        cores: The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a
             :obj:`joblib.parallel_backend` context. ``-1`` means using all processors.
-        seed: `int` (default `19491001`)
-            Random seed to ensure the reproducibility of each run.
-        kwargs:
-            Additional arguments that will be passed to each nearest neighbor search algorithm.
-
-    Returns
-    -------
-        fate_bias: `pandas.DataFrame`
-            A DataFrame that stores the fate bias for each cell state (row) to each cell group (column).
+        seed: Random seed to ensure the reproducibility of each run.
+        kwargs: Additional arguments that will be passed to each nearest neighbor search algorithm.
+
+    Returns:
+        fate_bias: A DataFrame that stores the fate bias for each cell state (row) to each cell group (column).
     """
 
     if dist_threshold is None:
         dist_threshold = 1
 
     if group not in adata.obs.keys():
         raise ValueError(f"The group {group} you provided is not a key of .obs attribute.")
@@ -589,15 +535,15 @@
 #
 #     adata.uns['prediction'] = gene_exprs
 #     return adata
 
 
 def andecestor(
     adata: AnnData,
-    init_cells: list,
+    init_cells: List,
     init_states: Optional[np.ndarray] = None,
     cores: int = 1,
     t_end: int = 50,
     basis: str = "umap",
     n_neighbors: int = 5,
     direction: str = "forward",
     interpolation_num: int = 250,
@@ -605,51 +551,37 @@
     metric: str = "euclidean",
     metric_kwds: dict = None,
     seed: int = 19491001,
     **kwargs,
 ) -> None:
     """Predict the ancestors or descendants of a group of initial cells (states) with the given vector field function.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the reconstructed vector field function in the `uns` attribute.
-        init_cells: `list`
-            Cell name or indices of the initial cell states for the historical or future cell state prediction with
+    Args:
+        adata: AnnData object that contains the reconstructed vector field function in the `uns` attribute.
+        init_cells: Cell name or indices of the initial cell states for the historical or future cell state prediction with
             numerical integration. If the names in init_cells not found in the adata.obs_name, it will be treated as
             cell indices and must be integers.
-        init_states: `numpy.ndarray` or None (default: None)
-            Initial cell states for the historical or future cell state prediction with numerical integration.
-        basis: `str` or None (default: `None`)
-            The embedding data to use for predicting cell fate.
-        cores: `int` (default: 1):
-            Number of cores to calculate nearest neighbor graph.
-        t_end: `float` (default None)
-            The length of the time period from which to predict cell state forward or backward over time. This is used
+        init_states: Initial cell states for the historical or future cell state prediction with numerical integration.
+        basis: The key in `adata.obsm` that points to the embedding data to use for predicting cell fate.
+        cores: Number of cores to calculate nearest neighbor graph.
+        t_end: The length of the time period from which to predict cell state forward or backward over time. This is used
             by the odeint function.
-        n_neighbors:
-            Number of nearest neighbos.
-        direction: `string` (default: both)
-            The direction to predict the cell fate. One of the `forward`, `backward`or `both` string.
-        interpolation_num: `int` (default: 100)
-            The number of uniformly interpolated time points.
-        metric: `str` or callable, default='euclidean'
-            The distance metric to use for the tree.  The default metric is , and with p=2 is equivalent to the standard
-            Euclidean metric. See the documentation of :class:`DistanceMetric` for a list of available metrics. If
-            metric is "precomputed", X is assumed to be a distance matrix and must be square during fit. X may be a
-            :term:`sparse graph`, in which case only "nonzero" elements may be considered neighbors.
-        metric_kwds : dict, default=None
-            Additional keyword arguments for the metric function.
-        seed: `int` (default `19491001`)
-            Random seed to ensure the reproducibility of each run.
-        kwargs:
-            Additional arguments that will be passed to each nearest neighbor search algorithm.
+        n_neighbors: Number of nearest neighbors.
+        direction: The direction to predict the cell fate. One of the `forward`, `backward` or `both` string.
+        interpolation_num: The number of uniformly interpolated time points.
+        metric: The distance metric to use for the tree.  The default metric is 'euclidean', and with p=2 is
+            equivalent to the standard Euclidean metric. See the documentation of :class:`DistanceMetric` for
+            a list of available metrics. If metric is "precomputed", X is assumed to be a distance matrix and
+            must be square during fit. X may be a :term:`sparse graph`, in which case only "nonzero" elements
+            may be considered neighbors.
+        metric_kwds : Additional keyword arguments for the metric function.
+        seed: Random seed to ensure the reproducibility of each run.
+        kwargs: Additional arguments that will be passed to each nearest neighbor search algorithm.
 
-    Returns
-    -------
+    Returns:
         Nothing but update the adata object with a new column in `.obs` that stores predicted ancestors or descendants.
     """
 
     logger = LoggerManager.gen_logger("dynamo-andecestor")
     logger.log_time()
 
     main_info("retrieve vector field function.")
```

### Comparing `dynamo-release-1.2.0/dynamo/prediction/perturbation.py` & `dynamo-release-1.3.0/dynamo/prediction/perturbation.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-from typing import Callable, Union
+from typing import Callable, Optional, Union
 
 import anndata
 import numpy as np
+from anndata import AnnData
 from scipy.sparse import csr_matrix
 
 from ..dynamo_logger import LoggerManager
 from ..tools.cell_velocities import cell_velocities
 from ..vectorfield import SvcVectorField
-from ..vectorfield.scVectorField import vector_field_function_knockout
+from ..vectorfield.scVectorField import KOVectorField, vector_field_function_knockout
 from ..vectorfield.vector_calculus import (
     jacobian,
-    rank_cell_groups,
-    rank_cells,
-    rank_genes,
     vecfld_from_adata,
     vector_transformation,
 )
+from ..vectorfield.rank_vf import rank_cell_groups, rank_cells, rank_genes
 from .utils import expr_to_pca, pca_to_expr, z_score, z_score_inv
 
 
 def KO(
     adata: anndata.AnnData,
     KO_genes: Union[str, list],
     vecfld: Union[None, Callable] = None,
@@ -28,50 +27,36 @@
     emb_basis: str = "umap",
     velocity_ko_wt_difference: bool = False,
     add_ko_basis_key: Union[str, None] = None,
     add_embedding_key: Union[str, None] = None,
     store_vf_ko: bool = False,
     add_vf_ko_key: Union[str, None] = None,
     return_vector_field_class: bool = True,
-):
+) -> Optional[KOVectorField]:
     """In silico knockout genes (and thus the vector field function) and prediction of cell fate after knockout.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object with the vector field function for PCA learned.
-        KO_genes:
-             The gene or list of genes that will be used to perform in-silico knockout.
-        vecfld:
-            The vector field function.
-        vf_key:
-            A key to the vector field functions in adata.uns.
-        basis:
-            The basis in which the vector field function is created.
-        emb_basis:
-            The embedding basis where the perturbed (KO) vector field function will be projected to.
-        velocity_ko_wt_difference:
-            Whether to use the difference from perturbed (KO) vector field to wildtype vector field in embedding space
+    Args:
+        adata: an Anndata object with the vector field function for PCA learned.
+        KO_genes: The gene or list of genes that will be used to perform in-silico knockout.
+        vecfld: The vector field function.
+        vf_key: A key to the vector field functions in adata.uns.
+        basis: The basis in which the vector field function is created.
+        emb_basis: The embedding basis where the perturbed (KO) vector field function will be projected to.
+        velocity_ko_wt_difference: Whether to use the difference from perturbed (KO) vector field to wildtype vector field in embedding space
             instead of raw perturbation (KO) vector field. Using the difference may reveal the perturbation (KO) effects more
             clearly.
-        add_ko_basis_key:
-            The key name for the velocity corresponds to the `basis` name whose associated vector field is perturbed
+        add_ko_basis_key: The key name for the velocity corresponds to the `basis` name whose associated vector field is perturbed
             (KO).
-        add_embedding_key:
-            The key name for the velocity corresponds to the `embedding` name to which the high dimensional perturbed
+        add_embedding_key: The key name for the velocity corresponds to the `embedding` name to which the high dimensional perturbed
             (KO) vector field will be projected to.
-        store_vf_ko:
-            Whether to store the perturbed (KO) vector field function. By default it is False.
-        add_vf_ko_key:
-            The key to store the perturbed (KO) vector field function in adata.uns.
-        return_vector_field_class:
-            Whether to return the perturbed (KO) vector field class. By default it is True.
+        store_vf_ko: Whether to store the perturbed (KO) vector field function. By default it is False.
+        add_vf_ko_key: The key to store the perturbed (KO) vector field function in adata.uns.
+        return_vector_field_class: Whether to return the perturbed (KO) vector field class. By default it is True.
 
-    Returns
-    -------
+    Returns:
         If return_vector_field_class is True, return the perturbed (KO) vector field class and update objected with
         perturbed (KO) vector field in both the PCA and low dimension space. If return_vector_field_class is False,
         return nothing but updates the adata object.
     """
 
     logger = LoggerManager.gen_logger("dynamo-KO")
 
@@ -131,32 +116,32 @@
 
 
 def perturbation(
     adata: anndata.AnnData,
     genes: Union[str, list],
     expression: Union[float, list] = 10,
     perturb_mode: str = "raw",
-    cells: Union[list, np.ndarray, None] = None,
+    cells: Optional[Union[list, np.ndarray]] = None,
     zero_perturb_genes_vel: bool = False,
-    pca_key: Union[str, np.ndarray, None] = None,
-    PCs_key: Union[str, np.ndarray, None] = None,
-    pca_mean_key: Union[str, np.ndarray, None] = None,
+    pca_key: Optional[Union[str, np.ndarray]] = None,
+    PCs_key: Optional[Union[str, np.ndarray]] = None,
+    pca_mean_key: Optional[Union[str, np.ndarray]] = None,
     basis: str = "pca",
     emb_basis: str = "umap",
     jac_key: str = "jacobian_pca",
-    X_pca: Union[np.ndarray, None] = None,
-    delta_Y: Union[np.ndarray, None] = None,
+    X_pca: Optional[np.ndarray] = None,
+    delta_Y: Optional[np.ndarray] = None,
     projection_method: str = "fp",
     pertubation_method: str = "j_delta_x",
     J_jv_delta_t: float = 1,
     delta_t: float = 1,
-    add_delta_Y_key: str = None,
-    add_transition_key: str = None,
-    add_velocity_key: str = None,
-    add_embedding_key: str = None,
+    add_delta_Y_key: Optional[str] = None,
+    add_transition_key: Optional[str] = None,
+    add_velocity_key: Optional[str] = None,
+    add_embedding_key: Optional[str] = None,
 ):
     """In silico perturbation of single-cells and prediction of cell fate after perturbation.
 
     To simulate genetic perturbation and its downstream effects, we take advantage of the analytical Jacobian from our
     vector field function. In particular, we first calculate the perturbation velocity vector:
 
     .. math::
@@ -167,68 +152,44 @@
     if overexpress gene i to expression 10 but downexpress gene j to -10 but keep others not changed, we have
     delta X = [0, 0, 0, delta x_i = 10, 0, 0, .., x_j = -10, 0, 0, 0]). Because Jacobian encodes the instantaneous
     changes of velocity of any genes after increasing any other gene, J \\dot \\delta X will produce the perturbation
     effect vector after propagating the genetic perturbation (\\delta_X) through the gene regulatory network. We then
     use X_pca and \\delta_Y as a pair (just like M_s and velocity_S) to project the perturbation vector to low
     dimensional space. The \\delta_Y can be also used to identify the strongest responders of the genetic perturbation.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        genes:
-            The gene or list of genes that will be used to perform in-silico perturbation.
-        expression:
-             The numerical value or list of values that will be used to encode the genetic perturbation. High positive
+    Args:
+        adata: an Annodata object.
+        genes: The gene or list of genes that will be used to perform in-silico perturbation.
+        expression: The numerical value or list of values that will be used to encode the genetic perturbation. High positive
              values indicates up-regulation while low negative value repression.
-        perturb_mode:
-            The mode for perturbing the gene expression vector, either `raw` or `z_score`.
-        cells:
-            The list of the cell indices that we will perform the perturbation.
-        zero_perturb_genes_vel:
-            Whether to set the peturbed genes' perturbation velocity vector values to be zero.
-        pca_key:
-            The key that corresponds to pca embedding. Can also be the actual embedding matrix.
-        PCs_key:
-            The key that corresponds to PC loading embedding. Can also be the actual loading matrix.
-        pca_mean_key:
-            The key that corresponds to means values that used for pca projection. Can also be the actual means matrix.
-        basis:
-            The key that corresponds to the basis from which the vector field is reconstructed.
-        jac_key:
-            The key to the jacobian matrix.
-        X_pca:
-            The pca embedding matrix.
-        delta_Y:
-            The actual perturbation matrix. This argument enables more customized perturbation schemes.
-        projection_method:
-            The approach that will be used to project the high dimensional perturbation effect vector to low dimensional
+        perturb_mode: The mode for perturbing the gene expression vector, either `raw` or `z_score`.
+        cells: The list of the cell indices that we will perform the perturbation.
+        zero_perturb_genes_vel: Whether to set the peturbed genes' perturbation velocity vector values to be zero.
+        pca_key: The key that corresponds to pca embedding. Can also be the actual embedding matrix.
+        PCs_key: The key that corresponds to PC loading embedding. Can also be the actual loading matrix.
+        pca_mean_key: The key that corresponds to means values that used for pca projection. Can also be the actual means matrix.
+        basis: The key that corresponds to the basis from which the vector field is reconstructed.
+        jac_key: The key to the jacobian matrix.
+        X_pca: The pca embedding matrix.
+        delta_Y: The actual perturbation matrix. This argument enables more customized perturbation schemes.
+        projection_method: The approach that will be used to project the high dimensional perturbation effect vector to low dimensional
             space.
-        pertubation_method:
-            The approach that will be used to calculate the perturbation effect vector after in-silico genetic
+        pertubation_method: The approach that will be used to calculate the perturbation effect vector after in-silico genetic
             perturbation. Can only be one of `"j_delta_x", "j_x_prime", "j_jv", "f_x_prime", "f_x_prime_minus_f_x_0"`
-        J_jv_delta_t:
-            If pertubation_method is `j_jv`, this will be used to determine the $\\delta x = jv \\delta t_{jv}$
-        delta_t:
-            This will be used to determine the $\\delta Y = jv \\delta t$
-        add_delta_Y_key:
-            The key that will be used to store the perturbation effect matrix. Both the pca dimension matrix (stored in
+        J_jv_delta_t: If pertubation_method is `j_jv`, this will be used to determine the $\\delta x = jv \\delta t_{jv}$
+        delta_t: This will be used to determine the $\\delta Y = jv \\delta t$
+        add_delta_Y_key: The key that will be used to store the perturbation effect matrix. Both the pca dimension matrix (stored in
             obsm) or the matrix of the original gene expression space (stored in .layers) will use this key. By default
             it is None and is set to be `method + '_perturbation'`.
-        add_transition_key: str or None (default: None)
-            The dictionary key that will be used for storing the transition matrix in .obsp.
-        add_velocity_key: str or None (default: None)
-            The dictionary key that will be used for storing the low dimensional velocity projection matrix in .obsm.
-        add_embedding_key: str or None (default: None)
-            The dictionary key that will be used for storing the low dimensional velocity projection matrix in .obsm.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            Returns an updated :class:`~anndata.AnnData` with perturbation effect matrix, projected perturbation vectors
+        add_transition_key: The dictionary key that will be used for storing the transition matrix in .obsp.
+        add_velocity_key: The dictionary key that will be used for storing the low dimensional velocity projection matrix in .obsm.
+        add_embedding_key: The dictionary key that will be used for storing the low dimensional velocity projection matrix in .obsm.
+
+    Returns:
+        adata: Returns an updated :class:`~anndata.AnnData` with perturbation effect matrix, projected perturbation vectors
             , and a cell transition matrix based on the perturbation vectors.
 
     """
 
     if pertubation_method.lower() not in ["j_delta_x", "j_x_prime", "j_jv", "f_x_prime", "f_x_prime_minus_f_x_0"]:
         raise ValueError(
             f"your method is set to be {pertubation_method.lower()} but must be one of `j_delta_x`, `j_x_prime`, "
@@ -380,80 +341,68 @@
     logger.info(
         f"you can use dyn.pl.streamline_plot(adata, basis='{emb_basis}_perturbation') to visualize the "
         f"perturbation vector"
     )
     adata.obsm[embedding_key] = adata.obsm["X_" + emb_basis].copy()
 
 
-def rank_perturbation_genes(adata, pkey="j_delta_x_perturbation", prefix_store="rank", **kwargs):
+def rank_perturbation_genes(
+    adata: AnnData, pkey: str = "j_delta_x_perturbation", prefix_store: str = "rank", **kwargs
+) -> AnnData:
     """Rank genes based on their raw and absolute perturbation effects for each cell group.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the gene-wise perturbation effect vectors.
-        pkey: str (default: 'perturbation_vector')
-            The perturbation key.
-        prefix_store: str (default: 'rank')
-            The prefix added to the key for storing the returned ranking information in adata.
-        kwargs:
-            Keyword arguments passed to `vf.rank_genes`.
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            AnnData object which has the rank dictionary for perturbation effects in `.uns`.
+    Args:
+        adata: AnnData object that contains the gene-wise perturbation effect vectors.
+        pkey: The perturbation key.
+        prefix_store: The prefix added to the key for storing the returned ranking information in adata.
+        kwargs: Keyword arguments passed to `vf.rank_genes`.
+
+    Returns:
+        adata: AnnData object which has the rank dictionary for perturbation effects in `.uns`.
     """
     rdict = rank_genes(adata, pkey, **kwargs)
     rdict_abs = rank_genes(adata, pkey, abs=True, **kwargs)
     adata.uns[prefix_store + "_" + pkey] = rdict
     adata.uns[prefix_store + "_abs_" + pkey] = rdict_abs
     return adata
 
 
-def rank_perturbation_cells(adata, pkey="j_delta_x_perturbation", prefix_store="rank", **kwargs):
+def rank_perturbation_cells(
+    adata: AnnData, pkey: str = "j_delta_x_perturbation", prefix_store: str = "rank", **kwargs
+) -> AnnData:
     """Rank cells based on their raw and absolute perturbation for each cell group.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the gene-wise velocities.
-        pkey: str (default: 'perturbation_vector')
-            The perturbation key.
-        prefix_store: str (default: 'rank')
-            The prefix added to the key for storing the returned in adata.
-        kwargs:
-            Keyword arguments passed to `vf.rank_cells`.
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            AnnData object which has the rank dictionary for perturbation effects in `.uns`.
+    Args:
+        adata: AnnData object that contains the gene-wise velocities.
+        pkey: The perturbation key.
+        prefix_store: The prefix added to the key for storing the returned in adata.
+        kwargs: Keyword arguments passed to `vf.rank_cells`.
+
+    Returns:
+        adata: AnnData object which has the rank dictionary for perturbation effects in `.uns`.
     """
     rdict = rank_cells(adata, pkey, **kwargs)
     rdict_abs = rank_cells(adata, pkey, abs=True, **kwargs)
     adata.uns[prefix_store + "_" + pkey + "_cells"] = rdict
     adata.uns[prefix_store + "_abs_" + pkey + "_cells"] = rdict_abs
     return adata
 
 
-def rank_perturbation_cell_clusters(adata, pkey="j_delta_x_perturbation", prefix_store="rank", **kwargs):
+def rank_perturbation_cell_clusters(
+    adata: AnnData, pkey: str = "j_delta_x_perturbation", prefix_store: str = "rank", **kwargs
+) -> AnnData:
     """Rank cells based on their raw and absolute perturbation for each cell group.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the gene-wise velocities.
-        pkey: str (default: 'perturbation_vector')
-            The perturbation key.
-        prefix_store: str (default: 'rank')
-            The prefix added to the key for storing the returned in adata.
-        kwargs:
-            Keyword arguments passed to `vf.rank_cells`.
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            AnnData object which has the rank dictionary for perturbation effects in `.uns`.
+    Args:
+        adata: AnnData object that contains the gene-wise velocities.
+        pkey: The perturbation key.
+        prefix_store: The prefix added to the key for storing the returned in adata.
+        kwargs: Keyword arguments passed to `vf.rank_cells`.
+
+    Returns:
+        adata: AnnData object which has the rank dictionary for perturbation effects in `.uns`.
     """
     rdict = rank_cell_groups(adata, pkey, **kwargs)
     rdict_abs = rank_cell_groups(adata, pkey, abs=True, **kwargs)
     adata.uns[prefix_store + "_" + pkey + "_cell_groups"] = rdict
     adata.uns[prefix_store + "_abs_" + pkey + "_cells_groups"] = rdict_abs
     return adata
```

### Comparing `dynamo-release-1.2.0/dynamo/prediction/state_graph.py` & `dynamo-release-1.3.0/dynamo/prediction/state_graph.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,14 @@
-from typing import List, Union
+from typing import List, Optional, Union
 
 import anndata
 import numpy as np
 import pandas as pd
 import scipy.sparse as sp
+from anndata import AnnData
 from scipy.sparse import csr_matrix
 from scipy.sparse.csgraph import shortest_path
 from scipy.spatial import cKDTree
 
 from ..dynamo_logger import LoggerManager, main_info, main_warning
 from ..prediction.fate import _fate
 from ..tools.clustering import neighbors
@@ -38,47 +39,38 @@
     adata: anndata.AnnData,
     group: str,
     basis: str = "umap",
     n_neighbors: int = 30,
     neighbor_key: Union[str, None] = None,
     graph_mat: np.ndarray = None,
     state_graph_method: str = "vf",
-):
+) -> np.ndarray:
     """This function prune a cell group transiton graph based on cell similarity graph (kNN graph).
 
     The pruning algorithm is as following: assuming the vf based cell-type transition graph is `m` (cell type x cell
     type matrix); the `M` matrix as the cell to cell-type assignment matrix (row is the cell and column the cell type;
     if i-th cell is j-th cell type, the `M_{ij}` is 1). the knn graph between cells based on the umap embedding (or
     others) is `n` (number of cells x number of cells matrix). We compute `t(M) n M` to get a cell-type by cell type
     connectivity graph M' (basically this propagates the cell type to cell matrix to the cell-cell knn graph and then
     lump the transition down to cell-type). Lastly, `g * M'`  will give pruned graph, where `g` is the vector field
     based cell-type transition graph. As you can see the resultant graph considers both vector field based connection
     and the similarity relationship of cells in expression space.
 
-    Parameters
-    ----------
-    adata:
-        AnnData object.
-    group:
-        Cell graph that will be used to build transition graph and lineage tree.
-    basis:
-         The basis that will be used to build the k-nearest neighbor graph when neighbor_key is not set.
-    n_neighbors:
-        The number of neighbors that will be used to build the k-nn graph, passed to `dyn.tl.neighbors` function. Not
-        used when neighbor_key provided.
-    neighbor_key:
-         The nearest neighbor graph key in `adata.obsp`. This nearest neighbor graph will be used to build a
-         gene-expression space based cell-type level connectivity graph.
-    state_graph_method:
-         Method that will be used to build the initial state graph.
-
-    Returns
-    -------
-    M:
-        The pruned cell state transition graph.
+    Args:
+        adata: AnnData object.
+        group: Cell graph that will be used to build transition graph and lineage tree.
+        basis: The basis that will be used to build the k-nearest neighbor graph when neighbor_key is not set.
+        n_neighbors: The number of neighbors that will be used to build the k-nn graph, passed to `dyn.tl.neighbors` function. Not
+            used when neighbor_key provided.
+        neighbor_key: The nearest neighbor graph key in `adata.obsp`. This nearest neighbor graph will be used to build a
+            gene-expression space based cell-type level connectivity graph.
+        state_graph_method: Method that will be used to build the initial state graph.
+
+    Returns:
+        M: The pruned cell state transition graph.
     """
 
     logger = LoggerManager.gen_logger("dynamo-prune_transition")
     logger.log_time()
     from patsy import dmatrix
 
     if group not in adata.obs.columns:
@@ -122,63 +114,51 @@
 
     logger.finish_progress(progress_name="prune_transition")
 
     return M
 
 
 def state_graph(
-    adata,
-    group,
-    method="vf",
-    transition_mat_key="pearson_transition_matrix",
-    approx=False,
-    eignum=5,
-    basis="umap",
-    layer=None,
-    arc_sample=False,
-    sample_num=100,
-    prune_graph=False,
+    adata: AnnData,
+    group: str,
+    method: str = "vf",
+    transition_mat_key: str = "pearson_transition_matrix",
+    approx: bool = False,
+    eignum: int = 5,
+    basis: Optional[str] = "umap",
+    layer: Optional[str] = None,
+    arc_sample: bool = False,
+    sample_num: int = 100,
+    prune_graph: bool = False,
     **kwargs,
-):
+) -> AnnData:
     """Estimate the transition probability between cell types using method of vector field integrations or Markov chain
     lumping.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that will be used to calculate a cell type (group) transition graph.
-        group: `str`
-            The attribute to group cells (column names in the adata.obs).
-        method: `str` (default: 'vf')
-            The method that will be used to construct lumped cell state graph. Must be one of {`vf` or `markov`}
-        transition_mat_key: `str` (default: 'pearson_transition_matrix')
-            The key that corresponds to the transition graph used in the KernelMarkovChain class for lumping.
-        approx: `bool` (default: False)
-            Whether to use streamplot to get the integration lines from each cell.
-        eignum: `int` (default: 5)
-            The number of eigen-vectors when performing the eigen-decomposition to obtain the stationary
+    Args:
+        adata: AnnData object that will be used to calculate a cell type (group) transition graph.
+        group: The attribute to group cells (column names in the adata.obs).
+        method: The method that will be used to construct lumped cell state graph. Must be one of {`vf` or `markov`}
+        transition_mat_key: The key that corresponds to the transition graph used in the KernelMarkovChain class for lumping.
+        approx: Whether to use streamplot to get the integration lines from each cell.
+        eignum: The number of eigen-vectors when performing the eigen-decomposition to obtain the stationary
             distribution. 5 should be sufficient as the stationary distribution will be the first eigenvector. This also
             accelerates the calculation.
-        basis: `str` or None (default: `umap`)
-            The embedding data to use for predicting cell fate. If `basis` is either `umap` or `pca`, the reconstructed
+        basis: The embedding data to use for predicting cell fate. If `basis` is either `umap` or `pca`, the reconstructed
             trajectory will be projected back to high dimensional space via the `inverse_transform` function.
-        layer: `str` or None (default: `None`)
-            Which layer of the data will be used for predicting cell fate with the reconstructed vector field function.
+        layer: Which layer of the data will be used for predicting cell fate with the reconstructed vector field function.
             The layer once provided, will override the `basis` argument and then predicting cell fate in high
             dimensional space.
-        sample_num: `int` (default: 100)
-            The number of cells to sample in each group that will be used for calculating the transitoin graph between
+        arc_sample: Whether to uniformly sample data points on the arc curve that are generated from vector field cell fate trajectory predictions.
+        sample_num: The number of cells to sample in each group that will be used for calculating the transitoin graph between
             cell groups. This is required for facilitating the calculation.
-        prune_graph: `bool` (default: `False`)
-            Whether to prune the transition graph based on cell similarities in `basis` bases.
-        kwargs:
-            Additional parameters that will be passed to `prune_transition` function.
+        prune_graph: Whether to prune the transition graph based on cell similarities in `basis` bases.
+        kwargs: Additional parameters that will be passed to `prune_transition` function.
 
-    Returns
-    -------
+    Returns:
         An updated adata object that is added with the `group + '_graph'` key, including the transition graph
         and the average transition time.
     """
     logger = LoggerManager.get_main_logger()
     timer_logger = LoggerManager.get_temp_timer_logger()
     timer_logger.log_time()
 
@@ -339,81 +319,67 @@
         )
     adata.uns[group + "_graph"] = {"group_graph": grp_graph, "group_avg_time": grp_avg_time, "group_names": uniq_grp}
     timer_logger.finish_progress(progress_name="State graph estimation")
     return adata
 
 
 def tree_model(
-    adata: anndata.AnnData,
+    adata: AnnData,
     group: str,
     progenitor: str,
     terminators: List[str],
     basis: str = "umap",
     n_neighbors: int = 30,
-    neighbor_key: Union[str, None] = None,
-    graph_mat: np.ndarray = None,
+    neighbor_key: Optional[str] = None,
+    graph_mat: Optional[np.ndarray] = None,
     state_graph_method: str = "vf",
     prune_graph: bool = True,
     row_norm: bool = True,
 ) -> pd.DataFrame:
     """This function learns a tree model of cell states (types).
 
     It is based on the shortest path from the source to target cells of the pruned vector field based cell-type
     transition graph. The pruning was done by restricting cell state transition that are only between cell states that
     are nearby in gene expression space (often low gene expression space).
 
-    Parameters
-    ----------
-    adata:
-        AnnData object.
-    group:
-        Cell graph that will be used to build transition graph and lineage tree.
-    progenitor:
-        The source cell type name of the lineage tree.
-    terminators:
-         The terminal cell type names of the lineage tree.
-    basis:
-         The basis that will be used to build the k-nearest neighbor graph when neighbor_key is not set.
-    n_neighbors:
-        The number of neighbors that will be used to build the k-nn graph, passed to `dyn.tl.neighbors` function. Not
-        used when neighbor_key provided.
-    neighbor_key:
-         The nearest neighbor graph key in `adata.obsp`. This nearest neighbor graph will be used to build a
-         gene-expression space based cell-type level connectivity graph.
-    state_graph_method:
-         Method that will be used to build the initial state graph.
-    prune_graph: `bool` (default: `True`)
-        Whether to prune the transition graph based on cell similarities in `basis` bases first before learning tree
-        model.
-    row_norm: `bool` (default: `True`)
-        Whether to normalize each row so that each row sum up to be 1. Note that row, columns in transition matrix
-        correspond to source and targets in dynamo by default.
-
-    Returns
-    -------
-    res:
-        The final tree model of cell groups. See following example on how to visualize the tree via dynamo.
-
-    Examples
-    --------
-    >>> import dynamo as dyn
-    >>> adata = dyn.sample_data.pancreatic_endocrinogenesis()
-    >>> dyn.pp.recipe_monocle(adata)
-    >>> dyn.tl.dynamics(adata)
-    >>> dyn.tl.cell_velocities(adata)
-    >>> dyn.vf.VectorField(adata, basis='umap', pot_curl_div=False)
-    >>> dyn.pd.state_graph(adata, group='clusters', basis='umap')
-    >>> res = dyn.pd.tree_model(adata, group='clusters', basis='umap')
-    >>> # in the following we first copy the state_graph result to a new key and then replace the `group_graph` key of
-    >>> # the state_graph result and visualize tree model via dynamo.
-    >>> adata.obs['clusters2'] = adata.obs['clusters'].copy()
-    >>> adata.uns['clusters2_graph'] = adata.uns['clusters_graph'].copy()
-    >>> adata.uns['clusters2_graph']['group_graph'] = res
-    >>> dyn.pl.state_graph(adata, group='clusters2', keep_only_one_direction=False, transition_threshold=None,
-    >>> color='clusters2', basis='umap', show_legend='on data')
+    Args:
+        adata: AnnData object.
+        group: Cell graph that will be used to build transition graph and lineage tree.
+        progenitor: The source cell type name of the lineage tree.
+        terminators: The terminal cell type names of the lineage tree.
+        basis: The basis that will be used to build the k-nearest neighbor graph when neighbor_key is not set.
+        n_neighbors: The number of neighbors that will be used to build the k-nn graph, passed to `dyn.tl.neighbors` function. Not
+            used when neighbor_key provided.
+        neighbor_key: The nearest neighbor graph key in `adata.obsp`. This nearest neighbor graph will be used to build a
+            gene-expression space based cell-type level connectivity graph.
+        state_graph_method: Method that will be used to build the initial state graph.
+        prune_graph: Whether to prune the transition graph based on cell similarities in `basis` bases first before learning tree
+            model.
+        row_norm: Whether to normalize each row so that each row sum up to be 1. Note that row, columns in transition matrix
+            correspond to source and targets in dynamo by default.
+
+    Returns:
+        res: The final tree model of cell groups. See following example on how to visualize the tree via dynamo.
+
+    Examples:
+        >>> import dynamo as dyn
+        >>> adata = dyn.sample_data.pancreatic_endocrinogenesis()
+        >>> dyn.pp.recipe_monocle(adata)
+        >>> dyn.tl.dynamics(adata)
+        >>> dyn.tl.cell_velocities(adata)
+        >>> dyn.vf.VectorField(adata, basis='umap', pot_curl_div=False)
+        >>> dyn.pd.state_graph(adata, group='clusters', basis='umap')
+        >>> res = dyn.pd.tree_model(adata, group='clusters', basis='umap')
+        >>> # in the following we first copy the state_graph result to a new key and then replace the `group_graph` key of
+        >>> # the state_graph result and visualize tree model via dynamo.
+        >>> adata.obs['clusters2'] = adata.obs['clusters'].copy()
+        >>> adata.uns['clusters2_graph'] = adata.uns['clusters_graph'].copy()
+        >>> adata.uns['clusters2_graph']['group_graph'] = res
+        >>> dyn.pl.state_graph(adata, group='clusters2', keep_only_one_direction=False, transition_threshold=None,
+        >>> color='clusters2', basis='umap', show_legend='on data')
     """
 
     logger = LoggerManager.gen_logger("dynamo-tree_model")
     logger.log_time()
 
     data = adata.obs
     groups = data[group]
```

### Comparing `dynamo-release-1.2.0/dynamo/prediction/tscRNA_seq.py` & `dynamo-release-1.3.0/dynamo/prediction/tscRNA_seq.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,51 +1,41 @@
-from typing import Optional, Union
+from typing import List, Optional, Union
 
 import anndata
 from scipy.sparse import csr_matrix
 
 from ..dynamo_logger import LoggerManager, main_exception, main_warning
 from ..utils import copy_adata
 from .utils import init_r0_pulse
 
 
 def get_pulse_r0(
     adata: anndata.AnnData,
-    genes: Union[list, str] = "use_for_dynamics",
+    genes: Union[List, str] = "use_for_dynamics",
     tkey: str = "X_total",
     nkey: str = "X_new",
     gamma_k_key: str = "gamma_k",
     add_init_r0_key: str = "init_r0_pulse",
     copy: bool = False,
 ) -> Union[anndata.AnnData, None]:
 
     """Get the total RNA at the initial time point for a kinetic experiment with the formula:
            :math:`r_0 = \frac{(r - l)}{(1 - k)}`, where :math: `k = 1 - e^{- \gamma t}
 
-    Parameters
-    ----------
-        adata:
-            an Annodata object
-        genes: `list`
-            A list of gene names that are going to be visualized.
-        tkey:
-            the key for normalized total layer in adata.layers.
-        nkey:
-            the key for normalized new layer in adata.layers.
-        gamma_k_key:
-            the key for the parameter k for each gene in adata.var.
-        add_init_r0_key:
-            the key that will be used to store the intial total RNA estimated, in adata.layers.
-        copy:
-            Whether copy the adata object.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An new or updated anndata object, based on copy parameter, that are updated with Size_Factor, normalized
+    Args:
+        adata: an Annodata object
+        genes: A list of gene names that are going to be visualized.
+        tkey: the key for normalized total layer in adata.layers.
+        nkey: the key for normalized new layer in adata.layers.
+        gamma_k_key: the key for the parameter k for each gene in adata.var.
+        add_init_r0_key: the key that will be used to store the initial total RNA estimated, in adata.layers.
+        copy: Whether copy the adata object.
+
+    Returns:
+        adata: An new or updated anndata object, based on copy parameter, that are updated with Size_Factor, normalized
             expression values, X and reduced dimensions, etc.
     """
 
     logger = LoggerManager.gen_logger("dynamo-estimate-initial-total-RNA")
     logger.log_time()
 
     adata = copy_adata(adata) if copy else adata
```

### Comparing `dynamo-release-1.2.0/dynamo/prediction/utils.py` & `dynamo-release-1.3.0/dynamo/prediction/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Callable
+from typing import Callable, Union
 
 # from anndata._core.views import ArrayView
 import numpy as np
 from scipy import interpolate
 from scipy.integrate import solve_ivp
 from tqdm import tqdm
 
@@ -13,64 +13,58 @@
 # import scipy.sparse as sp
 from ..vectorfield.topography import dup_osc_idx_iter
 
 # ---------------------------------------------------------------------------------------------------
 # initial state related
 
 
-def init_r0_pulse(r, l, k):
+def init_r0_pulse(
+    r: Union[float, np.ndarray], l: Union[float, np.ndarray], k: Union[float, np.ndarray]
+) -> Union[float, np.ndarray]:
     """calculate initial total RNA via ODE formula of RNA kinetics for one-shot/kinetics experiment
 
-    Parameters
-    ----------
-        r:
-            total RNA at current time point.
-        l:
-            labeled RNA at current time point.
-        k:
-            $k = 1 - e^{-\gamma t}$
-
-    Returns
-    -------
-        r0:
-            The intial total RNA at the beginning of the one-shot or kinetics experiment.
+    Args:
+        r: total RNA at current time point.
+        l: labeled RNA at current time point.
+        k: $k = 1 - e^{-\gamma t}$
+
+    Returns:
+        r0: The intial total RNA at the beginning of the one-shot or kinetics experiment.
     """
     r0 = (r - l) / (1 - k)
 
     return r0
 
 
-def init_l0_chase(l, gamma, t):
-    """calculate initial total RNA via ODE formula of RNA kinetics for degradation experiment
+def init_l0_chase(
+    l: Union[float, np.ndarray], gamma: Union[float, np.ndarray], t: Union[float, np.ndarray]
+) -> Union[float, np.ndarray]:
+    """calculate initial labeled RNA (l0) via ODE formula of RNA kinetics for degradation experiment
 
     Note that this approach only estimate the initial labeled RNA based on first-order decay model. To get the intial r0
     we can also assume cells with extreme total RNA as steady state cells and use that to estimate transcription rate.
 
-    Parameters
-    ----------
-        l:
-            labeled RNA(s)
-        gamma:
-            degradation rate(s)
-        t:
-            labeling time(s)
-
-    Returns
-    -------
-        l0:
-            The initial labeled RNA at the beginning of a degradation experiment.
+    Args:
+        l: labeled RNA(s)
+        gamma: degradation rate(s)
+        t:labeling time(s)
+
+    Returns:
+        l0: The initial labeled RNA at the beginning of a degradation experiment.
     """
     l0 = l / np.exp(-gamma * t)
 
     return l0
 
 
 # ---------------------------------------------------------------------------------------------------
 # integration related
 
+# TODO – a majority of the code below can be rewritten with the Trajectory class
+
 
 def integrate_vf_ivp(
     init_states,
     t,
     integration_direction,
     f: Callable,
     args=None,
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/CnmfPreprocessor.py` & `dynamo-release-1.3.0/dynamo/preprocessing/CnmfPreprocessor.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/Preprocessor.py` & `dynamo-release-1.3.0/dynamo/preprocessing/Preprocessor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,154 +1,165 @@
-from typing import Callable, List, Optional
+from typing import Any, Callable, Dict, List, Optional
 
 try:
     from typing import Literal
 except ImportError:
     from typing_extensions import Literal
 
 import numpy as np
-import pandas as pd
 from anndata import AnnData
 
 from ..configuration import DKM
 from ..dynamo_logger import (
     LoggerManager,
+    main_debug,
     main_info,
     main_info_insert_adata,
     main_warning,
 )
-from ..external import (
+from ..tools.connectivity import neighbors as default_neighbors
+from ..tools.utils import update_dict
+from .cell_cycle import cell_cycle_scores
+from .external import (
     normalize_layers_pearson_residuals,
     sctransform,
     select_genes_by_pearson_residuals,
 )
-from ..tools.connectivity import neighbors as default_neighbors
-from .preprocess import normalize_cell_expr_by_size_factors_legacy, pca_monocle
-from .preprocessor_utils import _infer_labeling_experiment_type
-from .preprocessor_utils import (
-    filter_cells_by_outliers as monocle_filter_cells_by_outliers,
-)
-from .preprocessor_utils import (
-    filter_genes_by_outliers as monocle_filter_genes_by_outliers,
-)
-from .preprocessor_utils import (
-    is_log1p_transformed_adata,
-    log1p_adata,
-    normalize_cell_expr_by_size_factors,
-    select_genes_by_dispersion_general,
-)
+from .gene_selection import select_genes_by_seurat_recipe, select_genes_monocle
+from .normalization import calc_sz_factor, normalize
+from .pca import pca
+from .QC import basic_stats
+from .QC import filter_cells_by_outliers as monocle_filter_cells_by_outliers
+from .QC import filter_genes_by_outliers as monocle_filter_genes_by_outliers
+from .QC import regress_out_parallel
+from .transform import Freeman_Tukey, log, log1p, log2
 from .utils import (
+    _infer_labeling_experiment_type,
+    calc_new_to_total_ratio,
     collapse_species_adata,
     convert2symbol,
     convert_layers2csr,
     detect_experiment_datatype,
     unique_var_obs_adata,
 )
 
 
 class Preprocessor:
     def __init__(
         self,
-        collapse_speicies_adata_function: Callable = collapse_species_adata,
+        collapse_species_adata_function: Callable = collapse_species_adata,
         convert_gene_name_function: Callable = convert2symbol,
         filter_cells_by_outliers_function: Callable = monocle_filter_cells_by_outliers,
-        filter_cells_by_outliers_kwargs: dict = {},
+        filter_cells_by_outliers_kwargs: Dict[str, Any] = {},
         filter_genes_by_outliers_function: Callable = monocle_filter_genes_by_outliers,
-        filter_genes_by_outliers_kwargs: dict = {},
-        normalize_by_cells_function: Callable = normalize_cell_expr_by_size_factors,
-        normalize_by_cells_function_kwargs: dict = {},
-        select_genes_function: Callable = select_genes_by_dispersion_general,
-        select_genes_kwargs: dict = {},
+        filter_genes_by_outliers_kwargs: Dict[str, Any] = {},
+        normalize_by_cells_function: Callable = normalize,
+        normalize_by_cells_function_kwargs: Dict[str, Any] = {},
+        size_factor_function: Callable = calc_sz_factor,
+        size_factor_kwargs: Dict[str, Any] = {},
+        select_genes_function: Callable = select_genes_monocle,
+        select_genes_kwargs: Dict[str, Any] = {},
         normalize_selected_genes_function: Callable = None,
-        normalize_selected_genes_kwargs: dict = {},
-        use_log1p: bool = True,
-        log1p_kwargs: dict = {},
-        pca_function: bool = pca_monocle,
-        pca_kwargs: dict = {},
+        normalize_selected_genes_kwargs: Dict[str, Any] = {},
+        norm_method: Callable = log1p,
+        norm_method_kwargs: Dict[str, Any] = {},
+        pca_function: Callable = pca,
+        pca_kwargs: Dict[str, Any] = {},
         gene_append_list: List[str] = [],
         gene_exclude_list: List[str] = [],
         force_gene_list: Optional[List[str]] = None,
-        sctransform_kwargs={},
+        sctransform_kwargs: Dict[str, Any] = {},
+        regress_out_kwargs: Dict[List[str], Any] = {},
+        cell_cycle_score_enable: bool = False,
+        cell_cycle_score_kwargs: Dict[str, Any] = {},
     ) -> None:
         """Preprocessor constructor.
 
         The default preprocess functions are those of monocle recipe by default.
-        You can pass your own Callable objects (functions) to this constructor directly, which wil be used in the
-        preprocess steps later. These functions parameters are saved into Preprocessor instances. You can set these
-        attributes directly to your own implementation.
+        You can pass your own Callable objects (functions) to this constructor directly, which wil be used in
+        the preprocess steps later. These functions parameters are saved into Preprocessor instances.
+        You can set these attributes directly to your own implementation.
 
         Args:
-            collapse_speicies_adata_function: function for collapsing the species data. Defaults to
+            collapse_species_adata_function: function for collapsing the species data. Defaults to
                 collapse_species_adata.
             convert_gene_name_function: transform gene names, by default convert2symbol, which transforms unofficial
                 gene names to official gene names. Defaults to convert2symbol.
             filter_cells_by_outliers_function: filter cells by thresholds. Defaults to monocle_filter_cells_by_outliers.
             filter_cells_by_outliers_kwargs: arguments that will be passed to filter_cells_by_outliers. Defaults to {}.
             filter_genes_by_outliers_function: filter genes by thresholds. Defaults to monocle_filter_genes_by_outliers.
             filter_genes_by_outliers_kwargs: arguments that will be passed to filter_genes_by_outliers. Defaults to {}.
             normalize_by_cells_function: function for performing cell-wise normalization. Defaults to
                 normalize_cell_expr_by_size_factors.
             normalize_by_cells_function_kwargs: arguments that will be passed to normalize_by_cells_function. Defaults
                 to {}.
-            select_genes_function: function for selecting gene features. Defaults to select_genes_by_dispersion_general.
+            select_genes_function: function for selecting gene features. Defaults to select_genes_monocle.
             select_genes_kwargs: arguments that will be passed to select_genes. Defaults to {}.
             normalize_selected_genes_function: function for normalize selected genes. Defaults to None.
             normalize_selected_genes_kwargs: arguments that will be passed to normalize_selected_genes. Defaults to {}.
-            use_log1p: whether to use log1p to normalize layers in adata. Defaults to True.
-            log1p_kwargs: arguments passed to use_log1p. Defaults to {}.
-            pca_function: function to perform pca. Defaults to pca_monocle.
+            norm_method: whether to use a method to normalize layers in adata. Defaults to True.
+            norm_method_kwargs: arguments passed to norm_method. Defaults to {}.
+            pca_function: function to perform pca. Defaults to pca in utils.py.
             pca_kwargs: arguments that will be passed pca. Defaults to {}.
-            gene_append_list: ensure that a list of genes show up in selected genes in monocle recipe pipeline. Defaults
-                to [].
-            gene_exclude_list: exclude a list of genes in monocle recipe pipeline. Defaults to [].
-            force_gene_list: use this gene list as selected genes in monocle recipe pipeline. Defaults to None.
+            gene_append_list: ensure that a list of genes show up in selected genes across all the recipe pipeline.
+                Defaults to [].
+            gene_exclude_list: exclude a list of genes across all the recipe pipeline. Defaults to [].
+            force_gene_list: use this gene list as selected genes across all the recipe pipeline. Defaults to None.
             sctransform_kwargs: arguments passed into sctransform function. Defaults to {}.
+            regress_out_kwargs: arguments passed into regress_out function. Defaults to {}.
         """
 
+        self.basic_stats = basic_stats
         self.convert_layers2csr = convert_layers2csr
         self.unique_var_obs_adata = unique_var_obs_adata
-        self.log1p = log1p_adata
-        self.log1p_kwargs = log1p_kwargs
+        self.norm_method = norm_method
+        self.norm_method_kwargs = norm_method_kwargs
         self.sctransform = sctransform
 
         self.filter_cells_by_outliers = filter_cells_by_outliers_function
         self.filter_genes_by_outliers = filter_genes_by_outliers_function
         self.normalize_by_cells = normalize_by_cells_function
+        self.calc_size_factor = size_factor_function
+        self.calc_new_to_total_ratio = calc_new_to_total_ratio
         self.select_genes = select_genes_function
         self.normalize_selected_genes = normalize_selected_genes_function
-        self.use_log1p = use_log1p
-
+        self.regress_out = regress_out_parallel
         self.pca = pca_function
         self.pca_kwargs = pca_kwargs
 
         # self.n_top_genes = n_top_genes
         self.convert_gene_name = convert_gene_name_function
-        self.collapse_species_adata = collapse_speicies_adata_function
+        self.collapse_species_adata = collapse_species_adata_function
         self.gene_append_list = gene_append_list
         self.gene_exclude_list = gene_exclude_list
         self.force_gene_list = force_gene_list
 
         # kwargs pass to the functions above
         self.filter_genes_by_outliers_kwargs = filter_genes_by_outliers_kwargs
         self.normalize_by_cells_function_kwargs = normalize_by_cells_function_kwargs
         self.filter_cells_by_outliers_kwargs = filter_cells_by_outliers_kwargs
+        self.size_factor_kwargs = size_factor_kwargs
         self.select_genes_kwargs = select_genes_kwargs
         self.sctransform_kwargs = sctransform_kwargs
         self.normalize_selected_genes_kwargs = normalize_selected_genes_kwargs
+        self.cell_cycle_score_enable = cell_cycle_score_enable
+        self.cell_cycle_score = cell_cycle_scores
+        self.cell_cycle_score_kwargs = cell_cycle_score_kwargs
+        self.regress_out_kwargs = regress_out_kwargs
 
     def add_experiment_info(
         self, adata: AnnData, tkey: Optional[str] = None, experiment_type: Optional[str] = None
     ) -> None:
-        """Infer the experiment type and experiment layers stored in the AnnData object and record the info in unstructured metadata (.uns).
+        """Infer the experiment type and experiment layers stored in the AnnData
+        object and record the info in unstructured metadata (.uns).
 
         Args:
             adata: an AnnData object.
             tkey: the key for time information (labeling time period for the cells) in .obs. Defaults to None.
             experiment_type: the experiment type. If set to None, the experiment type would be inferred from the data.
-                Defaults to None.
 
         Raises:
             ValueError: the tkey is invalid.
         """
 
         if DKM.UNS_PP_KEY not in adata.uns.keys():
             adata.uns[DKM.UNS_PP_KEY] = {}
@@ -158,22 +169,29 @@
             has_splicing,
             has_labeling,
             splicing_labeling,
             has_protein,
         ) = detect_experiment_datatype(adata)
         # check whether tkey info exists if has_labeling
         if has_labeling:
-            main_info("data contains labeling info, checking tkey:" + str(tkey))
-            if tkey not in adata.obs.keys():
-                raise ValueError("tkey:%s encoding the labeling time is not existed in your adata." % (str(tkey)))
+            main_debug("data contains labeling info, checking tkey:" + str(tkey))
             if tkey is not None and adata.obs[tkey].max() > 60:
                 main_warning(
                     "Looks like you are using minutes as the time unit. For the purpose of numeric stability, "
                     "we recommend using hour as the time unit."
                 )
+            if tkey not in adata.obs.keys():
+                if (tkey is None) and (DKM.UNS_PP_TKEY in adata.obs.keys()):
+                    tkey = DKM.UNS_PP_TKEY
+                    main_warning(
+                        "No 'tkey' value was given despite 'tkey' information in the adata, "
+                        "so we will use 'time' in the adata as the default."
+                    )
+                else:
+                    raise ValueError("tkey:%s encoding the labeling time is not existed in your adata." % (str(tkey)))
 
         adata.uns["pp"]["tkey"] = tkey
         adata.uns["pp"]["has_splicing"] = has_splicing
         adata.uns["pp"]["has_labeling"] = has_labeling
         adata.uns["pp"]["has_protein"] = has_protein
         adata.uns["pp"]["splicing_labeling"] = splicing_labeling
         # infer and set experiment type
@@ -208,195 +226,253 @@
             layers = ["X", "spliced", "unspliced"]
         adata.uns["pp"]["experiment_layers"] = layers
         adata.uns["pp"]["experiment_total_layers"] = total_layers
 
     def standardize_adata(self, adata: AnnData, tkey: str, experiment_type: str) -> None:
         """Process the AnnData object to make it meet the standards of dynamo.
 
-        The index of the observations would be ensured to be unique.
-        The layers with sparse matrix would be converted to compressed csr_matrix.
-        DKM.allowed_layer_raw_names() will be used to define only_splicing, only_labeling and splicing_labeling keys.
-        The genes would be renamed to their official name.
+        The index of the observations would be ensured to be unique. The layers with sparse matrix would be converted to
+        compressed csr_matrix. DKM.allowed_layer_raw_names() will be used to define only_splicing, only_labeling and
+        splicing_labeling keys. The genes would be renamed to their official name.
 
         Args:
             adata: an AnnData object.
             tkey: the key for time information (labeling time period for the cells) in .obs.
             experiment_type: the experiment type.
         """
 
         adata.uns["pp"] = {}
-        adata.uns["pp"]["norm_method"] = None
+        adata.uns["pp"]["X_norm_method"] = None
+        adata.uns["pp"]["layers_norm_method"] = None
+
+        main_debug("applying convert_gene_name function...")
+        self.convert_gene_name(adata)
+
+
+        self.basic_stats(adata)
         self.add_experiment_info(adata, tkey, experiment_type)
         main_info_insert_adata("tkey=%s" % tkey, "uns['pp']", indent_level=2)
-        main_info_insert_adata("experiment_type=%s" % experiment_type, "uns['pp']", indent_level=2)
-        main_info("making adata observation index unique...")
-        self.unique_var_obs_adata(adata)
+        main_info_insert_adata("experiment_type=%s" % adata.uns["pp"]["experiment_type"], "uns['pp']", indent_level=2)
+
         self.convert_layers2csr(adata)
+        self.collapse_species_adata(adata)
 
-        if self.collapse_species_adata:
-            main_info("applying collapse species adata...")
-            self.collapse_species_adata(adata)
-
-        if self.convert_gene_name:
-            main_info("applying convert_gene_name function...")
-            self.convert_gene_name(adata)
-            main_info("making adata observation index unique after gene name conversion...")
-            self.unique_var_obs_adata(adata)
+        main_debug("making adata observation index unique after gene name conversion...")
+        self.unique_var_obs_adata(adata)
 
     def _filter_cells_by_outliers(self, adata: AnnData) -> None:
         """Select valid cells based on the method specified as the preprocessor's `filter_cells_by_outliers`.
 
         Args:
             adata: an AnnData object.
         """
 
         if self.filter_cells_by_outliers:
-            main_info("filtering outlier cells...")
-            main_info("cell filter kwargs:" + str(self.filter_cells_by_outliers_kwargs))
+            main_debug("filtering outlier cells...")
+            main_debug("cell filter kwargs:" + str(self.filter_cells_by_outliers_kwargs))
             self.filter_cells_by_outliers(adata, **self.filter_cells_by_outliers_kwargs)
 
     def _filter_genes_by_outliers(self, adata: AnnData) -> None:
         """Select valid genes based on the method specified as the preprocessor's `filter_genes_by_outliers`.
 
         Args:
             adata: an AnnData object.
         """
 
         if self.filter_genes_by_outliers:
-            main_info("filtering outlier genes...")
-            main_info("gene filter kwargs:" + str(self.filter_genes_by_outliers_kwargs))
+            main_debug("filtering outlier genes...")
+            main_debug("gene filter kwargs:" + str(self.filter_genes_by_outliers_kwargs))
             self.filter_genes_by_outliers(adata, **self.filter_genes_by_outliers_kwargs)
 
+    def _calc_size_factor(self, adata: AnnData) -> None:
+        """Calculate the size factor of each cell based on method specified as the preprocessor's `calc_size_factor`.
+
+        Args:
+            adata: an AnnData object.
+        """
+
+        if self.calc_size_factor:
+            main_debug("size factor calculation...")
+            main_debug("size_factor_kwargs kwargs:" + str(self.size_factor_kwargs))
+            self.calc_size_factor(
+                adata,
+                total_layers=adata.uns["pp"]["experiment_total_layers"],
+                layers=adata.uns["pp"]["experiment_layers"],
+                **self.size_factor_kwargs
+            )
+
     def _select_genes(self, adata: AnnData) -> None:
         """selecting gene by features, based on method specified as the preprocessor's `select_genes`.
 
         Args:
             adata: an AnnData object.
         """
 
         if self.select_genes:
-            main_info("selecting genes...")
-            main_info("select_genes kwargs:" + str(self.select_genes_kwargs))
+            main_debug("selecting genes...")
+            main_debug("select_genes kwargs:" + str(self.select_genes_kwargs))
             self.select_genes(adata, **self.select_genes_kwargs)
 
     def _append_gene_list(self, adata: AnnData) -> None:
-        """Add genes to the feature gene list detected by the preprocessing steps.
+        """Add genes to the feature gene list detected by the preprocessing.
 
         Args:
             adata: an AnnData object.
         """
 
-        if self.gene_append_list is not None:
+        if len(self.gene_append_list) > 0:
             append_genes = adata.var.index.intersection(self.gene_append_list)
             adata.var.loc[append_genes, DKM.VAR_USE_FOR_PCA] = True
             main_info("appended %d extra genes as required..." % len(append_genes))
 
     def _exclude_gene_list(self, adata: AnnData) -> None:
-        """Remove genes from the feature gene list detected by the preprocessing steps.
+        """Remove genes from the feature gene list detected by the preprocessing.
 
         Args:
             adata: an AnnData object.
         """
 
-        if self.gene_exclude_list is not None:
+        if len(self.gene_exclude_list) > 0:
             exclude_genes = adata.var.index.intersection(self.gene_exclude_list)
             adata.var.loc[exclude_genes, DKM.VAR_USE_FOR_PCA] = False
             main_info("excluded %d genes as required..." % len(exclude_genes))
 
     def _force_gene_list(self, adata: AnnData) -> None:
-        """Use the provided gene list as the feature gene list, overwrite the gene list detected by the preprocessing steps.
+        """Use the provided gene list as the feature gene list, overwrite the gene list detected by the preprocessing.
 
         Args:
             adata: an AnnData object.
         """
 
         if self.force_gene_list is not None:
             adata.var.loc[:, DKM.VAR_USE_FOR_PCA] = False
             forced_genes = adata.var.index.intersection(self.force_gene_list)
             adata.var.loc[forced_genes, DKM.VAR_USE_FOR_PCA] = True
             main_info(
                 "OVERWRITE all gene selection results above according to user gene list inputs. %d genes in use."
                 % len(forced_genes)
             )
-        else:
-            main_info("self.force_gene_list is None, skipping filtering by gene list...")
 
     def _normalize_selected_genes(self, adata: AnnData) -> None:
         """Normalize selected genes with method specified in the preprocessor's `normalize_selected_genes`
 
         Args:
             adata: an AnnData object.
         """
 
-        if not callable(self.normalize_selected_genes):
-            main_info(
-                "skipping normalize by selected genes as preprocessor normalize_selected_genes is not callable..."
-            )
-            return
-
-        main_info("normalizing selected genes...")
-        self.normalize_selected_genes(adata, **self.normalize_selected_genes_kwargs)
+        if callable(self.normalize_selected_genes):
+            main_debug("normalizing selected genes...")
+            self.normalize_selected_genes(adata, **self.normalize_selected_genes_kwargs)
 
     def _normalize_by_cells(self, adata: AnnData) -> None:
         """Performing cell-wise normalization based on method specified as the preprocessor's `normalize_by_cells`.
 
         Args:
             adata: an AnnData object.
         """
 
-        if not callable(self.normalize_by_cells):
-            main_info("skipping normalize by cells as preprocessor normalize_by_cells is not callable...")
-            return
+        if callable(self.normalize_by_cells):
+            main_debug("applying normalize by cells function...")
+            self.normalize_by_cells(adata, **self.normalize_by_cells_function_kwargs)
 
-        main_info("applying normalize by cells function...")
-        self.normalize_by_cells(adata, **self.normalize_by_cells_function_kwargs)
+    def _norm_method(self, adata: AnnData) -> None:
+        """Perform a normalization method on the data with args specified in the preprocessor's `norm_method_kwargs`.
 
-    def _log1p(self, adata: AnnData) -> None:
-        """Perform log1p on the data with args specified in the preprocessor's `log1p_kwargs`.
+        Args:
+            adata: an AnnData object.
+        """
+
+        if callable(self.norm_method):
+            main_debug("applying a normalization method transformation on expression matrix data...")
+            self.norm_method(adata, **self.norm_method_kwargs)
+
+    def _regress_out(self, adata: AnnData) -> None:
+        """Perform regressing out with args specified in the preprocessor's `regress_out_kwargs`.
 
         Args:
             adata: an AnnData object.
         """
 
-        if self.use_log1p:
-            if is_log1p_transformed_adata(adata):
-                main_warning(
-                    "Your adata.X maybe log1p transformed before. If you are sure that your adata is not log1p transformed, please ignore this warning. Dynamo will do log1p transformation still."
-                )
-            # TODO: the following line is for monocle recipe and later dynamics matrix recovery
-            # refactor with dynamics module
-            adata.uns["pp"]["norm_method"] = "log1p"
-            main_info("applying log1p transformation on expression matrix data (adata.X)...")
-            self.log1p(adata, **self.log1p_kwargs)
+        if self.regress_out:
+            main_info("regressing out...")
+            self.regress_out(adata, **self.regress_out_kwargs)
 
     def _pca(self, adata: AnnData) -> None:
-        """Perform pca reduction with args specified in the preprocessor's `pca_kwargs`.
+        """Perform principal component analysis reduction with args specified in the preprocessor's `pca_kwargs`.
 
         Args:
             adata: an AnnData object.
         """
 
         if self.pca:
-            main_info("reducing dimension by PCA...")
+            main_info("PCA dimension reduction")
             self.pca(adata, **self.pca_kwargs)
 
-    def config_monocle_recipe(
-        self, adata: AnnData, n_top_genes: int = 2000, gene_selection_method: str = "SVR"
+    def _calc_ntr(self, adata: AnnData) -> None:
+        """Calculate the size factor of each cell based on method specified as the preprocessor's `calc_size_factor`.
+
+        Args:
+            adata: an AnnData object.
+        """
+
+        if self.calc_new_to_total_ratio:
+            main_debug("ntr calculation...")
+            # calculate NTR for every cell:
+            ntr, var_ntr = self.calc_new_to_total_ratio(adata)
+            adata.obs["ntr"] = ntr
+            adata.var["ntr"] = var_ntr
+
+    def _cell_cycle_score(self, adata: AnnData) -> None:
+        """Estimate cell cycle stage of each cell based on its gene expression pattern.
+
+        Args:
+            adata: an AnnData object.
+        """
+
+        if self.cell_cycle_score_enable:
+            main_debug("cell cycle scoring...")
+            try:
+                self.cell_cycle_score(adata, **self.cell_cycle_score_kwargs)
+            except Exception:
+                main_warning(
+                    "\nDynamo is not able to perform cell cycle staging for you automatically. \n"
+                    "Since dyn.pl.phase_diagram in dynamo by default colors cells by its cell-cycle stage, \n"
+                    "you need to set color argument accordingly if confronting errors related to this."
+                )
+
+    def preprocess_adata_seurat_wo_pca(
+        self, adata: AnnData, tkey: Optional[str] = None, experiment_type: Optional[str] = None
     ) -> None:
+        """Preprocess the anndata object according to standard preprocessing in Seurat recipe without PCA.
+        This can be used to test different dimension reduction methods.
+        """
+        main_info("Running preprocessing pipeline...")
+        temp_logger = LoggerManager.gen_logger("preprocessor-seurat_wo_pca")
+        temp_logger.log_time()
+
+        self.standardize_adata(adata, tkey, experiment_type)
+        self._filter_cells_by_outliers(adata)
+        self._filter_genes_by_outliers(adata)
+        self._normalize_by_cells(adata)
+        self._select_genes(adata)
+        self._norm_method(adata)
+
+        temp_logger.finish_progress(progress_name="preprocess by seurat wo pca recipe")
+
+    def config_monocle_recipe(self, adata: AnnData, n_top_genes: int = 2000) -> None:
         """Automatically configure the preprocessor for monocle recipe.
 
         Args:
             adata: an AnnData object.
             n_top_genes: Number of top feature genes to select in the preprocessing step. Defaults to 2000.
-            gene_selection_method: Which sorting method to be used to select genes. Defaults to "SVR".
         """
 
         n_obs, n_genes = adata.n_obs, adata.n_vars
         n_cells = n_obs
-        self.use_log1p = False
+
         self.filter_cells_by_outliers = monocle_filter_cells_by_outliers
         self.filter_cells_by_outliers_kwargs = {
             "filter_bool": None,
             "layer": "all",
             "min_expr_genes_s": min(50, 0.01 * n_genes),
             "min_expr_genes_u": min(25, 0.01 * n_genes),
             "min_expr_genes_p": min(2, 0.01 * n_genes),
@@ -417,170 +493,198 @@
             "min_avg_exp_p": 0,
             "max_avg_exp": np.inf,
             "min_count_s": 0,
             "min_count_u": 0,
             "min_count_p": 0,
             "shared_count": 30,
         }
-        self.select_genes = select_genes_by_dispersion_general
-        self.select_genes_kwargs = {
-            "recipe": "monocle",
-            "monocle_kwargs": {
-                "sort_by": gene_selection_method,
-                "n_top_genes": n_top_genes,
-                "keep_filtered": True,
-                "SVRs_kwargs": {
-                    "min_expr_cells": 0,
-                    "min_expr_avg": 0,
-                    "max_expr_avg": np.inf,
-                    "svr_gamma": None,
-                    "winsorize": False,
-                    "winsor_perc": (1, 99.5),
-                    "sort_inverse": False,
-                },
-                "only_bools": True,
-            },
-        }
+        self.select_genes = select_genes_monocle
+        self.select_genes_kwargs = {"n_top_genes": n_top_genes, "SVRs_kwargs": {"relative_expr": False}}
         self.normalize_selected_genes = None
-        self.normalize_by_cells = normalize_cell_expr_by_size_factors
+        self.normalize_by_cells = normalize
+        self.norm_method = log1p
+
+        self.regress_out_kwargs = update_dict({"obs_keys": []}, self.regress_out_kwargs)
 
-        # recipe monocle log1p all raw data in normalize_by_cells (dynamo version), so we do not need extra log1p transform.
-        self.use_log1p = False
-        self.pca = pca_monocle
+        self.pca = pca
         self.pca_kwargs = {"pca_key": "X_pca"}
 
+        self.cell_cycle_score_kwargs = {
+            "layer": None,
+            "gene_list": None,
+            "refine": True,
+            "threshold": 0.3,
+            "copy": False,
+        }
+
     def preprocess_adata_monocle(
         self, adata: AnnData, tkey: Optional[str] = None, experiment_type: Optional[str] = None
     ) -> None:
         """Preprocess the AnnData object based on Monocle style preprocessing recipe.
 
         Args:
             adata: an AnnData object.
             tkey: the key for time information (labeling time period for the cells) in .obs. Defaults to None.
-            experiment_type: the experiment type of the data. If not provided, would be inferred from the data. Defaults
-                to None.
+            experiment_type: the experiment type of the data. If not provided, would be inferred from the data.
         """
 
-        main_info("Running preprocessing pipeline...")
+        main_info("Running monocle preprocessing pipeline...")
         temp_logger = LoggerManager.gen_logger("preprocessor-monocle")
         temp_logger.log_time()
 
         self.standardize_adata(adata, tkey, experiment_type)
-
         self._filter_cells_by_outliers(adata)
         self._filter_genes_by_outliers(adata)
-        self._filter_cells_by_outliers(adata)
+
+        # The following size factor calculation is a prerequisite for monocle recipe preprocess in preprocessor.
+        self._calc_size_factor(adata)
+        self._normalize_by_cells(adata)
         self._select_genes(adata)
 
-        # gene selection has been completed above. Now we need to append/delete/force selected gene list required by users.
+        # append/delete/force selected gene list required by users.
         self._append_gene_list(adata)
         self._exclude_gene_list(adata)
         self._force_gene_list(adata)
 
-        self._normalize_selected_genes(adata)
-        self._normalize_by_cells(adata)
+        self._norm_method(adata)
+
+        if len(self.regress_out_kwargs["obs_keys"]) > 0:
+            self._regress_out(adata)
 
-        self._log1p(adata)
         self._pca(adata)
+        self._calc_ntr(adata)
+        self._cell_cycle_score(adata)
 
-        temp_logger.finish_progress(progress_name="preprocess")
+        temp_logger.finish_progress(progress_name="Preprocessor-monocle")
 
     def config_seurat_recipe(self, adata: AnnData) -> None:
         """Automatically configure the preprocessor for using the seurat style recipe.
 
         Args:
             adata: an AnnData object.
         """
 
         self.config_monocle_recipe(adata)
-        self.select_genes = select_genes_by_dispersion_general
-        self.select_genes_kwargs = {"recipe": "seurat", "n_top_genes": 2000}
-        self.normalize_by_cells_function_kwargs = {"skip_log": True}
+        self.select_genes = select_genes_by_seurat_recipe
+        self.select_genes_kwargs = {
+            "algorithm": "seurat_dispersion",
+            "n_top_genes": 2000,
+        }
         self.pca_kwargs = {"pca_key": "X_pca"}
         self.filter_genes_by_outliers_kwargs = {"shared_count": 20}
-        self.use_log1p = True
-        self.log1p_kwargs = {"layers": ["X"]}
+        self.regress_out_kwargs = update_dict({"obs_keys": []}, self.regress_out_kwargs)
 
     def preprocess_adata_seurat(
         self, adata: AnnData, tkey: Optional[str] = None, experiment_type: Optional[str] = None
     ) -> None:
         """The preprocess pipeline in Seurat based on dispersion, implemented by dynamo authors.
 
-        Stuart and Butler et al. Comprehensive Integration of Single-Cell Data. Cell (2019)
-        Butler et al. Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nat Biotechnol
+        Stuart and Butler et al. Comprehensive Integration of Single-Cell Data.
+        Cell (2019) Butler et al. Integrating single-cell transcriptomic data
+        across different conditions, technologies, and species. Nat Biotechnol
 
         Args:
             adata: an AnnData object
             tkey: the key for time information (labeling time period for the cells) in .obs. Defaults to None.
-            experiment_type: the experiment type of the data. If not provided, would be inferred from the data. Defaults
-                to None.
+            experiment_type: the experiment type of the data. If not provided, would be inferred from the data.
         """
 
         temp_logger = LoggerManager.gen_logger("preprocessor-seurat")
         temp_logger.log_time()
-        main_info("Applying Seurat recipe preprocessing...")
+        main_info("Running Seurat recipe preprocessing...")
 
         self.standardize_adata(adata, tkey, experiment_type)
+        self._filter_cells_by_outliers(adata)
         self._filter_genes_by_outliers(adata)
+
+        self._calc_size_factor(adata)
         self._normalize_by_cells(adata)
         self._select_genes(adata)
-        self._log1p(adata)
+
+        # append/delete/force selected gene list required by users.
+        self._append_gene_list(adata)
+        self._exclude_gene_list(adata)
+        self._force_gene_list(adata)
+
+        self._norm_method(adata)
+
+        if len(self.regress_out_kwargs["obs_keys"]) > 0:
+            self._regress_out(adata)
+
         self._pca(adata)
-        temp_logger.finish_progress(progress_name="preprocess by seurat recipe")
+        temp_logger.finish_progress(progress_name="Preprocessor-seurat")
 
     def config_sctransform_recipe(self, adata: AnnData) -> None:
         """Automatically configure the preprocessor for using the sctransform style recipe.
 
         Args:
             adata: an AnnData object.
         """
 
-        self.use_log1p = False
         raw_layers = DKM.get_raw_data_layers(adata)
+        raw_layers = [layer for layer in raw_layers if layer != DKM.X_LAYER]
         self.filter_cells_by_outliers_kwargs = {"keep_filtered": False}
         self.filter_genes_by_outliers_kwargs = {
             "inplace": True,
             "min_cell_s": 5,
             "min_count_s": 1,
             "min_cell_u": 5,
             "min_count_u": 1,
         }
-        self.select_genes_kwargs = {"inplace": True}
-        self.sctransform_kwargs = {"layers": raw_layers, "n_top_genes": 2000}
+        self.select_genes_kwargs = {"n_top_genes": 3000}
+        self.sctransform_kwargs = {"n_top_genes": 2000}
+        self.normalize_by_cells_function_kwargs = {"layers": raw_layers}
+        self.normalize_by_cells = normalize
+        self.regress_out_kwargs = update_dict({"obs_keys": []}, self.regress_out_kwargs)
         self.pca_kwargs = {"pca_key": "X_pca", "n_pca_components": 50}
 
     def preprocess_adata_sctransform(
         self, adata: AnnData, tkey: Optional[str] = None, experiment_type: Optional[str] = None
     ) -> None:
         """Python implementation of https://github.com/satijalab/sctransform.
 
         Hao and Hao et al. Integrated analysis of multimodal single-cell data. Cell (2021)
 
         Args:
             adata: an AnnData object
-            tkey: the key for time information (labeling time period for the cells) in .obs. Defaults to None.
-            experiment_type: the experiment type of the data. If not provided, would be inferred from the data. Defaults
-                to None.
+            tkey: the key for time information (labeling time period for the
+                cells) in .obs. Defaults to None.
+            experiment_type: the experiment type of the data. If not provided,
+                would be inferred from the data. Defaults to None.
         """
 
         temp_logger = LoggerManager.gen_logger("preprocessor-sctransform")
         temp_logger.log_time()
-        main_info("Applying Sctransform recipe preprocessing...")
+        main_info("Running Sctransform recipe preprocessing...")
 
         self.standardize_adata(adata, tkey, experiment_type)
-
         self._filter_cells_by_outliers(adata)
         self._filter_genes_by_outliers(adata)
+
+        main_warning(
+            "Sctransform recipe will subset the data first with default gene selection function for "
+            "efficiency. If you want to disable this, please perform sctransform without recipe."
+        )
+        self._calc_size_factor(adata)
         self._select_genes(adata)
         # TODO: if inplace in select_genes is True, the following subset is unnecessary.
         adata._inplace_subset_var(adata.var["use_for_pca"])
+
+        # append/delete/force selected gene list required by users.
+        self._append_gene_list(adata)
+        self._exclude_gene_list(adata)
+        self._force_gene_list(adata)
+
         self.sctransform(adata, **self.sctransform_kwargs)
+        self._normalize_by_cells(adata)
+        if len(self.regress_out_kwargs["obs_keys"]) > 0:
+            self._regress_out(adata)
         self._pca(adata)
 
-        temp_logger.finish_progress(progress_name="preprocess by sctransform recipe")
+        temp_logger.finish_progress(progress_name="Preprocessor-sctransform")
 
     def config_pearson_residuals_recipe(self, adata: AnnData) -> None:
         """Automatically configure the preprocessor for using the Pearson residuals style recipe.
 
         Args:
             adata: an AnnData object.
         """
@@ -588,131 +692,145 @@
         self.filter_cells_by_outliers = None
         self.filter_genes_by_outliers = None
         self.normalize_by_cells = None
         self.select_genes = select_genes_by_pearson_residuals
         self.select_genes_kwargs = {"n_top_genes": 2000}
         self.normalize_selected_genes = normalize_layers_pearson_residuals
         # select layers in adata to be normalized
-        normalize_layers = DKM.get_raw_data_layers(adata)
+        normalize_layers = DKM.X_LAYER
         self.normalize_selected_genes_kwargs = {"layers": normalize_layers, "copy": False}
+        self.regress_out_kwargs = update_dict({"obs_keys": []}, self.regress_out_kwargs)
         self.pca_kwargs = {"pca_key": "X_pca", "n_pca_components": 50}
-        self.use_log1p = False
 
     def preprocess_adata_pearson_residuals(
         self, adata: AnnData, tkey: Optional[str] = None, experiment_type: Optional[str] = None
     ) -> None:
         """A pipeline proposed in Pearson residuals (Lause, Berens & Kobak, 2021).
 
-        Lause, J., Berens, P. & Kobak, D. Analytic Pearson residuals for normalization of single-cell RNA-seq UMI data. Genome Biol 22, 258 (2021). https://doi.org/10.1186/s13059-021-02451-7
+        Lause, J., Berens, P. & Kobak, D. Analytic Pearson residuals for normalization of single-cell RNA-seq UMI data.
+        Genome Biol 22, 258 (2021). https://doi.org/10.1186/s13059-021-02451-7
 
         Args:
             adata: an AnnData object
-            tkey: the key for time information (labeling time period for the cells) in .obs. Defaults to None.
-            experiment_type: the experiment type of the data. If not provided, would be inferred from the data. Defaults
-                to None.
+            tkey: the key for time information (labeling time period for the
+                cells) in .obs. Defaults to None.
+            experiment_type: the experiment type of the data. If not provided,
+                would be inferred from the data. Defaults to None.
         """
 
-        temp_logger = LoggerManager.gen_logger("preprocessor-sctransform")
+        temp_logger = LoggerManager.gen_logger("Preprocessor-pearson residual")
         temp_logger.log_time()
         self.standardize_adata(adata, tkey, experiment_type)
+        self._filter_cells_by_outliers(adata)
+        self._filter_genes_by_outliers(adata)
 
         self._select_genes(adata)
+        # append/delete/force selected gene list required by users.
+        self._append_gene_list(adata)
+        self._exclude_gene_list(adata)
+        self._force_gene_list(adata)
+
         self._normalize_selected_genes(adata)
+        if len(self.regress_out_kwargs["obs_keys"]) > 0:
+            self._regress_out(adata)
+
         self._pca(adata)
 
-        temp_logger.finish_progress(progress_name="preprocess by pearson residual recipe")
+        temp_logger.finish_progress(progress_name="Preprocessor-pearson residual")
 
     def config_monocle_pearson_residuals_recipe(self, adata: AnnData) -> None:
         """Automatically configure the preprocessor for using the Monocle-Pearson-residuals style recipe.
 
         Useful when you want to use Pearson residual to obtain feature genes and perform PCA but also using the standard
         size-factor normalization and log1p analyses to normalize data for RNA velocity and vector field analyses.
 
         Args:
             adata: an AnnData object.
         """
 
         self.config_monocle_recipe(adata)
         # self.filter_cells_by_outliers = None
         # self.filter_genes_by_outliers = None
-        self.normalize_by_cells = normalize_cell_expr_by_size_factors
+        self.normalize_by_cells = normalize
         self.select_genes = select_genes_by_pearson_residuals
         self.select_genes_kwargs = {"n_top_genes": 2000}
         self.normalize_selected_genes = normalize_layers_pearson_residuals
-
         self.normalize_selected_genes_kwargs = {"layers": ["X"], "copy": False}
-
+        self.regress_out_kwargs = update_dict({"obs_keys": []}, self.regress_out_kwargs)
         self.pca_kwargs = {"pca_key": "X_pca", "n_pca_components": 50}
-        self.use_log1p = False
 
     def preprocess_adata_monocle_pearson_residuals(
         self, adata: AnnData, tkey: Optional[str] = None, experiment_type: Optional[str] = None
     ) -> None:
         """A combined pipeline of monocle and pearson_residuals.
 
         Results after running pearson_residuals can contain negative values, an undesired feature for later RNA velocity
         analysis. This function combine pearson_residual and monocle recipes so that it uses Pearson residual to obtain
         feature genes and perform PCA but also uses monocle recipe to generate X_spliced, X_unspliced, X_new, X_total or
         other data values for RNA velocity and downstream vector field analyses.
 
         Args:
             adata: an AnnData object
             tkey: the key for time information (labeling time period for the cells) in .obs. Defaults to None.
-            experiment_type: the experiment type of the data. If not provided, would be inferred from the data. Defaults
-                to None.
+            experiment_type: the experiment type of the data. If not provided, would be inferred from the data.
         """
 
         temp_logger = LoggerManager.gen_logger("preprocessor-monocle-pearson-residual")
         temp_logger.log_time()
         self.standardize_adata(adata, tkey, experiment_type)
+        self._filter_cells_by_outliers(adata)
+        self._filter_genes_by_outliers(adata)
         self._select_genes(adata)
+
+        # append/delete/force selected gene list required by users.
+        self._append_gene_list(adata)
+        self._exclude_gene_list(adata)
+        self._force_gene_list(adata)
+
         X_copy = adata.X.copy()
         self._normalize_by_cells(adata)
         adata.X = X_copy
         self._normalize_selected_genes(adata)
-        # use monocle to pprocess adata
-        # self.config_monocle_recipe(adata_copy)
-        # self.pca = None # do not do pca in this monocle
-        # self.preprocess_adata_monocle(adata_copy)
-        # for layer in adata_copy.layers:
-        #     if DKM.is_layer_X_key(layer):
-        #         adata.layers[layer] = adata.
+        if len(self.regress_out_kwargs["obs_keys"]) > 0:
+            self._regress_out(adata)
 
         self.pca(adata, **self.pca_kwargs)
-        temp_logger.finish_progress(progress_name="preprocess by monocle pearson residual recipe")
+        temp_logger.finish_progress(progress_name="Preprocessor-monocle-pearson-residual")
 
     def preprocess_adata(
         self,
         adata: AnnData,
         recipe: Literal[
             "monocle", "seurat", "sctransform", "pearson_residuals", "monocle_pearson_residuals"
         ] = "monocle",
         tkey: Optional[str] = None,
+        experiment_type: Optional[str] = None,
     ) -> None:
         """Preprocess the AnnData object with the recipe specified.
 
         Args:
             adata: An AnnData object.
             recipe: The recipe used to preprocess the data. Defaults to "monocle".
             tkey: the key for time information (labeling time period for the cells) in .obs. Defaults to None.
+            experiment_type: the experiment type of the data. If not provided, would be inferred from the data.
 
         Raises:
             NotImplementedError: the recipe is invalid.
         """
 
         if recipe == "monocle":
             self.config_monocle_recipe(adata)
-            self.preprocess_adata_monocle(adata, tkey=tkey)
+            self.preprocess_adata_monocle(adata, tkey=tkey, experiment_type=experiment_type)
         elif recipe == "seurat":
             self.config_seurat_recipe(adata)
-            self.preprocess_adata_seurat(adata, tkey=tkey)
+            self.preprocess_adata_seurat(adata, tkey=tkey, experiment_type=experiment_type)
         elif recipe == "sctransform":
             self.config_sctransform_recipe(adata)
-            self.preprocess_adata_sctransform(adata, tkey=tkey)
+            self.preprocess_adata_sctransform(adata, tkey=tkey, experiment_type=experiment_type)
         elif recipe == "pearson_residuals":
             self.config_pearson_residuals_recipe(adata)
-            self.preprocess_adata_pearson_residuals(adata, tkey=tkey)
+            self.preprocess_adata_pearson_residuals(adata, tkey=tkey, experiment_type=experiment_type)
         elif recipe == "monocle_pearson_residuals":
             self.config_monocle_pearson_residuals_recipe(adata)
-            self.preprocess_adata_monocle_pearson_residuals(adata, tkey=tkey)
+            self.preprocess_adata_monocle_pearson_residuals(adata, tkey=tkey, experiment_type=experiment_type)
         else:
-            raise NotImplementedError("preprocess recipe chosen not implemented: %s" % (recipe))
+            raise NotImplementedError("preprocess recipe chosen not implemented: %s" % recipe)
```

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/__init__.py` & `dynamo-release-1.3.0/dynamo/preprocessing/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,80 +1,89 @@
 """Mapping Vector Field of Single Cells
 """
 
 from .cell_cycle import cell_cycle_scores
 from .dynast import lambda_correction
-from .preprocess import (
-    Gini,
-    SVRs,
-    calc_sz_factor_legacy,
+from .external import (
+    harmony_debatch,
+    integrate,
+    normalize_layers_pearson_residuals,
+    sctransform,
+    select_genes_by_pearson_residuals,
+)
+from .normalization import calc_sz_factor, normalize
+from .QC import (
+    basic_stats,
+    filter_genes_by_clusters,
     filter_cells_by_outliers,
-    filter_cells_legacy,
-    filter_genes_by_clusters_,
     filter_genes_by_outliers,
-    get_svr_filter,
-    highest_frac_genes,
-    normalize_cell_expr_by_size_factors_legacy,
-    recipe_monocle,
-    recipe_velocyto,
-    select_genes_monocle,
+    filter_genes_by_pattern,
 )
-from .preprocessor_utils import *
+from .pca import pca, top_pca_genes
+from .transform import log1p, log1p_adata_layer
 from .utils import (
-    basic_stats,
     compute_gene_exp_fraction,
     convert2symbol,
-    cook_dist,
     decode,
-    filter_genes_by_pattern,
-    pca_monocle,
+    get_svr_filter,
     relative2abs,
     scale,
-    top_pca_genes,
+)
+from .deprecated import (
+    cook_dist,
+    calc_sz_factor_legacy,
+    normalize_cell_expr_by_size_factors,
+    filter_cells_legacy,
+    recipe_monocle,
+    recipe_velocyto,
 )
 
 filter_cells = filter_cells_by_outliers
 filter_genes = filter_genes_by_outliers
-log1p = log1p_adata
-normalize_cells = normalize_cell_expr_by_size_factors
+log1p = log1p
+normalize_cells = normalize
 
 from .CnmfPreprocessor import CnmfPreprocessor
-from .preprocess_monocle_utils import estimate_dispersion, top_table
+from .gene_selection import calc_Gini, calc_dispersion_by_svr, highest_frac_genes, select_genes_monocle
 from .Preprocessor import Preprocessor
 
 __all__ = [
+    "calc_sz_factor",
     "filter_cells",
     "filter_genes",
-    "log1p",
     "normalize_cells",
     "lambda_correction",
     "calc_sz_factor_legacy",
-    "normalize_cell_expr_by_size_factors",
+    "normalize_layers_pearson_residuals",
+    "normalize",
     "recipe_monocle",
     "recipe_velocyto",
-    "Gini",
-    "top_table",
-    "estimate_dispersion",
+    "calc_Gini",
     "filter_cells_by_outliers",
     "select_genes_monocle",
+    "select_genes_by_pearson_residuals",
     "filter_genes",
     "filter_genes_by_outliers",
-    "filter_genes_by_clusters_",
-    "SVRs",
+    "filter_genes_by_clusters",
+    "calc_dispersion_by_svr",
     "get_svr_filter",
     "highest_frac_genes",
     "cell_cycle_scores",
     "basic_stats",
     "cook_dist",
-    "pca_monocle",
+    "pca",
     "top_pca_genes",
     "relative2abs",
     "scale",
+    "sctransform",
     "convert2symbol",
     "filter_genes_by_pattern",
     "decode",
     "Preprocessor",
     "CnmfPreprocessor",
     "log1p",
-    "log1p_adata",
     "log1p_adata_layer",
+    "harmony_debatch",
+    "integrate",
+    "normalize_cell_expr_by_size_factors",
+    "filter_cells_legacy",
 ]
```

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/cell_cycle.py` & `dynamo-release-1.3.0/dynamo/preprocessing/cell_cycle.py`

 * *Files 1% similar despite different names*

```diff
@@ -436,15 +436,15 @@
     cell_cycle_scores = normalized_phase_scores_corr.corr()
     tmp = -len(phase_list)
     cell_cycle_scores = cell_cycle_scores[tmp:].transpose()[: -len(phase_list)]
 
     # pick maximal score as the phase for that cell
     cell_cycle_scores["cell_cycle_phase"] = cell_cycle_scores.idxmax(axis=1)
     cell_cycle_scores["cell_cycle_phase"] = cell_cycle_scores["cell_cycle_phase"].astype("category")
-    cell_cycle_scores["cell_cycle_phase"].cat.set_categories(phase_list, inplace=True)
+    cell_cycle_scores["cell_cycle_phase"].cat.set_categories(phase_list)
 
     def progress_ratio(x, phase_list):
         ind = phase_list.index(x["cell_cycle_phase"])
         return x[phase_list[(ind - 1) % len(phase_list)]] - x[phase_list[(ind + 1) % len(phase_list)]]
 
     # interpolate position within given cell cycle phase
     cell_cycle_scores["cell_cycle_progress"] = cell_cycle_scores.apply(
@@ -454,17 +454,17 @@
         ["cell_cycle_phase", "cell_cycle_progress"],
         ascending=[True, False],
         inplace=True,
     )
 
     # order of cell within cell cycle phase
     cell_cycle_scores["cell_cycle_order"] = cell_cycle_scores.groupby("cell_cycle_phase").cumcount()
-    cell_cycle_scores["cell_cycle_order"] = cell_cycle_scores.groupby("cell_cycle_phase")["cell_cycle_order"].apply(
-        lambda x: x / (len(x) - 1)
-    )
+    cell_cycle_scores["cell_cycle_order"] = cell_cycle_scores.groupby("cell_cycle_phase", group_keys=False)[
+        "cell_cycle_order"
+    ].apply(lambda x: x / (len(x) - 1))
 
     return cell_cycle_scores
 
 
 def cell_cycle_scores(
     adata: anndata.AnnData,
     layer: Union[str, None] = None,
@@ -504,15 +504,15 @@
     cell_cycle_scores = get_cell_phase(
         adata,
         layer=layer,
         refine=refine,
         gene_list=gene_list,
         threshold=threshold,
     )
-    temp_timer_logger.finish_progress(progress_name="cell phase estimation")
+    temp_timer_logger.finish_progress(progress_name="Cell Phase Estimation")
 
     cell_cycle_scores.index = adata.obs_names[cell_cycle_scores.index.values.astype("int")]
 
     logger.info_insert_adata("cell_cycle_phase", adata_attr="obs")
     adata.obs["cell_cycle_phase"] = cell_cycle_scores["cell_cycle_phase"].astype("category")
 
     # adata.obsm['cell_cycle_scores'] = cell_cycle_scores.set_index(adata.obs_names)
```

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/dynast.py` & `dynamo-release-1.3.0/dynamo/preprocessing/dynast.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/preprocess.py` & `dynamo-release-1.3.0/dynamo/preprocessing/deprecated.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,69 +1,535 @@
+import re
 import warnings
-from collections.abc import Iterable
-from typing import Callable, List, Optional, Tuple, Union
+from typing import Callable, Iterable, List, Optional, Tuple, Union
 
 try:
     from typing import Literal
 except ImportError:
     from typing_extensions import Literal
 
+import functools
+
 import anndata
 import numpy as np
+import numpy.typing as npt
 import pandas as pd
+import statsmodels.api as sm
 from anndata import AnnData
 from scipy.sparse import csr_matrix, issparse
 from sklearn.decomposition import FastICA
-from sklearn.utils import sparsefuncs
 
 from ..configuration import DKM, DynamoAdataConfig, DynamoAdataKeyManager
 from ..dynamo_logger import (
     LoggerManager,
-    main_critical,
+    main_debug,
     main_info,
     main_info_insert_adata_obsm,
-    main_info_insert_adata_uns,
     main_warning,
 )
 from ..tools.utils import update_dict
 from ..utils import copy_adata
 from .cell_cycle import cell_cycle_scores
-from .preprocess_monocle_utils import top_table
-from .preprocessor_utils import (
-    SVRs,
-    _infer_labeling_experiment_type,
-    filter_cells_by_outliers,
-    filter_genes_by_outliers,
-    normalize_cell_expr_by_size_factors,
-    select_genes_monocle,
-)
+from .gene_selection import calc_dispersion_by_svr
+from .normalization import calc_sz_factor, get_sz_exprs, normalize_mat_monocle, sz_util
+from .pca import pca
+from .QC import basic_stats, filter_genes_by_clusters, filter_genes_by_outliers
+from .transform import _Freeman_Tukey
 from .utils import (
-    Freeman_Tukey,
+    _infer_labeling_experiment_type,
     add_noise_to_duplicates,
-    basic_stats,
     calc_new_to_total_ratio,
-    clusters_stats,
     collapse_species_adata,
     compute_gene_exp_fraction,
     convert2symbol,
     convert_layers2csr,
-    cook_dist,
     detect_experiment_datatype,
     get_inrange_shared_counts_mask,
+    get_nan_or_inf_data_bool_mask,
     get_svr_filter,
-    get_sz_exprs,
     merge_adata_attrs,
-    normalize_mat_monocle,
-    pca_monocle,
-    sz_util,
     unique_var_obs_adata,
 )
 
 
-def calc_sz_factor_legacy(
+def deprecated(func):
+    @functools.wraps(func)
+    def wrapper(*args, **kwargs):
+        warnings.warn(
+            f"{func.__name__} is deprecated and will be removed in a future release. "
+            f"Please update your code to use the new replacement function.",
+            category=DeprecationWarning,
+            stacklevel=2,
+        )
+        return func(*args, **kwargs)
+
+    return wrapper
+
+
+# ---------------------------------------------------------------------------------------------------
+# implmentation of Cooks' distance (but this is for Poisson distribution fitting)
+
+# https://stackoverflow.com/questions/47686227/poisson-regression-in-statsmodels-and-r
+
+# from __future__ import division, print_function
+
+# https://stats.stackexchange.com/questions/356053/the-identity-link-function-does-not-respect-the-domain-of-the-gamma-
+# family
+def _weight_matrix_legacy(fitted_model: sm.Poisson) -> np.ndarray:
+    """Calculates weight matrix in Poisson regression.
+
+    Args:
+        fitted_model: a fitted Poisson model
+
+    Returns:
+        A diagonal weight matrix in Poisson regression.
+    """
+
+    return np.diag(fitted_model.fittedvalues)
+
+
+def _hessian_legacy(X: np.ndarray, W: np.ndarray) -> np.ndarray:
+    """Hessian matrix calculated as -X'*W*X.
+
+    Args:
+        X: the matrix of covariates.
+        W: the weight matrix.
+
+    Returns:
+        The result Hessian matrix.
+    """
+
+    return -np.dot(X.T, np.dot(W, X))
+
+
+def _hat_matrix_legacy(X: np.ndarray, W: np.ndarray) -> np.ndarray:
+    """Calculate hat matrix = W^(1/2) * X * (X'*W*X)^(-1) * X'*W^(1/2)
+
+    Args:
+        X: the matrix of covariates.
+        W: the diagonal weight matrix
+
+    Returns:
+        The result hat matrix
+    """
+
+    # W^(1/2)
+    Wsqrt = W ** (0.5)
+
+    # (X'*W*X)^(-1)
+    XtWX = -_hessian_legacy(X=X, W=W)
+    XtWX_inv = np.linalg.inv(XtWX)
+
+    # W^(1/2)*X
+    WsqrtX = np.dot(Wsqrt, X)
+
+    # X'*W^(1/2)
+    XtWsqrt = np.dot(X.T, Wsqrt)
+
+    return np.dot(WsqrtX, np.dot(XtWX_inv, XtWsqrt))
+
+
+@deprecated
+def cook_dist(*args, **kwargs):
+    _cook_dist_legacy(*args, **kwargs)
+
+
+def _cook_dist_legacy(model: sm.Poisson, X: np.ndarray, good: npt.ArrayLike) -> np.ndarray:
+    """calculate Cook's distance
+
+    Args:
+        model: a fitted Poisson model.
+        X: the matrix of covariates.
+        good: the dispersion table for MSE calculation.
+
+    Returns:
+        The result Cook's distance.
+    """
+
+    # Weight matrix
+    W = _weight_matrix_legacy(model)
+
+    # Hat matrix
+    H = _hat_matrix_legacy(X, W)
+    hii = np.diag(H)  # Diagonal values of hat matrix # fit.get_influence().hat_matrix_diag
+
+    # Pearson residuals
+    r = model.resid_pearson
+
+    # Cook's distance (formula used by R = (res/(1 - hat))^2 * hat/(dispersion * p))
+    # Note: dispersion is 1 since we aren't modeling overdispersion
+
+    resid = good.disp - model.predict(good)
+    rss = np.sum(resid**2)
+    MSE = rss / (good.shape[0] - 2)
+    # use the formula from: https://www.mathworks.com/help/stats/cooks-distance.html
+    cooks_d = r**2 / (2 * MSE) * hii / (1 - hii) ** 2  # (r / (1 - hii)) ** 2 *  / (1 * 2)
+
+    return cooks_d
+
+
+def _disp_calc_helper_NB_legacy(
+    adata: AnnData, layers: str = "X", min_cells_detected: int = 1
+) -> Tuple[List[str], List[pd.DataFrame]]:
+    """Helper function to calculate the dispersion parameter.
+
+    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
+
+    Args:
+        adata: an Anndata object.
+        layers: the layer of data used for dispersion fitting. Defaults to "X".
+        min_cells_detected: the minimal required number of cells with expression for selecting gene for dispersion
+            fitting. Defaults to 1.
+
+    Returns:
+        layers: a list of layers available.
+        res_list: a list of pd.DataFrames with mu, dispersion for each gene that passes filters.
+    """
+    main_warning(__name__ + " is deprecated.")
+    layers = DKM.get_available_layer_keys(adata, layers=layers, include_protein=False)
+
+    res_list = []
+    for layer in layers:
+        if layer == "raw":
+            CM = adata.raw.X
+            szfactors = adata.obs[layer + "Size_Factor"][:, None]
+        elif layer == "X":
+            CM = adata.X
+            szfactors = adata.obs["Size_Factor"][:, None]
+        else:
+            CM = adata.layers[layer]
+            szfactors = adata.obs[layer + "Size_Factor"][:, None]
+
+        if issparse(CM):
+            CM.data = np.round(CM.data, 0)
+            rounded = CM
+        else:
+            rounded = CM.round().astype("int")
+
+        lowerDetectedLimit = adata.uns["lowerDetectedLimit"] if "lowerDetectedLimit" in adata.uns.keys() else 1
+        nzGenes = (rounded > lowerDetectedLimit).sum(axis=0)
+        nzGenes = nzGenes > min_cells_detected
+
+        nzGenes = nzGenes.A1 if issparse(rounded) else nzGenes
+        if layer.startswith("X_"):
+            x = rounded[:, nzGenes]
+        else:
+            x = (
+                rounded[:, nzGenes].multiply(csr_matrix(1 / szfactors))
+                if issparse(rounded)
+                else rounded[:, nzGenes] / szfactors
+            )
+
+        xim = np.mean(1 / szfactors) if szfactors is not None else 1
+
+        f_expression_mean = x.mean(axis=0)
+
+        # For NB: Var(Y) = mu * (1 + mu / k)
+        # x.A.var(axis=0, ddof=1)
+        f_expression_var = (
+            (x.multiply(x).mean(0).A1 - f_expression_mean.A1**2) * x.shape[0] / (x.shape[0] - 1)
+            if issparse(x)
+            else x.var(axis=0, ddof=0) ** 2
+        )  # np.mean(np.power(x - f_expression_mean, 2), axis=0) # variance with n - 1
+        # https://scialert.net/fulltext/?doi=ajms.2010.1.15 method of moments
+        disp_guess_meth_moments = f_expression_var - xim * f_expression_mean  # variance - mu
+
+        disp_guess_meth_moments = disp_guess_meth_moments / np.power(
+            f_expression_mean, 2
+        )  # this is dispersion parameter (1/k)
+
+        res = pd.DataFrame(
+            {
+                "mu": np.array(f_expression_mean).flatten(),
+                "disp": np.array(disp_guess_meth_moments).flatten(),
+            }
+        )
+        res.loc[res["mu"] == 0, "mu"] = None
+        res.loc[res["mu"] == 0, "disp"] = None
+        res.loc[res["disp"] < 0, "disp"] = 0
+
+        res["gene_id"] = adata.var_names[nzGenes]
+
+        res_list.append(res)
+
+    return layers, res_list
+
+
+def _parametric_dispersion_fit_legacy(
+    disp_table: pd.DataFrame, initial_coefs: np.ndarray = np.array([1e-6, 1])
+) -> Tuple[sm.formula.glm, np.ndarray, pd.DataFrame]:
+    """Perform the dispersion parameter fitting with initial guesses of coefficients.
+
+    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
+
+    Args:
+        disp_table: A pandas dataframe with mu, dispersion for each gene that passes filters.
+        initial_coefs: Initial parameters for the gamma fit of the dispersion parameters. Defaults to
+            np.array([1e-6, 1]).
+
+    Returns:
+        A tuple (fit, coefs, good), where fit is a statsmodels fitting object, coefs contains the two resulting gamma
+        fitting coefficient, and good is the subsetted dispersion table that is subjected to Gamma fitting.
+    """
+    main_warning(__name__ + " is deprecated.")
+    coefs = initial_coefs
+    iter = 0
+    while True:
+        residuals = disp_table["disp"] / (coefs[0] + coefs[1] / disp_table["mu"])
+        good = disp_table.loc[(residuals > initial_coefs[0]) & (residuals < 10000), :]
+        # https://stats.stackexchange.com/questions/356053/the-identity-link-function-does-not-respect-the-domain-of-the
+        # -gamma-family
+        fit = sm.formula.glm(
+            "disp ~ I(1 / mu)",
+            data=good,
+            family=sm.families.Gamma(link=sm.genmod.families.links.identity),
+        ).train(start_params=coefs)
+
+        oldcoefs = coefs
+        coefs = fit.params
+
+        if coefs[0] < initial_coefs[0]:
+            coefs[0] = initial_coefs[0]
+        if coefs[1] < 0:
+            main_warning("Parametric dispersion fit may be failed.")
+
+        if np.sum(np.log(coefs / oldcoefs) ** 2 < coefs[0]):
+            break
+        iter += 1
+
+        if iter > 10:
+            main_warning("Dispersion fit didn't converge")
+            break
+        if not np.all(coefs > 0):
+            main_warning("Parametric dispersion fit may be failed.")
+
+    return fit, coefs, good
+
+
+def _estimate_dispersion_legacy(
+    adata: AnnData,
+    layers: str = "X",
+    modelFormulaStr: str = "~ 1",
+    min_cells_detected: int = 1,
+    removeOutliers: bool = False,
+) -> AnnData:
+    """This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
+
+    Args:
+        adata: an AnnData object.
+        layers: the layer(s) to be used for calculating dispersion. Default is "X" if there is no spliced layers.
+        modelFormulaStr: the model formula used to calculate dispersion parameters. Not used. Defaults to "~ 1".
+        min_cells_detected: the minimum number of cells detected for calculating the dispersion. Defaults to 1.
+        removeOutliers: whether to remove outliers when performing dispersion fitting. Defaults to False.
+
+    Raises:
+        Exception: there is no valid DataFrames with mu for genes.
+
+    Returns:
+        An updated annData object with dispFitInfo added to uns attribute as a new key.
+    """
+    main_warning(__name__ + " is deprecated.")
+    logger = LoggerManager.gen_logger("dynamo-preprocessing")
+    # mu = None
+    model_terms = [x.strip() for x in re.compile("~|\\*|\\+").split(modelFormulaStr)]
+    model_terms = list(set(model_terms) - set([""]))
+
+    cds_pdata = adata.obs  # .loc[:, model_terms]
+    cds_pdata["rowname"] = cds_pdata.index.values
+    layers, disp_tables = _disp_calc_helper_NB_legacy(adata[:, :], layers, min_cells_detected)
+    # disp_table['disp'] = np.random.uniform(0, 10, 11)
+    # disp_table = cds_pdata.apply(disp_calc_helper_NB(adata[:, :], min_cells_detected))
+
+    # cds_pdata <- dplyr::group_by_(dplyr::select_(rownames_to_column(pData(cds)), "rowname", .dots=model_terms), .dots
+    # =model_terms)
+    # disp_table <- as.data.frame(cds_pdata %>% do(disp_calc_helper_NB(cds[,.$rowname], cds@expressionFamily, min_cells_
+    # detected)))
+    for ind in range(len(layers)):
+        layer, disp_table = layers[ind], disp_tables[ind]
+
+        if disp_table is None:
+            raise Exception("Parametric dispersion fitting failed, please set a different lowerDetectionLimit")
+
+        disp_table = disp_table.loc[np.where(disp_table["mu"] != np.nan)[0], :]
+
+        res = _parametric_dispersion_fit_legacy(disp_table)
+        fit, coefs, good = res[0], res[1], res[2]
+
+        if removeOutliers:
+            # influence = fit.get_influence().cooks_distance()
+            # #CD is the distance and p is p-value
+            # (CD, p) = influence.cooks_distance
+
+            CD = cook_dist(fit, 1 / good["mu"][:, None], good)
+            cooksCutoff = 4 / good.shape[0]
+            main_debug("Removing " + str(len(CD[CD > cooksCutoff])) + " outliers")
+            outliers = CD > cooksCutoff
+            # use CD.index.values? remove genes that lost when doing parameter fitting
+            lost_gene = set(good.index.values).difference(set(range(len(CD))))
+            outliers[lost_gene] = True
+            res = _parametric_dispersion_fit_legacy(good.loc[~outliers, :])
+
+            fit, coefs = res[0], res[1]
+
+        def ans(q):
+            return coefs[0] + coefs[1] / q
+
+        if layer == "X":
+            logger.info_insert_adata("dispFitInfo", "uns")
+            adata.uns["dispFitInfo"] = {
+                "disp_table": good,
+                "disp_func": ans,
+                "coefs": coefs,
+            }
+        else:
+            logger.info_insert_adata(layer + "_dispFitInfo", "uns")
+            adata.uns[layer + "_dispFitInfo"] = {
+                "disp_table": good,
+                "disp_func": ans,
+                "coefs": coefs,
+            }
+
+    return adata
+
+
+def _top_table_legacy(
+    adata: AnnData, layer: str = "X", mode: Literal["dispersion", "gini"] = "dispersion"
+) -> pd.DataFrame:
+    """Retrieve a table that contains gene names and other info whose dispersions/gini index are highest.
+
+    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
+
+    Get information of the top layer.
+
+    Args:
+        adata: an AnnData object.
+        layer: the layer(s) that would be searched for. Defaults to "X".
+        mode: either "dispersion" or "gini", deciding whether dispersion data or gini data would be acquired. Defaults
+            to "dispersion".
+
+    Raises:
+        KeyError: if mode is set to dispersion but there is no available dispersion model.
+
+    Returns:
+        The data frame of the top layer with the gene_id, mean_expression, dispersion_fit and dispersion_empirical as
+        the columns.
+    """
+
+    layer = DKM.get_available_layer_keys(adata, layers=layer, include_protein=False)[0]
+
+    if layer in ["X"]:
+        key = "dispFitInfo"
+    else:
+        key = layer + "_dispFitInfo"
+
+    if mode == "dispersion":
+        if adata.uns[key] is None:
+            _estimate_dispersion_legacy(adata, layers=[layer])
+            raise KeyError(
+                "Error: for adata.uns.key=%s, no dispersion model found. Please call estimate_dispersion() before calling this function"
+                % key
+            )
+
+        top_df = pd.DataFrame(
+            {
+                "gene_id": adata.uns[key]["disp_table"]["gene_id"],
+                "mean_expression": adata.uns[key]["disp_table"]["mu"],
+                "dispersion_fit": adata.uns[key]["disp_func"](adata.uns[key]["disp_table"]["mu"]),
+                "dispersion_empirical": adata.uns[key]["disp_table"]["disp"],
+            }
+        )
+        top_df = top_df.set_index("gene_id")
+
+    elif mode == "gini":
+        top_df = adata.var[layer + "_gini"]
+
+    return top_df
+
+
+def _calc_mean_var_dispersion_general_mat_legacy(
+    data_mat: Union[np.ndarray, csr_matrix], axis: int = 0
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """calculate mean, variance, and dispersion of a matrix.
+
+    Args:
+        data_mat: the matrix to be evaluated, either a ndarray or a scipy sparse matrix.
+        axis: the axis along which calculation is performed. Defaults to 0.
+
+    Returns:
+        A tuple (mean, var, dispersion) where mean is the mean of the array along the given axis, var is the variance of
+        the array along the given axis, and dispersion is the dispersion of the array along the given axis.
+    """
+
+    if not issparse(data_mat):
+        return _calc_mean_var_dispersion_ndarray_legacy(data_mat, axis)
+    else:
+        return _calc_mean_var_dispersion_sparse_legacy(data_mat, axis)
+
+
+def _calc_mean_var_dispersion_ndarray_legacy(
+    data_mat: np.ndarray, axis: int = 0
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """calculate mean, variance, and dispersion of a non-sparse matrix.
+
+    Args:
+        data_mat: the matrix to be evaluated.
+        axis: the axis along which calculation is performed. Defaults to 0.
+
+    Returns:
+        A tuple (mean, var, dispersion) where mean is the mean of the array along the given axis, var is the variance of
+        the array along the given axis, and dispersion is the dispersion of the array along the given axis.
+    """
+
+    # per gene mean, var and dispersion
+    mean = np.nanmean(data_mat, axis=axis).flatten()
+
+    # <class 'anndata._core.views.ArrayView'> has bug after using operator "==" (e.g. mean == 0), which changes mean.
+    mean = np.array(mean)
+    mean[mean == 0] += 1e-7  # prevent division by zero
+    var = np.nanvar(data_mat, axis=axis)
+    dispersion = var / mean
+    return mean.flatten(), var.flatten(), dispersion.flatten()
+
+
+def _calc_mean_var_dispersion_sparse_legacy(
+    sparse_mat: csr_matrix, axis: int = 0
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """calculate mean, variance, and dispersion of a matrix.
+
+    Args:
+        sparse_mat: the sparse matrix to be evaluated.
+        axis: the axis along which calculation is performed. Defaults to 0.
+
+    Returns:
+        A tuple (mean, var, dispersion), where mean is the mean of the array along the given axis, var is the variance
+        of the array along the given axis, and dispersion is the dispersion of the array along the given axis.
+    """
+
+    sparse_mat = sparse_mat.copy()
+    nan_mask = get_nan_or_inf_data_bool_mask(sparse_mat.data)
+    temp_val = (sparse_mat != 0).sum(axis)
+    sparse_mat.data[nan_mask] = 0
+    nan_count = temp_val - (sparse_mat != 0).sum(axis)
+
+    non_nan_count = sparse_mat.shape[axis] - nan_count
+    mean = (sparse_mat.sum(axis) / sparse_mat.shape[axis]).A1
+    mean[mean == 0] += 1e-7  # prevent division by zero
+
+    # same as numpy var behavior: denominator is N, var=(data_arr-mean)/N
+    var = np.power(sparse_mat - mean, 2).sum(axis) / sparse_mat.shape[axis]
+    dispersion = var / mean
+    return np.array(mean).flatten(), np.array(var).flatten(), np.array(dispersion).flatten()
+
+
+@deprecated
+def calc_sz_factor_legacy(*args, **kwargs):
+    _calc_sz_factor_legacy(*args, **kwargs)
+
+
+def _calc_sz_factor_legacy(
     adata_ori: anndata.AnnData,
     layers: Union[str, list] = "all",
     total_layers: Union[list, None] = None,
     splicing_total_layers: bool = False,
     X_total_layers: bool = False,
     locfunc: Callable = np.nanmean,
     round_exprs: bool = False,
@@ -185,15 +651,20 @@
             adata.obs["initial_" + layer + "_cell_size"] = cell_total
 
     adata_ori = merge_adata_attrs(adata_ori, adata, attr="obs")
 
     return adata_ori
 
 
-def normalize_cell_expr_by_size_factors_legacy(
+@deprecated
+def normalize_cell_expr_by_size_factors(*args, **kwargs):
+    _normalize_cell_expr_by_size_factors_legacy(*args, **kwargs)
+
+
+def _normalize_cell_expr_by_size_factors_legacy(
     adata: anndata.AnnData,
     layers: str = "all",
     total_szfactor: str = "total_Size_Factor",
     splicing_total_layers: bool = False,
     X_total_layers: bool = False,
     norm_method: Union[Callable, Literal["clr"], None] = None,
     pseudo_expr: int = 1,
@@ -267,15 +738,15 @@
         if layer in excluded_layers:
             szfactors, CM = get_sz_exprs(adata, layer, total_szfactor=None)
         else:
             szfactors, CM = get_sz_exprs(adata, layer, total_szfactor=total_szfactor)
 
         if norm_method is None and layer == "X":
             CM = normalize_mat_monocle(CM, szfactors, relative_expr, pseudo_expr, np.log1p)
-        elif norm_method in [np.log1p, np.log, np.log2, Freeman_Tukey, None] and layer != "protein":
+        elif norm_method in [np.log1p, np.log, np.log2, _Freeman_Tukey, None] and layer != "protein":
             CM = normalize_mat_monocle(CM, szfactors, relative_expr, pseudo_expr, norm_method)
         elif layer == "protein":  # norm_method == 'clr':
             if norm_method != "clr":
                 main_warning(
                     "For protein data, log transformation is not recommended. Using clr normalization by default."
                 )
             """This normalization implements the centered log-ratio (CLR) normalization from Seurat which is computed
@@ -301,204 +772,26 @@
             adata.X = CM
         elif layer == "protein" and "protein" in adata.obsm_keys():
             main_info_insert_adata_obsm("X_protein")
             adata.obsm["X_protein"] = CM
         else:
             adata.layers["X_" + layer] = CM
 
-        adata.uns["pp"]["norm_method"] = norm_method.__name__ if callable(norm_method) else norm_method
+        norm_method_key = "X_norm_method" if layer == DKM.X_LAYER else "layers_norm_method"
+        adata.uns["pp"][norm_method_key] = norm_method.__name__ if callable(norm_method) else norm_method
 
     return adata
 
 
-def Gini(adata: anndata.AnnData, layers: Union[Literal["all"], List[str]] = "all") -> anndata.AnnData:
-    """Calculate the Gini coefficient of a numpy array. https://github.com/thomasmaxwellnorman/perturbseq_demo/blob/master/perturbseq/util.py
-
-    Args:
-        adata: an AnnData object
-        layers: the layer(s) to be normalized. Defaults to "all".
-
-    Returns:
-        An updated anndata object with gini score for the layers (include .X) in the corresponding var columns (layer + '_gini').
-    """
-
-    # From: https://github.com/oliviaguest/gini
-    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif
-    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm
-
-    layers = DynamoAdataKeyManager.get_available_layer_keys(adata, layers)
-
-    for layer in layers:
-        if layer == "raw":
-            CM = adata.raw.X
-        elif layer == "X":
-            CM = adata.X
-        elif layer == "protein":
-            if "protein" in adata.obsm_keys():
-                CM = adata.obsm[layer]
-            else:
-                continue
-        else:
-            CM = adata.layers[layer]
-
-        n_features = adata.shape[1]
-        gini = np.zeros(n_features)
-
-        for i in np.arange(n_features):
-            # all values are treated equally, arrays must be 1d
-            cur_cm = CM[:, i].A if issparse(CM) else CM[:, i]
-            if np.amin(CM) < 0:
-                cur_cm -= np.amin(cur_cm)  # values cannot be negative
-            cur_cm += 0.0000001  # np.min(array[array!=0]) #values cannot be 0
-            cur_cm = np.sort(cur_cm)  # values must be sorted
-            # index per array element
-            index = np.arange(1, cur_cm.shape[0] + 1)
-            n = cur_cm.shape[0]  # number of array elements
-            gini[i] = (np.sum((2 * index - n - 1) * cur_cm)) / (n * np.sum(cur_cm))  # Gini coefficient
-
-        if layer in ["raw", "X"]:
-            adata.var["gini"] = gini
-        else:
-            adata.var[layer + "_gini"] = gini
-
-    return adata
-
-
-def disp_calc_helper_NB(
-    adata: anndata.AnnData, layers: str = "X", min_cells_detected: int = 1
-) -> Tuple[List[str], List[pd.DataFrame]]:
-    """Calculate the dispersion parameter of the negative binomial distribution.
-
-    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
-
-    Args:
-        adata: an adata object
-        layers: the layer of data used for dispersion fitting. Defaults to "X".
-        min_cells_detected: the minimal required number of cells with expression for selecting gene for dispersion
-            fitting. Defaults to 1.
-
-    Returns:
-        A tuple (layers, res_list), where layers is a list of layers available and res_list is a list of pd.DataFrame's
-        with mu, dispersion for each gene that passes filters.
-    """
-
-    layers = DynamoAdataKeyManager.get_available_layer_keys(adata, layers=layers, include_protein=False)
-
-    res_list = []
-    for layer in layers:
-        if layer == "raw":
-            CM = adata.raw.X
-            szfactors = adata.obs[layer + "Size_Factor"][:, None]
-        elif layer == "X":
-            CM = adata.X
-            szfactors = adata.obs["Size_Factor"][:, None]
-        else:
-            CM = adata.layers[layer]
-            szfactors = adata.obs[layer + "Size_Factor"][:, None]
-
-        if issparse(CM):
-            CM.data = np.round(CM.data, 0)
-            rounded = CM
-        else:
-            rounded = CM.round().astype("int")
-
-        lowerDetectedLimit = adata.uns["lowerDetectedLimit"] if "lowerDetectedLimit" in adata.uns.keys() else 1
-        nzGenes = (rounded > lowerDetectedLimit).sum(axis=0)
-        nzGenes = nzGenes > min_cells_detected
-
-        nzGenes = nzGenes.A1 if issparse(rounded) else nzGenes
-        if layer.startswith("X_"):
-            x = rounded[:, nzGenes]
-        else:
-            x = (
-                rounded[:, nzGenes].multiply(csr_matrix(1 / szfactors))
-                if issparse(rounded)
-                else rounded[:, nzGenes] / szfactors
-            )
-
-        xim = np.mean(1 / szfactors) if szfactors is not None else 1
-
-        f_expression_mean = x.mean(axis=0)
+@deprecated
+def filter_cells_legacy(*args, **kwargs):
+    _filter_cells_legacy(*args, **kwargs)
 
-        # For NB: Var(Y) = mu * (1 + mu / k)
-        # x.A.var(axis=0, ddof=1)
-        f_expression_var = (
-            (x.multiply(x).mean(0).A1 - f_expression_mean.A1**2) * x.shape[0] / (x.shape[0] - 1)
-            if issparse(x)
-            else x.var(axis=0, ddof=0) ** 2
-        )  # np.mean(np.power(x - f_expression_mean, 2), axis=0) # variance with n - 1
-        # https://scialert.net/fulltext/?doi=ajms.2010.1.15 method of moments
-        disp_guess_meth_moments = f_expression_var - xim * f_expression_mean  # variance - mu
 
-        disp_guess_meth_moments = disp_guess_meth_moments / np.power(
-            f_expression_mean, 2
-        )  # this is dispersion parameter (1/k)
-
-        res = pd.DataFrame(
-            {
-                "mu": np.array(f_expression_mean).flatten(),
-                "disp": np.array(disp_guess_meth_moments).flatten(),
-            }
-        )
-        res.loc[res["mu"] == 0, "mu"] = None
-        res.loc[res["mu"] == 0, "disp"] = None
-        res.loc[res["disp"] < 0, "disp"] = 0
-
-        res["gene_id"] = adata.var_names[nzGenes]
-
-        res_list.append(res)
-
-    return layers, res_list
-
-
-def vstExprs(
-    adata: anndata.AnnData,
-    expr_matrix: Union[np.ndarray, None] = None,
-    round_vals: bool = True,
-) -> np.ndarray:
-    """Variance stabilization transformation of the gene expression.
-
-    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
-
-    Args:
-        adata: an AnnData object.
-        expr_matrix: an matrix of values to transform. Must be normalized (e.g. by size factors) already. Defaults to
-            None.
-        round_vals: whether to round expression values to the nearest integer before applying the transformation.
-            Defaults to True.
-
-    Returns:
-        A numpy array of the gene expression after VST.
-    """
-
-    fitInfo = adata.uns["dispFitInfo"]
-
-    coefs = fitInfo["coefs"]
-    if expr_matrix is None:
-        ncounts = adata.X
-        if round_vals:
-            if issparse(ncounts):
-                ncounts.data = np.round(ncounts.data, 0)
-            else:
-                ncounts = ncounts.round().astype("int")
-    else:
-        ncounts = expr_matrix
-
-    def vst(q):  # c( "asymptDisp", "extraPois" )
-        return np.log(
-            (1 + coefs[1] + 2 * coefs[0] * q + 2 * np.sqrt(coefs[0] * q * (1 + coefs[1] + coefs[0] * q)))
-            / (4 * coefs[0])
-        ) / np.log(2)
-
-    res = vst(ncounts.toarray()) if issparse(ncounts) else vst(ncounts)
-
-    return res
-
-
-def filter_cells_legacy(
+def _filter_cells_legacy(
     adata: anndata.AnnData,
     filter_bool: Union[np.ndarray, None] = None,
     layer: str = "all",
     keep_filtered: bool = False,
     min_expr_genes_s: int = 50,
     min_expr_genes_u: int = 25,
     min_expr_genes_p: int = 1,
@@ -576,51 +869,20 @@
     else:
         adata._inplace_subset_obs(filter_bool)
         adata.obs["pass_basic_filter"] = True
 
     return adata
 
 
-def filter_genes_by_clusters_(
-    adata: anndata.AnnData,
-    cluster: str,
-    min_avg_U: float = 0.02,
-    min_avg_S: float = 0.08,
-    size_limit: int = 40,
-) -> np.ndarray:
-    """Prepare filtering genes on the basis of cluster-wise expression threshold.
-
-    This function is taken from velocyto in order to reproduce velocyto's DentateGyrus notebook.
-
-    Args:
-        adata: an Anndata object.
-        cluster: a column name in the adata.obs attribute which will be used for cluster specific expression filtering.
-        min_avg_U: include genes that have unspliced average bigger than `min_avg_U` in at least one of the clusters.
-            Defaults to 0.02.
-        min_avg_S: include genes that have spliced average bigger than `min_avg_U` in at least one of the clusters.
-            Defaults to 0.08.
-        size_limit: the max number of members to be considered in a cluster during calculation. Defaults to 40.
-
-    Returns:
-        A boolean array corresponding to genes selected.
-    """
-    U, S, cluster_uid = (
-        adata.layers["unspliced"],
-        adata.layers["spliced"],
-        adata.obs[cluster],
-    )
-    cluster_uid, cluster_ix = np.unique(cluster_uid, return_inverse=True)
-
-    U_avgs, S_avgs = clusters_stats(U, S, cluster_uid, cluster_ix, size_limit=size_limit)
-    clu_avg_selected = (U_avgs.max(1) > min_avg_U) & (S_avgs.max(1) > min_avg_S)
-
-    return clu_avg_selected
+@deprecated
+def filter_genes_by_outliers_legacy(*args, **kwargs):
+    _filter_genes_by_outliers_legacy(*args, **kwargs)
 
 
-def filter_genes_by_outliers_legacy(
+def _filter_genes_by_outliers_legacy(
     adata: anndata.AnnData,
     filter_bool: Union[np.ndarray, None] = None,
     layer: str = "all",
     min_cell_s: int = 1,
     min_cell_u: int = 1,
     min_cell_p: int = 1,
     min_avg_exp_s: float = 1e-10,
@@ -722,15 +984,20 @@
     filter_bool = filter_bool & detected_bool if filter_bool is not None else detected_bool
 
     adata.var["pass_basic_filter"] = np.array(filter_bool).flatten()
 
     return adata
 
 
-def recipe_monocle(
+@deprecated
+def recipe_monocle(*args, **kwargs):
+    _recipe_monocle_legacy(*args, **kwargs)
+
+
+def _recipe_monocle_legacy(
     adata: anndata.AnnData,
     reset_X: bool = False,
     tkey: Union[str, None] = None,
     t_label_keys: Union[str, List[str], None] = None,
     experiment_type: Optional[str] = None,
     normalized: Union[bool, None] = None,
     layer: Union[str, None] = None,
@@ -864,14 +1131,15 @@
         ValueError: genes_to_append contains invalid genes.
 
     Returns:
         A new updated anndata object if `copy` arg is `True`. In the object, Size_Factor, normalized expression values,
         X and reduced dimensions, etc., are updated. Otherwise, return None.
     """
 
+    main_warning(__name__ + " is deprecated.")
     logger = LoggerManager.gen_logger("dynamo-preprocessing")
     logger.log_time()
     keep_filtered_cells = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_cells, DynamoAdataConfig.RECIPE_MONOCLE_KEEP_FILTERED_CELLS_KEY
     )
     keep_filtered_genes = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_genes, DynamoAdataConfig.RECIPE_MONOCLE_KEEP_FILTERED_GENES_KEY
@@ -889,15 +1157,15 @@
     adata = convert2symbol(adata, scopes=scopes)
     n_cells, n_genes = adata.n_obs, adata.n_vars
 
     # Since convert2symbol may subset adata and generate a new AnnData object,
     # we should create all following data after convert2symbol (gene names)
     adata.uns["pp"] = {}
     if norm_method == "Freeman_Tukey":
-        norm_method = Freeman_Tukey
+        norm_method = _Freeman_Tukey
 
     basic_stats(adata)
     (
         has_splicing,
         has_labeling,
         splicing_labeling,
         has_protein,
@@ -1073,15 +1341,15 @@
         "shared_count": None,
     }
     if fc_kwargs is not None:
         filter_cells_kwargs.update(fc_kwargs)
 
     logger.info("filtering cells...")
     logger.info_insert_adata("pass_basic_filter", "obs")
-    adata = filter_cells_legacy(adata, keep_filtered=keep_filtered_cells, **filter_cells_kwargs)
+    adata = _filter_cells_legacy(adata, keep_filtered=keep_filtered_cells, **filter_cells_kwargs)
     logger.info(f"{adata.obs.pass_basic_filter.sum()} cells passed basic filters.")
 
     filter_genes_kwargs = {
         "filter_bool": None,
         "layer": "all",
         "min_cell_s": max(5, 0.01 * n_cells),
         "min_cell_u": max(5, 0.005 * n_cells),
@@ -1097,15 +1365,15 @@
     }
     if fg_kwargs is not None:
         filter_genes_kwargs.update(fg_kwargs)
 
     # set pass_basic_filter for genes
     logger.info("filtering gene...")
     logger.info_insert_adata("pass_basic_filter", "var")
-    adata = filter_genes_by_outliers_legacy(
+    adata = _filter_genes_by_outliers_legacy(
         adata,
         **filter_genes_kwargs,
     )
     logger.info(f"{adata.var.pass_basic_filter.sum()} genes passed basic filters.")
 
     if adata.var.pass_basic_filter.sum() == 0:
         logger.error(
@@ -1115,15 +1383,15 @@
     if adata.obs.pass_basic_filter.sum() == 0:
         logger.error("No cells pass basic filters. Please check your data or arguments, for example, fc_kwargs.")
         raise Exception()
 
     # calculate sz factor
     logger.info("calculating size factor...")
     if not _has_szFactor_normalized or "Size_Factor" not in adata.obs_keys():
-        adata = calc_sz_factor_legacy(
+        adata = _calc_sz_factor_legacy(
             adata,
             total_layers=total_layers,
             scale_to=scale_to,
             splicing_total_layers=splicing_total_layers,
             X_total_layers=X_total_layers,
             layers=layer if type(layer) is list else "all",
             genes_use_for_norm=genes_use_for_norm,
@@ -1149,15 +1417,15 @@
         if pass_basic_filter_num < n_top_genes:
             logger.warning(
                 f"only {pass_basic_filter_num} genes passed basic filtering, but you requested {n_top_genes} "
                 f"genes for feature selection. Try lowering the gene selection stringency: "
                 f"{select_genes_dict}",
             )
         logger.info("selecting genes in layer: %s, sort method: %s..." % (feature_selection_layer, feature_selection))
-        adata = select_genes_monocle_legacy(
+        adata = _select_genes_monocle_legacy(
             adata,
             layer=feature_selection_layer,
             sort_by=feature_selection,
             n_top_genes=n_top_genes,
             keep_filtered=True,  # TODO double check if should comply with the argument keep_filtered_genes
             SVRs_kwargs=select_genes_dict,
         )
@@ -1203,15 +1471,15 @@
         else:
             valid_ids = adata.var.index.difference(genes_to_exclude)
 
         if extra_n_top_genes > 0:
             # let us ignore the `inplace` parameter in pandas.Categorical.remove_unused_categories  warning.
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore")
-                filter_bool = select_genes_monocle_legacy(
+                filter_bool = _select_genes_monocle_legacy(
                     adata[:, valid_ids],
                     sort_by=feature_selection,
                     n_top_genes=extra_n_top_genes,
                     keep_filtered=True,  # no effect to adata
                     SVRs_kwargs=select_genes_dict,
                     only_bools=True,
                 )
@@ -1222,35 +1490,35 @@
         logger.info("Discarding genes that failed the filtering...")
         adata._inplace_subset_var(adata.var["use_for_pca"])
 
     # normalized data based on sz factor
     if not _has_log1p_transformed:
         total_szfactor = "total_Size_Factor" if total_layers is not None else None
         logger.info("size factor normalizing the data, followed by log1p transformation.")
-        adata = normalize_cell_expr_by_size_factors_legacy(
+        adata = _normalize_cell_expr_by_size_factors_legacy(
             adata,
             layers=layer if type(layer) is list else "all",
             total_szfactor=total_szfactor,
             splicing_total_layers=splicing_total_layers,
             X_total_layers=X_total_layers,
             norm_method=norm_method,
             pseudo_expr=pseudo_expr,
             relative_expr=relative_expr,
             keep_filtered=keep_filtered_genes,
             sz_method=sz_method,
             scale_to=scale_to,
         )
     else:
         layers = DynamoAdataKeyManager.get_available_layer_keys(adata, "all")
-        for layer in layers:
-            if layer != "X":
-                logger.info_insert_adata("X_" + layer, "layers")
-                adata.layers["X_" + layer] = adata.layers[layer].copy()
-        logger.info_insert_adata("norm_method", "uns['pp']", indent_level=2)
-        adata.uns["pp"]["norm_method"] = None
+        for tmp_layer in layers:
+            if tmp_layer != "X":
+                logger.info_insert_adata("X_" + tmp_layer, "layers")
+                adata.layers["X_" + tmp_layer] = adata.layers[tmp_layer].copy()
+        logger.info_insert_adata("layers_norm_method", "uns['pp']", indent_level=2)
+        adata.uns["pp"]["layers_norm_method"] = None
 
     # only use genes pass filter (based on use_for_pca) to perform dimension reduction.
     if layer is None:
         pca_input = adata.X[:, adata.var.use_for_pca.values]
     else:
         if "X" in layer:
             pca_input = adata.X[:, adata.var.use_for_pca.values]
@@ -1281,15 +1549,15 @@
         )
 
     adata.var.iloc[bad_genes, adata.var.columns.tolist().index("use_for_pca")] = False
     pca_input = pca_input[:, valid_ind]
     logger.info("applying %s ..." % (method.upper()))
 
     if method == "pca":
-        adata = pca_monocle(adata, pca_input, num_dim, "X_" + method.lower())
+        adata = pca(adata, pca_input, num_dim, "X_" + method.lower())
         # TODO remove adata.obsm["X"] in future, use adata.obsm.X_pca instead
         adata.obsm["X"] = adata.obsm["X_" + method.lower()]
 
     elif method == "ica":
         fit = FastICA(
             num_dim,
             algorithm="deflation",
@@ -1339,15 +1607,20 @@
     logger.finish_progress(progress_name="recipe_monocle preprocess")
 
     if copy:
         return adata
     return None
 
 
-def recipe_velocyto(
+@deprecated
+def recipe_velocyto(*args, **kwargs):
+    _recipe_velocyto_legacy(*args, **kwargs)
+
+
+def _recipe_velocyto_legacy(
     adata: anndata.AnnData,
     total_layers: Union[List[str], None] = None,
     method: str = "pca",
     num_dim: int = 30,
     norm_method: Union[Callable, str, None] = None,
     pseudo_expr: int = 1,
     feature_selection: str = "SVR",
@@ -1394,15 +1667,15 @@
 
     adata = filter_cells_legacy(adata, filter_bool=np.array(filter_bool).flatten())
 
     filter_bool = filter_genes_by_outliers(adata, min_cell_s=30, min_count_s=40, shared_count=None)
 
     adata = adata[:, filter_bool]
 
-    adata = SVRs(
+    adata = calc_dispersion_by_svr(
         adata,
         layers=["spliced"],
         min_expr_cells=2,
         max_expr_avg=35,
         min_expr_avg=0,
     )
 
@@ -1413,19 +1686,19 @@
         adata,
         min_cell_s=0,
         min_count_s=0,
         min_count_u=25,
         min_cell_u=20,
         shared_count=None,
     )
-    filter_bool_cluster = filter_genes_by_clusters_(adata, min_avg_S=0.08, min_avg_U=0.01, cluster=cluster)
+    filter_bool_cluster = filter_genes_by_clusters(adata, min_avg_S=0.08, min_avg_U=0.01, cluster=cluster)
 
     adata = adata[:, filter_bool_gene & filter_bool_cluster]
 
-    adata = normalize_cell_expr_by_size_factors_legacy(
+    adata = normalize_cell_expr_by_size_factors(
         adata,
         total_szfactor=None,
         norm_method=norm_method,
         pseudo_expr=pseudo_expr,
         relative_expr=relative_expr,
         keep_filtered=keep_filtered_genes,
     )
@@ -1434,15 +1707,15 @@
     valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
     valid_ind = np.array(valid_ind).flatten()
     adata.var.use_for_pca[np.where(adata.var.use_for_pca)[0][~valid_ind]] = False
 
     CM = CM[:, valid_ind]
 
     if method == "pca":
-        adata, fit, _ = pca_monocle(adata, CM, num_dim, "X_" + method.lower(), return_all=True)
+        adata, fit, _ = pca(adata, CM, num_dim, "X_" + method.lower(), return_all=True)
         # adata.obsm['X_' + method.lower()] = reduce_dim
 
     elif method == "ica":
         cm_genesums = CM.sum(axis=0)
         valid_ind = (np.isfinite(cm_genesums)) + (cm_genesums != 0)
         valid_ind = np.array(valid_ind).flatten()
         CM = CM[:, valid_ind]
@@ -1468,113 +1741,20 @@
     ntr = calc_new_to_total_ratio(adata)
     if ntr is not None:
         adata.obs["ntr"] = ntr
 
     return adata
 
 
-def highest_frac_genes(
-    adata: AnnData,
-    store_key: str = "highest_frac_genes",
-    n_top: int = 30,
-    gene_prefix_list: List[str] = None,
-    gene_prefix_only: bool = False,
-    layer: Union[str, None] = None,
-) -> anndata.AnnData:
-    """Compute top genes df and store results in `adata.uns`
+@deprecated
+def select_genes_monocle_legacy(*args, **kwargs):
+    _select_genes_monocle_legacy(*args, **kwargs)
 
-    Args:
-        adata: an AnnData object
-        store_key: key for storing expression percent results. Defaults to "highest_frac_genes".
-        n_top: number of top genes to show. Defaults to 30.
-        gene_prefix_list: a list of gene name prefixes used for gathering/calculating expression percents from genes
-            with these prefixes. Defaults to None.
-        gene_prefix_only: whether to calculate percentages for gene groups with the specified prefixes only. It only
-            takes effect if gene prefix list is provided. Defaults to False.
-        layer: layer on which the gene percents will be computed. Defaults to None.
 
-    Returns:
-        An updated adata with top genes df stored in `adata.uns`
-    """
-
-    gene_mat = adata.X
-    if layer is not None:
-        gene_mat = DKM.select_layer_data(layer)
-    # compute gene percents at each cell row
-    cell_expression_sum = gene_mat.sum(axis=1).flatten()
-    # get rid of cells that have all zero counts
-    not_all_zero = cell_expression_sum != 0
-    filtered_adata = adata[not_all_zero, :]
-    cell_expression_sum = cell_expression_sum[not_all_zero]
-    main_info("%d rows(cells or subsets) are not zero. zero total RNA cells are removed." % np.sum(not_all_zero))
-
-    valid_gene_set = set()
-    prefix_to_genes = {}
-    _adata = filtered_adata
-    if gene_prefix_list is not None:
-        prefix_to_genes = {prefix: [] for prefix in gene_prefix_list}
-        for name in _adata.var_names:
-            for prefix in gene_prefix_list:
-                length = len(prefix)
-                if name[:length] == prefix:
-                    valid_gene_set.add(name)
-                    prefix_to_genes[prefix].append(name)
-                    break
-        if len(valid_gene_set) == 0:
-            main_critical("NO VALID GENES FOUND WITH REQUIRED GENE PREFIX LIST, GIVING UP PLOTTING")
-            return None
-        if gene_prefix_only:
-            # gathering gene prefix set data
-            df = pd.DataFrame(index=_adata.obs.index)
-            for prefix in prefix_to_genes:
-                if len(prefix_to_genes[prefix]) == 0:
-                    main_info("There is no %s gene prefix in adata." % prefix)
-                    continue
-                df[prefix] = _adata[:, prefix_to_genes[prefix]].X.sum(axis=1)
-            # adata = adata[:, list(valid_gene_set)]
-
-            _adata = AnnData(X=df)
-            gene_mat = _adata.X
-
-    # compute gene's total percents in the dataset
-    gene_percents = np.array(gene_mat.sum(axis=0))
-    gene_percents = (gene_percents / gene_mat.shape[1]).flatten()
-    # obtain top genes
-    sorted_indices = np.argsort(-gene_percents)
-    selected_indices = sorted_indices[:n_top]
-    gene_names = _adata.var_names[selected_indices]
-
-    gene_X_percents = gene_mat / cell_expression_sum.reshape([-1, 1])
-
-    # assemble a dataframe
-    selected_gene_X_percents = np.array(gene_X_percents)[:, selected_indices]
-    selected_gene_X_percents = np.squeeze(selected_gene_X_percents)
-
-    top_genes_df = pd.DataFrame(
-        selected_gene_X_percents,
-        index=adata.obs_names,
-        columns=gene_names,
-    )
-    gene_percents_df = pd.Series(gene_percents, index=_adata.var_names)
-
-    main_info_insert_adata_uns(store_key)
-    adata.uns[store_key] = {
-        "top_genes_df": top_genes_df,
-        "gene_mat": gene_mat,
-        "layer": layer,
-        "selected_indices": selected_indices,
-        "gene_prefix_list": gene_prefix_list,
-        "show_individual_prefix_gene": gene_prefix_only,
-        "gene_percents": gene_percents_df,
-    }
-
-    return adata
-
-
-def select_genes_monocle_legacy(
+def _select_genes_monocle_legacy(
     adata: anndata.AnnData,
     layer: str = "X",
     total_szfactor: str = "total_Size_Factor",
     keep_filtered: bool = True,
     sort_by: str = "SVR",
     n_top_genes: int = 2000,
     SVRs_kwargs: dict = {},
@@ -1611,25 +1791,25 @@
         else np.ones(adata.shape[1], dtype=bool)
     )
 
     if adata.shape[1] <= n_top_genes:
         filter_bool = np.ones(adata.shape[1], dtype=bool)
     else:
         if sort_by == "dispersion":
-            table = top_table(adata, layer, mode="dispersion")
+            table = _top_table_legacy(adata, layer, mode="dispersion")
             valid_table = table.query("dispersion_empirical > dispersion_fit")
             valid_table = valid_table.loc[
                 set(adata.var.index[filter_bool]).intersection(valid_table.index),
                 :,
             ]
             gene_id = np.argsort(-valid_table.loc[:, "dispersion_empirical"])[:n_top_genes]
             gene_id = valid_table.iloc[gene_id, :].index
             filter_bool = adata.var.index.isin(gene_id)
         elif sort_by == "gini":
-            table = top_table(adata, layer, mode="gini")
+            table = _top_table_legacy(adata, layer, mode="gini")
             valid_table = table.loc[filter_bool, :]
             gene_id = np.argsort(-valid_table.loc[:, "gini"])[:n_top_genes]
             gene_id = valid_table.index[gene_id]
             filter_bool = gene_id.isin(adata.var.index)
         elif sort_by == "SVR":
             SVRs_args = {
                 "min_expr_cells": 0,
@@ -1637,23 +1817,25 @@
                 "max_expr_avg": np.inf,
                 "svr_gamma": None,
                 "winsorize": False,
                 "winsor_perc": (1, 99.5),
                 "sort_inverse": False,
             }
             SVRs_args = update_dict(SVRs_args, SVRs_kwargs)
-            adata = SVRs(
+
+            adata = calc_dispersion_by_svr(
                 adata,
                 layers=[layer],
                 total_szfactor=total_szfactor,
                 filter_bool=filter_bool,
                 **SVRs_args,
             )
 
             filter_bool = get_svr_filter(adata, layer=layer, n_top_genes=n_top_genes, return_adata=False)
+            condition = filter_bool == True
 
     # filter genes by gene expression fraction as well
     adata.var["frac"], invalid_ids = compute_gene_exp_fraction(X=adata.X, threshold=exprs_frac_for_gene_exclusion)
     genes_to_exclude = (
         list(adata.var_names[invalid_ids])
         if genes_to_exclude is None
         else genes_to_exclude + list(adata.var_names[invalid_ids])
```

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/preprocess_monocle_utils.py` & `dynamo-release-1.3.0/dynamo/tools/utils_reduceDimension.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,302 +1,318 @@
-import re
-from typing import List, Tuple
+import warnings
+from typing import List, Optional, Tuple
+
+import numpy as np
 
 try:
     from typing import Literal
-except ImportError:
+except:
     from typing_extensions import Literal
 
-import anndata
-import numpy as np
-import pandas as pd
-import statsmodels.api as sm
-from scipy.sparse import csr_matrix, issparse
-
-from ..configuration import DKM, DynamoAdataConfig, DynamoAdataKeyManager
-from ..dynamo_logger import (
-    LoggerManager,
-    main_critical,
-    main_info,
-    main_info_insert_adata_obsm,
-    main_warning,
-)
-from .utils import cook_dist
-
-
-def parametric_dispersion_fit(
-    disp_table: pd.DataFrame, initial_coefs: np.ndarray = np.array([1e-6, 1])
-) -> Tuple[sm.formula.glm, np.ndarray, pd.DataFrame]:
-    """Perform the dispersion parameter fitting with initial guesses of coefficients.
-
-    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
-
-    Args:
-        disp_table: A pandas dataframe with mu, dispersion for each gene that passes filters.
-        initial_coefs: Initial parameters for the gamma fit of the dispersion parameters. Defaults to
-            np.array([1e-6, 1]).
-
-    Returns:
-        A tuple (fit, coefs, good), where fit is a statsmodels fitting object, coefs contains the two resulting gamma
-        fitting coefficient, and good is the the subsetted dispersion table that is subjected to Gamma fitting.
-    """
-
-    coefs = initial_coefs
-    iter = 0
-    while True:
-        residuals = disp_table["disp"] / (coefs[0] + coefs[1] / disp_table["mu"])
-        good = disp_table.loc[(residuals > initial_coefs[0]) & (residuals < 10000), :]
-        # https://stats.stackexchange.com/questions/356053/the-identity-link-function-does-not-respect-the-domain-of-the
-        # -gamma-family
-        fit = sm.formula.glm(
-            "disp ~ I(1 / mu)",
-            data=good,
-            family=sm.families.Gamma(link=sm.genmod.families.links.identity),
-        ).train(start_params=coefs)
-
-        oldcoefs = coefs
-        coefs = fit.params
-
-        if coefs[0] < initial_coefs[0]:
-            coefs[0] = initial_coefs[0]
-        if coefs[1] < 0:
-            main_warning("Parametric dispersion fit may be failed.")
-
-        if np.sum(np.log(coefs / oldcoefs) ** 2 < coefs[0]):
-            break
-        iter += 1
-
-        if iter > 10:
-            main_warning("Dispersion fit didn't converge")
-            break
-        if not np.all(coefs > 0):
-            main_warning("Parametric dispersion fit may be failed.")
-
-    return fit, coefs, good
-
-
-def disp_calc_helper_NB(
-    adata: anndata.AnnData, layers: str = "X", min_cells_detected: int = 1
-) -> Tuple[List[str], List[pd.DataFrame]]:
-    """Helper function to calculate the dispersion parameter.
-
-    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
-
-    Args:
-        adata: an Anndata object.
-        layers: the layer of data used for dispersion fitting. Defaults to "X".
-        min_cells_detected: the minimal required number of cells with expression for selecting gene for dispersion
-            fitting. Defaults to 1.
-
-    Returns:
-        layers: a list of layers available.
-        res_list: a list of pd.DataFrames with mu, dispersion for each gene that passes filters.
-    """
-    layers = DynamoAdataKeyManager.get_available_layer_keys(adata, layers=layers, include_protein=False)
-
-    res_list = []
-    for layer in layers:
-        if layer == "raw":
-            CM = adata.raw.X
-            szfactors = adata.obs[layer + "Size_Factor"][:, None]
-        elif layer == "X":
-            CM = adata.X
-            szfactors = adata.obs["Size_Factor"][:, None]
-        else:
-            CM = adata.layers[layer]
-            szfactors = adata.obs[layer + "Size_Factor"][:, None]
-
-        if issparse(CM):
-            CM.data = np.round(CM.data, 0)
-            rounded = CM
-        else:
-            rounded = CM.round().astype("int")
+from anndata import AnnData
 
-        lowerDetectedLimit = adata.uns["lowerDetectedLimit"] if "lowerDetectedLimit" in adata.uns.keys() else 1
-        nzGenes = (rounded > lowerDetectedLimit).sum(axis=0)
-        nzGenes = nzGenes > min_cells_detected
-
-        nzGenes = nzGenes.A1 if issparse(rounded) else nzGenes
-        if layer.startswith("X_"):
-            x = rounded[:, nzGenes]
-        else:
-            x = (
-                rounded[:, nzGenes].multiply(csr_matrix(1 / szfactors))
-                if issparse(rounded)
-                else rounded[:, nzGenes] / szfactors
-            )
-
-        xim = np.mean(1 / szfactors) if szfactors is not None else 1
-
-        f_expression_mean = x.mean(axis=0)
-
-        # For NB: Var(Y) = mu * (1 + mu / k)
-        # x.A.var(axis=0, ddof=1)
-        f_expression_var = (
-            (x.multiply(x).mean(0).A1 - f_expression_mean.A1**2) * x.shape[0] / (x.shape[0] - 1)
-            if issparse(x)
-            else x.var(axis=0, ddof=0) ** 2
-        )  # np.mean(np.power(x - f_expression_mean, 2), axis=0) # variance with n - 1
-        # https://scialert.net/fulltext/?doi=ajms.2010.1.15 method of moments
-        disp_guess_meth_moments = f_expression_var - xim * f_expression_mean  # variance - mu
-
-        disp_guess_meth_moments = disp_guess_meth_moments / np.power(
-            f_expression_mean, 2
-        )  # this is dispersion parameter (1/k)
-
-        res = pd.DataFrame(
-            {
-                "mu": np.array(f_expression_mean).flatten(),
-                "disp": np.array(disp_guess_meth_moments).flatten(),
-            }
-        )
-        res.loc[res["mu"] == 0, "mu"] = None
-        res.loc[res["mu"] == 0, "disp"] = None
-        res.loc[res["disp"] < 0, "disp"] = 0
-
-        res["gene_id"] = adata.var_names[nzGenes]
-
-        res_list.append(res)
-
-    return layers, res_list
+from ..configuration import DKM
+from ..dynamo_logger import main_info_insert_adata_obsm
+from ..preprocessing.pca import pca
+from .connectivity import (
+    _gen_neighbor_keys,
+    knn_to_adj,
+    umap_conn_indices_dist_embedding,
+)
+from .psl_py import psl
+from .utils import log1p_, update_dict
 
 
-def estimate_dispersion(
-    adata: anndata.AnnData,
-    layers: str = "X",
-    modelFormulaStr: str = "~ 1",
-    min_cells_detected: int = 1,
-    removeOutliers: bool = False,
-) -> anndata.AnnData:
-    """This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
+# ---------------------------------------------------------------------------------------------------
+def prepare_dim_reduction(
+    adata: AnnData,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    basis: str = "pca",
+    dims: Optional[List[int]] = None,
+    n_pca_components: int = 30,
+    n_components: int = 2,
+) -> Tuple[np.ndarray, int, str]:
+    """Prepare the data for dimension reduction.
 
     Args:
         adata: an AnnData object.
-        layers: the layer(s) to be used for calculating dispersion. Default is "X" if there is no spliced layers.
-        modelFormulaStr: the model formula used to calculate dispersion parameters. Not used. Defaults to "~ 1".
-        min_cells_detected: the minimum number of cells detected for calculating the dispersion. Defaults to 1.
-        removeOutliers: whether to remove outliers when performing dispersion fitting. Defaults to False.
+        genes: the list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
+            all genes will be used. Defaults to None.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        basis: the space that will be used for clustering. Defaults to "pca".
+        dims: the list of dimensions that will be selected for clustering. If `None`, all dimensions will be used.
+            Defaults to None.
+        n_pca_components: Number of input PCs (principle components) that will be used for further non-linear dimension
+            reduction. If n_pca_components is larger than the existing #PC in adata.obsm['X_pca'] or input layer's
+            corresponding pca space (layer_pca), pca will be rerun with n_pca_components PCs requested. Defaults to 30.
+        n_components: the dimension of the space to embed into. Defaults to 2.
 
     Raises:
-        Exception: there is no valid DataFrames with mu for genes.
+        ValueError: _description_
+        ValueError: _description_
+        ValueError: _description_
 
     Returns:
-        An updated annData object with dispFitInfo added to uns attribute as a new key.
+        A tuple (X_data, n_components, basis) where `X_data` is the extracted data from adata to perform dimension
+        reduction, `n_components` is the target dimension of the space to embed into, `basis` is the space that would be
+        used for clustering.
     """
 
-    logger = LoggerManager.gen_logger("dynamo-preprocessing")
-    # mu = None
-    model_terms = [x.strip() for x in re.compile("~|\\*|\\+").split(modelFormulaStr)]
-    model_terms = list(set(model_terms) - set([""]))
-
-    cds_pdata = adata.obs  # .loc[:, model_terms]
-    cds_pdata["rowname"] = cds_pdata.index.values
-    layers, disp_tables = disp_calc_helper_NB(adata[:, :], layers, min_cells_detected)
-    # disp_table['disp'] = np.random.uniform(0, 10, 11)
-    # disp_table = cds_pdata.apply(disp_calc_helper_NB(adata[:, :], min_cells_detected))
-
-    # cds_pdata <- dplyr::group_by_(dplyr::select_(rownames_to_column(pData(cds)), "rowname", .dots=model_terms), .dots
-    # =model_terms)
-    # disp_table <- as.data.frame(cds_pdata %>% do(disp_calc_helper_NB(cds[,.$rowname], cds@expressionFamily, min_cells_
-    # detected)))
-    for ind in range(len(layers)):
-        layer, disp_table = layers[ind], disp_tables[ind]
-
-        if disp_table is None:
-            raise Exception("Parametric dispersion fitting failed, please set a different lowerDetectionLimit")
-
-        disp_table = disp_table.loc[np.where(disp_table["mu"] != np.nan)[0], :]
-
-        res = parametric_dispersion_fit(disp_table)
-        fit, coefs, good = res[0], res[1], res[2]
-
-        if removeOutliers:
-            # influence = fit.get_influence().cooks_distance()
-            # #CD is the distance and p is p-value
-            # (CD, p) = influence.cooks_distance
-
-            CD = cook_dist(fit, 1 / good["mu"][:, None], good)
-            cooksCutoff = 4 / good.shape[0]
-            main_info("Removing " + str(len(CD[CD > cooksCutoff])) + " outliers")
-            outliers = CD > cooksCutoff
-            # use CD.index.values? remove genes that lost when doing parameter fitting
-            lost_gene = set(good.index.values).difference(set(range(len(CD))))
-            outliers[lost_gene] = True
-            res = parametric_dispersion_fit(good.loc[~outliers, :])
-
-            fit, coefs = res[0], res[1]
-
-        def ans(q):
-            return coefs[0] + coefs[1] / q
-
-        if layer == "X":
-            logger.info_insert_adata("dispFitInfo", "uns")
-            adata.uns["dispFitInfo"] = {
-                "disp_table": good,
-                "disp_func": ans,
-                "coefs": coefs,
-            }
+    if genes is not None:
+        genes = adata.var_names.intersection(genes).to_list()
+        if len(genes) == 0:
+            raise ValueError("no genes from your genes list appear in your adata object.")
+    if layer is not None:
+        if not DKM.check_if_layer_exist(adata, layer):
+            raise ValueError(f"the layer {layer} you provided is not included in the adata object!")
+
+    prefix = "X_" if layer is None else layer + "_"
+
+    if basis is not None:
+        if basis.split(prefix)[-1] not in [
+            "pca",
+            "umap",
+            "trimap",
+            "tsne",
+            "diffmap",
+        ]:
+            raise ValueError(
+                "basis (or the suffix of basis) can only be one of ['pca', 'umap', 'trimap', 'tsne', 'diffmap']."
+            )
+        if basis.startswith(prefix):
+            basis = basis
         else:
-            logger.info_insert_adata(layer + "_dispFitInfo", "uns")
-            adata.uns[layer + "_dispFitInfo"] = {
-                "disp_table": good,
-                "disp_func": ans,
-                "coefs": coefs,
-            }
+            basis = prefix + basis
 
-    return adata
+    if basis is None:
+        if layer is None:
+            if genes is not None:
+                X_data = adata[:, genes].X
+            else:
+                X_data = adata.X if "use_for_pca" not in adata.var.keys() else adata[:, adata.var.use_for_pca].X
+        else:
+            if genes is not None:
+                X_data = adata[:, genes].layers[layer]
+            else:
+                X_data = (
+                    adata.layers[layer]
+                    if "use_for_pca" not in adata.var.keys()
+                    else adata[:, adata.var.use_for_pca].layers[layer]
+                )
 
+            X_data = log1p_(adata, X_data)
+    else:
+        pca_key = "X_pca" if layer is None else layer + "_pca"
+        n_pca_components = max(max(dims), n_pca_components) if dims is not None else n_pca_components
 
-def top_table(
-    adata: anndata.AnnData, layer: str = "X", mode: Literal["dispersion", "gini"] = "dispersion"
-) -> pd.DataFrame:
-    """Retrieve a table that contains gene names and other info whose dispersions/gini index are highest.
+        if basis not in adata.obsm.keys():
+            if genes is not None or pca_key not in adata.obsm.keys() or adata.obsm[pca_key].shape[1] < n_pca_components:
+                if layer is None:
+                    if genes is not None:
+                        CM = adata[:, genes].X
+                    else:
+                        CM = adata.X if "use_for_pca" not in adata.var.keys() else adata[:, adata.var.use_for_pca].X
+                else:
+                    if genes is not None:
+                        CM = adata[:, genes].layers[layer]
+                    else:
+                        CM = (
+                            adata.layers[layer]
+                            if "use_for_pca" not in adata.var.keys()
+                            else adata[:, adata.var.use_for_pca].layers[layer]
+                        )
+
+                    CM = log1p_(adata, CM)
+
+                cm_genesums = CM.sum(axis=0)
+                valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
+                valid_ind = np.array(valid_ind).flatten()
+                CM = CM[:, valid_ind]
+                adata, fit, _ = pca(
+                    adata,
+                    CM,
+                    n_pca_components=n_pca_components,
+                    pca_key=pca_key,
+                    return_all=True,
+                )
 
-    This function is partly based on Monocle R package (https://github.com/cole-trapnell-lab/monocle3).
+                # valid genes used for dimension reduction calculation
+                adata.uns["pca_valid_ind"] = valid_ind
 
-    Get information of the top layer.
+        if pca_key in adata.obsm.keys():
+            X_data = adata.obsm[pca_key]
+        else:
+            if genes is not None:
+                CM = adata[:, genes].layers[layer]
+            else:
+                CM = (
+                    adata.layers[layer]
+                    if "use_for_pca" not in adata.var.keys()
+                    else adata[:, adata.var.use_for_pca].layers[layer]
+                )
+
+            CM = log1p_(adata, CM)
+
+            cm_genesums = CM.sum(axis=0)
+            valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
+            valid_ind = np.array(valid_ind).flatten()
+            CM = CM[:, valid_ind]
+            adata, fit, _ = pca(adata, CM, n_pca_components=n_pca_components, pca_key=pca_key, return_all=True)
+
+            # valid genes used for dimension reduction calculation
+            adata.uns["pca_valid_ind"] = valid_ind
+            X_data = adata.obsm[pca_key]
+
+    if dims is not None:
+        X_data = X_data[:, dims]
+
+    return X_data, n_components, basis
+
+
+def run_reduce_dim(
+    adata: AnnData,
+    X_data: np.ndarray,
+    n_components: int,
+    n_pca_components: int,
+    reduction_method: Literal["trimap", "diffusion_map", "tsne", "umap", "psl"],
+    embedding_key: str,
+    n_neighbors: int,
+    neighbor_key: str,
+    cores: int,
+    **kwargs,
+) -> AnnData:
+    """Perform dimension reduction.
 
     Args:
         adata: an AnnData object.
-        layer: the layer(s) that would be searched for. Defaults to "X".
-        mode: either "dispersion" or "gini", deciding whether dispersion data or gini data would be acquired. Defaults
-            to "dispersion".
+        X_data: the user supplied data that will be used for dimension reduction directly.
+        n_components: the dimension of the space to embed into.
+        n_pca_components: Number of input PCs (principle components) that will be used for further non-linear dimension
+            reduction.
+        reduction_method: Non-linear dimension reduction method to further reduce dimension based on the top
+            n_pca_components PCA components.
+        embedding_key: The str in .obsm that will be used as the key to save the reduced embedding space.
+        n_neighbors: the number of nearest neighbors when constructing adjacency matrix.
+        neighbor_key: The str in .uns that will be used as the key to save the nearest neighbor graph.
+        cores: the number of threads used for calculation.
+        kwargs: other kwargs passed to umap calculation (see `umap_conn_indices_dist_embedding`).
 
     Raises:
-        KeyError: if mode is set to dispersion but there is no available dispersion model.
+        ImportError: `trimap` cannot be imported.
+        ImportError: `FItSNE` cannot be imported.
+        Exception: invalid `reduction_method`.
 
     Returns:
-        The data frame of the top layer with the gene_id, mean_expression, dispersion_fit and dispersion_empirical as
-        the columns.
+        The updated AnnData object with reduced dimension space and related parameters.
     """
 
-    layer = DynamoAdataKeyManager.get_available_layer_keys(adata, layers=layer, include_protein=False)[0]
+    if reduction_method == "trimap":
+        try:
+            import trimap
+        except ImportError as exception:
+            raise ImportError(
+                "Please 1) check if trimap is installed in your environment. 2) if you can import trimap successfully in your python console."
+            )
 
-    if layer in ["X"]:
-        key = "dispFitInfo"
-    else:
-        key = layer + "_dispFitInfo"
+        triplemap = trimap.TRIMAP(
+            n_inliers=20,
+            n_outliers=10,
+            n_random=10,
+            distance="euclidean",  # cosine
+            weight_adj=1000.0,
+            apply_pca=False,
+        )
+        X_dim = triplemap.fit_transform(X_data)
 
-    if mode == "dispersion":
-        if adata.uns[key] is None:
-            estimate_dispersion(adata, layers=[layer])
-
-        if adata.uns[key] is None:
-            raise KeyError(
-                "Error: for adata.uns.key=%s, no dispersion model found. Please call estimate_dispersion() before calling this function"
-                % key
+        main_info_insert_adata_obsm(embedding_key, log_level=20)
+        adata.obsm[embedding_key] = X_dim
+        adata.uns[neighbor_key] = {
+            "params": {"n_neighbors": n_neighbors, "method": reduction_method},
+            # "connectivities": "connectivities",
+            # "distances": "distances",
+            # "indices": "indices",
+        }
+    elif reduction_method.lower() == "diffusion_map":
+        # support Yan's diffusion map here
+        pass
+    elif reduction_method.lower() == "tsne":
+        try:
+            from fitsne import FItSNE
+        except ImportError:
+            raise ImportError(
+                "Please first install fitsne to perform accelerated tSNE method. Install instruction is "
+                "provided here: https://pypi.org/project/fitsne/"
             )
 
-        top_df = pd.DataFrame(
-            {
-                "gene_id": adata.uns[key]["disp_table"]["gene_id"],
-                "mean_expression": adata.uns[key]["disp_table"]["mu"],
-                "dispersion_fit": adata.uns[key]["disp_func"](adata.uns[key]["disp_table"]["mu"]),
-                "dispersion_empirical": adata.uns[key]["disp_table"]["disp"],
-            }
-        )
-        top_df = top_df.set_index("gene_id")
+        X_dim = FItSNE(X_data, nthreads=cores)  # use FitSNE
 
-    elif mode == "gini":
-        top_df = adata.var[layer + "_gini"]
+        # bh_tsne = TSNE(n_components = n_components)
+        # X_dim = bh_tsne.fit_transform(X)
+        main_info_insert_adata_obsm(embedding_key, log_level=20)
+        adata.obsm[embedding_key] = X_dim
+        adata.uns[neighbor_key] = {
+            "params": {"n_neighbors": n_neighbors, "method": reduction_method},
+            # "connectivities": "connectivities",
+            # "distances": "distances",
+            # "indices": "indices",
+        }
+    elif reduction_method == "umap":
+        _umap_kwargs = {
+            "n_components": n_components,
+            "metric": "euclidean",
+            "min_dist": 0.5,
+            "spread": 1.0,
+            "max_iter": None,
+            "alpha": 1.0,
+            "gamma": 1.0,
+            "negative_sample_rate": 5,
+            "init_pos": "spectral",
+            "random_state": 0,
+            "densmap": False,
+            "dens_lambda": 2.0,
+            "dens_frac": 0.3,
+            "dens_var_shift": 0.1,
+            "output_dens": False,
+            "verbose": False,
+        }
+        umap_kwargs = update_dict(_umap_kwargs, kwargs)
+
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            (
+                mapper,
+                graph,
+                knn_indices,
+                knn_dists,
+                X_dim,
+            ) = umap_conn_indices_dist_embedding(X_data, n_neighbors, **umap_kwargs)
+
+        main_info_insert_adata_obsm(embedding_key, log_level=20)
+        adata.obsm[embedding_key] = X_dim
+        knn_dists = knn_to_adj(knn_indices, knn_dists)
+        adata.uns[neighbor_key] = {
+            "params": {"n_neighbors": n_neighbors, "method": reduction_method},
+            # "connectivities": "connectivities",
+            # "distances": "distances",
+            "indices": knn_indices,
+        }
+
+        layer = neighbor_key.split("_")[0] if neighbor_key.__contains__("_") else None
+        neighbor_result_prefix = "" if layer is None else layer
+        conn_key, dist_key, neighbor_key = _gen_neighbor_keys(neighbor_result_prefix)
+
+        adata.uns["umap_fit"] = {
+            "fit": mapper,
+            "n_pca_components": n_pca_components,
+        }
+    elif reduction_method == "psl":
+        adj_mat, X_dim = psl(X_data, d=n_components, K=n_neighbors)  # this need to be updated
+        main_info_insert_adata_obsm(embedding_key, log_level=20)
+        adata.obsm[embedding_key] = X_dim
+        adata.uns[neighbor_key] = adj_mat
 
-    return top_df
+    else:
+        raise Exception("reduction_method {} is not supported.".format(reduction_method))
+
+    return adata
```

### Comparing `dynamo-release-1.2.0/dynamo/preprocessing/utils.py` & `dynamo-release-1.3.0/dynamo/preprocessing/utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,35 +1,27 @@
 import warnings
-from typing import Callable, Iterable, List, Tuple, Union
+from typing import Iterable, List, Optional, Tuple, Union
 
 try:
     from typing import Literal
 except ImportError:
     from typing_extensions import Literal
 
 import anndata
 import numpy as np
 import numpy.typing as npt
 import pandas as pd
 import scipy
 import scipy.sparse
 import statsmodels.api as sm
 from anndata import AnnData
-from scipy.sparse import csr_matrix, issparse
-from sklearn.decomposition import PCA, TruncatedSVD
+from scipy.sparse import csr_matrix, issparse, spmatrix
 
 from ..configuration import DKM, DynamoAdataKeyManager
-from ..dynamo_logger import (
-    LoggerManager,
-    main_debug,
-    main_exception,
-    main_info,
-    main_info_insert_adata_var,
-    main_warning,
-)
+from ..dynamo_logger import LoggerManager, main_debug, main_info, main_warning
 from ..utils import areinstance
 
 
 # ---------------------------------------------------------------------------------------------------
 # symbol conversion related
 def convert2gene_symbol(input_names: List[str], scopes: Union[List[str], None] = "ensembl.gene") -> pd.DataFrame:
     """Convert ensemble gene id to official gene names using mygene package.
@@ -129,16 +121,17 @@
             "Your adata object uses non-official gene names as gene index. \n"
             "Dynamo is converting those names to official gene names."
         )
         official_gene_df = convert2gene_symbol(adata.var_names, scopes)
         merge_df = adata.var.merge(official_gene_df, left_on="query", right_on="query", how="left").set_index(
             adata.var.index
         )
-        adata.var = merge_df
         valid_ind = np.where(merge_df["notfound"] != True)[0]  # noqa: E712
+        merge_df.pop("notfound")
+        adata.var = merge_df
 
         if subset is True:
             adata._inplace_subset_var(valid_ind)
             adata.var.index = adata.var["symbol"].values.copy()
         else:
             indices = np.array(adata.var.index)
             indices[valid_ind] = adata.var.loc[valid_ind, "symbol"].values.copy()
@@ -171,180 +164,48 @@
 
     valid_ids = np.where(frac > threshold)[0]
 
     return frac, valid_ids
 
 
 # ---------------------------------------------------------------------------------------------------
-# implmentation of Cooks' distance (but this is for Poisson distribution fitting)
-
-# https://stackoverflow.com/questions/47686227/poisson-regression-in-statsmodels-and-r
-
-# from __future__ import division, print_function
-
-# https://stats.stackexchange.com/questions/356053/the-identity-link-function-does-not-respect-the-domain-of-the-gamma-
-# family
-def _weight_matrix(fitted_model: sm.Poisson) -> np.ndarray:
-    """Calculates weight matrix in Poisson regression.
-
-    Args:
-        fitted_model: a fitted Poisson model
-
-    Returns:
-        A diagonal weight matrix in Poisson regression.
-    """
-
-    return np.diag(fitted_model.fittedvalues)
-
-
-def _hessian(X: np.ndarray, W: np.ndarray) -> np.ndarray:
-    """Hessian matrix calculated as -X'*W*X.
-
-    Args:
-        X: the matrix of covariates.
-        W: the weight matrix.
-
-    Returns:
-        The result Hessian matrix.
-    """
-
-    return -np.dot(X.T, np.dot(W, X))
-
-
-def _hat_matrix(X: np.ndarray, W: np.ndarray) -> np.ndarray:
-    """Calculate hat matrix = W^(1/2) * X * (X'*W*X)^(-1) * X'*W^(1/2)
-
-    Args:
-        X: the matrix of covariates.
-        W: the diagonal weight matrix
-
-    Returns:
-        The result hat matrix
-    """
-
-    # W^(1/2)
-    Wsqrt = W ** (0.5)
-
-    # (X'*W*X)^(-1)
-    XtWX = -_hessian(X=X, W=W)
-    XtWX_inv = np.linalg.inv(XtWX)
-
-    # W^(1/2)*X
-    WsqrtX = np.dot(Wsqrt, X)
-
-    # X'*W^(1/2)
-    XtWsqrt = np.dot(X.T, Wsqrt)
-
-    return np.dot(WsqrtX, np.dot(XtWX_inv, XtWsqrt))
-
-
-def cook_dist(model: sm.Poisson, X: np.ndarray, good: npt.ArrayLike) -> np.ndarray:
-    """calculate Cook's distance
-
-    Args:
-        model: a fitted Poisson model.
-        X: the matrix of covariates.
-        good: the dispersion table for MSE calculation.
-
-    Returns:
-        The result Cook's distance.
-    """
-
-    # Weight matrix
-    W = _weight_matrix(model)
-
-    # Hat matrix
-    H = _hat_matrix(X, W)
-    hii = np.diag(H)  # Diagonal values of hat matrix # fit.get_influence().hat_matrix_diag
-
-    # Pearson residuals
-    r = model.resid_pearson
-
-    # Cook's distance (formula used by R = (res/(1 - hat))^2 * hat/(dispersion * p))
-    # Note: dispersion is 1 since we aren't modeling overdispersion
-
-    resid = good.disp - model.predict(good)
-    rss = np.sum(resid**2)
-    MSE = rss / (good.shape[0] - 2)
-    # use the formula from: https://www.mathworks.com/help/stats/cooks-distance.html
-    cooks_d = r**2 / (2 * MSE) * hii / (1 - hii) ** 2  # (r / (1 - hii)) ** 2 *  / (1 * 2)
-
-    return cooks_d
-
-
-# ---------------------------------------------------------------------------------------------------
 # preprocess utilities
-def filter_genes_by_pattern(
-    adata: anndata.AnnData,
-    patterns: Tuple[str] = ("MT-", "RPS", "RPL", "MRPS", "MRPL", "ERCC-"),
-    drop_genes: bool = False,
-) -> Union[List[bool], None]:
-    """Utility function to filter mitochondria, ribsome protein and ERCC spike-in genes, etc.
+def _infer_labeling_experiment_type(adata: anndata.AnnData, tkey: str) -> Literal["one-shot", "kin", "deg"]:
+    """Returns the experiment type of `adata` according to `tkey`s
 
     Args:
-        adata: an AnnData object.
-        patterns: the patterns used to filter genes. Defaults to ("MT-", "RPS", "RPL", "MRPS", "MRPL", "ERCC-").
-        drop_genes: whether inplace drop the genes from the AnnData object. Defaults to False.
+        adata: an AnnData Object.
+        tkey: the key for time in `adata.obs`.
 
     Returns:
-        A list of indices of matched genes if `drop_genes` is False. Otherwise, returns none.
+        The experiment type, must be one of "one-shot", "kin" or "deg".
     """
 
-    logger = LoggerManager.gen_logger("dynamo-utils")
-
-    matched_genes = pd.Series(adata.var_names).str.startswith(patterns).to_list()
-    logger.info(
-        "total matched genes is " + str(sum(matched_genes)),
-        indent_level=1,
-    )
-    if sum(matched_genes) > 0:
-        if drop_genes:
-            gene_bools = np.ones(adata.n_vars, dtype=bool)
-            gene_bools[matched_genes] = False
-            logger.info(
-                "inplace subset matched genes ... ",
-                indent_level=1,
-            )
-            # let us ignore the `inplace` parameter in pandas.Categorical.remove_unused_categories  warning.
-            with warnings.catch_warnings():
-                warnings.simplefilter("ignore")
-
-                adata._inplace_subset_var(gene_bools)
-
-            logger.finish_progress(progress_name="filter_genes_by_pattern")
-            return None
-        else:
-            logger.finish_progress(progress_name="filter_genes_by_pattern")
-            return matched_genes
-
+    experiment_type = None
+    tkey_val = np.array(adata.obs[tkey], dtype="float")
+    if len(np.unique(tkey_val)) == 1:
+        experiment_type = "one-shot"
+    else:
+        labeled_frac = adata.layers["new"].T.sum(0) / adata.layers["total"].T.sum(0)
+        xx = labeled_frac.A1 if issparse(adata.layers["new"]) else labeled_frac
 
-def basic_stats(adata: anndata.AnnData) -> None:
-    """Generate basic stats of the adata, including number of genes, number of cells, and number of mitochondria genes.
+        yy = tkey_val
+        xm, ym = np.mean(xx), np.mean(yy)
+        cov = np.mean(xx * yy) - xm * ym
+        var_x = np.mean(xx * xx) - xm * xm
 
-    Args:
-        adata: an AnnData object.
-    """
+        k = cov / var_x
 
-    adata.obs["nGenes"], adata.obs["nCounts"] = np.array((adata.X > 0).sum(1)), np.array((adata.X).sum(1))
-    adata.var["nCells"], adata.var["nCounts"] = np.array((adata.X > 0).sum(0).T), np.array((adata.X).sum(0).T)
-    if adata.var_names.inferred_type == "bytes":
-        adata.var_names = adata.var_names.astype("str")
-    mito_genes = adata.var_names.str.upper().str.startswith("MT-")
-
-    if sum(mito_genes) > 0:
-        try:
-            adata.obs["pMito"] = np.array(adata.X[:, mito_genes].sum(1) / adata.obs["nCounts"].values.reshape((-1, 1)))
-        except:  # noqa E722
-            main_exception(
-                "no mitochondria genes detected; looks like your var_names may be corrupted (i.e. "
-                "include nan values). If you don't believe so, please report to us on github or "
-                "via xqiu@wi.mit.edu"
-            )
-    else:
-        adata.obs["pMito"] = 0
+        # total labeled RNA amount will increase (decrease) in kinetic (degradation) experiments over time.
+        experiment_type = "kin" if k > 0 else "deg"
+    main_debug(
+        f"\nDynamo has detected that your labeling data is from a kin experiment. \nIf the experiment type is incorrect, "
+        f"please provide the correct experiment_type (one-shot, kin, or deg)."
+    )
+    return experiment_type
 
 
 def unique_var_obs_adata(adata: anndata.AnnData) -> anndata.AnnData:
     """Function to make the obs and var attribute's index unique
 
     Args:
         adata: an AnnData object.
@@ -394,15 +255,15 @@
             origin_df: the DataFrame to be merged into.
             diff_df: the DataFrame to be merged.
 
         Returns:
             The merged DataFrame.
         """
 
-        _columns = set(diff_df.columns).difference(origin_df.columns)
+        _columns = list(set(diff_df.columns).difference(origin_df.columns))
         new_df = origin_df.merge(diff_df[_columns], how="left", left_index=True, right_index=True)
         return new_df.loc[origin_df.index, :]
 
     if attr == "var":
         adata_ori.var = _merge_by_diff(adata_ori.var, adata.var)
     elif attr == "obs":
         obs_df = _merge_by_diff(adata_ori.obs, adata.obs)
@@ -410,14 +271,20 @@
             raise ValueError(
                 "Left join generates more rows. Please check if you obs names are unique before calling this fucntion."
             )
         adata_ori.obs = obs_df
     return adata_ori
 
 
+def clip_by_perc(layer_mat):
+    """Returns a new matrix by clipping the layer_mat according to percentage."""
+    # TODO implement this function (currently not used)
+    return
+
+
 def get_inrange_shared_counts_mask(
     adata: anndata.AnnData, layers: List[str], min_shared_count: int, count_by: Literal["gene", "cells"] = "gene"
 ) -> np.ndarray:
     """Generate the mask showing the genes having counts more than the provided minimal count.
 
     Args:
         adata: an AnnData object.
@@ -484,48 +351,48 @@
     return (
         np.array(_sum.sum(sum_dim_index).A1 >= min_shared_count)
         if issparse(adata.layers[layers[0]])
         else np.array(_sum.sum(sum_dim_index) >= min_shared_count)
     )
 
 
-def clusters_stats(
-    U: pd.DataFrame, S: pd.DataFrame, clusters_uid: np.ndarray, cluster_ix: np.ndarray, size_limit: int = 40
-) -> Tuple[np.ndarray, np.ndarray]:
-    """Calculate the averages per cluster for unspliced and spliced data.
+def get_nan_or_inf_data_bool_mask(arr: np.ndarray) -> np.ndarray:
+    """Returns the mask of arr with the same shape, indicating whether each index is nan/inf or not.
 
     Args:
-        U: the unspliced DataFrame.
-        S: the spliced DataFrame.
-        clusters_uid: the uid of the clusters.
-        cluster_ix: the indices of the clusters in adata.obs.
-        size_limit: the max number of members to be considered in a cluster during calculation. Defaults to 40.
+        arr: an array
 
     Returns:
-        U_avgs: the average of clusters for unspliced data.
-        S_avgs: the average of clusters for spliced data.
+        A bool array indicating each element is nan/inf or not
     """
 
-    U_avgs = np.zeros((S.shape[1], len(clusters_uid)))
-    S_avgs = np.zeros((S.shape[1], len(clusters_uid)))
-    # avgU_div_avgS = np.zeros((S.shape[1], len(clusters_uid)))
-    # slopes_by_clust = np.zeros((S.shape[1], len(clusters_uid)))
+    mask = np.isnan(arr) | np.isinf(arr) | np.isneginf(arr)
+    return mask
 
-    for i, uid in enumerate(clusters_uid):
-        cluster_filter = cluster_ix == i
-        n_cells = np.sum(cluster_filter)
-        if n_cells > size_limit:
-            U_avgs[:, i], S_avgs[:, i] = (
-                U[cluster_filter, :].mean(0),
-                S[cluster_filter, :].mean(0),
-            )
-        else:
-            U_avgs[:, i], S_avgs[:, i] = U.mean(0), S.mean(0)
 
-    return U_avgs, S_avgs
+def get_gene_selection_filter(
+    valid_table: pd.Series,
+    n_top_genes: int = 2000,
+    basic_filter: Optional[pd.Series] = None,
+) -> np.ndarray:
+    """Generate the mask by sorting given table of scores.
+
+    Args:
+        valid_table: the scores used to sort the highly variable genes.
+        n_top_genes: number of top genes to be filtered. Defaults to 2000.
+        basic_filter: the filter to remove outliers. For example, the `adata.var["pass_basic_filter"]`.
+
+    Returns:
+        The filter mask as a bool array.
+    """
+    if basic_filter is None:
+        basic_filter = pd.Series(True, index=valid_table.index)
+    feature_gene_idx = np.argsort(-valid_table)[:n_top_genes]
+    feature_gene_idx = valid_table.index[feature_gene_idx]
+    return basic_filter.index.isin(feature_gene_idx)
 
 
 def get_svr_filter(
     adata: anndata.AnnData, layer: str = "spliced", n_top_genes: int = 3000, return_adata: bool = False
 ) -> Union[anndata.AnnData, np.ndarray]:
     """Generate the mask showing the genes with valid svr scores.
 
@@ -557,183 +424,64 @@
         filter_bool = np.zeros(adata.n_vars, dtype=bool)
         filter_bool[feature_gene_idx] = True
         res = filter_bool
 
     return res
 
 
-def sz_util(
-    adata: anndata.AnnData,
-    layer: str,
-    round_exprs: bool,
-    method: Literal["mean-geometric-mean-total", "geometric", "median"],
-    locfunc: Callable,
-    total_layers: List[str] = None,
-    CM: pd.DataFrame = None,
-    scale_to: Union[float, None] = None,
-) -> Tuple[pd.Series, pd.Series]:
-    """Calculate the size factor for a given layer.
+def seurat_get_mean_var(
+    X: Union[csr_matrix, np.ndarray],
+    ignore_zeros: bool = False,
+    perc: Union[float, List[float], None] = None,
+) -> Tuple[np.ndarray, np.ndarray]:
+    """Only used in seurat impl to match seurat and scvelo implementation result.
 
     Args:
-        adata: an AnnData object.
-        layer: the layer to operate on.
-        round_exprs: whether the gene expression should be rounded into integers.
-        method: the method used to calculate the expected total reads / UMI used in size factor calculation. Only
-            `mean-geometric-mean-total` / `geometric` and `median` are supported. When `median` is used, `locfunc` will
-            be replaced with `np.nanmedian`.
-        locfunc: the function to normalize the data.
-        total_layers: the layer(s) that can be summed up to get the total mRNA. For example, ["spliced", "unspliced"],
-            ["uu", "ul", "su", "sl"] or ["new", "old"], etc. Defaults to None.
-        CM: the data to operate on, overriding the layer. Defaults to None.
-        scale_to: the final total expression for each cell that will be scaled to. Defaults to None.
-
-    Raises:
-        NotImplementedError: method is invalid.
+        X: a matrix as np.ndarray or a sparse matrix as scipy sparse matrix. Rows are cells while columns are genes.
+        ignore_zeros: whether ignore columns with 0 only. Defaults to False.
+        perc: clip the gene expression values based on the perc or the min/max boundary of the values. Defaults to None.
 
     Returns:
-        A tuple (sfs, cell_total) where sfs is the size factors and cell_total is the initial cell size.
+        A tuple (mean, var) where mean is the mean of the columns after processing of the matrix and var is the variance
+        of the columns after processing of the matrix.
     """
 
-    adata = adata.copy()
+    data = X.data if issparse(X) else X
+    mask_nans = np.isnan(data) | np.isinf(data) | np.isneginf(data)
 
-    if layer == "_total_" and "_total_" not in adata.layers.keys():
-        if total_layers is not None:
-            if not isinstance(total_layers, list):
-                total_layers = [total_layers]
-            if len(set(total_layers).difference(adata.layers.keys())) == 0:
-                total = None
-                for t_key in total_layers:
-                    total = adata.layers[t_key] if total is None else total + adata.layers[t_key]
-                adata.layers["_total_"] = total
-
-    if layer == "raw":
-        CM = adata.raw.X if CM is None else CM
-    elif layer == "X":
-        CM = adata.X if CM is None else CM
-    elif layer == "protein":
-        if "protein" in adata.obsm_keys():
-            CM = adata.obsm["protein"] if CM is None else CM
-        else:
-            return None, None
-    else:
-        CM = adata.layers[layer] if CM is None else CM
+    n_nonzeros = (X != 0).sum(0)
+    n_counts = n_nonzeros if ignore_zeros else X.shape[0]
 
-    if round_exprs:
-        main_info("rounding expression data of layer: %s during size factor calculation" % (layer))
-        if issparse(CM):
-            CM.data = np.round(CM.data, 0)
+    if mask_nans.sum() > 0:
+        if issparse(X):
+            data[np.isnan(data) | np.isinf(data) | np.isneginf(data)] = 0
+            n_nans = n_nonzeros - (X != 0).sum(0)
         else:
-            CM = CM.round().astype("int")
-
-    cell_total = CM.sum(axis=1).A1 if issparse(CM) else CM.sum(axis=1)
-    cell_total += cell_total == 0  # avoid infinity value after log (0)
-
-    if method in ["mean-geometric-mean-total", "geometric"]:
-        sfs = cell_total / (np.exp(locfunc(np.log(cell_total))) if scale_to is None else scale_to)
-    elif method == "median":
-        sfs = cell_total / (np.nanmedian(cell_total) if scale_to is None else scale_to)
-    elif method == "mean":
-        sfs = cell_total / (np.nanmean(cell_total) if scale_to is None else scale_to)
-    else:
-        raise NotImplementedError(f"This method {method} is not supported!")
-
-    return sfs, cell_total
-
-
-def get_sz_exprs(
-    adata: anndata.AnnData, layer: str, total_szfactor: Union[str, None] = None
-) -> Tuple[np.ndarray, npt.ArrayLike]:
-    """Get the size factor from an AnnData object.
-
-    Args:
-        adata: an AnnData object.
-        layer: the layer for which to get the size factor.
-        total_szfactor: the key-name for total size factor entry in `adata.obs`. If not None, would override the layer
-            selected. Defaults to None.
-
-    Returns:
-        A tuple (szfactors, CM), where szfactors is the queried size factor and CM is the data of the layer
-        corresponding to the size factor.
-    """
-
-    if layer == "raw":
-        CM = adata.raw.X
-        szfactors = adata.obs[layer + "Size_Factor"].values[:, None]
-    elif layer == "X":
-        CM = adata.X
-        szfactors = adata.obs["Size_Factor"].values[:, None]
-    elif layer == "protein":
-        if "protein" in adata.obsm_keys():
-            CM = adata.obsm[layer]
-            szfactors = adata.obs["protein_Size_Factor"].values[:, None]
-        else:
-            CM, szfactors = None, None
-    else:
-        CM = adata.layers[layer]
-        szfactors = adata.obs[layer + "_Size_Factor"].values[:, None]
-
-    if total_szfactor is not None and total_szfactor in adata.obs.keys():
-        szfactors = adata.obs[total_szfactor][:, None]
-    elif total_szfactor is not None:
-        main_warning("`total_szfactor` is not `None` and it is not in adata object.")
-
-    return szfactors, CM
+            X[mask_nans] = 0
+            n_nans = mask_nans.sum(0)
+        n_counts -= n_nans
 
+    if perc is not None:
+        if np.size(perc) < 2:
+            perc = [perc, 100] if perc < 50 else [0, perc]
+        lb, ub = np.percentile(data, perc)
+        data = np.clip(data, lb, ub)
 
-def normalize_mat_monocle(
-    mat: np.ndarray, szfactors: np.ndarray, relative_expr: bool, pseudo_expr: int, norm_method: Callable = np.log1p
-) -> np.ndarray:
-    """Normalize the given array for monocle recipe.
-
-    Args:
-        mat: the array to operate on.
-        szfactors: the size factors corresponding to the array.
-        relative_expr: whether we need to divide gene expression values first by size factor before normalization.
-        pseudo_expr: a pseudocount added to the gene expression value before log/log2 normalization.
-        norm_method: the method used to normalize data. Defaults to np.log1p.
-
-    Returns:
-        The normalized array.
-    """
-
-    if norm_method == np.log1p:
-        pseudo_expr = 0
-    if relative_expr:
-        mat = mat.multiply(csr_matrix(1 / szfactors)) if issparse(mat) else mat / szfactors
-
-    if pseudo_expr is None:
-        pseudo_expr = 1
-
-    if issparse(mat):
-        mat.data = norm_method(mat.data + pseudo_expr) if norm_method is not None else mat.data
-        if norm_method is not None and norm_method.__name__ == "Freeman_Tukey":
-            mat.data -= 1
-    else:
-        mat = norm_method(mat + pseudo_expr) if norm_method is not None else mat
-
-    return mat
-
-
-def Freeman_Tukey(X: np.ndarray, inverse=False) -> np.ndarray:
-    """perform Freeman-Tukey transform or inverse transform on the given array.
-
-    Args:
-        X: a matrix.
-        inverse: whether to perform inverse Freeman-Tukey transform. Defaults to False.
-
-    Returns:
-        The transformed array.
-    """
-
-    if inverse:
-        res = np.sqrt(X) + np.sqrt((X + 1))
+    if issparse(X):
+        mean = (X.sum(0) / n_counts).A1
+        mean_sq = (X.multiply(X).sum(0) / n_counts).A1
     else:
-        res = (X**2 - 1) ** 2 / (4 * X**2)
-
-    return res
+        mean = X.sum(0) / n_counts
+        mean_sq = np.multiply(X, X).sum(0) / n_counts
+    n_cells = np.clip(X.shape[0], 2, None)  # to avoid division by zero
+    var = (mean_sq - mean**2) * (n_cells / (n_cells - 1))
+
+    mean = np.nan_to_num(mean)
+    var = np.nan_to_num(var)
+    return mean, var
 
 
 def anndata_bytestring_decode(adata_item: pd.DataFrame) -> None:
     """Decode contents of an annotation of an AnnData object inplace.
 
     Args:
         adata_item: an annotation of an AnnData object.
@@ -767,211 +515,93 @@
 
     decode_index(adata.obs)
     decode_index(adata.var)
     anndata_bytestring_decode(adata.obs)
     anndata_bytestring_decode(adata.var)
 
 
-# ---------------------------------------------------------------------------------------------------
-# pca
-
-
-def pca_monocle(
-    adata: AnnData,
-    X_data: np.ndarray = None,
-    n_pca_components: int = 30,
-    pca_key: str = "X",
-    pcs_key: str = "PCs",
-    genes_to_append: Union[List[str], None] = None,
-    layer: Union[List[str], str, None] = None,
-    return_all: bool = False,
-) -> Union[AnnData, Tuple[AnnData, Union[PCA, TruncatedSVD], np.ndarray]]:
-    """Perform PCA reduction for monocle recipe.
+def add_noise_to_duplicates(adata: anndata.AnnData, basis: str = "pca") -> None:
+    """Add noise to duplicated elements of the reduced array inplace.
 
     Args:
         adata: an AnnData object.
-        X_data: the data to perform dimension reduction on. Defaults to None.
-        n_pca_components: number of PCA components reduced to. Defaults to 30.
-        pca_key: the key to store the reduced data. Defaults to "X".
-        pcs_key: the key to store the principle axes in feature space. Defaults to "PCs".
-        genes_to_append: a list of genes should be inspected. Defaults to None.
-        layer: the layer(s) to perform dimension reduction on. Would be overrided by X_data. Defaults to None.
-        return_all: whether to return the PCA fit model and the reduced array together with the updated AnnData object.
-            Defaults to False.
-
-    Raises:
-        ValueError: layer provided is not invalid.
-        ValueError: list of genes to append is invalid.
-
-    Returns:
-        The the updated AnnData object with reduced data if `return_all` is False. Otherwise, a tuple (adata, fit,
-        X_pca), where adata is the updated AnnData object, fit is the fit model for dimension reduction, and X_pca is
-        the reduced array, will be returned.
+        basis: the type of dimension redduction. Defaults to "pca".
     """
 
-    # only use genes pass filter (based on use_for_pca) to perform dimension reduction.
-    if X_data is None:
-        if "use_for_pca" not in adata.var.keys():
-            adata.var["use_for_pca"] = True
-
-        if layer is None:
-            X_data = adata.X[:, adata.var.use_for_pca.values]
-        else:
-            if "X" in layer:
-                X_data = adata.X[:, adata.var.use_for_pca.values]
-            elif "total" in layer:
-                X_data = adata.layers["X_total"][:, adata.var.use_for_pca.values]
-            elif "spliced" in layer:
-                X_data = adata.layers["X_spliced"][:, adata.var.use_for_pca.values]
-            elif "protein" in layer:
-                X_data = adata.obsm["X_protein"]
-            elif type(layer) is str:
-                X_data = adata.layers["X_" + layer][:, adata.var.use_for_pca.values]
-            else:
-                raise ValueError(
-                    f"your input layer argument should be either a `str` or a list that includes one of `X`, "
-                    f"`total`, `protein` element. `Layer` currently is {layer}."
-                )
-
-        cm_genesums = X_data.sum(axis=0)
-        valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
-        valid_ind = np.array(valid_ind).flatten()
+    X_data = adata.obsm["X_" + basis]
+    min_val = abs(X_data).min()
 
-        bad_genes = np.where(adata.var.use_for_pca)[0][~valid_ind]
-        if genes_to_append is not None and len(adata.var.index[bad_genes].intersection(genes_to_append)) > 0:
-            raise ValueError(
-                f"The gene list passed to argument genes_to_append contains genes with no expression "
-                f"across cells or non finite values. Please check those genes:"
-                f"{set(bad_genes).intersection(genes_to_append)}!"
-            )
+    n_obs, n_var = X_data.shape
+    while True:
+        _, index = np.unique(X_data, axis=0, return_index=True)
+        duplicated_idx = np.setdiff1d(np.arange(n_obs), index)
 
-        adata.var.iloc[bad_genes, adata.var.columns.tolist().index("use_for_pca")] = False
-        X_data = X_data[:, valid_ind]
+        if len(duplicated_idx) == 0:
+            adata.obsm["X_" + basis] = X_data
+            break
+        else:
+            X_data[duplicated_idx, :] += np.random.normal(0, min_val / 1000, (len(duplicated_idx), n_var))
 
-    USE_TRUNCATED_SVD_THRESHOLD = 100000
-    if adata.n_obs < USE_TRUNCATED_SVD_THRESHOLD:
-        pca = PCA(
-            n_components=min(n_pca_components, X_data.shape[1] - 1),
-            svd_solver="arpack",
-            random_state=0,
-        )
-        fit = pca.fit(X_data.toarray()) if issparse(X_data) else pca.fit(X_data)
-        X_pca = fit.transform(X_data.toarray()) if issparse(X_data) else fit.transform(X_data)
-        adata.obsm[pca_key] = X_pca
-        adata.uns[pcs_key] = fit.components_.T
 
-        adata.uns["explained_variance_ratio_"] = fit.explained_variance_ratio_
-    else:
-        # unscaled PCA
-        fit = TruncatedSVD(
-            n_components=min(n_pca_components + 1, X_data.shape[1] - 1),
-            random_state=0,
-        )
-        # first columns is related to the total UMI (or library size)
-        X_pca = fit.fit_transform(X_data)[:, 1:]
-        adata.obsm[pca_key] = X_pca
-        adata.uns[pcs_key] = fit.components_.T
+def is_float_integer_arr(arr: Union[np.ndarray, spmatrix, list]) -> bool:
+    """Test if an array's elements are integers
 
-        adata.uns["explained_variance_ratio_"] = fit.explained_variance_ratio_[1:]
+    Args:
+        arr: an input array.
 
-    adata.uns["explained_variance_ratio_"] = fit.explained_variance_ratio_[1:]
-    adata.uns["pca_mean"] = fit.mean_ if hasattr(fit, "mean_") else None
+    Returns:
+        A flag whether all elements of the array are integers.
+    """
 
-    if return_all:
-        return adata, fit, X_pca
-    else:
-        return adata
+    if issparse(arr):
+        arr = arr.data
+    return np.all(np.equal(np.mod(arr, 1), 0))
 
 
-def pca_genes(PCs: list, n_top_genes: int = 100) -> np.ndarray:
-    """For each gene, if the gene is n_top in some principle component then it is valid. Return all such valid genes.
+def is_integer_arr(arr: Union[np.ndarray, spmatrix, list]) -> bool:
+    """Test if an array like obj's dtype is integer
 
     Args:
-        PCs: principle components(PC) of PCA
-        n_top_genes: number of gene candidates in EACH PC. Defaults to 100.
+        arr: an array like object.
 
     Returns:
-        A bool array indicating whether the gene is valid.
+        A flag whether the array's dtype is integer.
     """
 
-    valid_genes = np.zeros(PCs.shape[0], dtype=bool)
-    for q in PCs.T:
-        sorted_q = np.sort(np.abs(q))[::-1]
-        is_pc_top_n = np.abs(q) > sorted_q[n_top_genes]
-        valid_genes = np.logical_or(is_pc_top_n, valid_genes)
-    return valid_genes
+    return np.issubdtype(arr.dtype, np.integer) or np.issubdtype(arr.dtype, int)
 
 
-def top_pca_genes(
-    adata: AnnData,
-    pc_key: str = "PCs",
-    n_top_genes: int = 100,
-    pc_components: Union[int, None] = None,
-    adata_store_key: str = "top_pca_genes",
-) -> AnnData:
-    """Define top genes as any gene that is ``n_top_genes`` in some principle component.
+def is_nonnegative(mat: Union[np.ndarray, spmatrix, list]) -> bool:
+    """Test whether all elements of an array or sparse array are non-negative.
 
     Args:
-        adata: an AnnData object.
-        pc_key: component key stored in adata.uns. Defaults to "PCs".
-        n_top_genes: number of top genes as valid top genes in each component. Defaults to 100.
-        pc_components: number of top principle components to use. Defaults to None.
-        adata_store_key: the key for storing pca genes. Defaults to "top_pca_genes".
-
-    Raises:
-        Exception: invalid pc_key.
+        mat: an array in ndarray or sparse array in scipy spmatrix.
 
     Returns:
-        The AnnData object with top genes stored as values of adata.var[adata_store_key].
+        A flag whether all elements are non-negative.
     """
 
-    if pc_key in adata.uns.keys():
-        Q = adata.uns[pc_key]
-    elif pc_key in adata.varm.keys():
-        Q = adata.varm[pc_key]
-    else:
-        raise Exception(f"No PC matrix {pc_key} found in neither .uns nor .varm.")
-    if pc_components is not None:
-        if type(pc_components) == int:
-            Q = Q[:, :pc_components]
-        elif type(pc_components) == list:
-            Q = Q[:, pc_components]
-
-    pcg = pca_genes(Q, n_top_genes=n_top_genes)
-    genes = np.zeros(adata.n_vars, dtype=bool)
-    if DKM.VAR_USE_FOR_PCA in adata.var.keys():
-        genes[adata.var[DKM.VAR_USE_FOR_PCA]] = pcg
-    else:
-        genes = pcg
-    main_info_insert_adata_var(adata_store_key, indent_level=2)
-    adata.var[adata_store_key] = genes
-    return adata
+    if scipy.sparse.issparse(mat):
+        return np.all(mat.sign().data >= 0)
+    return np.all(np.sign(mat) >= 0)
 
 
-def add_noise_to_duplicates(adata: anndata.AnnData, basis: str = "pca") -> None:
-    """Add noise to duplicated elements of the reduced array inplace.
+def is_nonnegative_integer_arr(mat: Union[np.ndarray, spmatrix, list]) -> bool:
+    """Test if an array's elements are non-negative integers
 
     Args:
-        adata: an AnnData object.
-        basis: the type of dimension redduction. Defaults to "pca".
-    """
+        mat: an input array.
 
-    X_data = adata.obsm["X_" + basis]
-    min_val = abs(X_data).min()
+    Returns:
+        A flag whether all elements of the array are non-negative integers.
+    """
 
-    n_obs, n_var = X_data.shape
-    while True:
-        _, index = np.unique(X_data, axis=0, return_index=True)
-        duplicated_idx = np.setdiff1d(np.arange(n_obs), index)
-
-        if len(duplicated_idx) == 0:
-            adata.obsm["X_" + basis] = X_data
-            break
-        else:
-            X_data[duplicated_idx, :] += np.random.normal(0, min_val / 1000, (len(duplicated_idx), n_var))
+    if (not is_integer_arr(mat)) and (not is_float_integer_arr(mat)):
+        return False
+    return is_nonnegative(mat)
 
 
 # ---------------------------------------------------------------------------------------------------
 # labeling related
 
 
 def collapse_species_adata(adata: anndata.AnnData) -> None:
@@ -1123,45 +753,45 @@
     else:
         ntr, var_ntr = None, None
 
     return ntr, var_ntr
 
 
 def scale(
-    adata: anndata.AnnData,
+    adata: AnnData,
     layers: Union[List[str], str, None] = None,
-    scale_to_layer: Union[str, None] = None,
-    scale_to: float = 1e6,
+    scale_to_layer: Optional[str] = None,
+    scale_to: float = 1e4,
 ) -> anndata.AnnData:
     """Scale layers to a particular total expression value, similar to `normalize_expr_data` function.
 
     Args:
         adata: an AnnData object.
         layers: the layers to scale. Defaults to None.
         scale_to_layer: use which layer to calculate a global scale factor. If None, calculate each layer's own scale
             factor and scale all layers to same total value. Defaults to None.
         scale_to: the total expression value that layers are scaled to. Defaults to 1e6.
 
     Returns:
         The scaled AnnData object.
     """
 
-    layers = DynamoAdataKeyManager.get_available_layer_keys(adata, layers)
-    has_splicing, has_labeling, _ = detect_experiment_datatype(adata)
+    if layers is None:
+        layers = DynamoAdataKeyManager.get_available_layer_keys(adata, layers="all")
+    has_splicing, has_labeling = detect_experiment_datatype(adata)[:2]
 
     if scale_to_layer is None:
         scale_to_layer = "total" if has_labeling else None
         scale = scale_to / adata.layers[scale_to_layer].sum(1)
     else:
         scale = None
 
     for layer in layers:
-        if scale is None:
-            scale = scale_to / adata.layers[layer].sum(1)
-
+        # if scale is None:
+        scale = scale_to / adata.layers[layer].sum(1)
         adata.layers[layer] = csr_matrix(adata.layers[layer].multiply(scale))
 
     return adata
 
 
 # ---------------------------------------------------------------------------------------------------
 # ERCC related
@@ -1330,7 +960,40 @@
 
     rad = radians(degree)
     R = [
         [cos(rad), -sin(rad)],
         [sin(rad), cos(rad)],
     ]
     return np.array(R)
+
+
+def reset_adata_X(adata: AnnData, experiment_type: str, has_labeling: bool, has_splicing: bool):
+    if has_labeling:
+        if experiment_type.lower() in [
+            "one-shot",
+            "kin",
+            "mixture",
+            "mix_std_stm",
+            "kinetics",
+            "mix_pulse_chase",
+            "mix_kin_deg",
+        ]:
+            adata.X = adata.layers["total"].copy()
+        if experiment_type.lower() in ["deg", "degradation"] and has_splicing:
+            adata.X = adata.layers["spliced"].copy()
+        if experiment_type.lower() in ["deg", "degradation"] and not has_splicing:
+            main_warning(
+                "It is not possible to calculate RNA velocity from a degradation experiment which has no "
+                "splicing information."
+            )
+            adata.X = adata.layers["total"].copy()
+        else:
+            adata.X = adata.layers["total"].copy()
+    else:
+        adata.X = adata.layers["spliced"].copy()
+
+
+def del_raw_layers(adata: AnnData):
+    layers = list(adata.layers.keys())
+    for layer in layers:
+        if not layer.startswith("X_"):
+            del adata.layers[layer]
```

### Comparing `dynamo-release-1.2.0/dynamo/sample_data.py` & `dynamo-release-1.3.0/dynamo/sample_data.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/simulation/Gillespie.py` & `dynamo-release-1.3.0/dynamo/simulation/Gillespie.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,80 +2,65 @@
 # -*- coding: utf-8 -*-
 """
 Created on Wed Jan 30 11:21:25 2019
 
 @author: xqiu
 """
 
+from typing import Optional
+
 import anndata
 import pandas as pd
 import scipy.sparse
+from anndata import AnnData
 
 from .bif_os_inclusive_sim import osc_diff_dup, sim_diff, sim_osc, simulate
 from .utils import *
 
 
 # deterministic as well as noise
 def Gillespie(
-    a=None,
-    b=None,
-    la=None,
-    aa=None,
-    ai=None,
-    si=None,
-    be=None,
-    ga=None,
-    C0=np.zeros((5, 1)),
-    t_span=[0, 50],
-    n_traj=1,
-    t_eval=None,
-    dt=1,
-    method="basic",
-    verbose=False,
-):
+    a: Optional[float] = None,
+    b: Optional[float] = None,
+    la: Optional[float] = None,
+    aa: Optional[float] = None,
+    ai: Optional[float] = None,
+    si: Optional[float] = None,
+    be: Optional[float] = None,
+    ga: Optional[float] = None,
+    C0: np.ndarray = np.zeros((5, 1)),
+    t_span: List = [0, 50],
+    n_traj: int = 1,
+    t_eval: Optional[float] = None,
+    dt: float = 1,
+    method: str = "basic",
+    verbose: bool = False,
+) -> AnnData:
     """A simulator of RNA dynamics that includes RNA bursting, transcription, metabolic labeling, splicing, transcription, RNA/protein degradation
 
-    Parameters
-    ----------
-        a: `float` or None
-            rate of active promoter switches to inactive one
-        b: `float` or None
-            rate of inactive promoter switches to active one
-        la: `float` or None
-            lambda_: 4sU labelling rate
-        aa: `float` or None
-            transcription rate with active promoter
-        ai: `float` or None
-            transcription rate with inactive promoter
-        si: `float` or None
-            sigma, degradation rate
-        be: `float` or None
-            beta, splicing rate
-        ga: `float` or None
-            gamma: the fraction of labeled u turns to unlabeled s
-        C0: `numpy.ndarray` (default: np.zeros((5, 1)))
-            A numpy array with dimension of 5 x n_gene. Here 5 corresponds to the five species (s - promoter state, ul,
+    Args:
+        a: rate of active promoter switches to inactive one
+        b: rate of inactive promoter switches to active one
+        la: lambda_: 4sU labelling rate
+        aa: transcription rate with active promoter
+        ai: transcription rate with inactive promoter
+        si: sigma, degradation rate
+        be: beta, splicing rate
+        ga: gamma, the fraction of labeled u turns to unlabeled s
+        C0: A numpy array with dimension of 5 x n_gene. Here 5 corresponds to the five species (s - promoter state, ul,
             uu, sl, su) for each gene.
-        t_span:
-            list of between and end time of simulation
-        n_traj:
-            number of simulation trajectory to use
-        t_eval: `float` or None
-            the time points at which data is simulated
-        dt: `float` (default: `1`)
-            delta t used in simulation
-        method: `str` (default: basic)
-            method to simulate the expression dynamics
-        verbose: `bool` (default: False)
-            whether to report running information
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object containing the simulated data.
+        t_span: list of between and end time of simulation
+        n_traj: number of simulation trajectory to use
+        t_eval: the time points at which data is simulated
+        dt: delta t used in simulation
+        method: method to simulate the expression dynamics
+        verbose: whether to report running information
+
+    Returns:
+        adata: an Annodata object containing the simulated data.
     """
 
     gene_num, species_num = C0.shape[0:2]
     adata_no_splicing, P = None, None
 
     if method == "basic":
         if t_eval is None:
```

### Comparing `dynamo-release-1.2.0/dynamo/simulation/__init__.py` & `dynamo-release-1.3.0/dynamo/simulation/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 from .evaluation import evaluate
 from .Gillespie import Gillespie
 from .ODE import (
     Simulator,
     Ying_model,
     jacobian_bifur2genes,
-    neurogenesis,
+    neurongenesis,
     ode_bifur2genes,
     state_space_sampler,
     toggle,
 )
 from .simulate_anndata import (
     AnnDataSimulator,
     BifurcationTwoGenes,
     CellularModelSimulator,
     KinLabelingSimulator,
-    Neurogenesis,
+    Neurongenesis,
     OscillationTwoGenes,
     bifur2genes_params,
     bifur2genes_splicing_params,
-    neurogenesis_params,
+    neurongenesis_params,
     osc2genes_params,
     osc2genes_splicing_params,
 )
 from .utils import CellularSpecies, directMethod
```

### Comparing `dynamo-release-1.2.0/dynamo/simulation/bif_os_inclusive_sim.py` & `dynamo-release-1.3.0/dynamo/simulation/bif_os_inclusive_sim.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/simulation/evaluation.py` & `dynamo-release-1.3.0/dynamo/simulation/evaluation.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,31 +1,25 @@
 import numpy as np
 from sklearn.metrics import mean_squared_error
 
 from ..tools.utils import einsum_correlation
 
 
-def evaluate(reference, prediction, metric="cosine"):
+def evaluate(reference: np.ndarray, prediction: np.ndarray, metric: str = "cosine") -> float:
     """Function to evaluate the vector field related reference quantities vs. that from vector field prediction.
 
-    Parameters
-    ----------
-        reference: `numpy.ndarray`
-            The reference quantity of the vector field (for example, simulated velocity vectors at each point or trajectory,
+    Args:
+        reference: The reference quantity of the vector field (for example, simulated velocity vectors at each point or trajectory,
             or estimated RNA velocity vector)
-        prediction: `numpy.ndarray`
-            The predicted quantity of the vector field (for example, velocity vectors calculated based on reconstructed vector
+        prediction: The predicted quantity of the vector field (for example, velocity vectors calculated based on reconstructed vector
             field function at each point or trajectory, or reconstructed RNA velocity vector)
-        metric: `str`
-            The metric for benchmarking the vector field quantities after reconstruction.
+        metric: The metric for benchmarking the vector field quantities after reconstruction.
 
-    Returns
-    -------
-        res: `float`
-            The score between the reference vs. reconstructed quantities based on the metric.
+    Returns:
+        res: The score between the reference vs. reconstructed quantities based on the metric.
     """
 
     if metric == "cosine":
         true_normalized = reference / (np.linalg.norm(reference, axis=1).reshape(-1, 1) + 1e-20)
         predict_normalized = prediction / (np.linalg.norm(prediction, axis=1).reshape(-1, 1) + 1e-20)
 
         res = np.mean(true_normalized * predict_normalized) * prediction.shape[1]
```

### Comparing `dynamo-release-1.2.0/dynamo/simulation/simulate_anndata.py` & `dynamo-release-1.3.0/dynamo/simulation/simulate_anndata.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Callable, Union
+from typing import Callable, Dict, List, Optional, Union
 
 import anndata
 import numpy as np
 import pandas as pd
 
 # dynamo logger related
 from ..dynamo_logger import (
@@ -14,17 +14,17 @@
     main_warning,
 )
 from ..tools.sampling import sample
 from ..tools.utils import flatten, isarray
 from .ODE import (
     hill_act_func,
     hill_inh_func,
-    neurogenesis,
+    neurongenesis,
     ode_bifur2genes,
-    ode_neurogenesis,
+    ode_neurongenesis,
     ode_osc2genes,
 )
 from .utils import CellularSpecies, GillespieReactions, Reaction
 
 bifur2genes_params = {
     "gamma": [0.2, 0.2],
     "a": [0.5, 0.5],
@@ -59,32 +59,32 @@
     "a": [1.5, 0.5],
     "b": [1.0, 2.5],
     "S": [2.5, 2.5],
     "K": [2.5, 2.5],
     "m": [5, 5],
     "n": [10, 10],
 }
-neurogenesis_params = {
+neurongenesis_params = {
     "gamma": np.ones(12),
     "a": [2.2, 4, 3, 3, 3, 4, 5, 5, 3, 3, 3, 3],
     "K": [10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
     "n": 4 * np.ones(12),
 }
 
 
 class AnnDataSimulator:
     def __init__(
         self,
         reactions: GillespieReactions,
-        C0s,
-        param_dict,
+        C0s: Optional[np.ndarray],
+        param_dict: Dict,
         species: Union[None, CellularSpecies] = None,
-        gene_param_names=[],
-        required_param_names=[],
-        velocity_func=None,
+        gene_param_names: List = [],
+        required_param_names: List = [],
+        velocity_func: Optional[Callable] = None,
     ) -> None:
 
         # initialization of variables
         self.reactions = reactions
         self.C0s = np.atleast_2d(C0s)
         self.param_dict = param_dict
         self.gene_param_names = gene_param_names
@@ -243,52 +243,42 @@
 
         return adata
 
 
 class CellularModelSimulator(AnnDataSimulator):
     def __init__(
         self,
-        gene_names: list,
-        synthesis_param_names: list,
-        param_dict: dict,
-        molecular_param_names: list = [],
-        kinetic_param_names: list = [],
-        C0s: np.ndarray = None,
+        gene_names: List,
+        synthesis_param_names: List,
+        param_dict: Dict,
+        molecular_param_names: List = [],
+        kinetic_param_names: List = [],
+        C0s: Optional[np.ndarray] = None,
         r_aug: float = 1,
         tau: float = 1,
         n_C0s: int = 10,
-        velocity_func: Union[None, Callable] = None,
-        report_stoich=False,
+        velocity_func: Optional[Callable] = None,
+        report_stoich: bool = False,
     ) -> None:
         """
         An anndata simulator class handling models with synthesis, splicing (optional), and first-order degrdation reactions.
 
-        Parameters
-        ----------
-            gene_names: list
-                List of gene names.
-            synthesis_param_names: list
-                List of kinetic parameters used to calculate synthesis rates.
-            param_dict: dict
-                The parameter dictionary containing "a", "b", "S", "K", "m", "n", "beta" (optional), "gamma"
+        Args:
+            gene_names: List of gene names.
+            synthesis_param_names: List of kinetic parameters used to calculate synthesis rates.
+            param_dict: The parameter dictionary containing "a", "b", "S", "K", "m", "n", "beta" (optional), "gamma"
                 if `param_dict` has the key "beta", the simulation includes the splicing process and therefore has 4 species (`unspliced` and `spliced` for each gene).
-            molecular_param_names: list
-                List of names of parameters which has `number of molecules` in their dimensions. These parameters will be multiplied with `r_aug` for scaling.
-            kinetic_param_names: list
-                List of names of parameters which has `time` in their dimensions. These parameters will be multiplied with `tau` to scale the kinetics.
-            C0s: None or :class:`~numpy.ndarray`
-                Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions.
-            r_aug: float
-                Parameter which augments steady state copy number for r1 and r2. At steady state, r1_s ~ r*(a1+b1)/ga1; r2_s ~ r*(a2+b2)/ga2
-            tau: float
-                Time scale parameter which does not affect steady state solutions.
-            n_C0s: int
-                Number of augmented initial conditions, if C0s is `None`.
-            velocity_func: None or Callable
-                Function used to calculate velocity. If `None`, the velocity will not be calculated.
+            molecular_param_names: List of names of parameters which has `number of molecules` in their dimensions. These parameters will be multiplied with `r_aug` for scaling.
+            kinetic_param_names: List of names of parameters which has `time` in their dimensions. These parameters will be multiplied with `tau` to scale the kinetics.
+            C0s: Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions.
+            r_aug: Parameter which augments steady state copy number for r1 and r2. At steady state, r1_s ~ r*(a1+b1)/ga1; r2_s ~ r*(a2+b2)/ga2
+            tau: Time scale parameter which does not affect steady state solutions.
+            n_C0s: Number of augmented initial conditions, if C0s is `None`.
+            velocity_func: Function used to calculate velocity. If `None`, the velocity will not be calculated.
+            report_stoich: Whether to report the Stoichiometry Matrix.
         """
         self.splicing = True if "beta" in param_dict.keys() else False
         self.gene_names = gene_names
 
         # register species
         species = CellularSpecies(gene_names)
         if self.splicing:
@@ -512,33 +502,36 @@
         else:
             raise Exception("No simulated data has been generated; Run simulation first.")
 
         return adata
 
 
 class BifurcationTwoGenes(CellularModelSimulator):
-    def __init__(self, param_dict, C0s=None, r_aug=20, tau=3, n_C0s=10, gene_names=None, report_stoich=False) -> None:
+    def __init__(
+        self,
+        param_dict: Dict,
+        C0s: Optional[np.ndarray] = None,
+        r_aug: float = 20,
+        tau: float = 3,
+        n_C0s: int = 10,
+        gene_names: Optional[List] = None,
+        report_stoich: bool = False,
+    ) -> None:
         """
         Two gene toggle switch model anndata simulator.
 
-        Parameters
-        ----------
-            param_dict: dict
-                The parameter dictionary containing "a", "b", "S", "K", "m", "n", "beta" (optional), "gamma"
+        Args:
+            param_dict: The parameter dictionary containing "a", "b", "S", "K", "m", "n", "beta" (optional), "gamma"
                 if `param_dict` has the key "beta", the simulation includes the splicing process and therefore has 4 species (u and s for each gene).
-            C0s: None or :class:`~numpy.ndarray`
-                Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions based on the steady states.
-            r_aug: float
-                Parameter which augments steady state copy number for gene 1 (r1) and gene 2 (r2). At steady state, r1_s ~ r*(a1+b1)/ga1; r2_s ~ r*(a2+b2)/ga2
-            tau: float
-                Time scale parameter which does not affect steady state solutions.
-            n_C0s: int
-                Number of augmented initial conditions, if C0s is `None`.
-            gene_names: None or list
-                List of gene names. If `None`, "gene_1", "gene_2", etc., are used.
+            C0s: Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions based on the steady states.
+            r_aug: Parameter which augments steady state copy number for gene 1 (r1) and gene 2 (r2). At steady state, r1_s ~ r*(a1+b1)/ga1; r2_s ~ r*(a2+b2)/ga2
+            tau: Time scale parameter which does not affect steady state solutions.
+            n_C0s: Number of augmented initial conditions, if C0s is `None`.
+            gene_names: List of gene names. If `None`, "gene_1", "gene_2", etc., are used.
+            report_stoich: Whether to report the Stoichiometry Matrix.
         """
         if gene_names is None:
             gene_names = ["gene_1", "gene_2"]
 
         super().__init__(
             gene_names,
             synthesis_param_names=["a", "b", "S", "K", "m", "n"],
@@ -602,33 +595,36 @@
                 Reaction([], [x2], lambda C, x1=x1, x2=x2: rate_syn(C[x2], C[x1], 1), desc="synthesis")
             )
 
         super().register_reactions(reactions)
 
 
 class OscillationTwoGenes(CellularModelSimulator):
-    def __init__(self, param_dict, C0s=None, r_aug=20, tau=3, n_C0s=10, gene_names=None, report_stoich=False) -> None:
+    def __init__(
+        self,
+        param_dict: Dict,
+        C0s: Optional[np.ndarray] = None,
+        r_aug: float = 20,
+        tau: float = 3,
+        n_C0s: int = 10,
+        gene_names: Optional[List] = None,
+        report_stoich: bool = False,
+    ) -> None:
         """
         Two gene oscillation model anndata simulator. This is essentially a predator-prey model, where gene 1 (predator) inhibits gene 2 (prey) and gene 2 activates gene 1.
 
-        Parameters
-        ----------
-            param_dict: dict
-                The parameter dictionary containing "a", "b", "S", "K", "m", "n", "beta" (optional), "gamma"
+        Args:
+            param_dict: The parameter dictionary containing "a", "b", "S", "K", "m", "n", "beta" (optional), "gamma"
                 if `param_dict` has the key "beta", the simulation includes the splicing process and therefore has 4 species (u and s for each gene).
-            C0s: None or :class:`~numpy.ndarray`
-                Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions based on the steady states.
-            r_aug: float
-                Parameter which augments copy numbers for the two genes.
-            tau: float
-                Time scale parameter which does not affect steady state solutions.
-            n_C0s: int
-                Number of augmented initial conditions, if C0s is `None`.
-            gene_names: None or list
-                List of gene names. If `None`, "gene_1", "gene_2", etc., are used.
+            C0s: Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions based on the steady states.
+            r_aug: Parameter which augments copy numbers for the two genes.
+            tau: Time scale parameter which does not affect steady state solutions.
+            n_C0s: Number of augmented initial conditions, if C0s is `None`.
+            gene_names: List of gene names. If `None`, "gene_1", "gene_2", etc., are used.
+            report_stoich: Whether to report the Stoichiometry Matrix.
         """
         if gene_names is None:
             gene_names = ["gene_1", "gene_2"]
 
         super().__init__(
             gene_names,
             synthesis_param_names=["a", "b", "S", "K", "m", "n"],
@@ -696,32 +692,35 @@
             reactions.register_reaction(
                 Reaction([], [x2], lambda C, x1=x1, x2=x2: rate_syn_2(C[x2], C[x1], 1), desc="synthesis")
             )
 
         super().register_reactions(reactions)
 
 
-class Neurogenesis(CellularModelSimulator):
-    def __init__(self, param_dict, C0s=None, r_aug=20, tau=3, n_C0s=10, report_stoich=False) -> None:
+class Neurongenesis(CellularModelSimulator):
+    def __init__(
+        self,
+        param_dict: Dict,
+        C0s: Optional[np.ndarray] = None,
+        r_aug: float = 20,
+        tau: float = 3,
+        n_C0s: int = 10,
+        report_stoich: bool = False,
+    ) -> None:
         """
-        Neurogenesis model from Xiaojie Qiu, et. al, 2012. anndata simulator.
+        Neurongenesis model from Xiaojie Qiu, et. al, 2012. anndata simulator.
 
-        Parameters
-        ----------
-            param_dict: dict
-                The parameter dictionary.
+        Args:
+            param_dict: The parameter dictionary.
                 if `param_dict` has the key "beta", the simulation includes the splicing process and therefore has 4 species (u and s for each gene).
-            C0s: None or :class:`~numpy.ndarray`
-                Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions based on the steady states.
-            r_aug: float
-                Parameter which augments steady state copy number for gene 1 (r1) and gene 2 (r2). At steady state, r1_s ~ r*(a1+b1)/ga1; r2_s ~ r*(a2+b2)/ga2
-            tau: float
-                Time scale parameter which does not affect steady state solutions.
-            n_C0s: int
-                Number of augmented initial conditions, if C0s is `None`.
+            C0s: Initial conditions (# init cond. by # species). If None, the simulator automatically generates `n_C0s` initial conditions based on the steady states.
+            r_aug: Parameter which augments steady state copy number for gene 1 (r1) and gene 2 (r2). At steady state, r1_s ~ r*(a1+b1)/ga1; r2_s ~ r*(a2+b2)/ga2
+            tau: Time scale parameter which does not affect steady state solutions.
+            n_C0s: Number of augmented initial conditions, if C0s is `None`.
+            report_stoich: Whether to report the Stoichiometry Matrix.
         """
 
         gene_names = [
             "Pax6",
             "Mash1",
             "Zic1",
             "Brn2",
@@ -741,15 +740,15 @@
             param_dict=param_dict,
             molecular_param_names=["a", "K"],
             kinetic_param_names=["a"],
             C0s=C0s,
             r_aug=r_aug,
             tau=tau,
             n_C0s=n_C0s,
-            velocity_func=ode_neurogenesis,
+            velocity_func=ode_neurongenesis,
             report_stoich=report_stoich,
         )
 
     def auto_C0(self, r_aug, tau):
         # C0 = np.ones(self.get_n_species()) * r_aug
         C0 = np.zeros(self.get_n_species())
         # TODO: splicing case
```

### Comparing `dynamo-release-1.2.0/dynamo/simulation/utils.py` & `dynamo-release-1.3.0/dynamo/simulation/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,50 +1,47 @@
-from typing import Callable, List, Union
+from typing import Callable, List, Optional, Tuple, Union
 
 import numpy as np
 
 # dynamo logger related
 from ..dynamo_logger import (
     LoggerManager,
     main_critical,
     main_exception,
     main_info,
     main_tqdm,
     main_warning,
 )
 
 
-def directMethod(prop_fcn: Callable, update_fcn: Callable, tspan, C0, record_skip_steps=0, record_max_length=1e5):
+def directMethod(
+    prop_fcn: Callable,
+    update_fcn: Callable,
+    tspan: List,
+    C0: np.ndarray,
+    record_skip_steps: int = 0,
+    record_max_length: int = 1e5,
+) -> Tuple[np.ndarray, np.ndarray]:
     """Gillespie direct method.
 
-    Parameters
-    ----------
-        prop_fcn: Callable
-            a function that calculates the propensity for each reaction.
+    Args:
+        prop_fcn: a function that calculates the propensity for each reaction.
             input: an array of copy numbers of all species;
             output: an array of propensities of all reactions.
-        update_fcn: Callable
-            a function that determines how the copy number of each species increases or decreases after each reaction.
+        update_fcn: a function that determines how the copy number of each species increases or decreases after each reaction.
             input: (1) an array of current copy numbers of all species; (2) the index of the occurred reaction.
             output: an array of updated of copy numbers of all species.
-        tspan: list
-            a list of starting and end simulation time, e.g. [0, 100].
-        C0: :class:`~numpy.ndarray`
-            A 1d array of initial conditions.
-        record_skip_steps: int
-            The number of reaction steps skipped when recording the trajectories.
-        record_max_length: int
-            The maximum length for recording the trajectories.
-
-    Returns
-    -------
-        retT: :class:`~numpy.ndarray`
-            a 1d numpy array of time points.
-        retC: :class:`~numpy.ndarray`
-            a 2d numpy array (n_species x n_time_points) of copy numbers for each species at each time point.
+        tspan: a list of starting and end simulation time, e.g. [0, 100].
+        C0: A 1d array of initial conditions.
+        record_skip_steps: The number of reaction steps skipped when recording the trajectories.
+        record_max_length: The maximum length for recording the trajectories.
+
+    Returns:
+        retT: a 1d numpy array of time points.
+        retC: a 2d numpy array (n_species x n_time_points) of copy numbers for each species at each time point.
     """
     retC = np.zeros((len(C0), int(record_max_length)), np.float64)
     retT = np.zeros(int(record_max_length), np.float64)
     c = C0.flatten()
     t = tspan[0]
     retC[:, 0] = c
     retT[0] = t
@@ -347,15 +344,15 @@
 
     def get_n_genes(self):
         return len(self.gene_names)
 
     def get_species_names(self):
         return self.species_dict.keys()
 
-    def register_species(self, species_name: str, is_gene_species=True):
+    def register_species(self, species_name: str, is_gene_species: bool = True):
         if self.get_n_genes() == 0 and is_gene_species:
             raise Exception("There is no gene and therefore cannot register gene species.")
         if species_name in self.species_dict:
             raise Exception(f"You have already registered {species_name}.")
         else:
             self._species_names.append(species_name)
             if not is_gene_species:
@@ -363,15 +360,15 @@
                 self.species_dict[species_name] = self.num_species
                 self.num_species += 1
             else:
                 self._is_gene_species.append(True)
                 self.species_dict[species_name] = [i + self.num_species for i in range(self.get_n_genes())]
                 self.num_species += self.get_n_genes()
 
-    def get_index(self, species, gene=None):
+    def get_index(self, species: str, gene: Optional[Union[int, str]] = None):
         if not species in self.species_dict.keys():
             raise Exception(f"Unregistered species `{species}`")
         idx = self.species_dict[species]
         if gene is not None:
             if type(gene) == str and gene in self.gene_names:
                 idx = next(k for i, k in enumerate(idx) if self.gene_names[i] == gene)
             elif type(gene) == int and gene < self.get_n_genes():
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/Markov.py` & `dynamo-release-1.3.0/dynamo/tools/Markov.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/tools/__init__.py` & `dynamo-release-1.3.0/dynamo/tools/__init__.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/tools/_dynamics_deprecated.py` & `dynamo-release-1.3.0/dynamo/tools/_dynamics_deprecated.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/tools/cell_velocities.py` & `dynamo-release-1.3.0/dynamo/tools/cell_velocities.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,23 @@
-from typing import Union
+from typing import Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
-import anndata
 import numpy as np
 import scipy
-import scipy as scp
 import scipy.sparse as sp
+from anndata import AnnData
 from numba import jit
 from sklearn.decomposition import PCA
 from sklearn.utils import sparsefuncs
 
+from ..configuration import DKM
 from ..dynamo_logger import LoggerManager, main_info, main_warning
 from ..utils import areinstance
 from .connectivity import _gen_neighbor_keys, adj_to_knn, check_and_recompute_neighbors
 from .dimension_reduction import reduceDimension
 from .graph_calculus import calc_gaussian_weight, fp_operator, graphize_velocity
 from .Markov import ContinuousTimeMarkovChain, KernelMarkovChain, velocity_on_grid
 from .metric_velocity import gene_wise_confidence
@@ -30,160 +35,140 @@
     set_transition_genes,
     split_velocity_graph,
     update_dict,
 )
 
 
 def cell_velocities(
-    adata: anndata.AnnData,
+    adata: AnnData,
     ekey: Union[str, None] = None,
     vkey: Union[str, None] = None,
-    X: Union[np.array, scipy.sparse.csr_matrix, None] = None,
-    V: Union[np.array, scipy.sparse.csr_matrix, None] = None,
-    X_embedding: Union[str, None] = None,
-    transition_matrix: Union[np.array, scipy.sparse.csr_matrix, None] = None,
+    X: Union[np.array, sp.csr_matrix, None] = None,
+    V: Union[np.array, sp.csr_matrix, None] = None,
+    X_embedding: Union[np.ndarray, None] = None,
+    transition_matrix: Union[np.ndarray, sp.csr_matrix, None] = None,
     use_mnn: bool = False,
     n_pca_components: Union[int, None] = None,
-    transition_genes: Union[str, list, None] = None,
+    transition_genes: Union[str, List[str], List[bool], None] = None,
     min_r2: Union[float, None] = None,
     min_alpha: Union[float, None] = None,
     min_gamma: Union[float, None] = None,
     min_delta: Union[float, None] = None,
     basis: str = "umap",
     neighbor_key_prefix: str = "",
     adj_key: str = "distances",
     add_transition_key: str = None,
     add_velocity_key: str = None,
     n_neighbors: int = 30,
-    method: str = "pearson",
+    method: Literal["kmc", "fp", "cosine", "pearson", "transform"] = "pearson",
     neg_cells_trick: bool = True,
     calc_rnd_vel: bool = False,
-    xy_grid_nums: tuple = (50, 50),
+    xy_grid_nums: Tuple[int] = (50, 50),
     correct_density: bool = True,
     scale: bool = True,
     sample_fraction: Union[float, None] = None,
     random_seed: int = 19491001,
     enforce: bool = False,
     preserve_len: bool = False,
     **kernel_kwargs,
-) -> anndata.AnnData:
-    """Project high dimensional velocity vectors onto given low dimensional embeddings,
-    and/or compute cell transition probabilities.
-
-    When method='kmc', the Itô kernel is used which not only considers the correlation between the vector from any cell
-    to its nearest neighbors and its velocity vector but also the corresponding distances. We expect this new kernel
-    will enable us to visualize more intricate vector flow or steady states in low dimension. We also expect it will
-    improve the calculation of the stationary distribution or source states of sampled cells. The original
-    "correlation/cosine" velocity projection method is also supported. Kernels based on the reconstructed velocity field
-    is also possible.
-
-    With the `key` argument, `cell_velocities` can be called by `cell_accelerations` or `cell_curvature` to calculate
-    RNA acceleration/curvature vector for each cell.
-
-    Arguments
-    ---------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        ekey: str or None (optional, default None)
-            The dictionary key that corresponds to the gene expression in the layer attribute. By default, ekey and vkey
-            will be automatically detected from the adata object.
-        vkey: str or None (optional, default None)
-            The dictionary key that corresponds to the estimated velocity values in the layers attribute.
-        X: :class:`~numpy.ndarray` or :class:`~scipy.sparse.csr_matrix` or None (optional, default `None`)
-            The expression states of single cells (or expression states in reduced dimension, like pca, of single cells)
-        V: :class:`~numpy.ndarray` or :class:`~scipy.sparse.csr_matrix` or None (optional, default `None`)
-            The RNA velocity of single cells (or velocity estimates projected to reduced dimension, like pca, of single
-            cells). Note that X, V need to have the exact dimensionalities.
-        X_embedding: str or None (optional, default None)
-            The low expression reduced space (pca, umap, tsne, etc.) of single cells that RNA velocity will be projected
-            onto. Note X_embedding, X and V has to have the same cell/sample dimension and X_embedding should have
-            less feature dimension comparing that of X or V.
-        use_mnn: bool (optional, default False)
-            Whether to use mutual nearest neighbors for projecting the high dimensional velocity vectors. By default, we
-            don't use the mutual nearest neighbors. Mutual nearest neighbors are calculated from nearest neighbors
-            across different layers, which which accounts for cases where, for example, the cells from spliced
+) -> AnnData:
+    """Project high dimensional velocity vectors onto given low dimensional embeddings, and/or compute cell transition
+        probabilities.
+
+    Args:
+        adata: an AnnData object.
+        ekey: the dictionary key that corresponds to the gene expression in the layer attribute. If set to be None, ekey
+            will be automatically detected from the adata object. Defaults to None.
+        vkey: the dictionary key that corresponds to the estimated velocity values in the layers attribute. If set to be
+            None, vkey will be automatically detected from the adata object. Defaults to None.
+        X: the expression states of single cells (or expression states in reduced dimension, like pca, of single cells).
+            Defaults to None.
+        V: the RNA velocity of single cells (or velocity estimates projected to reduced dimension, like pca, of single
+            cells). Note that X, V need to have the exact dimensionalities. Defaults to None.
+        X_embedding: the low expression reduced space (pca, umap, tsne, etc.) of single cells that RNA velocity will be
+            projected onto. Note X_embedding, X and V has to have the same cell/sample dimension and X_embedding should
+            have less feature dimension comparing that of X or V. Defaults to None.
+        transition_matrix: the set of genes used for projection of hign dimensional velocity vectors. If None,
+            transition genes are determined based on the R2 of linear regression on phase planes. The argument can be
+            either a dictionary key of .var, a list of gene names, or a list of booleans of length .n_vars. Defaults to
+            None.
+        use_mnn: whether to use mutual nearest neighbors for projecting the high dimensional velocity vectors. By
+            default, we don't use the mutual nearest neighbors. Mutual nearest neighbors are calculated from nearest
+            neighbors across different layers, which accounts for cases where, for example, the cells from spliced
             expression may be nearest neighbors but far from nearest neighbors on unspliced data. Using mnn assumes your
-            data from different layers are reliable (otherwise it will destroy real signals).
-        n_pca_components: int (optional, default None)
-            The number of pca components to project the high dimensional X, V before calculating transition matrix for
-            velocity visualization. By default it is None and if method is `kmc`, n_pca_components will be reset to 30;
-            otherwise use all high dimensional data for velocity projection.
-        transition_genes: str, list, or None (optional, default None)
-            The set of genes used for projection of hign dimensional velocity vectors.
-            If None, transition genes are determined based on the R2 of linear regression on phase planes.
-            The argument can be either a dictionary key of .var, a list of gene names, or a list of booleans
-            of length .n_vars.
-        min_r2: float or None (optional, default None)
-            The minimal value of r-squared of the parameter fits for selecting transition genes.
-        min_alpha: float or None (optional, default None)
-            The minimal value of alpha kinetic parameter for selecting transition genes.
-        min_gamma: float or None (optional, default None)
-            The minimal value of gamma kinetic parameter for selecting transition genes.
-        min_delta: float or None (optional, default None)
-            The minimal value of delta kinetic parameter for selecting transition genes.
-        basis: str (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be `X_spliced_umap`
-            or `X_total_umap`, etc.
-        neighbor_key_prefix: str (optional, default `neighbors`)
-            The dictionary key prefix in .uns. Connectivity and distance matrix keys are also generate with this prefix in adata.obsp.
-        adj_key: str (optional, default `distances`)
-            The dictionary key for the adjacency matrix of the nearest neighbor graph in .obsp.
-        add_transition_key: str or None (default: None)
-            The dictionary key that will be used for storing the transition matrix in .obsp.
-        add_velocity_key: str or None (default: None)
-            The dictionary key that will be used for storing the low dimensional velocity projection matrix in .obsm.
-        method: str (optional, default `pearson`)
-            The method to calculate the transition matrix and project high dimensional vector to low dimension, either
-            `kmc`, `fp`, `cosine`, `pearson`, or `transform`. "kmc" is our new approach to learn the transition matrix
-            via diffusion approximation or an Itô kernel. "cosine" or "pearson" are the methods used in the original RNA
-            velocity paper or the scvelo paper (Note that scVelo implementation actually centers both dX and V, so its
-            cosine kernel is equivalent to pearson correlation kernel but we also provide the raw cosine kernel). "kmc"
-            option is arguable better than "correlation" or "cosine" as it not only considers the correlation but also
-            the distance of the nearest neighbors to the high dimensional velocity vector. Finally, the "transform"
-            method uses umap's transform method to transform new data points to the UMAP space. "transform" method is
-            NOT recommended. Kernels that are based on the reconstructed vector field in high dimension is also
-            possible.
-        neg_cells_trick: bool (optional, default True)
-            Whether we should handle cells having negative correlations in gene expression difference with high
-            dimensional velocity vector separately. This option was borrowed from scVelo package
+            data from different layers are reliable (otherwise it will destroy real signals). Defaults to False.
+        n_pca_components: the number of pca components to project the high dimensional X, V before calculating
+            transition matrix for velocity visualization. By default it is None and if method is `kmc`, n_pca_components
+            will be reset to 30; otherwise use all high dimensional data for velocity projection. Defaults to None.
+        transition_genes: the set of genes used for projection of hign dimensional velocity vectors. If None, transition
+            genes are determined based on the R2 of linear regression on phase planes. The argument can be either a
+            dictionary key of .var, a list of gene names, or a list of booleans of length .n_vars. Defaults to None.
+        min_r2: the minimal value of r-squared of the parameter fits for selecting transition genes. Defaults to None.
+        min_alpha: the minimal value of alpha kinetic parameter for selecting transition genes. Defaults to None.
+        min_gamma: the minimal value of gamma kinetic parameter for selecting transition genes. Defaults to None.
+        min_delta: the minimal value of delta kinetic parameter for selecting transition genes. Defaults to None.
+        basis: the dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be
+            `X_spliced_umap` or `X_total_umap`, etc. Defaults to "umap".
+        neighbor_key_prefix: the dictionary key prefix in .uns. Connectivity and distance matrix keys are also generate
+            with this prefix in adata.obsp. Defaults to "".
+        adj_key: the dictionary key for the adjacency matrix of the nearest neighbor graph in .obsp. Defaults to
+            "distances".
+        add_transition_key: the dictionary key that will be used for storing the transition matrix in .obsp. Defaults to
+            None.
+        add_velocity_key: the dictionary key that will be used for storing the low dimensional velocity projection
+            matrix in .obsm. Defaults to None.
+        n_neighbors: the number of neighbors to be used to calculate velocity projection. Defaults to 30.
+        method: the method to calculate the transition matrix and project high dimensional vector to low dimension,
+            either `kmc`, `fp`, `cosine`, `pearson`, or `transform`. "kmc" is our new approach to learn the transition
+            matrix via diffusion approximation or an Itô kernel. "cosine" or "pearson" are the methods used in the
+            original RNA velocity paper or the scvelo paper (Note that scVelo implementation actually centers both dX
+            and V, so its cosine kernel is equivalent to pearson correlation kernel but we also provide the raw cosine
+            kernel). "kmc" option is arguable better than "correlation" or "cosine" as it not only considers the
+            correlation but also the distance of the nearest neighbors to the high dimensional velocity vector. Finally,
+            the "transform" method uses umap's transform method to transform new data points to the UMAP space.
+            "transform" method is NOT recommended. Kernels that are based on the reconstructed vector field in high
+            dimension is also possible. Defaults to "pearson".
+        neg_cells_trick: whether to handle cells having negative correlations in gene expression difference with
+            high dimensional velocity vector separately. This option was borrowed from scVelo package
             (https://github.com/theislab/scvelo) and use in conjunction with "pearson" and "cosine" kernel. Not required
-            if method is set to be "kmc".
-        calc_rnd_vel: bool (default: False)
-            A logic flag to determine whether we will calculate the random velocity vectors which can be plotted
-            downstream as a negative control and used to adjust the quiver scale of the velocity field.
-        xy_grid_nums: tuple (default: (50, 50)).
-            A tuple of number of grids on each dimension.
-        correct_density: bool (default: True)
-            Whether to correct density when calculating the markov transition matrix.
-        scale: bool (default: False)
-            Whether to scale velocity when calculating the markov transition matrix, applicable to the `kmc` kernel.
-        sample_fraction: None or float (default: None)
-            The downsampled fraction of kNN for the purpose of acceleration, applicable to the `kmc` kernel.
-        random_seed: int (default: 19491001)
-            The random seed for numba to ensure consistency of the random velocity vectors. Default value 19491001 is a
-            special day for those who care.
-        key: str or None (default: None)
-            The prefix key that will be prefixed to the keys for storing calculated transition matrix, projection
-            vectors, etc.
-        preserve_len: bool (default: False)
-            Whether to preserve the length of high dimension vector length. When set to be True, the length  of low
-            dimension projected vector will be proportionally scaled to that of the high dimensional vector.
-        enforce: bool (default: False)
-            Whether to enforce 1) redefining use_for_transition column in obs attribute; However this is NOT executed if
-                                    the argument 'transition_genes' is not None.
-                               2) recalculation of the transition matrix.
-        kernel_kwargs: dict
-            A dictionary of paramters that will be passed to the kernel for constructing the transition matrix.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            Returns an updated :class:`~anndata.AnnData` with projected velocity vectors, and a cell transition matrix
-            calculated using either the Itô kernel method or similar methods from (La Manno et al. 2018).
+            if method is set to be "kmc". Defaults to True.
+        calc_rnd_vel: whether to calculate the random velocity vectors which can be plotted downstream as a negative
+            control and used to adjust the quiver scale of the velocity field. Defaults to False.
+        xy_grid_nums: a tuple of number of grids on each dimension. Defaults to (50, 50).
+        correct_density: whether to correct density when calculating the markov transition matrix. Defaults to True.
+        scale: whether to scale velocity when calculating the markov transition matrix, applicable to the `kmc` kernel.
+            Defaults to True.
+        sample_fraction: the downsampled fraction of kNN for the purpose of acceleration, applicable to the `kmc`
+            kernel. Defaults to None.
+        random_seed: the random seed for numba to ensure consistency of the random velocity vectors. Default value
+            19491001 is a special day for those who care. Defaults to 19491001.
+        enforce: Whether to enforce
+            1) redefining use_for_transition column in obs attribute; However this is NOT executed if the argument
+                'transition_genes' is not None.
+            2) recalculation of the transition matrix. Defaults to False.
+        preserve_len: whether to preserve the length of high dimension vector length. When set to be True, the length of
+            low dimension projected vector will be proportionally scaled to that of the high dimensional vector.
+            Defaults to False.
+        kernel_kwargs: kwargs that would be passed to the kernel for constructing the transition matrix.
+
+    Raises:
+        Exception: neighborhood info is invalid.
+        TypeError: transition gene list is invalid.
+        ValueError: provided transition genes do not have velocity data.
+        Exception: `X` and `V` have different dimensions.
+        Exception: `X` and `X_embedding` has different number of samples.
+        Exception: Number of dimension of `X` is smaller than the one of `X_embedding`.
+        Exception: Most calculated velocity is theoretically invalid.
+        NotImplementedError: the mode provided in kernel_kwargs is invalid.
+
+    Returns:
+        an updated AnnData object with projected velocity vectors, and a cell transition matrix calculated using either
+        the Itô kernel method or similar methods from (La Manno et al. 2018).
     """
+
     conn_key, dist_key, neighbor_key = _gen_neighbor_keys(neighbor_key_prefix)
     mapper_r = get_mapper_inverse()
     layer = mapper_r[ekey] if (ekey is not None and ekey in mapper_r.keys()) else ekey
     ekey, vkey, layer = get_ekey_vkey_from_adata(adata) if (ekey is None or vkey is None) else (ekey, vkey, layer)
 
     if calc_rnd_vel:
         numba_random_seed(random_seed)
@@ -331,18 +316,18 @@
             "The number of dimensions of X is smaller than that of the embedding. Try lower the min_r2, "
             "min_gamma thresholds."
         )
 
     V = V.A if sp.issparse(V) else V
     X = X.A if sp.issparse(X) else X
     finite_inds = get_finite_inds(V)
-    X, V = X[:, finite_inds], V[:, finite_inds]
 
-    if sum(finite_inds) != X.shape[0]:
+    if sum(finite_inds) != X.shape[1]:
         main_info(f"{X.shape[1] - sum(finite_inds)} genes are removed because of nan velocity values.")
+        X, V = X[:, finite_inds], V[:, finite_inds]
         if transition_genes is not None:  # if X, V is provided by the user, transition_genes will be None
             adata.var.loc[np.array(transition_genes)[~finite_inds], "use_for_transition"] = False
 
     if finite_inds.sum() < 5 and len(finite_inds) > 100:
         raise Exception(
             f"there are only {finite_inds.sum()} genes have finite velocity values. "
             f"Please make sure the {vkey} is correctly calculated! And if you run kinetic parameters "
@@ -537,20 +522,20 @@
         umap_trans, n_pca_components = (
             adata.uns["umap_fit"]["fit"],
             adata.uns["umap_fit"]["n_pca_components"],
         )
 
         if "pca_fit" not in adata.uns_keys() or type(adata.uns["pca_fit"]) == str:
             CM = adata.X[:, adata.var.use_for_dynamics.values]
-            from ..preprocessing.utils import pca_monocle
+            from ..preprocessing.pca import pca
 
-            adata, pca_fit, X_pca = pca_monocle(adata, CM, n_pca_components, "X", return_all=True)
+            adata, pca_fit, X_pca = pca(adata, CM, n_pca_components, "X", return_all=True)
             adata.uns["pca_fit"] = pca_fit
 
-        X_pca, pca_fit = adata.obsm["X"], adata.uns["pca_fit"]
+        X_pca, pca_fit = adata.obsm[DKM.X_PCA], adata.uns["pca_fit"]
         V = adata[:, adata.var.use_for_dynamics.values].layers[vkey] if vkey in adata.layers.keys() else None
         CM, V = CM.A if sp.issparse(CM) else CM, V.A if sp.issparse(V) else V
         V[np.isnan(V)] = 0
         Y_pca = pca_fit.transform(CM + V)
 
         Y = umap_trans.transform(Y_pca)
 
@@ -611,63 +596,57 @@
             "D": D_rnd,
         }
 
     return adata
 
 
 def confident_cell_velocities(
-    adata,
-    group,
-    lineage_dict,
-    ekey="M_s",
-    vkey="velocity_S",
-    basis="umap",
-    confidence_threshold=0.85,
-    only_transition_genes=False,
-):
-    """Confidently compute transition probability and project high dimension velocity vector to existing low dimension
+    adata: AnnData,
+    group: str,
+    lineage_dict: Dict[str, Union[List[str], str]],
+    ekey: Optional[str] = "M_s",
+    vkey: Optional[str] = "velocity_S",
+    basis: str = "umap",
+    confidence_threshold: float = 0.85,
+    only_transition_genes: bool = False,
+) -> AnnData:
+    """Compute transition probability and perform velocity projection
+
+    Confidently compute transition probability and project high dimension velocity vector to existing low dimension
     embeddings using progenitors and mature cell groups priors.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        group: str
-            The column key/name that identifies the cell state grouping information of cells. This will be used for
+    Args:
+        adata: an AnnData object.
+        group: the column key/name that identifies the cell state grouping information of cells. This will be used for
             calculating gene-wise confidence score in each cell state.
-        lineage_dict: dict
-            A dictionary describes lineage priors. Keys correspond to the group name from `group` that corresponding
-            to the state of one progenitor type while values correspond to the group names from `group` of one or
-            multiple terminal cell states. The best practice for determining terminal cell states are those fully
+        lineage_dict: a dictionary describes lineage priors. Keys correspond to the group name from `group` that
+            corresponding to the state of one progenitor type while values correspond to the group names from `group` of
+            one or multiple terminal cell states. The best practice for determining terminal cell states are those fully
             functional cells instead of intermediate cell states. Note that in python a dictionary key cannot be a list,
             so if you have two progenitor types converge into one terminal cell state, you need to create two records
             each with the same terminal cell as value but different progenitor as the key. Value can be either a string
             for one cell group or a list of string for multiple cell groups.
-        ekey: str or None (default: `M_s`)
-            The layer that will be used to retrieve data for identifying the gene is in induction or repression phase at
-            each cell state. If `None`, .X is used.
-        vkey: str or None (default: `velocity_S`)
-            The layer that will be used to retrieve velocity data for calculating gene-wise confidence. If `None`,
-            `velocity_S` is used.
-        basis: str (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute.
-        confidence_threshold: float (optional, default 0.85)
-            The minimal threshold of the mean of the average progenitors and the average mature cells prior based
-            gene-wise velocity confidence score. Only genes with score larger than this will be considered as confident
-            transition genes for velocity projection.
-        only_transition_genes: bool (optional, default False)
-            Whether only use previous identified transition genes for confident gene selection, followed by velocity
-            projection.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            Returns an updated `~anndata.AnnData` with only confident genes based transition_matrix and projected
-            embedding of high dimension velocity vectors in the existing embeddings of current cell state, calculated
-            using either the cosine kernel method from (La Manno et al. 2018) or the Itô kernel for the FP method, etc.
+        ekey: the layer that will be used to retrieve data for identifying the gene is in induction or repression phase
+            at each cell state. If `None`, .X is used. Defaults to "M_s".
+        vkey: the layer that will be used to retrieve velocity data for calculating gene-wise confidence. If `None`,
+            `velocity_S` is used. Defaults to "velocity_S".
+        basis: the dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Defaults to "umap".
+        confidence_threshold: the minimal threshold of the mean of the average progenitors and the average mature cells
+            prior based gene-wise velocity confidence score. Only genes with score larger than this will be considered
+            as confident transition genes for velocity projection. Defaults to 0.85.
+        only_transition_genes: whether only use previous identified transition genes for confident gene selection,
+            followed by velocity projection. Defaults to False.
+
+    Raises:
+        Exception: RNA velocity not evaluated.
+
+    Returns:
+        An updated `~anndata.AnnData` with only confident genes based transition_matrix and projected embedding of high
+        dimension velocity vectors in the existing embeddings of current cell state, calculated using either the cosine
+        kernel method from (La Manno et al. 2018) or the Itô kernel for the FP method, etc.
     """
 
     if not any([i.startswith("velocity") for i in adata.layers.keys()]):
         raise Exception(
             "You need to first run `dyn.tl.dynamics(adata)` to estimate kinetic parameters and obtain "
             "raw RNA velocity before running this function."
         )
@@ -713,33 +692,32 @@
         basis=basis,
         transition_genes=confident_genes,
     )
 
     return adata
 
 
-def stationary_distribution(adata, method="kmc", direction="both", calc_rnd=True):
+def stationary_distribution(
+    adata: AnnData,
+    method: str = "kmc",
+    direction: Literal["both", "forward", "backward"] = "both",
+    calc_rnd: bool = True,
+) -> None:
     """Compute stationary distribution of cells using the transition matrix.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        method: str (default: `kmc`)
-            The method to calculate the stationary distribution.
-        direction: str (default: `both`)
-            The direction of diffusion for calculating the stationary distribution, can be one of `both`, `forward`,
-            `backward`.
-        calc_rnd: bool (default: True)
-            Whether to also calculate the stationary distribution from the control randomized transition matrix.
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            Returns an updated `~anndata.AnnData` with source, sink stationary distributions and the randomized results,
-            depending on the direction and calc_rnd arguments.
+    Update the AnnData object with source, sink stationary distributions and the randomized results, depending on the
+    `direction` and `calc_rnd` arguments.
+
+    Args:
+        adata: an AnnData object.
+        method: the method to calculate the stationary distribution. Defaults to "kmc".
+        direction: the direction of diffusion for calculating the stationary distribution, can be one of `both`,
+            `forward`, `backward`. Defaults to "both".
+        calc_rnd: whether to also calculate the stationary distribution from the control randomized transition matrix.
+            Defaults to True.
     """
 
     # row is the source and columns are targets
     T = adata.obsp["transition_matrix"]
 
     if method == "kmc":
         kmc = KernelMarkovChain()
@@ -789,67 +767,56 @@
         elif direction == "backward":
             adata.obs["source_steady_state_distribution"] = diffusion(T, backward=True)
             if calc_rnd:
                 T_rnd = adata.obsp["transition_matrix_rnd"]
                 adata.obs["source_steady_state_distribution_rnd"] = diffusion(T_rnd, backward=True)
 
 
-def generalized_diffusion_map(adata, **kwargs):
+def generalized_diffusion_map(adata: AnnData, **kwargs) -> None:
     """Apply the diffusion map algorithm on the transition matrix build from Itô kernel.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the constructed transition matrix.
-        kwargs:
-            Additional parameters that will be passed to the diffusion_map_embedding function.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that updated with X_diffusion_map embedding in obsm attribute.
+    Update the AnnData object with X_diffusion_map embedded in obsm attribute.
+
+    Args:
+        adata: an AnnData object with the constructed transition matrix.
+        kwargs: additional kwargs that will be passed to `diffusion_map_embedding function`.
     """
 
     kmc = KernelMarkovChain()
     kmc.P = adata.obsp["transition_matrix"]
     dm_args = {"n_dims": 2, "t": 1}
     dm_args.update(kwargs)
     dm = kmc.diffusion_map_embedding(*dm_args)
 
     adata.obsm["X_diffusion_map"] = dm
 
 
-def diffusion(M, P0=None, steps=None, backward=False):
+def diffusion(
+    M: np.ndarray, P0: Optional[np.ndarray] = None, steps: Optional[int] = None, backward: bool = False
+) -> np.ndarray:
     """Find the state distribution of a Markov process.
 
-    Parameters
-    ----------
-        M: :class:`~numpy.ndarray` (dimension n x n, where n is the cell number)
-            The transition matrix.
-        P0: :class:`~numpy.ndarray` (default None; dimension is n, )
-            The initial cell state.
-        steps: int (default None)
-            The random walk steps on the Markov transitioin matrix.
-        backward: bool (default False)
-            Whether the backward transition will be considered.
-
-    Returns
-    -------
-        Mu: :class:`~numpy.ndarray`
-            The state distribution of the Markov process.
+    Args:
+        M: the transition matrix with dimension of n x n, where n is the cell number.
+        P0: The initial cell state with dimension of n. Defaults to None.
+        steps: the random walk steps on the Markov transition matrix. Defaults to None.
+        backward: whether the backward transition will be considered. Defaults to False.
+
+    Returns:
+        The state distribution of the Markov process.
     """
 
     if backward is True:
         M = M.T
         M = M / M.sum(1)
 
     if steps is None:
         # code inspired from  https://github.com/prob140/prob140/blob/master/prob140/markov_chains.py#L284
 
-        eigenvalue, eigen = scp.linalg.eig(
+        eigenvalue, eigen = scipy.linalg.eig(
             M, left=True, right=False
         )  # if not sp.issparse(M) else eigs(M) # source is on the row
 
         eigenvector = np.real(eigen) if not sp.issparse(M) else np.real(eigen.T)
         eigenvalue_1_ind = np.isclose(eigenvalue, 1)
         mu = eigenvector[:, eigenvalue_1_ind] / np.sum(eigenvector[:, eigenvalue_1_ind])
 
@@ -863,52 +830,78 @@
             if P0 is None
             else P0.dot(np.linalg.matrix_power(M, steps))
         )
 
     return mu
 
 
-def expected_return_time(M, backward=False):
+def expected_return_time(M: np.ndarray, backward=False) -> np.ndarray:
     """Find the expected returning time.
 
-    Parameters
-    ----------
-        M: :class:`~numpy.ndarray` (dimension n x n, where n is the cell number)
-            The transition matrix.
-        backward: bool (default False)
-            Whether the backward transition will be considered.
-
-    Returns
-    -------
-        T: :class:`~numpy.ndarray`
-            The expected return time (1 / steady_state_probability).
+    Args:
+        M: the transition matrix.
+        backward: whether the backward transition will be considered. Defaults to False.
 
+    Returns:
+        The expected return time (1 / steady_state_probability).
     """
+
     steady_state = diffusion(M, P0=None, steps=None, backward=backward)
 
     T = 1 / steady_state
     return T
 
 
 def kernels_from_velocyto_scvelo(
-    X,
-    X_embedding,
-    V,
-    adj_mat,
-    neg_cells_trick,
-    xy_grid_nums,
-    kernel="pearson",
-    n_recurse_neighbors=2,
-    max_neighs=None,
-    transform="sqrt",
-    use_neg_vals=True,
-    correct_density=True,
-):
-    """utility function for calculating the transition matrix and low dimensional velocity embedding via the original
-    pearson correlation kernel (La Manno et al., 2018) or the cosine kernel from scVelo (Bergen et al., 2019)."""
+    X: np.ndarray,
+    X_embedding: np.ndarray,
+    V: np.ndarray,
+    adj_mat: np.ndarray,
+    neg_cells_trick: bool,
+    xy_grid_nums: Tuple[int],
+    kernel: Literal["pearson", "cosine"] = "pearson",
+    n_recurse_neighbors: int = 2,
+    max_neighs: Optional[int] = None,
+    transform: Literal["log", "logratio", "linear", "sqrt"] = "sqrt",
+    use_neg_vals: bool = True,
+    correct_density: bool = True,
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
+    """Utility function for calculating the transition matrix and low dimensional velocity.
+
+    Two kernels, the original
+    pearson correlation kernel (La Manno et al., 2018) or the cosine kernel from scVelo (Bergen et al., 2019) are
+    available for calculation.
+
+    Args:
+        X: the expression state of single cells.
+        X_embedding: the low expression reduced space (pca, umap, tsne, etc.) of single cells that RNA velocity will be
+            projected onto. Note X_embedding, X and V has to have the same cell/sample dimension and X_embedding should
+            have less feature dimension comparing that of X or V.
+        V: the RNA velocity of single cells (or velocity estimates projected to reduced dimension, like pca, of single
+            cells). Note that X, V need to have the exact dimensionalities.
+        adj_mat: the neighbor indices.
+        neg_cells_trick: whether to handle cells having negative correlations in gene expression difference with high
+            dimensional velocity vector separately.
+        xy_grid_nums: a tuple of number of grids on each dimension.
+        kernel: the method to calculate correlation between X and velocity vector Y_i for gene i.  Defaults to
+            "pearson".
+        n_recurse_neighbors: the order of neighbors to be used to calculate velocity graph. Defaults to 2.
+        max_neighs: the max number of neighbors to be used to calculate velocity graph. If the number of neighbors
+            within the order of n_recurse_neighbors is larger than this value, a random subset will be chosen. Defaults
+            to None.
+        transform: the method to transform the original velocity matrix. Defaults to "sqrt".
+        use_neg_vals: whether to use negative values during handle cells having negative correlations. Defaults to True.
+        correct_density: whether to correct density when calculating the markov transition matrix. Defaults to True.
+
+    Returns:
+        A tuple (T, delta_X, X_grid, V_grid, D) where T is the transition matrix, delta_X is the low dimensional
+        velocity, X_grid is a grid for plotting velocities and V_grid is the velocities on the grid. D is the diffusion
+        matrix.
+    """
+
     n = X.shape[0]
     if adj_mat is not None:
         rows = []
         cols = []
         vals = []
 
     delta_X = np.zeros((n, X_embedding.shape[1]))
@@ -984,89 +977,31 @@
     )
 
     return T, delta_X, X_grid, V_grid, D
 
 
 # utility functions for calculating the random cell velocities
 @jit(nopython=True)
-def numba_random_seed(seed):
+def numba_random_seed(seed: int) -> None:
     """Same as np.random.seed but for numba. Function adapated from velocyto.
 
-    Parameters
-    ----------
-        seed: int
-            Random seed value
-
+    Args:
+        seed: the random seed value
     """
+
     np.random.seed(seed)
 
 
 @jit(nopython=True)
-def permute_rows_nsign(A):
-    """Permute in place the entries and randomly switch the sign for each row of a matrix independently. Function
-    adapted from velocyto
-
-    Parameters
-    ----------
-        A: :class:`~numpy.ndarray`
-            A numpy array that will be permuted.
+def permute_rows_nsign(A: np.ndarray) -> None:
+    """Permute in place the entries and randomly switch the sign for each row of a matrix independently.
+
+    The function is adapted from velocyto.
+
+    Args:
+        A: a numpy array that will be permuted.
     """
 
     plmi = np.array([+1, -1])
     for i in range(A.shape[1]):
         np.random.shuffle(A[:, i])
         A[:, i] = A[:, i] * np.random.choice(plmi, size=A.shape[0])
-
-
-"""This function can be removed now
-def embed_velocity(
-    adata,
-    x_basis,
-    v_basis="velocity",
-    emb_basis="X",
-    velocity_gene_tag="transition_genes",
-    num_pca=100,
-    n_recurse_neighbors=2,
-    M_diff=0.25,
-    adaptive_local_kernel=True,
-    normalize_velocity=True,
-    return_kmc=False,
-    **kmc_kwargs,
-):
-    if velocity_gene_tag is not None:
-        X = adata.layers[x_basis][:, adata.var[velocity_gene_tag]]
-        V = adata.layers[v_basis][:, adata.var[velocity_gene_tag]]
-    else:
-        X = adata.layers[x_basis]
-        V = adata.layers[v_basis]
-
-    X = log1p_(adata, X)
-
-    X_emb = adata.obsm[emb_basis]
-    Idx = adata.uns["neighbors"]["indices"]
-
-    if num_pca is not None:
-        pca = PCA()
-        pca.fit(X)
-        X_pca = pca.transform(X)
-        Y_pca = pca.transform(X + V)
-        V_pca = Y_pca - X_pca
-    else:
-        X_pca = X
-        V_pca = V
-
-    kmc = KernelMarkovChain()
-    kmc.fit(
-        X_pca[:, :num_pca],
-        V_pca[:, :num_pca],
-        neighbor_idx=Idx,
-        n_recurse_neighbors=n_recurse_neighbors,
-        M_diff=M_diff,
-        adaptive_local_kernel=adaptive_local_kernel,
-        **kmc_kwargs,
-    )
-
-    Uc = kmc.compute_density_corrected_drift(X_emb, normalize_vector=normalize_velocity)
-    if return_kmc:
-        return Uc, kmc
-    else:
-        return Uc"""
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/clustering.py` & `dynamo-release-1.3.0/dynamo/tools/clustering.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,89 +1,83 @@
-from typing import Optional, Union
+from typing import Any, Iterable, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
-import anndata
 import numpy as np
 import pandas as pd
 from anndata import AnnData
-from scipy.sparse import csr_matrix
-from sklearn.neighbors import NearestNeighbors
+from scipy.sparse import csr_matrix, issparse
 
 from ..configuration import DKM
 from ..dynamo_logger import main_info
-from ..preprocessing.preprocessor_utils import filter_genes_by_outliers as filter_genes
-from ..preprocessing.preprocessor_utils import log1p_adata as log1p
-from ..preprocessing.preprocessor_utils import normalize_cell_expr_by_size_factors
-from ..preprocessing.utils import pca_monocle
+from ..preprocessing.normalization import normalize
+from ..preprocessing.QC import filter_genes_by_outliers as filter_genes
+from ..preprocessing.pca import pca
+from ..preprocessing.transform import log1p
 from ..utils import LoggerManager, copy_adata
 from .connectivity import _gen_neighbor_keys, neighbors
 from .utils import update_dict
 from .utils_reduceDimension import prepare_dim_reduction, run_reduce_dim
 
 
 def hdbscan(
-    adata,
-    X_data=None,
-    genes=None,
-    layer=None,
-    basis="pca",
-    dims=None,
-    n_pca_components=30,
-    n_components=2,
-    result_key=None,
-    copy=False,
+    adata: AnnData,
+    X_data: Optional[np.ndarray] = None,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    basis: str = "pca",
+    dims: Optional[List[int]] = None,
+    n_pca_components: int = 30,
+    n_components: int = 2,
+    result_key: Optional[str] = None,
+    copy: bool = False,
     **hdbscan_kwargs
-):
+) -> Optional[AnnData]:
     """Apply hdbscan to cluster cells in the space defined by basis.
 
     HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander
-    (https://doi.org/10.1007/978-3-642-37456-2_14) which extends DBSCAN by converting
-    it into a hierarchical clustering algorithm, followed by using a technique to extract
-    a flat clustering based in the stability of clusters. Here you can use hdbscan to
-    cluster your data in any space specified by `basis`. The data that used to produced
-    from this space can be specified by `layer`. Thus, you are able to use either the
-    unspliced or new RNA data for dimension reduction and clustering. HDBSCAN is a density
-    based method, it thus requires you to perform clustering on relatively low dimension,
-    for example top 30 PCs or top 5 umap dimension with at least several thousands of cells.
-    In practice, HDBSCAN will assign -1 for cells that have low local density and thus not
-    able to confidentially assign to any clusters.
+    (https://doi.org/10.1007/978-3-642-37456-2_14) which extends DBSCAN by converting it into a hierarchical clustering
+    algorithm, followed by using a technique to extract a flat clustering based in the stability of clusters. Here you
+    can use hdbscan to cluster your data in any space specified by `basis`. The data that used to produced from this
+    space can be specified by `layer`. Thus, you are able to use either the unspliced or new RNA data for dimension
+    reduction and clustering. HDBSCAN is a density based method, it thus requires you to perform clustering on
+    relatively low dimension, for example top 30 PCs or top 5 umap dimension with at least several thousands of cells.
+    In practice, HDBSCAN will assign -1 for cells that have low local density and thus not able to confidentially assign
+    to any clusters.
 
     The hdbscan package from Leland McInnes, John Healy, Steve Astels Revision is used.
 
-    Parameters
-    ----------
-    adata: :class:`~anndata.AnnData`
-        AnnData object.
-    X_data: `np.ndarray` (default: `None`)
-        The user supplied data that will be used for clustering directly.
-    genes: `list` or None (default: `None`)
-        The list of genes that will be used to subset the data for dimension reduction and clustering. If `None`, all
-        genes will be used.
-    layer: `str` or None (default: `None`)
-        The layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is used.
-    basis: `str` or None (default: `None`)
-        The space that will be used for clustering. Valid names includes, for example, `pca`, `umap`, `velocity_pca`
-        (that is, you can use velocity for clustering), etc.
-    dims: `list` or None (default: `None`)
-        The list of dimensions that will be selected for clustering. If `None`, all dimensions will be used.
-    n_pca_components: `int` (default: `30`)
-        The number of pca components that will be used.
-    n_components: `int` (default: `2`)
-        The number of dimension that non-linear dimension reduction will be projected to.
-    copy:
-        Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments.
-    hdbscan_kwargs: `dict`
-        Additional parameters that will be passed to hdbscan function.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An updated AnnData object with the clustering updated. `hdbscan` and `hdbscan_prob` are two newly added
-            columns from .obs, corresponding to either the Cluster results or the probability of each cell belong to a
-            cluster. `hdbscan` key in .uns corresponds to a dictionary that includes additional results returned from
-            hdbscan run.
+    Args:
+        adata: an AnnData object.
+        X_data: the user supplied data that will be used for clustering directly. Defaults to None.
+        genes: the list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
+            all genes will be used. Defaults to None.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        basis: the space that will be used for clustering. Valid names includes, for example, `pca`, `umap`,
+            `velocity_pca` (that is, you can use velocity for clustering), etc. Defaults to "pca".
+        dims: the list of dimensions that will be selected for clustering. If `None`, all dimensions will be used.
+            Defaults to None.
+        n_pca_components: the number of pca components that will be used. Defaults to 30.
+        n_components: the number of dimension that non-linear dimension reduction will be projected to. Defaults to 2.
+        result_key: the key for storing clustering results in .obs and .uns. Defaults to None.
+        copy: whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments.
+            Defaults to False.
+
+    Raises:
+        ImportError: hdbscan not installed.
+
+    Returns:
+        An updated AnnData object with the clustering updated. `hdbscan` and `hdbscan_prob` are two newly added columns
+        from .obs, corresponding to either the Cluster results or the probability of each cell belong to a cluster.
+        `hdbscan` key in .uns corresponds to a dictionary that includes additional results returned from hdbscan run.
+        Returned if `copy` is true.
     """
 
     try:
         from hdbscan import HDBSCAN
     except ImportError:
         raise ImportError("You need to install the package `hdbscan`." "install hdbscan via `pip install hdbscan`")
 
@@ -151,18 +145,18 @@
     }
 
     h_kwargs = update_dict(h_kwargs, hdbscan_kwargs)
     cluster = HDBSCAN(**h_kwargs)
     cluster.fit(X_data)
 
     if result_key is None:
-        key = "hdbscan"
-    adata.obs[key] = cluster.labels_.astype("str")
-    adata.obs[key + "_prob"] = cluster.probabilities_
-    adata.uns[key] = {
+        result_key = "hdbscan"
+    adata.obs[result_key] = cluster.labels_.astype("str")
+    adata.obs[result_key + "_prob"] = cluster.probabilities_
+    adata.uns[result_key] = {
         "hdbscan": cluster.labels_.astype("str"),
         "probabilities_": cluster.probabilities_,
         "cluster_persistence_": cluster.cluster_persistence_,
         "outlier_scores_": cluster.outlier_scores_,
         "exemplars_": cluster.exemplars_,
     }
 
@@ -170,46 +164,81 @@
 
     if copy:
         return adata
     return None
 
 
 def leiden(
-    adata,
-    use_weight=True,
-    weight=None,
-    initial_membership=None,
-    adj_matrix=None,
-    adj_matrix_key=None,
-    result_key=None,
-    layer=None,
-    obsm_key=None,
-    selected_cluster_subset: list = None,
-    selected_cell_subset=None,
-    directed=False,
-    copy=False,
+    adata: AnnData,
+    resolution: float = 1.0,
+    use_weight: bool = False,
+    weights: Optional[Union[str, Iterable]] = None,
+    initial_membership: Optional[List[int]] = None,
+    adj_matrix: Optional[csr_matrix] = None,
+    adj_matrix_key: Optional[str] = None,
+    seed: Optional[int] = None,
+    result_key: Optional[str] = None,
+    layer: Optional[str] = None,
+    obsm_key: Optional[str] = None,
+    selected_cluster_subset: Optional[Tuple[str, List[int]]] = None,
+    selected_cell_subset: Optional[List[int]] = None,
+    directed: bool = True,
+    copy: bool = False,
     **kwargs
-) -> anndata.AnnData:
-    """Apply leiden clustering to adata.
-    For other community detection general parameters, please refer to ``dynamo's`` :py:meth:`~dynamo.tl.cluster_community` function.
-    "The Leiden algorithm is an improvement of the Louvain algorithm. The Leiden algorithm consists of three phases: (1) local moving of nodes, (2) refinement of the partition (3) aggregation of the network based on the refined partition, using the non-refined partition to create an initial partition for the aggregate network." - cdlib
-
-    Parameters
-    ----------
-    weight :
-         weights of edges. Can be either an iterable or an edge attribute. Default None
-    initial_membership : optional
-        list of int Initial membership for the partition. If None then defaults to a singleton partition. Default None, by default None
+) -> AnnData:
+    """Apply leiden clustering to the input adata.
+
+    For other general community detection related parameters, please refer to ``dynamo's``
+    :py:meth:`~dynamo.tl.cluster_community` function.
+
+    The Leiden algorithm is an improvement of the Louvain algorithm. Based on the cdlib package, the Leiden algorithm
+    consists of three phases:
+    (1) local moving of nodes,
+    (2) refinement of the partition,
+    (3) aggregation of the network based on the refined partition, using the non-refined partition to create an initial
+    partition for the aggregate network.
+
+    Please note that since 2/23/23, we have replaced the integrated louvain method from cdlib package with that from the
+    original leidenalg package.
+
+    Args:
+        adata: an adata object
+        resolution: the resolution of the clustering that determines the level of detail in the clustering process.
+            An increase in this value will result in the generation of a greater number of clusters.
+        use_weight: whether to use the weight of the edges in the clustering process. Default False.
+        weights: weights of edges. Can be either an iterable (list of double) or an edge attribute.
+        initial_membership: list of int. Initial membership for the partition.
+            If None then defaults to a singleton partition.
+        adj_matrix: the adjacency matrix to use for the cluster_community function.
+        adj_matrix_key: the key of the adjacency matrix in adata.obsp used for the cluster_community function.
+        seed: seed for the random number generator. By default uses a random seed if nothing is specified.
+        result_key: the key to use for saving clustering results which will be included in both adata.obs and adata.uns.
+        layer: the adata layer where cluster algorithms will work on.
+        obsm_key: the key of the obsm that points to the expression embedding to be used for dyn.tl.neighbors to
+            calculate the nearest neighbor graph.
+        selected_cluster_subset: a tuple of 2 elements (cluster_key, allowed_clusters) filtering cells in adata based on
+            cluster_key in adata.obs and only reserves cells in the allowed clusters.
+        selected_cell_subset: a list of cell indices to cluster.
+        directed: whether the graph is directed.
+        copy: return a copy instead of writing to adata.
+        **kwargs: additional arguments to pass to the cluster_community function.
 
+    Returns:
+        adata: An updated AnnData object with the leiden clustering results added. The adata is updated up with the
+        `result_key` key to use for saving clustering results which will be included in both adata.obs and adata.uns.
+        adata.obs[result_key] saves the clustering identify of each cell where the adata.uns[result_key] saves the
+        relevant parameters for the leiden clustering .
     """
 
     kwargs.update(
         {
-            "weight": weight,
+            "resolution_parameter": resolution,
+            "weights": weights,
             "initial_membership": initial_membership,
+            "seed": seed,
         }
     )
 
     return cluster_community(
         adata,
         method="leiden",
         use_weight=use_weight,
@@ -223,46 +252,82 @@
         directed=directed,
         copy=copy,
         **kwargs
     )
 
 
 def louvain(
-    adata,
-    resolution=1.0,
-    use_weight=True,
-    adj_matrix=None,
-    adj_matrix_key=None,
-    randomize=False,
-    result_key=None,
-    layer=None,
-    obsm_key=None,
-    selected_cluster_subset: list = None,
-    selected_cell_subset=None,
-    directed=False,
-    copy=False,
+    adata: AnnData,
+    resolution: float = 1.0,
+    use_weight: bool = False,
+    weights: Optional[Union[str, Iterable]] = None,
+    initial_membership: Optional[List[int]] = None,
+    adj_matrix: Optional[csr_matrix] = None,
+    adj_matrix_key: Optional[str] = None,
+    seed: Optional[int] = None,
+    result_key: Optional[str] = None,
+    layer: Optional[str] = None,
+    obsm_key: Optional[str] = None,
+    selected_cluster_subset: Optional[Tuple[str, List[int]]] = None,
+    selected_cell_subset: Optional[List[int]] = None,
+    directed: bool = True,
+    copy: bool = False,
     **kwargs
-) -> anndata.AnnData:
-    """Louvain implementation from cdlib.
-    For other community detection general parameters, please refer to ``dynamo's`` :py:meth:`~dynamo.tl.cluster_community` function.
-    "Louvain maximizes a modularity score for each community. The algorithm optimises the modularity in two elementary phases: (1) local moving of nodes; (2) aggregation of the network. In the local moving phase, individual nodes are moved to the community that yields the largest increase in the quality function. In the aggregation phase, an aggregate network is created based on the partition obtained in the local moving phase. Each community in this partition becomes a node in the aggregate network. The two phases are repeated until the quality function cannot be increased further." - cdlib
-
-    Parameters
-    ----------
-    resolution : float, optional
-        change the size of the communities, default to 1.
-    randomize : bool, optional
-        "randomState instance or None, optional (default=None). If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random." - cdlib
+) -> AnnData:
+    """Apply louvain clustering to adata.
+
+    For other general community detection related parameters,
+    please refer to ``dynamo's`` :py:meth:`~dynamo.tl.cluster_community` function.
+
+    Based on the cdlib package, the Louvain algorithm optimises the modularity in two elementary phases:
+    (1) local moving of nodes;
+    (2) aggregation of the network.
+    In the local moving phase, individual nodes are moved to the community that yields the largest increase in the
+    quality function. In the aggregation phase, an aggregate network is created based on the partition obtained in the
+    local moving phase. Each community in this partition becomes a node in the aggregate network. The two phases are
+    repeated until the quality function cannot be increased further.
+
+    Please note that since 2/23/23, we have replaced the integrated louvain method from cdlib package with that from the
+    original louvain package.
+
+    Args:
+        adata: an adata object
+        resolution: the resolution of the clustering that determines the level of detail in the clustering process.
+            An increase in this value will result in the generation of a greater number of clusters.
+        use_weight: whether to use the weight of the edges in the clustering process. Default False
+        weights: weights of edges. Can be either an iterable (list of double) or an edge attribute.
+        initial_membership: list of int. Initial membership for the partition.
+            If None then defaults to a singleton partition.
+        adj_matrix: the adjacency matrix to use for the cluster_community function. Default None
+        adj_matrix_key: adj_matrix_key in adata.obsp used for the cluster_community function. Default None
+        seed: seed for the random number generator. By default uses a random seed if nothing is specified.
+        result_key: the key to use for saving clustering results which will be included in both adata.obs and adata.uns.
+        layer: the adata layer where cluster algorithms will work on.
+        obsm_key: the key of the obsm that points to the expression embedding to be used for dyn.tl.neighbors to
+            calculate the nearest neighbor graph.
+        selected_cluster_subset: a tuple of 2 elements (cluster_key, allowed_clusters) filtering cells in adata based on
+            cluster_key in adata.obs and only reserves cells in the allowed clusters.
+        selected_cell_subset: a list of cell indices to cluster.
+        directed: whether the graph is directed.
+        copy: return a copy instead of writing to adata.
+        **kwargs: additional arguments to pass to the clustering function.
+
+    Returns:
+        adata: An updated AnnData object with the leiden clustering results added. The adata is updated up with the
+        `result_key` key to use for saving clustering results which will be included in both adata.obs and adata.uns.
+        adata.obs[result_key] saves the clustering identify of each cell where the adata.uns[result_key] saves the
+        relevant parameters for the leiden clustering .
     """
-    if directed:
-        raise ValueError("CDlib does not support directed graph for Louvain community detection for now.")
+
     kwargs.update(
         {
-            "resolution": resolution,
-            "randomize": randomize,
+            "resolution_parameter": resolution,
+            "weights": weights,
+            "initial_membership": initial_membership,
+            "seed": seed,
         }
     )
 
     return cluster_community(
         adata,
         method="louvain",
         use_weight=use_weight,
@@ -276,31 +341,54 @@
         directed=directed,
         copy=copy,
         **kwargs
     )
 
 
 def infomap(
-    adata,
-    use_weight=True,
-    adj_matrix=None,
-    adj_matrix_key=None,
-    result_key=None,
-    layer=None,
-    obsm_key=None,
-    selected_cluster_subset: list = None,
-    selected_cell_subset=None,
-    directed=False,
-    copy=False,
+    adata: AnnData,
+    use_weight: bool = True,
+    adj_matrix: Union[np.ndarray, csr_matrix, None] = None,
+    adj_matrix_key: Optional[str] = None,
+    result_key: Optional[str] = None,
+    layer: Optional[str] = None,
+    obsm_key: Optional[str] = None,
+    selected_cluster_subset: Optional[Tuple[str, str]] = None,
+    selected_cell_subset: Union[List[int], List[str], None] = None,
+    directed: bool = False,
+    copy: bool = False,
     **kwargs
-) -> anndata.AnnData:
+) -> AnnData:
     """Apply infomap community detection algorithm to cluster adata.
-    For other community detection general parameters, please refer to ``dynamo's`` :py:meth:`~dynamo.tl.cluster_community` function.
-    "Infomap is based on ideas of information theory. The algorithm uses the probability flow of random walks on a network as a proxy for information flows in the real system and it decomposes the network into modules by compressing a description of the probability flow." - cdlib
+
+    For other community detection general parameters, please refer to `dynamo`'s `tl.cluster_community` function.
+    "Infomap is based on ideas of information theory. The algorithm uses the probability flow of random walks on a
+    network as a proxy for information flows in the real system and it decomposes the network into modules by
+    compressing a description of the probability flow." - cdlib
+
+    Args:
+        adata: an AnnData object.
+        use_weight: whether to use graph weight or not. False means to use connectivities only (0/1 integer values).
+            Defaults to True.
+        adj_matrix: adj_matrix used for clustering. Defaults to None.
+        adj_matrix_key: the key for adj_matrix stored in adata.obsp. Defaults to None.
+        result_key: the key where the results will be stored in obs. Defaults to None.
+        layer: the adata layer on which cluster algorithms will work. Defaults to None.
+        obsm_key: the key in obsm corresponding to the data that would be used for finding neighbors. Defaults to None.
+        selected_cluster_subset: a tuple of (cluster_key, allowed_clusters).Filtering cells in adata based on
+            cluster_key in adata.obs and only reserve cells in the allowed clusters. Defaults to None.
+        selected_cell_subset: a subset of cells in adata that would be clustered. Could be a list of indices or a list
+            of cell names. Defaults to None.
+        directed: whether the edges in the graph should be directed. Defaults to False.
+        copy: whether to return a new updated AnnData object or updated the original one inplace. Defaults to False.
+
+    Returns:
+        An updated AnnData object if `copy` is set to be true.
     """
+
     kwargs.update({})
 
     return cluster_community(
         adata,
         method="infomap",
         use_weight=use_weight,
         result_key=result_key,
@@ -314,55 +402,56 @@
         copy=copy,
         **kwargs
     )
 
 
 def cluster_community(
     adata: AnnData,
-    method: str = "leiden",
-    result_key: Union[str, None] = None,
-    adj_matrix: Union[list, np.array, csr_matrix, None] = None,
-    adj_matrix_key: Union[str, None] = None,
-    use_weight: bool = True,
+    method: Literal["leiden", "louvain", "infomap"] = "leiden",
+    result_key: Optional[str] = None,
+    adj_matrix: Optional[Union[list, np.array, csr_matrix]] = None,
+    adj_matrix_key: Optional[str] = None,
+    use_weight: bool = False,
     no_community_label: int = -1,
-    layer: Union[str, None] = None,
-    obsm_key: Union[str, None] = None,
-    cell_subsets: list = None,
-    cluster_and_subsets: list = None,
+    layer: Optional[str] = None,
+    obsm_key: Optional[str] = None,
+    cell_subsets: Optional[List[int]] = None,
+    cluster_and_subsets: Optional[Tuple[str, List[int]]] = None,
     directed: bool = True,
     copy: bool = False,
     **kwargs
-) -> Union[AnnData, None]:
-    """A base function for detecting communities and inserting results into adata with algorithms specified parameters passed in.
-    Adjacent matrix retrieval priority: adj_matrix > adj_matrix_key > others
-
-    Parameters
-    ----------
-    adata
-        adata object
-    method
-        community detection method, by default "leiden"
-    result_key
-        the key where the results are stored in obs, by default None
-    adj_matrix
-        adj_matrix used for clustering, by default None
-    adj_matrix_key
-        adj_matrix_key in adata.obsp used for clustering
-    use_weight
-        if using graph weight or not, by default False meaning using connectivities only (0/1 integer values)
-    no_community_label
-        the label value used for nodes not contained in any community, by default -1
-    layer
-        some adata layer which cluster algorithms will work on, by default None
-    cell_subsets
-        cluster only a subset of cells in adata, by default None
-    cluster_and_subsets
-        A tuple of 2 elements (cluster_key, allowed_clusters).filtering cells in adata based on cluster_key in adata.obs and only reserve cells in the allowed clusters, by default None
-    directed
-        if the edges in the graph should be directed, by default False
+) -> Optional[AnnData]:
+    """A base function for detecting communities and inserting results into adata with algorithms specified parameters
+    passed in. Adjacent matrix retrieval priority: adj_matrix > adj_matrix_key > others
+
+    Args:
+        adata: an AnnData object.
+        method: the algorithm to cluster the AnnData object. Can be one of "leiden", "louvain", or "infomap". Defaults
+            to "leiden".
+        result_key: the key where the results will be stored in obs. Defaults to None.
+        adj_matrix: adj_matrix used for clustering. Defaults to None.
+        adj_matrix_key: the key for adj_matrix stored in adata.obsp. Defaults to None.
+        use_weight: whether to use graph weight or not. False means to use connectivities only (0/1 integer values).
+            Defaults to True.
+        no_community_label: the label value used for nodes not contained in any community. Defaults to -1.
+        layer: the adata layer on which cluster algorithms will work. Defaults to None.
+        obsm_key: the key in obsm corresponding to the data that would be used for finding neighbors. Defaults to None.
+        cell_subsets: a subset of cells in adata that would be clustered. Could be a list of indices or a list
+            of cell names. Defaults to None.
+        cluster_and_subsets: a tuple of (cluster_key, allowed_clusters).Filtering cells in adata based on
+            cluster_key in adata.obs and only reserve cells in the allowed clusters. Defaults to None.
+        directed: whether the edges in the graph should be directed. Defaults to False.
+        copy: whether to return a new updated AnnData object or updated the original one inplace. Defaults to False.
+
+    Raises:
+        ValueError: `adj_matrix_key` and `layer` conflicted.
+        ValueError: `adj_matrix_key` not found in .obsp.
+
+    Returns:
+        An updated AnnData object if `copy` is set to be true.
     """
 
     adata = copy_adata(adata) if copy else adata
     if (layer is not None) and (adj_matrix_key is not None):
         raise ValueError("Please supply one of adj_matrix_key and layer")
     if use_weight:
         conn_type = DKM.OBSP_ADJ_MAT_DIST
@@ -387,15 +476,15 @@
             if layer is None:
                 if obsm_key is None:
                     neighbors(adata)
                 else:
                     X_data = adata.obsm[obsm_key]
                     neighbors(adata, X_data=X_data, result_prefix=obsm_key)
             else:
-                main_info("using PCA genes for clustering based on adata.var.use_for_pca...")
+                main_info("using PCA genes for clustering based on adata.var.use_for_pca ...")
                 X_data = adata[:, adata.var.use_for_pca].layers[layer]
                 neighbors(adata, X_data=X_data, result_prefix=layer)
 
         if not (adj_matrix_key in adata.obsp):
             raise ValueError("%s does not exist in adata.obsp" % adj_matrix_key)
 
         graph_sparse_matrix = adata.obsp[adj_matrix_key]
@@ -424,28 +513,30 @@
             cluster_and_subsets[0],
             cluster_and_subsets[1],
         )
         valid_indices_bools = np.isin(adata.obs[cluster_col], allowed_clusters)
         valid_indices = np.argwhere(valid_indices_bools).flatten()
         graph_sparse_matrix = graph_sparse_matrix[valid_indices, :][:, valid_indices]
 
-    if not use_weight:
-        graph_sparse_matrix.data = 1
-
     community_result = cluster_community_from_graph(
         method=method, graph_sparse_matrix=graph_sparse_matrix, directed=directed, **kwargs
     )
 
     labels = np.zeros(len(adata), dtype=int) + no_community_label
 
     # No subset required case, use all indices
     if valid_indices is None:
         valid_indices = np.arange(0, len(adata))
-    for i, community in enumerate(community_result.communities):
-        labels[valid_indices[community]] = i
+
+    if hasattr(community_result, "membership"):
+        labels[valid_indices] = community_result.membership
+    else:
+        for i, community in enumerate(community_result.communities):
+            labels[valid_indices[community]] = i
+
     # clusters need to be categorical strings
     adata.obs[result_key] = pd.Categorical(labels.astype(str))
 
     adata.uns[result_key] = {
         "method": method,
         "adj_matrix_key": adj_matrix_key,
         "use_weight": use_weight,
@@ -455,113 +546,140 @@
         "cluster_and_subsets": cluster_and_subsets,
         "directed": directed,
     }
     if copy:
         return adata
 
 
-def cluster_community_from_graph(graph=None, graph_sparse_matrix=None, method="louvain", directed=False, **kwargs):
-    # -> NodeClustering:
-    """Detect communities based on graph inputs and selected methods with arguments passed in kwargs."""
+def cluster_community_from_graph(
+    graph=None,
+    graph_sparse_matrix: Union[np.ndarray, csr_matrix, None] = None,
+    method: Literal["leiden", "louvain", "infomap"] = "louvain",
+    directed: bool = False,
+    **kwargs
+) -> Any:
+    """A function takes a graph as input and clusters its nodes into communities using one of three algorithms:
+    Leiden, Louvain, or Infomap.
+
+    Args:
+        graph (nx.Graph): the input graph that would be directly used for clustering. Defaults to None.
+        graph_sparse_matrix: a sparse matrix that would be converted to a graph if `graph` is not supplied.
+        method: the algorithm to cluster the AnnData object. Can be one of "leiden", "louvain", or "infomap".
+        directed: whether the edges in the graph should be directed. Defaults to False. Defaults to False.
+
+    Raises:
+        ImportError: cdlib or networkx not installed.
+        ValueError: neither graph nor graph_sparse_matrix is valid.
+        KeyError: resolution is not found in kwargs for louvain algorithm.
+        KeyError: randomize is not found in kwargs for louvain algorithm.
+        NotImplementedError: `method` is invalid.
+
+    Returns:
+        NodeClustering: a NodeClustering object that contains the communities detected by the chosen algorithm.
+    """
+
     logger = LoggerManager.get_main_logger()
     logger.info("Detecting communities on graph...")
+
     try:
-        import networkx as nx
-        from cdlib import algorithms
+        import igraph
+        import leidenalg
     except ImportError:
         raise ImportError(
-            "You need to install the excellent package `cdlib` if you want to use louvain or leiden " "for clustering."
+            "Please install networkx, igraph, leidenalg via "
+            "`pip install networkx igraph leidenalg` for clustering on graph."
         )
+
+    initial_membership = kwargs.pop("initial_membership", None)
+    weights = kwargs.pop("weights", None)
+    seed = kwargs.pop("seed", None)
+
     if graph is not None:
         # highest priority
-        pass
-    elif graph_sparse_matrix is not None:
-        logger.info("Converting graph_sparse_matrix to networkx object", indent_level=2)
-        # if graph matrix is with weight, then edge attr "weight" stores weight of edges
-        graph = nx.convert_matrix.from_scipy_sparse_matrix(graph_sparse_matrix, edge_attribute="weight")
-        for i in range(graph_sparse_matrix.shape[0]):
-            if not (i in graph.nodes):
-                graph.add_node(i)
+        main_info("using graph from arg for clustering...")
+    elif issparse(graph_sparse_matrix):
+        logger.info("Converting graph_sparse_matrix to igraph object", indent_level=2)
+        if directed:
+            graph = igraph.Graph.Weighted_Adjacency(graph_sparse_matrix, mode="directed")
+        else:
+            graph = igraph.Graph.Weighted_Adjacency(graph_sparse_matrix, mode="undirected")
     else:
         raise ValueError("Expected graph inputs are invalid")
 
-    if directed:
-        graph = graph.to_directed()
-    else:
-        graph = graph.to_undirected()
-
     if method == "leiden":
-        initial_membership, weights = None, None
-        if "initial_membership" in kwargs:
-            logger.info("Detecting community with initial_membership input from caller")
-            initial_membership = kwargs["initial_membership"]
-        if "weights" in kwargs:
-            weights = kwargs["weights"]
-
-        if initial_membership is not None:
-            main_info(
-                "Currently initial_membership for leiden has some issue and thus we ignore it. "
-                "We will support it in future."
-            )
-            initial_membership = None
+        # ModularityVertexPartition does not accept a resolution_parameter, instead RBConfigurationVertexPartition.
+        if kwargs["resolution_parameter"] != 1:
+            partition_type = leidenalg.RBConfigurationVertexPartition
+        else:
+            partition_type = leidenalg.ModularityVertexPartition
+            kwargs.pop("resolution_parameter")
+
+        coms = leidenalg.find_partition(
+            graph, partition_type, initial_membership=initial_membership, weights=weights, seed=seed, **kwargs
+        )
 
-        coms = algorithms.leiden(graph, weights=weights, initial_membership=initial_membership)
     elif method == "louvain":
-        if "resolution" not in kwargs:
-            raise KeyError("resolution not in louvain input parameters")
-        # if "weight" not in kwargs:
-        #     raise KeyError("weight not in louvain input parameters")
-        if "randomize" not in kwargs:
-            raise KeyError("randomize not in louvain input parameters")
-
-        resolution = kwargs["resolution"]
-        weight = "weight"
-        randomize = kwargs["randomize"]
-        coms = algorithms.louvain(graph, weight=weight, resolution=resolution, randomize=randomize)
+        try:
+            import louvain
+        except ImportError:
+            raise ImportError("Please install louvain via `pip install louvain==0.8.0` for clustering on graph.")
+
+        logger.warning("louvain is not maintained, we recommend using leiden instead.")
+        coms = louvain.find_partition(
+            graph,
+            louvain.RBConfigurationVertexPartition,
+            initial_membership=initial_membership,
+            weights=weights,
+            seed=seed,
+            **kwargs
+        )
     elif method == "infomap":
+        try:
+            import cdlib as algorithms
+        except ImportError:
+            raise ImportError("Please install cdlib via `pip install cdlib` for clustering on graph.")
         coms = algorithms.infomap(graph)
     else:
         raise NotImplementedError("clustering algorithm not implemented yet")
 
     logger.finish_progress(progress_name="Community clustering with %s" % (method))
 
     return coms
 
 
 def scc(
-    adata: anndata.AnnData,
+    adata: AnnData,
     min_cells: int = 100,
     spatial_key: str = "spatial",
     e_neigh: int = 30,
     s_neigh: int = 6,
     resolution: Optional[float] = None,
     copy: bool = False,
-) -> Optional[anndata.AnnData]:
+) -> Optional[AnnData]:
     """Spatially constrained clustering (scc) to identify continuous tissue domains.
 
     Args:
-        adata: an Anndata object, after normalization.
-        min_cells: minimal number of cells the gene expressed.
-        spatial_key: the key in `.obsm` that corresponds to the spatial coordinate of each bucket.
-        e_neigh: the number of nearest neighbor in gene expression space.
-        s_neigh: the number of nearest neighbor in physical space.
-        resolution: the resolution parameter of the leiden clustering algorithm.
-        copy: Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments.
+        adata: an normalized AnnData object.
+        min_cells: minimal number of cells the gene expressed. Defaults to 100.
+        spatial_key: the key in `.obsm` corresponding to the spatial coordinate of each bucket. Defaults to "spatial".
+        e_neigh: the number of nearest neighbor in gene expression space. Defaults to 30.
+        s_neigh: the number of nearest neighbor in physical space. Defaults to 6.
+        resolution: the resolution parameter of the leiden clustering algorithm. Defaults to None.
+        copy: whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments.
             Defaults to False.
 
     Returns:
-        Depends on the argument `copy` return either an `~anndata.AnnData` object with cluster info in "scc_e_{a}_s{b}"
-        or None.
+        An updated AnnData object with cluster info stored in `.obs[scc_e_{a}_s{b}]` if `copy` is set to be true.
     """
 
     filter_genes(adata, min_cell_s=min_cells)
     adata.uns["pp"] = {}
-    normalize_cell_expr_by_size_factors(adata, layers="X")
+    normalize(adata, layers="X")
     log1p(adata)
-    pca_monocle(adata, n_pca_components=30, pca_key="X_pca")
+    pca(adata, n_pca_components=30, pca_key="X_pca")
 
     neighbors(adata, n_neighbors=e_neigh)
     if "X_" + spatial_key not in adata.obsm.keys():
         adata.obsm["X_" + spatial_key] = adata.obsm[spatial_key].copy()
 
     neighbors(adata, n_neighbors=s_neigh, basis=spatial_key, result_prefix="spatial")
     conn = adata.obsp["connectivities"].copy()
@@ -572,33 +690,34 @@
 
     if copy:
         return adata
     return None
 
 
 def purity(
-    adata,
+    adata: AnnData,
     neighbor: int = 30,
     resolution: Optional[float] = None,
     spatial_key: str = "spatial",
     neighbors_key: str = "spatial_connectivities",
     cluster_key: str = "leiden",
 ) -> float:
-    """Calculate the puriority of the scc's clustering results.
+    """Calculate the purity of the scc's clustering results.
 
     Args:
-        adata: an adata object
-        neighbor: the number of nearest neighbor in physical space.
-        resolution: the resolution parameter of the leiden clustering algorithm.
-        spatial_key: the key in `.obsm` that corresponds to the spatial coordinate of each bucket.
-        neighbors_key: the key in `.obsp` that corresponds to the spatial nearest neighbor graph.
-        cluster_key: the key in `.obsm` that corresponds to the clustering identity.
+        adata: an AnnData object.
+        neighbor: the number of nearest neighbor in physical space. Defaults to 30.
+        resolution: the resolution parameter of the leiden clustering algorithm. Defaults to None.
+        spatial_key: the key in `.obsm` corresponding to the spatial coordinate of each bucket. Defaults to "spatial".
+        neighbors_key: the key in `.obsp` that corresponds to the spatial nearest neighbor graph. Defaults to
+            "spatial_connectivities".
+        cluster_key: the key in `.obsm` that corresponds to the clustering identity. Defaults to "leiden".
 
     Returns:
-        purity_score: the average purity score across cells.
+        The average purity score across cells.
     """
 
     if neighbors_key not in adata.obsp.keys():
         neighbors(adata, n_neighbors=neighbor, basis=spatial_key, result_prefix=neighbors_key.split("_")[0])
 
     neighbor_graph = adata.obsp[neighbors_key]
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/connectivity.py` & `dynamo-release-1.3.0/dynamo/tools/connectivity.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,46 +1,46 @@
+from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
 import warnings
 from copy import deepcopy
 from inspect import signature
 
 import numpy as np
 import scipy
 from anndata import AnnData
 from pynndescent.distances import true_angular
-from scipy.sparse import csr_matrix, issparse
+from scipy.sparse import coo_matrix, csr_matrix, issparse
 from sklearn.decomposition import TruncatedSVD
 from sklearn.utils import sparsefuncs
+from umap import UMAP
 
 from ..configuration import DynamoAdataKeyManager
 from ..docrep import DocstringProcessor
 from ..dynamo_logger import LoggerManager, main_info, main_warning
 from .utils import fetch_X_data, log1p_
 
 docstrings = DocstringProcessor()
 
 
-def adj_to_knn(adj, n_neighbors):
-    """convert the adjacency matrix of a nearest neighbor graph to the indices
-        and weights for a knn graph.
-
-    Arguments
-    ---------
-        adj: matrix (`.X`, dtype `float32`)
-            Adjacency matrix (n x n) of the nearest neighbor graph.
-        n_neighbors: 'int' (optional, default 15)
-            The number of nearest neighbors of the kNN graph.
-
-    Returns
-    -------
-        idx: :class:`~numpy.ndarray`
-            The matrix (n x n_neighbors) that stores the indices for each node's
-            n_neighbors nearest neighbors.
-        wgt: :class:`~numpy.ndarray`
-            The matrix (n x n_neighbors) that stores the weights on the edges
-            for each node's n_neighbors nearest neighbors.
+def adj_to_knn(adj: np.ndarray, n_neighbors: int = 15) -> Tuple[np.ndarray, np.ndarray]:
+    """Convert the adjacency matrix of a nearest neighbor graph to the indices and weights for a knn graph.
+
+    Args:
+        adj: adjacency matrix (n x n) of the nearest neighbor graph.
+        n_neighbors: the number of nearest neighbors of the kNN graph. Defaults to 15.
+
+    Returns:
+        A tuple (idx, wgt) where idx is the matrix (n x n_neighbors) storing the indices for each node's n_neighbors
+        nearest neighbors and wgt is the matrix (n x n_neighbors) storing the wights on the edges for each node's
+        n_neighbors nearest neighbors.
     """
 
     n_cells = adj.shape[0]
     idx = np.zeros((n_cells, n_neighbors), dtype=int)
     wgt = np.zeros((n_cells, n_neighbors), dtype=adj.dtype)
 
     for cur_cell in range(n_cells):
@@ -62,43 +62,53 @@
             idx_ = np.arange(1, (cur_n_neighbors + 1))
             idx[cur_cell, idx_] = cur_neighbors[1]
             wgt[cur_cell, idx_] = adj[cur_cell][:, cur_neighbors[1]].A
 
     return idx, wgt
 
 
-def knn_to_adj(knn_indices, knn_weights):
+def knn_to_adj(knn_indices: np.ndarray, knn_weights: np.ndarray) -> csr_matrix:
+    """Convert a knn graph's indices and weights to an adjacency matrix of the corresponding nearest neighbor graph.
+
+    Args:
+        knn_indices: the matrix (n x n_neighbors) storing the indices for each node's n_neighbors nearest neighbors in
+            the knn graph.
+        knn_weights: the matrix (n x n_neighbors) storing the wights on the edges for each node's n_neighbors nearest
+            neighbors in the knn graph.
+
+    Returns:
+        The converted adjacency matrix (n x n) of the corresponding nearest neighbor graph.
+    """
+
     adj = csr_matrix(
         (
             knn_weights.flatten(),
             (
                 np.repeat(knn_indices[:, 0], knn_indices.shape[1]),
                 knn_indices.flatten(),
             ),
         )
     )
     adj.eliminate_zeros()
 
     return adj
 
 
-def get_conn_dist_graph(knn, distances):
-    """Compute connection and distance sparse matrices
+def get_conn_dist_graph(knn: np.ndarray, distances: np.ndarray) -> Tuple[csr_matrix, csr_matrix]:
+    """Compute connection and distance sparse matrix.
 
-    Parameters
-    ----------
-        knn:
-            n_obs x n_neighbors, k nearest neighbor graph
-        distances:
-            KNN dists
-
-    Returns
-    -------
-        distance and connectivity matrices
+    Args:
+        knn: a matrix (n x n_neighbors) storing the indices for each node's n_neighbors nearest neighbors in knn graph.
+        distances: the distances to the n_neighbors the closest points in knn graph.
+
+    Returns:
+        A tuple (distances, connectivities), where distance is the distance sparse matrix and connectivities is the
+        connectivity sparse matrix.
     """
+
     n_obs, n_neighbors = knn.shape
     distances = csr_matrix(
         (
             distances.flatten(),
             (np.repeat(np.arange(n_obs), n_neighbors), knn.flatten()),
         ),
         shape=(n_obs, n_obs),
@@ -110,108 +120,104 @@
     connectivities.eliminate_zeros()
 
     return distances, connectivities
 
 
 @docstrings.get_sectionsf("umap_ann")
 def umap_conn_indices_dist_embedding(
-    X,
-    n_neighbors=30,
-    n_components=2,
-    metric="euclidean",
-    min_dist=0.1,
-    spread=1.0,
-    max_iter=None,
-    alpha=1.0,
-    gamma=1.0,
-    negative_sample_rate=5,
-    init_pos="spectral",
-    random_state=0,
-    densmap=False,
-    dens_lambda=2.0,
-    dens_frac=0.3,
-    dens_var_shift=0.1,
-    output_dens=False,
-    return_mapper=True,
-    verbose=False,
+    X: np.ndarray,
+    n_neighbors: int = 30,
+    n_components: int = 2,
+    metric: Union[str, Callable] = "euclidean",
+    min_dist: float = 0.1,
+    spread: float = 1.0,
+    max_iter: Optional[int] = None,
+    alpha: float = 1.0,
+    gamma: float = 1.0,
+    negative_sample_rate: float = 5,
+    init_pos: Union[Literal["spectral", "random"], np.ndarray] = "spectral",
+    random_state: Union[int, np.random.RandomState, None] = 0,
+    densmap: bool = False,
+    dens_lambda: float = 2.0,
+    dens_frac: float = 0.3,
+    dens_var_shift: float = 0.1,
+    output_dens: bool = False,
+    return_mapper: bool = True,
+    verbose: bool = False,
     **umap_kwargs,
-):
+) -> Union[
+    Tuple[UMAP, coo_matrix, np.ndarray, np.ndarray, np.ndarray],
+    Tuple[coo_matrix, np.ndarray, np.ndarray, np.ndarray],
+]:
     """Compute connectivity graph, matrices for kNN neighbor indices, distance matrix and low dimension embedding with
-    UMAP. This code is adapted from umap-learn:
+    UMAP.
+
+    This code is adapted from umap-learn:
     (https://github.com/lmcinnes/umap/blob/97d33f57459de796774ab2d7fcf73c639835676d/umap/umap_.py)
 
-    Arguments
-    ---------
-        X: sparse matrix (`.X`, dtype `float32`)
-            expression matrix (n_cell x n_genes)
-        n_neighbors: 'int' (optional, default 15)
-            The number of nearest neighbors to compute for each sample in ``X``.
-        n_components: 'int' (optional, default 2)
-            The dimension of the space to embed into.
-        metric: 'str' or `callable` (optional, default `cosine`)
-            The metric to use for the computation.
-        min_dist: 'float' (optional, default `0.1`)
-            The effective minimum distance between embedded points. Smaller values will result in a more
+
+    Args:
+        X: the expression matrix (n_cell x n_genes).
+        n_neighbors: the number of nearest neighbors to compute for each sample in `X`. Defaults to 30.
+        n_components: the dimension of the space to embed into. Defaults to 2.
+        metric: the metric to use for the computation. Defaults to "euclidean".
+        min_dist: the effective minimum distance between embedded points. Smaller values will result in a more
             clustered/clumped embedding where nearby points on the manifold are drawn closer together, while larger
-            values will result on a more even dispersal of points. The value should be set relative to the ``spread``
-            value, which determines the scale at which embedded points will be spread out.
-        spread: `float` (optional, default 1.0)
-            The effective scale of embedded points. In combination with min_dist this determines how clustered/clumped
-            the embedded points are.
-        max_iter: 'int' or None (optional, default None)
-            The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result
-            in more accurate embeddings. If None is specified a value will be selected based on the size of the input
-            dataset (200 for large datasets, 500 for small). This argument was refactored from n_epochs from UMAP-learn
-            to account for recent API changes in UMAP-learn 0.5.2.
-        alpha: `float` (optional, default 1.0)
-            Initial learning rate for the SGD.
-        gamma: `float` (optional, default 1.0)
-            Weight to apply to negative samples. Values higher than one will result in greater weight being given to
-            negative samples.
-        negative_sample_rate: `float` (optional, default 5)
-            The number of negative samples to select per positive sample in the optimization process. Increasing this
-            value will result in greater repulsive force being applied, greater optimization cost, but slightly more
-            accuracy. The number of negative edge/1-simplex samples to use per positive edge/1-simplex sample in
-             optimizing the low dimensional embedding.
-        init_pos: 'spectral':
-            How to initialize the low dimensional embedding. Use a spectral embedding of the fuzzy 1-skeleton
-        random_state: `int`, `RandomState` instance or `None`, optional (default: None)
-            If int, random_state is the seed used by the random number generator; If RandomState instance, random_state
-            is the random number generator; If None, the random number generator is the RandomState instance used by
-            `numpy.random`.
-        dens_lambda: float (optional, default 2.0)
-            Controls the regularization weight of the density correlation term
-            in densMAP. Higher values prioritize density preservation over the
-            UMAP objective, and vice versa for values closer to zero. Setting this
-            parameter to zero is equivalent to running the original UMAP algorithm.
-        dens_frac: float (optional, default 0.3)
-            Controls the fraction of epochs (between 0 and 1) where the
-            density-augmented objective is used in densMAP. The first
-            (1 - dens_frac) fraction of epochs optimize the original UMAP objective
-            before introducing the density correlation term.
-        dens_var_shift: float (optional, default 0.1)
-            A small constant added to the variance of local radii in the
-            embedding when calculating the density correlation objective to
-            prevent numerical instability from dividing by a small number
-        output_dens: float (optional, default False)
-            Determines whether the local radii of the final embedding (an inverse
-            measure of local density) are computed and returned in addition to
-            the embedding. If set to True, local radii of the original data
-            are also included in the output for comparison; the output is a tuple
-            (embedding, original local radii, embedding local radii). This option
-            can also be used when densmap=False to calculate the densities for
-            UMAP embeddings.
-        verbose: `bool` (optional, default False)
-                Controls verbosity of logging.
-
-    Returns
-    -------
-        graph, knn_indices, knn_dists, embedding_
-            A tuple of kNN graph (`graph`), indices of nearest neighbors of each cell (knn_indicies), distances of
-            nearest neighbors (knn_dists) and finally the low dimensional embedding (embedding_).
+            values will result on a more even dispersal of points. The value should be set relative to the `spread`
+            value, which determines the scale at which embedded points will be spread out. Defaults to 0.1.
+        spread: the effective scale of embedded points. In combination with min_dist this determines how
+            clustered/clumped the embedded points are. Defaults to 1.0.
+        max_iter: the number of training epochs to be used in optimizing the low dimensional embedding. Larger values
+            result in more accurate embeddings. If None is specified a value will be selected based on the size of the
+            input dataset (200 for large datasets, 500 for small). This argument was refactored from n_epochs from
+            UMAP-learn to account for recent API changes in UMAP-learn 0.5.2. Defaults to None.
+        alpha: initial learning rate for the SGD. Defaults to 1.0.
+        gamma: weight to apply to negative samples. Values higher than one will result in greater weight being given to
+            negative samples. Defaults to 1.0.
+        negative_sample_rate: the number of negative samples to select per positive sample in the optimization process.
+            Increasing this value will result in greater repulsive force being applied, greater optimization cost, but
+            slightly more accuracy. The number of negative edge/1-simplex samples to use per positive edge/1-simplex
+            sample in optimizing the low dimensional embedding. Defaults to 5.
+        init_pos: the method to initialize the low dimensional embedding. Where:
+            "spectral": use a spectral embedding of the fuzzy 1-skeleton.
+            "random": assign initial embedding positions at random.
+            np.ndarray: the array to define the initial position.
+            Defaults to "spectral".
+        random_state: the method to generate random numbers. If int, random_state is the seed used by the random number
+            generator; If RandomState instance, random_state is the random number generator; If None, the random number
+            generator is the RandomState instance used by `numpy.random`. Defaults to 0.
+        densmap: whether to use the density-augmented objective function to optimize the embedding according to the
+            densMAP algorithm. Defaults to False.
+        dens_lambda: controls the regularization weight of the density correlation term in densMAP. Higher values
+            prioritize density preservation over the UMAP objective, and vice versa for values closer to zero. Setting
+            this parameter to zero is equivalent to running the original UMAP algorithm. Defaults to 2.0.
+        dens_frac: controls the fraction of epochs (between 0 and 1) where the density-augmented objective is used in
+            densMAP. The first (1 - dens_frac) fraction of epochs optimize the original UMAP objective before
+            introducing the density correlation term. Defaults to 0.3.
+        dens_var_shift: a small constant added to the variance of local radii in the embedding when calculating the
+            density correlation objective to prevent numerical instability from dividing by a small number Defaults to
+            0.1.
+        output_dens: whether the local radii of the final embedding (an inverse measure of local density) are computed
+            and returned in addition to the embedding. If set to True, local radii of the original data are also
+            included in the output for comparison; the output is a tuple (embedding, original local radii, embedding
+            local radii). This option can also be used when densmap=False to calculate the densities for UMAP
+            embeddings. Defaults to False.
+        return_mapper: whether to return the data mapped onto the UMAP space. Defaults to True.
+        verbose: whether to log verbosely. Defaults to False.
+
+    Raises:
+        ValueError: `dense_lambda` is negative and thus invalid.
+        ValueError: `dense_frac` out of range (0.0 ~ 1.0)
+        ValueError: `dense_var_shift` is negative and thus invalid.
+
+    Returns:
+        A tuple ([mapper,] graph, knn_indices, knn_dists, embedding_). `mapper` is the data mapped onto umap space and
+        will be returned only if `return_mapper` is true. graph is the sparse matrix representing the graph,
+        `knn_indices` is the matrix storing indices of nearest neighbors of each cell, `knn_dists` is the distances to
+        the n_neighbors closest points in knn graph, and `embedding_` is the low dimensional embedding.
     """
 
     from sklearn.metrics import pairwise_distances
     from sklearn.utils import check_random_state
     from umap.umap_ import (
         find_ab_params,
         fuzzy_simplicial_set,
@@ -353,66 +359,84 @@
         ).fit(X)
 
         return mapper, graph, knn_indices, knn_dists, embedding_
     else:
         return graph, knn_indices, knn_dists, embedding_
 
 
-def mnn_from_list(knn_graph_list):
-    """Apply reduce function to calculate the mutual kNN."""
+CsrOrNdarray = TypeVar("CsrOrNdarray", csr_matrix, np.ndarray)
+
+
+def mnn_from_list(knn_graph_list: List[CsrOrNdarray]) -> CsrOrNdarray:
+    """Apply reduce function to calculate the mutual kNN.
+
+    Args:
+        knn_graph_list: a list of ndarray or csr_matrix representing a series of knn graphs.
+
+    Returns:
+        The calculated mutual knn, in same type as the input (ndarray of csr_matrix).
+    """
+
     import functools
 
     mnn = (
         functools.reduce(scipy.sparse.csr.csr_matrix.minimum, knn_graph_list)
         if issparse(knn_graph_list[0])
         else functools.reduce(scipy.minimum, knn_graph_list)
     )
 
     return mnn
 
 
-def normalize_knn_graph(knn):
+def normalize_knn_graph(knn: csr_matrix) -> csr_matrix:
+    """Normalize the knn graph so that each row will be sum up to 1.
+
+    Args:
+        knn: the sparse matrix containing the indices of nearest neighbors of each cell.
+
+    Returns:
+        The normalized matrix.
+    """
+
     """normalize the knn graph so that each row will be sum up to 1."""
     knn.setdiag(1)
     knn = knn.astype("float32")
     sparsefuncs.inplace_row_scale(knn, 1 / knn.sum(axis=1).A1)
 
     return knn
 
 
 def mnn(
-    adata,
-    n_pca_components=30,
-    n_neighbors=250,
-    layers="all",
-    use_pca_fit=True,
-    save_all_to_adata=False,
-):
-    """Function to calculate mutual nearest neighbor graph across specific data layers.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        n_pca_components: 'int' (optional, default `30`)
-            Number of PCA components.
-        layers: str or list (default: `all`)
-            The layer(s) to be normalized. Default is `all`, including RNA (X, raw) or spliced, unspliced, protein, etc.
-        use_pca_fit: `bool` (default: `True`)
-            Whether to use the precomputed pca model to transform different data layers or calculate pca for each data
-            layer separately.
-        save_all_to_adata: `bool` (default: `False`)
-            Whether to save_fig all calculated data to adata object.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An updated anndata object that are updated with the `mnn` or other relevant data that are calculated during
-            mnn calculation.
+    adata: AnnData,
+    n_pca_components: int = 30,
+    n_neighbors: int = 250,
+    layers: Union[str, List[str]] = "all",
+    use_pca_fit: bool = True,
+    save_all_to_adata: bool = False,
+) -> AnnData:
+    """Calculate mutual nearest neighbor graph across specific data layers.
+
+    Args:
+        adata: an AnnData object.
+        n_pca_components: the number of PCA components. Defaults to 30.
+        n_neighbors: the number of nearest neighbors to compute for each sample. Defaults to 250.
+        layers: the layer(s) to be normalized. When set to `'all'`, it will include RNA (X, raw) or spliced, unspliced,
+            protein, etc. Defaults to "all".
+        use_pca_fit: whether to use the precomputed pca model to transform different data layers or calculate pca for
+            each data layer separately. Defaults to True.
+        save_all_to_adata: whether to save_fig all calculated data to adata object. Defaults to False.
+
+    Raises:
+        Exception: no PCA fit result in .uns.
+
+    Returns:
+        An updated anndata object that are updated with the `mnn` or other relevant data that are calculated during mnn
+        calculation.
     """
+
     if use_pca_fit:
         if "pca_fit" in adata.uns.keys():
             fiter = adata.uns["pca_fit"]
         else:
             raise Exception("use_pca_fit is set to be True, but there is no pca fit results in .uns attribute.")
 
     layers = DynamoAdataKeyManager.get_available_layer_keys(adata, layers, False, False)
@@ -462,120 +486,105 @@
 
     mnn = mnn_from_list(knn_graph_list)
     adata.uns["mnn"] = normalize_knn_graph(mnn)
 
     return adata
 
 
-def _gen_neighbor_keys(result_prefix="") -> tuple:
+def _gen_neighbor_keys(result_prefix: str = "") -> Tuple[str, str, str]:
     """Generate neighbor keys for other functions to store/access info in adata.
 
-    Parameters
-    ----------
-        result_prefix : str, optional
-            generate keys based on this prefix, by default ""
-
-    Returns
-    -------
-        tuple:
-            A tuple consisting of (conn_key, dist_key, neighbor_key)
+    Args:
+        result_prefix: the prefix for keys. Defaults to "".
 
+    Returns:
+        A tuple (conn_key, dist_key, neighbor_key) for key of connectivity matrix, distance matrix, neighbor matrix,
+        respectively.
     """
+
     if result_prefix:
         result_prefix = result_prefix if result_prefix.endswith("_") else result_prefix + "_"
     if result_prefix is None:
         result_prefix = ""
 
     conn_key, dist_key, neighbor_key = (
         result_prefix + "connectivities",
         result_prefix + "distances",
         result_prefix + "neighbors",
     )
     return conn_key, dist_key, neighbor_key
 
 
 def neighbors(
-    adata,
-    X_data=None,
-    genes=None,
-    basis="pca",
-    layer=None,
-    n_pca_components=30,
-    n_neighbors=30,
-    method=None,
-    metric="euclidean",
-    metric_kwads=None,
-    cores=1,
-    seed=19491001,
-    result_prefix="",
+    adata: AnnData,
+    X_data: np.ndarray = None,
+    genes: Optional[List[str]] = None,
+    basis: str = "pca",
+    layer: Optional[str] = None,
+    n_pca_components: int = 30,
+    n_neighbors: int = 30,
+    method: Optional[str] = None,
+    metric: Union[str, Callable] = "euclidean",
+    metric_kwads: Dict[str, Any] = None,
+    cores: int = 1,
+    seed: int = 19491001,
+    result_prefix: str = "",
     **kwargs,
-):
-    """Function to search nearest neighbors of the adata object.
+) -> AnnData:
+    """Search nearest neighbors of the adata object.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        X_data: `np.ndarray` (default: `None`)
-            The user supplied data that will be used for nearest neighbor search directly.
-        genes: `list` or None (default: `None`)
-            The list of genes that will be used to subset the data for nearest neighbor search. If `None`, all genes
-            will be used.
-        basis: `str` (default: `pca`)
-            The space that will be used for nearest neighbor search. Valid names includes, for example, `pca`, `umap`,
-            `velocity_pca` or `X` (that is, you can use velocity for clustering), etc.
-        layers: str or None (default: `None`)
-            The layer to be used for nearest neighbor search.
-        n_pca_components: 'int' (optional, default `30`)
-            Number of PCA components. Applicable only if you will use pca `basis` for nearest neighbor search.
-        n_neighbors: `int` (optional, default `30`)
-            Number of nearest neighbors.
-        method: `str` or `None` (default: `None`)
-            The methoed used for nearest neighbor search. If `umap` or `pynn`, it relies on `pynndescent` package's
-            NNDescent for fast nearest neighbor search.
-        metric: `str` or callable, default='euclidean'
-            The distance metric to use for the tree.  The default metric is , and with p=2 is equivalent to the standard
-            Euclidean metric. See the documentation of :class:`DistanceMetric` for a list of available metrics. If
+    Args:
+        adata: an AnnData object.
+        X_data: the user supplied data that will be used for nearest neighbor search directly. Defaults to None.
+        genes: the list of genes that will be used to subset the data for nearest neighbor search. If `None`, all genes
+            will be used. Defaults to None.
+        basis: The space that will be used for nearest neighbor search. Valid names includes, for example, `pca`,
+            `umap`, `velocity_pca` or `X` (that is, you can use velocity for clustering), etc. Defaults to "pca".
+        layer: the layer to be used for nearest neighbor search. Defaults to None.
+        n_pca_components: number of PCA components. Applicable only if you will use pca `basis` for nearest neighbor
+            search. Defaults to 30.
+        n_neighbors: number of nearest neighbors. Defaults to 30.
+        method: the methoed used for nearest neighbor search. If `umap` or `pynn`, it relies on `pynndescent` package's
+            NNDescent for fast nearest neighbor search. Defaults to None.
+        metric: the distance metric to use for the tree. The default metric is euclidean, and with p=2 is equivalent to
+            the standard Euclidean metric. See the documentation of `DistanceMetric` for a list of available metrics. If
             metric is "precomputed", X is assumed to be a distance matrix and must be square during fit. X may be a
-            :term:`sparse graph`, in which case only "nonzero" elements may be considered neighbors.
-        metric_params : dict, default=None
-            Additional keyword arguments for the metric function.
-        cores: `int` (default: 1)
-            The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a
-            :obj:`joblib.parallel_backend` context. ``-1`` means using all processors.
-        seed: `int` (default `19491001`)
-            Random seed to ensure the reproducibility of each run.
-        result_prefix: `str` (default: `''`)
-            The key that will be used as the prefix of the connectivity, distance and neighbor keys in the returning
-            adata.
-        kwargs:
-            Additional arguments that will be passed to each nearest neighbor search algorithm.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An updated anndata object that are updated with the `indices`, `connectivity`, `distance` to the .obsp, as
-            well as a new `neighbors` key in .uns.
+            `sparse graph`, in which case only "nonzero" elements may be considered neighbors. Defaults to "euclidean".
+        metric_kwads: additional keyword arguments for the metric function. Defaults to None.
+        cores: the number of parallel jobs to run for neighbors search. `None` means 1 unless in a
+            `joblib.parallel_backend` context. `-1` means using all processors. Defaults to 1.
+        seed: random seed to ensure the reproducibility of each run. Defaults to 19491001.
+        result_prefix: the key that will be used as the prefix of the connectivity, distance and neighbor keys in the
+            returning adata. Defaults to "".
+        kwargs: additional arguments that will be passed to each nearest neighbor search algorithm.
+
+    Raises:
+        ImportError: `method` is invalid.
+
+    Returns:
+        An updated anndata object that are updated with the `indices`, `connectivity`, `distance` to the .obsp, as well
+        as a new `neighbors` key in .uns.
     """
+
     logger = LoggerManager.gen_logger("neighbors")
     logger.info("Start computing neighbor graph...")
     logger.log_time()
 
     if X_data is None:
         logger.info("X_data is None, fetching or recomputing...", indent_level=2)
         if basis == "pca" and "X_pca" not in adata.obsm_keys():
             logger.info("PCA as basis not X_pca not found, doing PCAs", indent_level=2)
-            from ..preprocessing.utils import pca_monocle
+            from ..preprocessing.pca import pca
 
             CM = adata.X if genes is None else adata[:, genes].X
             cm_genesums = CM.sum(axis=0)
             valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
             valid_ind = np.array(valid_ind).flatten()
             CM = CM[:, valid_ind]
-            adata, _, _ = pca_monocle(adata, CM, pca_key="X_pca", n_pca_components=n_pca_components, return_all=True)
+            adata, _, _ = pca(adata, CM, pca_key="X_pca", n_pca_components=n_pca_components, return_all=True)
 
             X_data = adata.obsm["X_pca"]
         else:
             logger.info("fetching X data from layer:%s, basis:%s" % (str(layer), str(basis)))
             genes, X_data = fetch_X_data(adata, genes, layer, basis)
 
     if method is None:
@@ -636,41 +645,36 @@
     }
 
     return adata
 
 
 def check_neighbors_completeness(
     adata: AnnData,
-    conn_key="connectivities",
-    dist_key="distances",
-    result_prefix="",
-    check_nonzero_row=True,
-    check_nonzero_col=False,
+    conn_key: str = "connectivities",
+    dist_key: str = "distances",
+    result_prefix: str = "",
+    check_nonzero_row: bool = True,
+    check_nonzero_col: bool = False,
 ) -> bool:
-    """Check if neighbor graph in adata is valid.
+    """Check if neighbor graph in the AnnData object is valid.
 
-    Parameters
-    ----------
-        adata : AnnData
-        conn_key : str, optional
-            connectivity key, by default "connectivities"
-        dist_key : str, optional
-            distance key, by default "distances"
-        result_prefix : str, optional
-            The result prefix in adata.uns for neighbor graph related data, by default ""
-        check_nonzero_row:
-            Whether to check if row sums of neighbor graph distance or connectivity matrix are nonzero. Row sums correspond to out-degrees by convention.
-        check_nonzero_col:
-            Whether to check if column sums of neighbor graph distance or connectivity matrix are nonzero. Column sums correspond to in-degrees by convention.
-
-    Returns
-    -------
-        bool
-            whether the neighbor graph is valid or not. (If valid, return True)
+    Args:
+        adata: an AnnData object.
+        conn_key: the key for connectivity matrix. Defaults to "connectivities".
+        dist_key: the key for distance matrix. Defaults to "distances".
+        result_prefix: the result prefix in adata.uns for neighbor graph related data. Defaults to "".
+        check_nonzero_row: whether to check if row sums of neighbor graph distance or connectivity matrix are nonzero.
+            Row sums correspond to out-degrees by convention. Defaults to True.
+        check_nonzero_col: whether to check if column sums of neighbor graph distance or connectivity matrix are
+            nonzero. Column sums correspond to in-degrees by convention. Defaults to False.
+
+    Returns:
+        Whether the neighbor graph is valid or not.
     """
+
     is_valid = True
     conn_key, dist_key, neighbor_key = _gen_neighbor_keys(result_prefix)
     keys = [conn_key, dist_key, neighbor_key]
 
     # Old anndata version version
     # conn_mat = adata.uns[neighbor_key]["connectivities"]
     # dist_mat = adata.uns[neighbor_key]["distances"]
@@ -720,23 +724,22 @@
         if not is_col_valid:
             main_warning("Some column sums(in degree) in adata's neighbor graph are zero.")
         is_valid = is_valid and is_col_valid
 
     return is_valid
 
 
-def check_and_recompute_neighbors(adata: AnnData, result_prefix: str = ""):
+def check_and_recompute_neighbors(adata: AnnData, result_prefix: str = "") -> None:
     """Check if adata's neighbor graph is valid and recompute neighbor graph if necessary.
 
-    Parameters
-    ----------
-        adata:
-        result_prefix : str, optional
-            The result prefix in adata.uns for neighbor graph related data, by default ""
+    Args:
+        adata: an AnnData object.
+        result_prefix: the result prefix in adata.uns for neighbor graph related data. Defaults to "".
     """
+
     if result_prefix is None:
         result_prefix = ""
     conn_key, dist_key, neighbor_key = _gen_neighbor_keys(result_prefix)
 
     if not check_neighbors_completeness(adata, conn_key=conn_key, dist_key=dist_key, result_prefix=result_prefix):
         main_info("Neighbor graph is broken, recomputing....")
         neighbors(adata, result_prefix=result_prefix)
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/construct_velocity_tree.py` & `dynamo-release-1.3.0/dynamo/tools/construct_velocity_tree.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,27 @@
 import re
 
-import matplotlib
 import matplotlib.pyplot as plt
-import networkx as nx
 import numpy as np
 import scipy
-from scipy.sparse import csr_matrix
 from scipy.sparse.csgraph import shortest_path
 
-from .DDRTree_py import DDRTree_py
+from .DDRTree_py import DDRTree
 
 
-def remove_velocity_points(G, n):
+def remove_velocity_points(G: np.ndarray, n: int) -> np.ndarray:
+    """Modify a tree graph to remove the nodes themselves and recalculate the weights.
+
+    Args:
+        G: a smooth tree graph embedded in the low dimension space.
+        n: the number of genes (column num of the original data)
+
+    Returns:
+        The tree graph with a node itself removed and weight recalculated.
+    """
     for nodeid in range(n, 2 * n):
         nb_ids = []
         for nb_id in range(len(G[0])):
             if G[nodeid][nb_id] != 0:
                 nb_ids = nb_ids + [nb_id]
         num_nbs = len(nb_ids)
 
@@ -37,38 +43,55 @@
             # print('Add ege %s, %s\n',G.Nodes.Name {nb_ids(i)}, G.Nodes.Name {nb_ids(min_ind)});
             G[nodeid][nb_ids[0]] = 0
             G[nb_ids[0]][nodeid] = 0
 
     return G
 
 
-def calculate_angle(o, y, x):
+def calculate_angle(o: np.ndarray, y: np.ndarray, x: np.ndarray) -> float:
+    """Calculate the angle between two vectors.
+
+    Args:
+        o: coordination of the origin.
+        y: end point of the first vector.
+        x: end point of the second vector.
+
+    Returns:
+        The angle between the two vectors.
+    """
+
     yo = y - o
     norm_yo = yo / scipy.linalg.norm(yo)
     xo = x - o
     norm_xo = xo / scipy.linalg.norm(xo)
     angle = np.arccos(norm_yo.T * norm_xo)
     return angle
 
 
-def construct_velocity_tree_py(X1, X2):
+def construct_velocity_tree_py(X1: np.ndarray, X2: np.ndarray) -> None:
+    """Save a velocity tree graph with given data.
+
+    Args:
+        X1: epxression matrix.
+        X2: velocity matrix.
+    """
     n = X1.shape[1]
 
     # merge two data with a given time
     t = 0.5
     X_all = np.hstack((X1, X1 + t * X2))
 
     # parameter settings
     maxIter = 20
     eps = 1e-3
     sigma = 0.001
     gamma = 10
 
     # run DDRTree algorithm
-    W, Z, stree, Y, R, history = DDRTree_py(X_all, maxIter=maxIter, eps=eps, sigma=sigma, gamma=gamma)
+    W, Z, stree, Y, R, history = DDRTree(X_all, maxIter=maxIter, eps=eps, sigma=sigma, gamma=gamma)
 
     # draw velocity figure
 
     # quiver(Z(1, 1: 100), Z(2, 1: 100), Z(1, 101: 200)-Z(1, 1: 100), Z(2, 101: 200)-Z(2, 1: 100));
     # plot(Z(1, 1: 100), Z(2, 1: 100), 'ob');
     # plot(Z(1, 101: 200), Z(2, 101: 200), 'sr');
     G = stree
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/dimension_reduction.py` & `dynamo-release-1.3.0/dynamo/tools/dimension_reduction.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,88 +1,84 @@
-from typing import Union
+from typing import List, Optional
 
-import anndata
 import numpy as np
+from anndata import AnnData
 
 from ..dynamo_logger import LoggerManager
 from ..utils import copy_adata
 from .connectivity import _gen_neighbor_keys, neighbors
+from .utils import update_dict
 from .utils_reduceDimension import prepare_dim_reduction, run_reduce_dim
 
 
 def reduceDimension(
-    adata: anndata.AnnData,
+    adata: AnnData,
     X_data: np.ndarray = None,
-    genes: Union[list, None] = None,
-    layer: Union[str, None] = None,
-    basis: Union[str, None] = "pca",
-    dims: Union[list, None] = None,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    basis: Optional[str] = "pca",
+    dims: Optional[List[int]] = None,
     n_pca_components: int = 30,
     n_components: int = 2,
     n_neighbors: int = 30,
     reduction_method: str = "umap",
-    embedding_key: Union[str, None] = None,
-    neighbor_key: Union[str, None] = None,
+    embedding_key: Optional[str] = None,
+    neighbor_key: Optional[str] = None,
     enforce: bool = False,
     cores: int = 1,
     copy: bool = False,
     **kwargs,
-) -> Union[anndata.AnnData, None]:
-    """Compute a low dimension reduction projection of an annodata object first with PCA, followed by non-linear
+) -> Optional[AnnData]:
+    """Compute a low dimension reduction projection of an AnnData object first with PCA, followed by non-linear
     dimension reduction methods
 
-    Arguments
-    ---------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        X_data:
-            The user supplied data that will be used for dimension reduction directly.
-        genes:
-            The list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
-            all genes will be used.
-        layer:
-            The layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is used.
-        basis:
-            The space that will be used for clustering. Valid names includes, for example, `pca`, `umap`, `velocity_pca`
-            (that is, you can use velocity for clustering), etc.
-        dims:
-            The list of dimensions that will be selected for clustering. If `None`, all dimensions will be used.
-        n_pca_components:
-            Number of input PCs (principle components) that will be used for further non-linear dimension reduction.. If n_pca_components is larger than the existing #PC in adata.obsm['X_pca'] or input layer's corresponding pca space (layer_pca), pca will be rerun with n_pca_components PCs requested.
-        n_components:
-            The dimension of the space to embed into.
-        n_neighbors:
-            Number of nearest neighbors when constructing adjacency matrix.
-        reduction_method:
-            Non-linear dimension reduction method to further reduce dimension based on the top n_pca_components PCA
-            components. Currently, PSL
-            (probablistic structure learning, a new dimension reduction by us), tSNE (fitsne instead of traditional tSNE
-            used) or umap are supported.
-        embedding_key:
-            The str in .obsm that will be used as the key to save the reduced embedding space. By default it is None and
-            embedding key is set as layer + reduction_method. If layer is None, it will be "X_neighbors".
-        neighbor_key:
-            The str in .uns that will be used as the key to save the nearest neighbor graph. By default it is None and
-            neighbor_key key is set as layer + "_neighbors". If layer is None, it will be "X_neighbors".
-        cores:
-            Number of cores. Used only when the tSNE reduction_method is used.
-
-    Returns
-    -------
-       adata: :class:`~anndata.AnnData`
-            An new or updated anndata object, based on copy parameter, that are updated with reduced dimension data for
-            data from different layers.
+    Args:
+        adata: an AnnData object.
+        X_data: the user supplied data that will be used for dimension reduction directly. Defaults to None.
+        genes: the list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
+            all genes will be used. Defaults to None.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        basis: the space that will be used for clustering. Valid names includes, for example, `pca`, `umap`,
+            `velocity_pca` (that is, you can use velocity for clustering), etc. Defaults to "pca".
+        dims: the list of dimensions that will be selected for clustering. If `None`, all dimensions will be used.
+            Defaults to None.
+        n_pca_components: Number of input PCs (principle components) that will be used for further non-linear dimension
+            reduction. If n_pca_components is larger than the existing #PC in adata.obsm['X_pca'] or input layer's
+            corresponding pca space (layer_pca), pca will be rerun with n_pca_components PCs requested. Defaults to 30.
+        n_components: the dimension of the space to embed into. Defaults to 2.
+        n_neighbors: the number of nearest neighbors when constructing adjacency matrix. Defaults to 30.
+        reduction_method: Non-linear dimension reduction method to further reduce dimension based on the top
+            n_pca_components PCA components. Currently, PSL (probablistic structure learning, a new dimension reduction
+            by us), tSNE (fitsne instead of traditional tSNE used) or umap are supported. Defaults to "umap".
+        embedding_key: The str in .obsm that will be used as the key to save the reduced embedding space. By default it
+            is None and embedding key is set as layer + reduction_method. If layer is None, it will be "X_neighbors".
+            Defaults to None.
+        neighbor_key: The str in .uns that will be used as the key to save the nearest neighbor graph. By default it is
+            None and neighbor_key key is set as layer + "_neighbors". If layer is None, it will be "X_neighbors".
+            Defaults to None.
+        enforce: whether to re-perform dimension reduction even if there is reduced basis in the AnnData object.
+            Defaults to False.
+        cores: the number of cores used for calculation. Used only when tSNE reduction_method is used. Defaults to 1.
+        copy: whether to return a copy of the AnnData object or update the object in place. Defaults to False.
+        kwargs: other kwargs that will be passed to umap.UMAP. for umap, min_dist is a noticeable kwargs that would
+            significantly influence the reduction result.
+
+    Returns:
+        An updated AnnData object updated with reduced dimension data for data from different layers, returned if `copy`
+        is true.
     """
 
     logger = LoggerManager.gen_logger("dynamo-dimension-reduction")
     logger.log_time()
 
     adata = copy_adata(adata) if copy else adata
 
-    logger.info("retrive data for non-linear dimension reduction...", indent_level=1)
+    logger.info("retrieve data for non-linear dimension reduction...", indent_level=1)
+
     if X_data is None:
         X_data, n_components, basis = prepare_dim_reduction(
             adata,
             genes=genes,
             layer=layer,
             basis=basis,
             dims=dims,
@@ -103,15 +99,15 @@
     if embedding_key is None:
         embedding_key = "X_" + reduction_method if layer is None else layer + "_" + reduction_method
     if neighbor_key is None:
         neighbor_result_prefix = "" if layer is None else layer
         conn_key, dist_key, neighbor_key = _gen_neighbor_keys(neighbor_result_prefix)
 
     if enforce or not has_basis:
-        logger.info(f"perform {reduction_method}...", indent_level=1)
+        logger.info(f"[{reduction_method.upper()}] using {basis} with n_pca_components = {n_pca_components}", indent_level=1)
         adata = run_reduce_dim(
             adata,
             X_data,
             n_components,
             n_pca_components,
             reduction_method,
             embedding_key,
@@ -119,21 +115,98 @@
             neighbor_key,
             cores,
             **kwargs,
         )
     if neighbor_key not in adata.uns_keys():
         neighbors(adata)
 
-    logger.finish_progress(progress_name="dimension_reduction projection")
+    logger.finish_progress(progress_name=reduction_method.upper())
 
     if copy:
         return adata
     return None
 
 
+def run_umap(
+    adata: AnnData,
+    X_data: np.ndarray = None,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    basis: Optional[str] = "pca",
+    dims: Optional[List[int]] = None,
+    n_pca_components: int = 30,
+    n_components: int = 2,
+    n_neighbors: int = 30,
+    embedding_key: Optional[str] = None,
+    neighbor_key: Optional[str] = None,
+    enforce: bool = False,
+    cores: int = 1,
+    copy: bool = False,
+    min_dist: float = 0.5,
+    **kwargs,
+) -> Optional[AnnData]:
+    """Compute a low dimension reduction projection of an AnnData object first with PCA, followed by UMAP.
+
+    This is a wrap for reduce Dimension, with the important min_dist value specified straightforwardly.
+
+    Args:
+        adata: an AnnData object.
+        X_data: the user supplied data that will be used for dimension reduction directly. Defaults to None.
+        genes: the list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
+            all genes will be used. Defaults to None.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        basis: the space that will be used for clustering. Valid names includes, for example, `pca`, `umap`,
+            `velocity_pca` (that is, you can use velocity for clustering), etc. Defaults to "pca".
+        dims: the list of dimensions that will be selected for clustering. If `None`, all dimensions will be used.
+            Defaults to None.
+        n_pca_components: Number of input PCs (principle components) that will be used for further non-linear dimension
+            reduction. If n_pca_components is larger than the existing #PC in adata.obsm['X_pca'] or input layer's
+            corresponding pca space (layer_pca), pca will be rerun with n_pca_components PCs requested. Defaults to 30.
+        n_components: the dimension of the space to embed into. Defaults to 2.
+        n_neighbors: the number of nearest neighbors when constructing adjacency matrix. Defaults to 30.
+        embedding_key: The str in .obsm that will be used as the key to save the reduced embedding space. By default it
+            is None and embedding key is set as layer + reduction_method. If layer is None, it will be "X_neighbors".
+            Defaults to None.
+        neighbor_key: The str in .uns that will be used as the key to save the nearest neighbor graph. By default it is
+            None and neighbor_key key is set as layer + "_neighbors". If layer is None, it will be "X_neighbors".
+            Defaults to None.
+        enforce: whether to re-perform dimension reduction even if there is reduced basis in the AnnData object.
+            Defaults to False.
+        cores: the number of cores used for calculation. Used only when tSNE reduction_method is used. Defaults to 1.
+        copy: whether to return a copy of the AnnData object or update the object in place. Defaults to False.
+        min_dist: the min_dist arg passed to umap.UMAP.
+        kwargs: other kwargs that will be passed to umap.UMAP. for umap, min_dist is a noticeable kwargs that would
+            significantly influence the reduction result.
+
+    Returns:
+        An updated AnnData object updated with reduced dimension data for data from different layers, returned if `copy`
+        is true.
+    """
+    kwargs = update_dict(kwargs, {"min_dist": min_dist})
+    return reduceDimension(
+        adata=adata,
+        X_data=X_data,
+        genes=genes,
+        layer=layer,
+        basis=basis,
+        dims=dims,
+        n_pca_components=n_pca_components,
+        n_components=n_components,
+        n_neighbors=n_neighbors,
+        reduction_method="umap",
+        embedding_key=embedding_key,
+        neighbor_key=neighbor_key,
+        enforce=enforce,
+        cores=cores,
+        copy=copy,
+        **kwargs,
+    )
+
+
 # @docstrings.with_indent(4)
 # def run_umap(X,
 #         n_neighbors=30,
 #         n_components=2,
 #         metric="euclidean",
 #         min_dist=0.1,
 #         spread=1.0,
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/dynamics.py` & `dynamo-release-1.3.0/dynamo/tools/dynamics.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,15 @@
 import inspect
 import warnings
-from typing import Optional, Union
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import numpy as np
 import pandas as pd
 from anndata import AnnData
 from scipy.sparse import SparseEfficiencyWarning, csr_matrix, issparse
 from tqdm import tqdm
 
@@ -46,253 +51,243 @@
 
 warnings.simplefilter("ignore", SparseEfficiencyWarning)
 
 
 # incorporate the model selection code soon
 def dynamics(
     adata: AnnData,
-    filter_gene_mode: str = "final",
+    filter_gene_mode: Literal["final", "basic", "no"] = "final",
     use_smoothed: bool = True,
-    assumption_mRNA: str = "auto",
-    assumption_protein="ss",
-    model: str = "auto",
-    est_method: str = "auto",
+    assumption_mRNA: Literal["ss", "kinetic", "auto"] = "auto",
+    assumption_protein: Literal["ss"] = "ss",
+    model: Literal["auto", "deterministic", "stochastic"] = "auto",
+    est_method: Literal["ols", "rlm", "ransac", "gmm", "negbin", "auto", "twostep", "direct"] = "auto",
     NTR_vel: bool = False,
     group: Optional[str] = None,
-    protein_names: Optional[list] = None,
+    protein_names: Optional[List[str]] = None,
     concat_data: bool = False,
     log_unnormalized: bool = True,
-    one_shot_method: str = "combined",
+    one_shot_method: Literal["combined", "sci-fate", "sci_fate"] = "combined",
     fraction_for_deg: bool = False,
     re_smooth: bool = False,
     sanity_check: bool = False,
     del_2nd_moments: Optional[bool] = None,
     cores: int = 1,
     tkey: str = None,
     **est_kwargs,
-):
-    """Inclusive model of expression dynamics considers splicing, metabolic labeling and protein translation. It
-    support learning high-dimensional velocity vector samples for droplet based (10x, inDrop, drop-seq, etc),
-    scSLAM-seq, NASC-seq sci-fate, scNT-seq, scEU-seq, cite-seq or REAP-seq datasets.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object.
-        filter_gene_mode: `str` (default: `final`)
-            The string for indicating which mode (one of, {'final', 'basic', 'no'}) of gene filter will be used.
-        use_smoothed: `bool` (default: 'True')
-            Whether to use the smoothed data when estimating kinetic parameters and calculating velocity for each gene.
-            When you have time-series data (`tkey` is not None), we recommend to smooth data among cells from each time
-            point.
-        assumption_mRNA: `str` `str` {`ss`, `kinetic`, `auto`}, (default: `auto`)
-            Parameter estimation assumption for mRNA. Available options are:
-            (1) 'ss': pseudo steady state;
-            (2) 'kinetic' or None: degradation and kinetic data without steady state assumption.
+) -> AnnData:
+    """Inclusive model of expression dynamics considers splicing, metabolic labeling and protein translation.
+
+    The function supports learning high-dimensional velocity vector samples for droplet based (10x, inDrop, drop-seq,
+    etc), scSLAM-seq, NASC-seq sci-fate, scNT-seq, scEU-seq, cite-seq or REAP-seq datasets.
+
+    Args:
+        adata: an AnnData object.
+        filter_gene_mode: The string for indicating which mode of gene filter will be used. Defaults to "final".
+        use_smoothed: whether to use the smoothed data when estimating kinetic parameters and calculating velocity for
+            each gene. When you have time-series data (`tkey` is not None), we recommend to smooth data among cells from
+            each time point. Defaults to True.
+        assumption_mRNA: Parameter estimation assumption for mRNA. Available options are:
+                (1) 'ss': pseudo steady state;
+                (2) 'kinetic' or None: degradation and kinetic data without steady state assumption.
+                (3) 'auto': dynamo will choose a reasonable assumption of the system under study automatically.
             If no labelling data exists, assumption_mRNA will automatically set to be 'ss'. For one-shot experiment,
             assumption_mRNA is set to be None. However we will use steady state assumption to estimate parameters alpha
             and gamma either by a deterministic linear regression or the first order decay approach in line of the
             sci-fate paper;
-            (3) 'auto': dynamo will choose a reasonable assumption of the system under study automatically.
-        assumption_protein: `str`, (default: `ss`)
-            Parameter estimation assumption for protein. Available options are:
-            (1) 'ss': pseudo steady state;
-        model: `str` {`auto`, `deterministic`, `stochastic`} (default: `auto`)
-            String indicates which estimation model will be used.
-            (1) 'deterministic': The method based on `deterministic` ordinary differential equations;
-            (2) 'stochastic' or `moment`: The new method from us that is based on `stochastic` master equations;
+            Defaults to "auto".
+        assumption_protein: Parameter estimation assumption for protein. Available options are:
+                (1) 'ss': pseudo steady state;
+            Defaults to "ss".
+        model: String indicates which estimation model will be used.
+            Available options are:
+                (1) 'deterministic': The method based on `deterministic` ordinary differential equations;
+                (2) 'stochastic' or `moment`: The new method from us that is based on `stochastic` master equations;
             Note that `kinetic` model doesn't need to assumes the `experiment_type` is not `conventional`. As other
             labeling experiments, if you specify the `tkey`, dynamo can also apply `kinetic` model on `conventional`
             scRNA-seq datasets. A "model_selection" model will be supported soon in which alpha, beta and gamma will be
             modeled as a function of time.
-        est_method: `str` {`ols`, `rlm`, `ransac`, `gmm`, `negbin`, `auto`} This parameter should be used in conjunction
-        with `model` parameter.
-            * Available options when the `model` is 'ss' include:
-            (1) 'ols': The canonical method or Ordinary Least Squares regression from the seminar RNA velocity paper
-            based on deterministic ordinary differential equations;
-            (2) 'rlm': The robust linear models from statsmodels. Robust Regression provides an alternative to OLS
-            regression by lowering the restrictions on assumptions and dampens the effect of outliers in order to fit
-            majority of the data.
-            (3) 'ransac': RANSAC (RANdom SAmple Consensus) algorithm for robust linear regression. RANSAC is an
-             iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data
-             set. RANSAC implementation is based on RANSACRegressor function from sklearn package. Note that if `rlm` or
-             `ransac` failed, it will roll back to the `ols` method. In addition, `ols`, `rlm` and `ransac` can be only
-             used in conjunction with the `deterministic` model.
-            (4) 'gmm': The new generalized methods of moments from us that is based on master equations, similar to the
-            "moment" model in the excellent scVelo package;
-            (5) 'negbin': The new method from us that models steady state RNA expression as a negative binomial
-            distribution, also built upon on master equations.
+            Defaults to "auto".
+        est_method: This parameter should be used in conjunction with `model` parameter.
+            Available options when the `model` is 'ss' include:
+                (1) 'ols': The canonical method or Ordinary Least Squares regression from the seminar RNA velocity paper
+                    based on deterministic ordinary differential equations;
+                (2) 'rlm': The robust linear models from statsmodels. Robust Regression provides an alternative to OLS
+                    regression by lowering the restrictions on assumptions and dampens the effect of outliers in order
+                    to fit majority of the data.
+                (3) 'ransac': RANSAC (RANdom SAmple Consensus) algorithm for robust linear regression. RANSAC is an
+                    iterative algorithm for the robust estimation of parameters from a subset of inliers from the
+                    complete dataset. RANSAC implementation is based on RANSACRegressor function from sklearn package.
+                    Note that if `rlm` or `ransac` failed, it will roll back to the `ols` method. In addition, `ols`,
+                    `rlm` and `ransac` can be only used in conjunction with the `deterministic` model.
+                (4) 'gmm': The new generalized methods of moments from us that is based on master equations, similar to
+                    the "moment" model in the excellent scVelo package;
+                (5) 'negbin': The new method from us that models steady state RNA expression as a negative binomial
+                    distribution, also built upon on master equations.
+                (6) 'auto': dynamo will choose the suitable estimation method based on the `assumption_mRNA`,
+                    `experiment_type` and `model` parameter.
             Note that all those methods require using extreme data points (except negbin, which use all data points) for
             estimation. Extreme data points are defined as the data from cells whose expression of unspliced / spliced
             or new / total RNA, etc. are in the top or bottom, 5%, for example. `linear_regression` only considers the
             mean of RNA species (based on the `deterministic` ordinary different equations) while moment based methods
             (`gmm`, `negbin`) considers both first moment (mean) and second moment (uncentered variance) of RNA species
             (based on the `stochastic` master equations).
-            (4) 'auto': dynamo will choose the suitable estimation method based on the `assumption_mRNA`,
-            `experiment_type` and `model` parameter.
             The above method are all (generalized) linear regression based method. In order to return estimated
             parameters (including RNA half-life), it additionally returns R-squared (either just for extreme data points
             or all data points) as well as the log-likelihood of the fitting, which will be used for transition matrix
             and velocity embedding.
-            * Available options when the `assumption_mRNA` is 'kinetic' include:
-            (1) 'auto': dynamo will choose the suitable estimation method based on the `assumption_mRNA`,
-            `experiment_type` and `model` parameter.
-            * Available options when the `model` is 'ss' include:
-            (1) `twostep`: first for each time point, estimate K (1-e^{-rt}) using the total and new RNA data. Then
-            use regression via t-np.log(1-K) to get degradation rate gamma. When splicing and labeling data both exist,
-            replacing new/total with ul/u can be used to estimate beta. Suitable for velocity estimation.
-            (2) `direct` (default): method that directly uses the kinetic model to estimate rate parameters, generally
-            not good for velocity estimation.
+            Available options when the `assumption_mRNA` is 'kinetic' include:
+                (1) 'auto': dynamo will choose the suitable estimation method based on the `assumption_mRNA`,
+                    `experiment_type` and `model` parameter.
+                (2) `twostep`: first for each time point, estimate K (1-e^{-rt}) using the total and new RNA data. Then
+                    use regression via t-np.log(1-K) to get degradation rate gamma. When splicing and labeling data both
+                    exist, replacing new/total with ul/u can be used to estimate beta. Suitable for velocity estimation.
+                (3) `direct` (default): method that directly uses the kinetic model to estimate rate parameters,
+                    generally not good for velocity estimation.
             Under `kinetic` model, choosing estimation is `experiment_type` dependent. For `kinetics` experiments,
             dynamo supposes methods including RNA bursting or without RNA bursting. Dynamo also adaptively estimates
             parameters, based on whether the data has splicing or without splicing.
             Under `kinetic` assumption, the above method uses non-linear least square fitting. In order to return
             estimated parameters (including RNA half-life), it additionally returns the log-likelihood of the
             fitting, which will be used for transition matrix and velocity embedding.
             All `est_method` uses least square to estimate optimal parameters with latin cubic sampler for initial
-            sampling.
-        NTR_vel: `bool` (default: `False`)
-            Whether to use NTR (new/total ratio) velocity for labeling datasets.
-        group: `str` or None (default: `None`)
-            The column key/name that identifies the grouping information (for example, clusters that correspond to
+            sampling. Defaults to "auto".
+        NTR_vel: whether to use NTR (new/total ratio) velocity for labeling datasets. Defaults to False.
+        group: the column key/name that identifies the grouping information (for example, clusters that correspond to
             different cell types) of cells. This will be used to calculate 1/2 st moments and covariance for each cells
             in each group. It will also enable estimating group-specific (i.e cell-type specific) kinetic parameters.
-        protein_names: `List`
-            A list of gene names corresponds to the rows of the measured proteins in the `X_protein` of the `obsm`
-            attribute. The names have to be included in the adata.var.index.
-        concat_data: `bool` (default: `False`)
-            Whether to concatenate data before estimation. If your data is a list of matrices for each time point, this
-            need to be set as True.
-        log_unnormalized: `bool` (default: `True`)
-            Whether to log transform the unnormalized data.
-        one_shot_method: `str` (default: `combined`)
-            The method that will be used for estimating kinetic parameters for one-shot experiment data. Can be one of
-            {"combined", "sci-fate", "sci_fate"}.
-            (1). the "sci-fate" method directly solves gamma with the first-order decay model;
-            (2). the "combined" model uses the linear regression under steady state to estimate relative gamma, and then
-                 calculate absolute gamma (degradation rate), beta (splicing rate) and cell-wise alpha (transcription
-                 rate).
-        fraction_for_deg: `bool` (default: `False`)
-            Whether to use the fraction of labeled RNA instead of the raw labeled RNA to estimate the degradation
-            parameter.
-        re_smooth: `bool` (default: `False`)
-            Whether to re-smooth the adata and also recalculate 1/2 moments or covariance.
-        sanity_check: `bool` (default: `False`)
-            Whether to perform sanity-check before estimating kinetic parameters and velocity vectors, currently only
-            applicable to kinetic or degradation metabolic labeling based scRNA-seq data. The basic idea is that for
-            kinetic (degradation) experiment, the total labelled RNA for each gene should increase (decrease) over time.
-            If they don't satisfy this criteria, those genes will be ignored during the estimation.
-        del_2nd_moments: `bool` (default: `False`)
-            Whether to remove second moments or covariances. Default it is `False` so this avoids recalculating 2nd
-            moments or covariance but it may take a lot memory when your dataset is big. Set this to `True` when your
-            data is huge (like > 25, 000 cells or so) to reducing the memory footprint.
-        cores: `int` (default: 1):
-            Number of cores to run the estimation. If cores is set to be > 1, multiprocessing will be used to parallel
-            the parameter estimation. Currently only applicable cases when assumption_mRNA is `ss` or cases when
-            experiment_type is either "one-shot" or "mix_std_stm".
-        tkey:
-            The column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data. If `tkey` is None, then  `adata.uns["pp"]["tkey"]` will be checked and used if exists.
-        **est_kwargs
-            Other arguments passed to the fit method (steady state models) or estimation methods (kinetic models).
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An updated AnnData object with estimated kinetic parameters, inferred velocity and estimation related
-            information included. The estimated kinetic parameters are currently appended to .obs (should move to .obsm
-            with the key `dynamics` later). Depends on the estimation method, experiment type and whether you applied
-            estimation for each groups via `group`, the number of returned parameters can be variable. For conventional
-            scRNA-seq (including cite-seq or other types of protein/RNA coassays) and somethings metabolic labeling data
-            , the parameters will  at mostly include:
-                alpha: Transcription rate
-                beta: Splicing rate
-                gamma: Spliced RNA degradation rate
-                eta: Translation rate (only applicable to RNA/protein coassay)
-                delta: Protein degradation rate (only applicable to RNA/protein coassay)
-                alpha_b: intercept of alpha fit
-                beta_b: intercept of beta fit
-                gamma_b: intercept of gamma fit
-                eta_b: intercept of eta fit (only applicable to RNA/protein coassay)
-                delta_b: intercept of delta fit (only applicable to RNA/protein coassay)
-                alpha_r2: r-squared for goodness of fit of alpha estimation
-                beta_r2: r-squared for goodness of fit of beta estimation
-                gamma_r2: r-squared for goodness of fit of gamma estimation
-                eta_r2: r-squared for goodness of fit of eta estimation (only applicable to RNA/protein coassay)
-                delta_r2: r-squared for goodness of fit of delta estimation (only applicable to RNA/protein coassay)
-                alpha_logLL: loglikelihood of alpha estimation (only applicable to stochastic model)
-                beta_loggLL: loglikelihood of beta estimation (only applicable to stochastic model)
-                gamma_logLL: loglikelihood of gamma estimation (only applicable to stochastic model)
-                eta_logLL: loglikelihood of eta estimation (only applicable to stochastic model and RNA/protein coassay)
-                delta_loggLL: loglikelihood of delta estimation (only applicable to stochastic model and RNA/protein
-                    coassay)
-                uu0: estimated amount of unspliced unlabeled RNA at time 0 (only applicable to data with both splicing
-                    and labeling)
-                ul0: estimated amount of unspliced labeled RNA at time 0 (only applicable to data with both splicing
-                    and labeling)
-                su0: estimated amount of spliced unlabeled RNA at time 0 (only applicable to data with both splicing
-                    and labeling)
-                sl0: estimated amount of spliced labeled RNA at time 0 (only applicable to data with both splicing and
-                    labeling)
-                U0: estimated amount of unspliced RNA (uu + ul) at time 0
-                S0: estimated amount of spliced (su + sl) RNA at time 0
-                total0: estimated amount of spliced (U + S) RNA at time 0
-                half_life: Spliced mRNA's half-life (log(2) / gamma)
-
-            Note that all data points are used when estimating r2 although only extreme data points are used for
-            estimating r2. This is applicable to all estimation methods, either `linear_regression`, `gmm` or `negbin`.
-            By default we set the intercept to be 0.
-
-            For metabolic labeling data, the kinetic parameters will at most include:
-                alpha: Transcription rate (effective - when RNA promoter switching considered)
-                beta: Splicing rate
-                gamma: Spliced RNA degradation rate
-                a: Switching rate from active promoter state to inactive promoter state
-                b: Switching rate from inactive promoter state to active promoter state
-                alpha_a: Transcription rate for active promoter
-                alpha_i: Transcription rate for inactive promoter
-                cost: cost of the kinetic parameters estimation
-                logLL: loglikelihood of kinetic parameters estimation
-                alpha_r2: r-squared for goodness of fit of alpha estimation
-                beta_r2: r-squared for goodness of fit of beta estimation
-                gamma_r2: r-squared for goodness of fit of gamma estimation
-                uu0: estimated amount of unspliced unlabeled RNA at time 0 (only applicable to data with both splicing
-                    and labeling)
-                ul0: estimated amount of unspliced labeled RNA at time 0 (only applicable to data with both splicing
-                    and labeling)
-                su0: estimated amount of spliced unlabeled RNA at time 0 (only applicable to data with both splicing
-                    and labeling)
-                sl0: estimated amount of spliced labeled RNA at time 0 (only applicable to data with both splicing and
-                    labeling)
-                u0: estimated amount of unspliced RNA (including uu, ul) at time 0
-                s0: estimated amount of spliced (including su, sl) RNA at time 0
-                total0: estimated amount of spliced (including U, S) RNA at time 0
-                p_half_life: half-life for unspliced mRNA
-                half_life: half-life for spliced mRNA
-
-            If sanity_check has performed, a column with key `sanity_check` will also included which indicates which
-            gene passes filter (`filter_gene_mode`) and sanity check. This is only applicable to kinetic and degradation
-            metabolic labeling experiments.
-
-            In addition, the `dynamics` key of the .uns attribute corresponds to a dictionary that includes the
-            following keys:
-                t: An array like object that indicates the time point of each cell used during parameters estimation
-                    (applicable only to kinetic models)
-                group: The group that you used to estimate parameters group-wise
-                X_data: The input that was used for estimating parameters (applicable only to kinetic models)
-                X_fit_data: The data that was fitted during parameters estimation (applicable only to kinetic models)
-                asspt_mRNA: Assumption of mRNA dynamics (steady state or kinetic)
-                experiment_type: Experiment type (either conventional or metabolic labeling based)
-                normalized: Whether to normalize data
-                model: Model used for the parameter estimation (either auto, deterministic or stochastic)
-                has_splicing: Does the adata has splicing? detected automatically
-                has_labeling: Does the adata has labelling? detected automatically
-                has_protein: Does the adata has protein information? detected automatically
-                use_smoothed: Whether to use smoothed data (or first moment, done via local average of neighbor cells)
-                NTR_vel: Whether to estimate NTR velocity
-                log_unnormalized: Whether to log transform unnormalized data.
+            Defaults to None.
+        protein_names: a list of gene names corresponds to the rows of the measured proteins in the `X_protein` of the
+            `obsm` attribute. The names have to be included in the adata.var.index. Defaults to None.
+        concat_data: whether to concatenate data before estimation. If your data is a list of matrices for each time
+            point, this need to be set as True. Defaults to False.
+        log_unnormalized: whether to log transform the unnormalized data. Defaults to True.
+        one_shot_method: The method that will be used for estimating kinetic parameters for one-shot experiment data.
+            (1) the "sci-fate" method directly solves gamma with the first-order decay model;
+            (2) the "combined" model uses the linear regression under steady state to estimate relative gamma, and then
+                calculate absolute gamma (degradation rate), beta (splicing rate) and cell-wise alpha (transcription
+                rate). Defaults to "combined".
+        fraction_for_deg: whether to use the fraction of labeled RNA instead of the raw labeled RNA to estimate the
+            degradation parameter. Defaults to False.
+        re_smooth: whether to re-smooth the adata and also recalculate 1/2 moments or covariance. Defaults to False.
+        sanity_check: whether to perform sanity-check before estimating kinetic parameters and velocity vectors,
+            currently only applicable to kinetic or degradation metabolic labeling based scRNA-seq data. The basic idea
+            is that for kinetic (degradation) experiment, the total labelled RNA for each gene should increase
+            (decrease) over time. If they don't satisfy this criteria, those genes will be ignored during the
+            estimation. Defaults to False.
+        del_2nd_moments: whether to remove second moments or covariances. Default it is `False` so this avoids
+            recalculating 2nd moments or covariance but it may take a lot memory when your dataset is big. Set this to
+            `True` when your data is huge (like > 25, 000 cells or so) to reducing the memory footprint. Defaults to
+            None.
+        cores: number of cores to run the estimation. If cores is set to be > 1, multiprocessing will be used to
+            parallel the parameter estimation. Currently only applicable cases when assumption_mRNA is `ss` or cases
+            when experiment_type is either "one-shot" or "mix_std_stm". Defaults to 1.
+        tkey: the column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data. If `tkey`
+            is None, then  `adata.uns["pp"]["tkey"]` will be checked and used if exists. Defaults to None.
+        **est_kwargs: Other arguments passed to the fit method (steady state models) or estimation methods (kinetic
+            models).
+
+    Raises:
+        ValueError: preprocessing not performed.
+        Exception: No gene pass filter.
+        Exception: Too few valid genes.
+
+    Returns:
+        An updated AnnData object with estimated kinetic parameters, inferred velocity and estimation related
+        information included. The estimated kinetic parameters are currently appended to .obs (should move to .obsm with
+        the key `dynamics` later). Depends on the estimation method, experiment type and whether you applied estimation
+        for each groups via `group`, the number of returned parameters can be variable. For conventional scRNA-seq
+        (including cite-seq or other types of protein/RNA coassays) and somethings metabolic labeling data, the
+        parameters will  at mostly include:
+            alpha: Transcription rate
+            beta: Splicing rate
+            gamma: Spliced RNA degradation rate
+            eta: Translation rate (only applicable to RNA/protein coassay)
+            delta: Protein degradation rate (only applicable to RNA/protein coassay)
+            alpha_b: intercept of alpha fit
+            beta_b: intercept of beta fit
+            gamma_b: intercept of gamma fit
+            eta_b: intercept of eta fit (only applicable to RNA/protein coassay)
+            delta_b: intercept of delta fit (only applicable to RNA/protein coassay)
+            alpha_r2: r-squared for goodness of fit of alpha estimation
+            beta_r2: r-squared for goodness of fit of beta estimation
+            gamma_r2: r-squared for goodness of fit of gamma estimation
+            eta_r2: r-squared for goodness of fit of eta estimation (only applicable to RNA/protein coassay)
+            delta_r2: r-squared for goodness of fit of delta estimation (only applicable to RNA/protein coassay)
+            alpha_logLL: loglikelihood of alpha estimation (only applicable to stochastic model)
+            beta_loggLL: loglikelihood of beta estimation (only applicable to stochastic model)
+            gamma_logLL: loglikelihood of gamma estimation (only applicable to stochastic model)
+            eta_logLL: loglikelihood of eta estimation (only applicable to stochastic model and RNA/protein coassay)
+            delta_loggLL: loglikelihood of delta estimation (only applicable to stochastic model and RNA/protein
+                coassay)
+            uu0: estimated amount of unspliced unlabeled RNA at time 0 (only applicable to data with both splicing
+                and labeling)
+            ul0: estimated amount of unspliced labeled RNA at time 0 (only applicable to data with both splicing
+                and labeling)
+            su0: estimated amount of spliced unlabeled RNA at time 0 (only applicable to data with both splicing
+                and labeling)
+            sl0: estimated amount of spliced labeled RNA at time 0 (only applicable to data with both splicing and
+                labeling)
+            U0: estimated amount of unspliced RNA (uu + ul) at time 0
+            S0: estimated amount of spliced (su + sl) RNA at time 0
+            total0: estimated amount of spliced (U + S) RNA at time 0
+            half_life: Spliced mRNA's half-life (log(2) / gamma)
+
+        Note that all data points are used when estimating r2 although only extreme data points are used for
+        estimating r2. This is applicable to all estimation methods, either `linear_regression`, `gmm` or `negbin`.
+        By default we set the intercept to be 0.
+
+        For metabolic labeling data, the kinetic parameters will at most include:
+            alpha: Transcription rate (effective - when RNA promoter switching considered)
+            beta: Splicing rate
+            gamma: Spliced RNA degradation rate
+            a: Switching rate from active promoter state to inactive promoter state
+            b: Switching rate from inactive promoter state to active promoter state
+            alpha_a: Transcription rate for active promoter
+            alpha_i: Transcription rate for inactive promoter
+            cost: cost of the kinetic parameters estimation
+            logLL: loglikelihood of kinetic parameters estimation
+            alpha_r2: r-squared for goodness of fit of alpha estimation
+            beta_r2: r-squared for goodness of fit of beta estimation
+            gamma_r2: r-squared for goodness of fit of gamma estimation
+            uu0: estimated amount of unspliced unlabeled RNA at time 0 (only applicable to data with both splicing
+                and labeling)
+            ul0: estimated amount of unspliced labeled RNA at time 0 (only applicable to data with both splicing
+                and labeling)
+            su0: estimated amount of spliced unlabeled RNA at time 0 (only applicable to data with both splicing
+                and labeling)
+            sl0: estimated amount of spliced labeled RNA at time 0 (only applicable to data with both splicing and
+                labeling)
+            u0: estimated amount of unspliced RNA (including uu, ul) at time 0
+            s0: estimated amount of spliced (including su, sl) RNA at time 0
+            total0: estimated amount of spliced (including U, S) RNA at time 0
+            p_half_life: half-life for unspliced mRNA
+            half_life: half-life for spliced mRNA
+
+        If sanity_check has performed, a column with key `sanity_check` will also included which indicates which
+        gene passes filter (`filter_gene_mode`) and sanity check. This is only applicable to kinetic and degradation
+        metabolic labeling experiments.
+
+        In addition, the `dynamics` key of the .uns attribute corresponds to a dictionary that includes the
+        following keys:
+            t: An array like object that indicates the time point of each cell used during parameters estimation
+                (applicable only to kinetic models)
+            group: The group that you used to estimate parameters group-wise
+            X_data: The input that was used for estimating parameters (applicable only to kinetic models)
+            X_fit_data: The data that was fitted during parameters estimation (applicable only to kinetic models)
+            asspt_mRNA: Assumption of mRNA dynamics (steady state or kinetic)
+            experiment_type: Experiment type (either conventional or metabolic labeling based)
+            normalized: Whether to normalize data
+            model: Model used for the parameter estimation (either auto, deterministic or stochastic)
+            has_splicing: Does the adata has splicing? detected automatically
+            has_labeling: Does the adata has labelling? detected automatically
+            has_protein: Does the adata has protein information? detected automatically
+            use_smoothed: Whether to use smoothed data (or first moment, done via local average of neighbor cells)
+            NTR_vel: Whether to estimate NTR velocity
+            log_unnormalized: Whether to log transform unnormalized data.
     """
 
     del_2nd_moments = DynamoAdataConfig.use_default_var_if_none(
         del_2nd_moments, DynamoAdataConfig.DYNAMICS_DEL_2ND_MOMENTS_KEY
     )
     if "pp" not in adata.uns_keys():
         raise ValueError(f"\nPlease run `dyn.pp.receipe_monocle(adata)` before running this function!")
@@ -345,17 +340,14 @@
 
         if len(M_layers) < 2 or re_smooth:
             main_info("removing existing M layers:%s..." % (str(list(M_layers))), indent_level=2)
             for i in M_layers:
                 del adata.layers[i]
             main_info("making adata smooth...", indent_level=2)
 
-            if filter_gene_mode.lower() == "final" and "X_pca" in adata.obsm.keys():
-                adata.obsm["X"] = adata.obsm["X_pca"]
-
             if group is not None and group in adata.obs.columns:
                 moments(adata, genes=valid_bools, group=group)
             else:
                 moments(adata, genes=valid_bools, group=tkey)
         elif tkey is not None:
             main_warning(
                 f"You used tkey {tkey} (or group {group}), but you have calculated local smoothing (1st moment) "
@@ -979,27 +971,92 @@
     if del_2nd_moments:
         remove_2nd_moments(adata)
 
     return adata
 
 
 def kinetic_model(
-    subset_adata,
-    tkey,
-    model,
-    est_method,
-    experiment_type,
-    has_splicing,
-    splicing_labeling,
-    has_switch,
-    param_rngs,
-    data_type="sfs",
-    return_ntr=False,
+    subset_adata: AnnData,
+    tkey: str,
+    model: Literal["auto", "deterministic", "stochastic"],
+    est_method: Literal["twostep", "direct"],
+    experiment_type: str,
+    has_splicing: bool,
+    splicing_labeling: bool,
+    has_switch: bool,
+    param_rngs: Dict[str, List[int]],
+    data_type: Literal["smoothed", "sfs"] = "sfs",
+    return_ntr: bool = False,
     **est_kwargs,
-):
+) -> Tuple[
+    Union[Dict[str, Any], pd.DataFrame],
+    np.ndarray,
+    Optional[np.ndarray],
+    Optional[np.ndarray],
+    Optional[Dict[str, List[int]]],
+    List[np.ndarray],
+    List[np.ndarray],
+]:
+    """Calculate the parameters required for velocity estimation when the mRNA assumption is 'kinetic'.
+
+    Args:
+        subset_adata: an AnnData object with invalid genes trimmed.
+        tkey: the column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data. If `tkey`
+            is None, then `adata.uns["pp"]["tkey"]` will be checked and used if exists.
+        model: String indicates which estimation model will be used.
+            Available options are:
+                (1) 'deterministic': The method based on `deterministic` ordinary differential equations;
+                (2) 'stochastic' or `moment`: The new method from us that is based on `stochastic` master equations;
+            Note that `kinetic` model doesn't need to assume the `experiment_type` is not `conventional`. As other
+            labeling experiments, if you specify the `tkey`, dynamo can also apply `kinetic` model on `conventional`
+            scRNA-seq datasets. A "model_selection" model will be supported soon in which alpha, beta and gamma will be
+            modeled as a function of time.
+        est_method: Available options when the `assumption_mRNA` is 'kinetic' include:
+                (1) 'auto': dynamo will choose the suitable estimation method based on the `assumption_mRNA`,
+                    `experiment_type` and `model` parameter.
+                (2) `twostep`: first for each time point, estimate K (1-e^{-rt}) using the total and new RNA data. Then
+                    use regression via t-np.log(1-K) to get degradation rate gamma. When splicing and labeling data both
+                    exist, replacing new/total with ul/u can be used to estimate beta. Suitable for velocity estimation.
+                (3) `direct` (default): method that directly uses the kinetic model to estimate rate parameters,
+                    generally not good for velocity estimation.
+            Under `kinetic` model, choosing estimation is `experiment_type` dependent. For `kinetics` experiments,
+            dynamo supposes methods including RNA bursting or without RNA bursting. Dynamo also adaptively estimates
+            parameters, based on whether the data has splicing or without splicing.
+            Under `kinetic` assumption, the above method uses non-linear least square fitting. In order to return
+            estimated parameters (including RNA half-life), it additionally returns the log-likelihood of the
+            fitting, which will be used for transition matrix and velocity embedding.
+            All `est_method` uses least square to estimate optimal parameters with latin cubic sampler for initial
+            sampling.
+        experiment_type: the experiment type of the data.
+        has_splicing: whether the object containing unspliced and spliced data
+        splicing_labeling: hether the object containing both splicing and labelling data
+        has_switch: whether there should be switch for stochastic model.
+        param_rngs: the range set for each parameter.
+        data_type: the data type, could be "smoothed" or "sfs". Defaults to "sfs".
+        return_ntr: whether to deal with new/total ratio. Defaults to False.
+
+    Raises:
+        NotImplementedError: `model` is invalid.
+        NotImplementedError: `model` is invalid.
+        NotImplementedError: `model` is invalid.
+        NotImplementedError: `model` is invalid.
+        NotImplementedError: `experiment_type` is invalid.
+        NotImplementedError: mix_pulse_chase/mix_kin_deg experiment can only use `deterministic` model.
+        Exception: pulse_time_series experiment type not implemented.
+        Exception: dual_labeling experiment type not implemented.
+        Exception: experiment type invalid.
+
+    Returns:
+        A tuple (Estm_df, half_life, cost, logLL, _param_ranges, X_data, X_fit_data), where Estm_df contains the
+        parameters required for mRNA velocity calculation, half_life is for half-life of spliced mRNA, cost is for the
+        cost of kinetic parameters estimation, logLL is for loglikelihood of kinetic parameters estimation,
+        _param_ranges is for the intended range of parameter estimation, X_data is for the data used for parameter
+        estimation, and X_fit_data is for the data that get fitted during parameter estimation.
+    """
+
     """est_method can be either `twostep` (two-step model) or `direct`. data_type can either 'sfs' or 'smoothed'."""
     logger = LoggerManager.gen_logger("dynamo-kinetic-model")
     logger.info("experiment type: %s, method: %s, model: %s" % (experiment_type.lower(), str(est_method), str(model)))
     time = subset_adata.obs[tkey].astype("float").values
     if experiment_type.lower() == "kin":
         if est_method == "twostep":
             if has_splicing:
@@ -1371,15 +1428,15 @@
                         "oo0": [0, 1000],
                     }
                     Est = Mixture_KinDeg_NoSwitching(
                         Moments_NoSwitchingNoSplicing(),
                         Moments_NoSwitchingNoSplicing(),
                     )
                 else:
-                    raise Exception(
+                    raise NotImplementedError(
                         f"model {model} with kinetic assumption is not implemented. "
                         f"current supported models for kinetics experiments include: stochastic, deterministic, "
                         f"mixture, mixture_deterministic_stochastic or mixture_stochastic_stochastic"
                     )
     elif experiment_type.lower() == "deg":
         if has_splicing and splicing_labeling:
             layers = (
@@ -1473,15 +1530,15 @@
             else:
                 raise NotImplementedError(
                     f"model {model} with kinetic assumption is not implemented. "
                     f"current supported models for degradation experiment include: "
                     f"stochastic, deterministic."
                 )
     elif experiment_type.lower() == "mix_std_stm":
-        raise Exception(f"experiment {experiment_type} with kinetic assumption is not implemented")
+        raise NotImplementedError(f"experiment {experiment_type} with kinetic assumption is not implemented")
     elif experiment_type.lower() in ["mix_pulse_chase", "mix_kin_deg"]:
         total_layer = "M_t" if ("M_t" in subset_adata.layers.keys() and data_type == "smoothed") else "X_total"
 
         if model.lower() in ["deterministic"]:
             layer = "M_n" if ("M_n" in subset_adata.layers.keys() and data_type == "smoothed") else "X_new"
             X, X_raw = prepare_data_no_splicing(
                 subset_adata,
@@ -1646,23 +1703,14 @@
         )
 
         Estm_df = pd.DataFrame(np.vstack(Estm), columns=[*all_keys[: len(Estm[0])]])
         Estm_df["gamma_k"] = gamma_k  # gamma_k = gamma / beta
         Estm_df["beta"] = Estm_df["gamma"] / gamma_k  # gamma_k = gamma / beta
         Estm_df["gamma_r2"] = gamma_all_r2
 
-        return (
-            Estm_df,
-            half_life,
-            cost,
-            logLL,
-            _param_ranges,
-            X_data,
-            X_fit_data,
-        )
     elif experiment_type.lower() in ["mix_pulse_chase", "mix_kin_deg"] and est_method == "twostep":
         if has_splicing:
             layers = (
                 ["M_u", "M_s"] if ("M_u" in subset_adata.layers.keys() and data_type == "smoothed") else ["X_u", "X_s"]
             )
             U, S = (
                 subset_adata.layers[layers[0]].T,
@@ -1689,22 +1737,39 @@
             Estm_df["gamma_k"] = Estm_df["gamma"]  # fix a bug in pl.dynamics
     else:
         Estm_df = pd.DataFrame(np.vstack(Estm), columns=[*all_keys[: len(Estm[0])]])
 
     return Estm_df, half_life, cost, logLL, _param_ranges, X_data, X_fit_data
 
 
-def fbar(a, b, alpha_a, alpha_i):
+def fbar(a: np.ndarray, b: np.ndarray, alpha_a: np.ndarray, alpha_i: np.ndarray) -> Optional[np.ndarray]:
+    """Calculate transcription rate based on switching rate and transcription rate.
+
+    Args:
+        a: switching rate from active promoter state to inactive promoter state.
+        b: switching rate from inactive promoter state to active promoter state.
+        alpha_a: transcription rate for active promoter.
+        alpha_i: transcription rate for inactive promoter.
+
+    Returns:
+        The transcription rate (effective - when RNA promoter switching considered).
+    """
     if any([i is None for i in [a, b, alpha_a, alpha_i]]):
         return None
     else:
         return b / (a + b) * alpha_a + a / (a + b) * alpha_i
 
 
-def _get_dispatcher():
+def _get_dispatcher() -> dict:
+    """Build a dict containing simulators for several kinds of dynamics.
+
+    Returns:
+        A dict containing simulators for several kinds of dynamics.
+    """
+
     dispatcher = {
         "Deterministic": Deterministic,
         "Deterministic_NoSplicing": Deterministic_NoSplicing,
         "Moments_NoSwitching": Moments_NoSwitching,
         "Moments_NoSwitchingNoSplicing": Moments_NoSwitchingNoSplicing,
         "Mixture_KinDeg_NoSwitching": Mixture_KinDeg_NoSwitching,
     }
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/dynamo_bk.py` & `dynamo-release-1.3.0/dynamo/tools/dynamo_bk.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/tools/dynamo_fitting.py` & `dynamo-release-1.3.0/dynamo/tools/dynamo_fitting.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/tools/graph_operators.py` & `dynamo-release-1.3.0/dynamo/tools/graph_operators.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 import numpy as np
 from igraph import Graph
 from scipy.linalg import qr
 from scipy.sparse import csr_matrix
 
 
-def gradop(g):
+def gradop(g: csr_matrix):
     e = np.array(g.get_edgelist())
     ne = g.ecount()
     i, j, x = np.tile(range(ne), 2), e.T.flatten(), np.repeat([-1, 1], ne)
 
     return csr_matrix((x, (i, j)), shape=(ne, g.vcount()))
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/growth.py` & `dynamo-release-1.3.0/dynamo/tools/growth.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,80 +1,76 @@
 """Mapping Vector Field of Single Cells
 """
 
 # module to deal with reaction/diffusion/advection.
 # code was loosely based on PBA, WOT and PRESCIENT.
 
+from typing import Any, Callable, Dict, List, Optional, Union
+
 import numpy as np
+import numpy.typing as npt
 import pandas as pd
+from anndata import AnnData
 from scipy.sparse import issparse
 from sklearn.neighbors import NearestNeighbors
 
 
 def score_cells(
-    adata,
-    genes=None,
-    layer=None,
-    basis=None,
-    n_neighbors=30,
-    beta=0.1,
-    iteration=5,
-    metric="euclidean",
-    metric_kwds=None,
-    cores=1,
-    seed=19491001,
-    return_score=True,
+    adata: AnnData,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    basis: Optional[str] = None,
+    n_neighbors: int = 30,
+    beta: float = 0.1,
+    iteration: int = 5,
+    metric: Union[str, Callable] = "euclidean",
+    metric_kwds: Optional[Dict[str, Any]] = None,
+    cores: int = 1,
+    seed: int = 19491001,
+    return_score: bool = True,
     **kwargs,
-):
+) -> Optional[np.ndarray]:
     """Score cells based on a set of genes.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the reconstructed vector field function in the `uns` attribute.
-        genes: `list` or None (default: None)
-            The gene names whose gene expression will be used for predicting cell fate. By default (when genes is set to
-            None), the genes used for velocity embedding (var.use_for_transition) will be used for vector field
-            reconstruction. Note that the genes to be used need to have velocity calculated and corresponds to those used
-            in the `dyn.tl.VectorField` function.
-        layer: `str` or None (default: 'X')
-            Which layer of the data will be used for predicting cell fate with the reconstructed vector field function.
-            The layer once provided, will override the `basis` argument and then predicting cell fate in high dimensional
-            space.
-        basis: `str` or None (default: `None`)
-            The embedding data to use for predicting cell fate. If `basis` is either `umap` or `pca`, the reconstructed
-            trajectory will be projected back to high dimensional space via the `inverse_transform` function.
-        n_neighbors: `int` (default: `30`)
-            Number of nearest neighbors.
-        beta: `float` (default: `0.1`)
-            The weight that will apply to the current query cell.
-        iteration: `int` (default: `0.5`)
-            Number of smooth iterations.
-        metric: `str` or callable, default='euclidean'
-            The distance metric to use for the tree.  The default metric is , and with p=2 is equivalent to the standard
-            Euclidean metric. See the documentation of :class:`DistanceMetric` for a list of available metrics. If metric
-            is "precomputed", X is assumed to be a distance matrix and must be square during fit. X may be a
-            :term:`sparse graph`, in which case only "nonzero" elements may be considered neighbors.
-        metric_kwds : dict, default=None
-            Additional keyword arguments for the metric function.
-        cores: `int` (default: 1)
-            The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
-            ``-1`` means using all processors.
-        seed: `int` (default `19491001`)
-            Random seed to ensure the reproducibility of each run.
-        return_score: `bool` (default: `False`)
-            Whether to return the score. If False, save the smoothed score to `cell_scores` column in the `.obs`
-            attribute and also to the dictionary corresponding to the `score_cells` key in the .uns attribute.
-        kwargs:
-            Additional arguments that will be passed to each nearest neighbor search algorithm.
-
-    Returns
-    -------
-        Depending on return_score, it either return the cell scores or an updated adata object that contains the cell
-        score information.
+    Args:
+        adata: an AnnData object that contains the reconstructed vector field function in the `uns` attribute.
+        genes: the gene names whose gene expression will be used for predicting cell fate. By default (when genes is set
+            to None), the genes used for velocity embedding (var.use_for_transition) will be used for vector field
+            reconstruction. Note that the genes to be used need to have velocity calculated and corresponds to those
+            used in the `dyn.tl.VectorField` function. Defaults to None.
+        layer: which layer of the data will be used for predicting cell fate with the reconstructed vector field
+            function. The layer once provided, will override the `basis` argument and then predicting cell fate in high
+            dimensional space. Defaults to None.
+        basis: the embedding data to use for predicting cell fate. If `basis` is either `umap` or `pca`, the
+            reconstructed trajectory will be projected back to high dimensional space via the `inverse_transform`
+            function. Defaults to None.
+        n_neighbors: number of nearest neighbors. Defaults to 30.
+        beta: the weight that will apply to the current query cell. Defaults to 0.1.
+        iteration: number of smooth iterations. Defaults to 5.
+        metric: the distance metric to use for the tree.  The default metric is , and with p=2 is equivalent to the
+            standard Euclidean metric. See the documentation of :class:`DistanceMetric` for a list of available metrics.
+            If metric is "precomputed", X is assumed to be a distance matrix and must be square during fit. X may be a
+            `sparse graph`, in which case only "nonzero" elements may be considered neighbors. Defaults to "euclidean".
+        metric_kwds: additional keyword arguments for the metric function. Defaults to None.
+        cores: the number of parallel jobs to run for neighbors search. `None` means 1 unless in a
+            `joblib.parallel_backend` context. `-1` means using all processors. Defaults to 1.
+        seed: random seed to ensure the reproducibility of each run. Defaults to 19491001.
+        return_score: whether to return the score. If False, save the smoothed score to `cell_scores` column in the
+            `.obs` attribute and also to the dictionary corresponding to the `score_cells` key in the .uns attribute.
+            Defaults to True.
+
+    Raises:
+        ValueError: X_pca unavailable in .obsm.
+        ValueError: basis not available in .obsm.
+        ValueError: genes not provided and no "use_for_pca" in .obs.
+        ValueError: input genes have no overlap with genes in the AnnData object.
+
+    Returns:
+        The calculated cell scores if `return` score is true, otherwise the scores would be updated as annotations of
+        the AnnData object inplace.
     """
 
     if basis is None and "X_pca" not in adata.obsm.keys():
         raise ValueError(f"Your adata doesn't have 'X_pca' basis in .obsm.")
     elif basis is not None and "X_" + basis not in adata.obsm.keys():
         raise ValueError(f"Your adata doesn't have the {basis} you inputted in .obsm attribute of your adata.")
 
@@ -137,63 +133,52 @@
             "layer": layer,
             "basis": basis,
         }
         adata.obs["cell_score"] = smoothed_score
 
 
 def cell_growth_rate(
-    adata,
-    group,
-    source,
-    target,
-    L0=0.3,
-    L=1.2,
-    k=1e-3,
-    birth_genes=None,
-    death_genes=None,
-    clone_column=None,
+    adata: AnnData,
+    group: Optional[str],
+    source: Optional[str],
+    target: Optional[str],
+    L0: float = 0.3,
+    L: float = 1.2,
+    k: float = 1e-3,
+    birth_genes: Optional[List[str]] = None,
+    death_genes: Optional[List[str]] = None,
+    clone_column: Optional[str] = None,
     **kwargs,
-):
+) -> AnnData:
     """Estimate the growth rate via clone information or logistic equation of population dynamics.
 
-    Growth rate is calculated as 1) number_of_cell_at_source_time_in_the_clone / number_of_cell_at_end_time_in_the_clone
-    when there is clone information (`[clone_column, time_column, source_time, target_time]` are all not None); 2)
-    estimate via logistic equation of population growth and death.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the reconstructed vector field function in the `uns` attribute.
-        group: str or None (default: `None`)
-            The column key in .obs points to the collection time of each cell, required for calculating growth rate with
-            clone information.
-        source: str or None (default: `None`)
-            The column key in .obs points to the starting point from collection time of each cell, required for
+    Args:
+        adata: an AnnData object.
+        group: The column key in .obs points to the collection time of each cell, required for calculating growth rate
+            with clone information.
+        source: The column key in .obs points to the starting point from collection time of each cell, required for
             calculating growth rate with clone information.
-        target: str or None (default: `None`)
-            The column key in .obs points to the end point from collection time of each cell, required for
+        target: The column key in .obs points to the end point from collection time of each cell, required for
             calculating growth rate with clone information.
-        L0: float (default: `0.3`)
-            The base growth/death rate.
-        L: float (default: `1.2`)
-            The maximum growth/death rate.
-        k: float (default: `0.001)
-            The steepness of the curve.
-        birth_genes: list or None (default: `None`)
-            The gene list associated with the cell cycle process. If None, GSEA's KEGG_CELL_CYCLE will be used.
-        death_genes: list or None (default: `None`)
-            The gene list associated with the cell cycle process. If None, GSEA's KEGG_APOPTOSIS will be used.
-        clone_column: str or None (default: `None`)
-            The column key in .obs points to the clone id if there is any. If a cell doesn't belong to any clone, the
-            clone id of that cell should be assigned as `np.nan`
-        kwargs
-            Additional arguments that will be passed to score_cells function.
+        L0: The base growth/death rate. Defaults to 0.3.
+        L: The maximum growth/death rate. Defaults to 1.2.
+        k: The steepness of the curve. Defaults to 1e-3.
+        birth_genes: The gene list associated with the cell cycle process. If None, GSEA's KEGG_CELL_CYCLE will be used.
+            Defaults to None.
+        death_genes: The gene list associated with the cell cycle process. If None, GSEA's KEGG_APOPTOSIS will be used.
+            Defaults to None.
+        clone_column: The column key in .obs points to the clone id if there is any. If a cell doesn't belong to any
+            clone, the clone id of that cell should be assigned as `np.nan`. Defaults to None.
+        kwargs: Additional arguments that will be passed to score_cells function.
+
+    Raises:
+        ValueError: `clone_name` or `group` not in .obs.
+        ValueError: `source` or `target` not in .obs.
 
-    Returns
-    -------
+    Returns:
         An updated adata object that includes `growth_rate` column or `growth_rate, birth_score, death_score` in its
         `.obs` attribute when the clone based or purely expression based growth rate was calculated.
     """
 
     # calculate growth rate when there is clone information.
     all_clone_info = [clone_column, group, source, target]
 
@@ -262,13 +247,32 @@
 
     adata.obs["growth_rate"] = np.nan
     adata.obs.loc[source_mask_, "growth_rate"] = growth_rates
 
     return adata
 
 
-def n_descentants(birth, death, dt):
+def n_descentants(birth: npt.ArrayLike, death: npt.ArrayLike, dt: npt.ArrayLike) -> npt.ArrayLike:
+    """Calculate the number of descentants for a given cell after some given time.
+
+    Args:
+        birth: logged birth number per unit time.
+        death: logged death number per unit time.
+        dt: total time.
+
+    Returns:
+        The number of descentants for a given cell after some given time.
+    """
     return np.exp(dt * (birth - death))
 
 
-def growth_rate(n, dt):
+def growth_rate(n: npt.ArrayLike, dt: npt.ArrayLike) -> npt.ArrayLike:
+    """Calculate the logged growth rate.
+
+    Args:
+        n: increased cell number.
+        dt: time interval
+
+    Returns:
+        The growth rate.
+    """
     return np.log(n) / dt
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/markers.py` & `dynamo-release-1.3.0/dynamo/tools/markers.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,81 +1,84 @@
+from typing import List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
 import warnings
 from collections import Counter
 
 import numpy as np
 import pandas as pd
 import patsy
 import statsmodels.api as sm
 import statsmodels.formula.api as smf
+from anndata import AnnData
 from patsy import bs, cr, dmatrix
 from scipy import stats
 from scipy.sparse import issparse
 from scipy.stats import mannwhitneyu
+from statsmodels.genmod.generalized_linear_model import GLMResultsWrapper
 from statsmodels.sandbox.stats.multicomp import multipletests
 from tqdm import tqdm
 
 from ..dynamo_logger import (
     main_critical,
     main_exception,
     main_info,
     main_tqdm,
     main_warning,
 )
-from ..preprocessing.utils import Freeman_Tukey
+from ..preprocessing.transform import _Freeman_Tukey
 from ..tools.connectivity import _gen_neighbor_keys, check_and_recompute_neighbors
 from .utils import fetch_X_data
 from .utils_markers import fdr, specificity
 
 
 def moran_i(
-    adata,
-    X_data=None,
-    genes=None,
-    layer=None,
-    weighted=True,
-    assumption="permutation",
-    local_moran=False,
-):
-    """Identify genes with strong spatial autocorrelation with Moran's I test. This can be used to identify genes that are
-    potentially related to critical dynamic process. Moran's I test is first introduced in single cell genomics analysis
-    in (Cao, et al, 2019). Note that moran_i supports performing spatial autocorrelation analysis for any layer or
-    normalized data in your adata object. That is you can either use the total, new, unspliced or velocity, etc. for the
-    Moran's I analysis.
-
-    Global Moran's I test is based on pysal. More details can be found at:
-    http://geodacenter.github.io/workbook/5a_global_auto/lab5a.html#morans-i
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        X_data: `np.ndarray` (default: `None`)
-            The user supplied data that will be used for Moran's I calculation directly.
-        genes: `list` or None (default: `None`)
-            The list of genes that will be used to subset the data for dimension reduction and clustering. If `None`, all
-            genes will be used.
-        layer: `str` or None (default: `None`)
-            The layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is used.
-        weighted: `bool` (default: `True`)
-            Whether to consider edge weights in the spatial weights graph.
-        assumption: `str` (default: `permutation`)
-            Assumption of the Moran's I test, can be one of {'permutation', 'normality', 'randomization'}.
-            Inference for Moran’s I is based on a null hypothesis of spatial randomness. The distribution of the
-            statistic under the null can be derived using either an assumption of normality (independent normal random
-            variates), or so-called randomization (i.e., each value is equally likely to occur at any location). An
-            alternative to an analytical derivation is a computational approach based on permutation. This calculates a
-            reference distribution for the statistic under the null hypothesis of spatial randomness by randomly permuting
-            the observed values over the locations. The statistic is computed for each of these randomly reshuffled data sets,
-            which yields a reference distribution.
-        local_moran: `bool` (default: `False`)
-            Whether to also calculate local Moran's I.
-
-    Returns
-    -------
-        Returns an updated `~anndata.AnnData` with a new key `'Moran_' + type` in the .uns attribute, storing the Moran' I
+    adata: AnnData,
+    X_data: Optional[np.ndarray] = None,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    weighted: bool = True,
+    assumption: Literal["permutation", "normality", "randomization"] = "permutation",
+    local_moran: bool = False,
+) -> AnnData:
+    """Identify genes with strong spatial autocorrelation with Moran's I test.
+
+    This can be used to identify genes that are potentially related to critical dynamic process. Moran's I test is first
+    introduced in single cell genomics analysis in (Cao, et al. 2019). Note that moran_i supports performing spatial
+    autocorrelation analysis for any layer or normalized data in your adata object. That is you can either use the
+    total, new, unspliced or velocity, etc. for the Moran's I analysis.
+
+    Args:
+        adata: an AnnData object.
+        X_data: the user supplied data that will be used for Moran's I calculation directly. Defaults to None.
+        genes: the list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
+            all genes will be used. Defaults to None.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        weighted: whether to consider edge weights in the spatial weights graph. Defaults to True.
+        assumption: assumption of the Moran's I test. Inference for Moran's I is based on a null hypothesis of spatial
+            randomness. The distribution of the statistic under the null can be derived using either an assumption of
+            normality (independent normal random variates), or so-called randomization (i.e., each value is equally
+            likely to occur at any location). An alternative to an analytical derivation is a computational approach
+            based on permutation. This calculates a reference distribution for the statistic under the null hypothesis
+            of spatial randomness by randomly permuting the observed values over the locations. The statistic is
+            computed for each of these randomly reshuffled data sets, which yields a reference distribution. Defaults to
+            "permutation".
+        local_moran: whether to also calculate local Moran's I. Defaults to False.
+
+    Raises:
+        ImportError: pysal not installed.
+        ValueError: `X_data` is provided but `genes` does not correspond to its columns.
+
+    Returns:
+        The updated AnnData object with a new key `'Moran_' + type` in the .uns attribute, storing the Moran' I
         test results.
     """
 
     try:
         import pysal
     except ImportError:
         raise ImportError(
@@ -156,76 +159,77 @@
     if local_moran:
         adata.uns["local_moran"] = l_moran
 
     return adata
 
 
 def find_group_markers(
-    adata,
-    group,
-    genes=None,
-    layer=None,
-    exp_frac_thresh=None,
-    log2_fc_thresh=None,
-    qval_thresh=0.05,
-    de_frequency=1,
-    subset_control_vals=None,
-):
+    adata: AnnData,
+    group: str,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    exp_frac_thresh: Optional[float] = None,
+    log2_fc_thresh: Optional[float] = None,
+    qval_thresh: float = 0.05,
+    de_frequency: int = 1,
+    subset_control_vals: Optional[bool] = None,
+) -> AnnData:
     """Find marker genes for each group of cells based on gene expression or velocity values as specified by the layer.
 
-    Tests each gene for differential expression between cells in one group to cells from all other groups via Mann-Whitney U
-    test. It also calculates the fraction of cells with non-zero expression, log 2 fold changes as well as the specificity
-    (calculated as 1 - Jessen-Shannon distance between the distribution of percentage of cells with expression across all
-    groups to the hypothetical perfect distribution in which only the test group of cells has expression). In addition,
-    Rank-biserial correlation (rbc) and qval are calculated. The rank biserial correlation is used to assess the relationship
-    between a dichotomous categorical variable and an ordinal variable. The rank biserial test is very similar to the
-    non-parametric Mann-Whitney U test that is used to compare two independent groups on an ordinal variable. Mann-Whitney
-    U tests are preferable to rank biserial correlations when comparing independent groups. Rank biserial correlations can
-    only be used with dichotomous (two levels) categorical variables. qval is calculated using Benjamini-Hochberg adjustment.
+    Tests each gene for differential expression between cells in one group to cells from all other groups via
+    Mann-Whitney U test. It also calculates the fraction of cells with non-zero expression, log 2 fold changes as well
+    as the specificity (calculated as 1 - Jessen-Shannon distance between the distribution of percentage of cells with
+    expression across all groups to the hypothetical perfect distribution in which only the test group of cells has
+    expression). In addition, Rank-biserial correlation (rbc) and qval are calculated. The rank biserial correlation is
+    used to assess the relationship between a dichotomous categorical variable and an ordinal variable. The rank
+    biserial test is very similar to the non-parametric Mann-Whitney U test that is used to compare two independent
+    groups on an ordinal variable. Mann-Whitney U tests are preferable to rank biserial correlations when comparing
+    independent groups. Rank biserial correlations can only be used with dichotomous (two levels) categorical variables.
+    qval is calculated using Benjamini-Hochberg adjustment.
 
-    Note that this function is designed in a general way so that you can either use the total, new, unspliced or velocity,
-    etc. to identify differentially expressed genes.
+    Note that this function is designed in a general way so that you can either use the total, new, unspliced or
+    velocity, etc. to identify differentially expressed genes.
 
     This function is adapted from https://github.com/KarlssonG/nabo/blob/master/nabo/_marker.py and Monocle 3alpha.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        group: `str` or None (default: `None`)
-            The column key/name that identifies the grouping information (for example, clusters that correspond to
-            different cell types or different time points) of cells. This will be used for calculating group-specific genes.
-        genes: `list` or None (default: `None`)
-            The list of genes that will be used to subset the data for dimension reduction and clustering. If `None`, all
-            genes will be used.
-        layer: `str` or None (default: `None`)
-            The layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is used.
-        exp_frac_thresh: `float` (default: None)
-            The minimum percentage of cells with expression for a gene to proceed differential expression test. If `layer`
-            is not `velocity` related (i.e. `velocity_S`), `exp_frac_thresh` by default is set to be 0.1, otherwise 0.
-        log2_fc_thresh: `float` (default: None)
-            The minimal threshold of log2 fold change for a gene to proceed differential expression test. If `layer` is
-            not `velocity` related (i.e. `velocity_S`), `log2_fc_thresh` by default is set to be 1, otherwise 0.
-        qval_thresh: `float` (default: 0.05)
-            The minimial threshold of qval to be considered as significant genes.
-        de_frequency:
-            Minimum number of clusters against a gene should be significantly differentially expressed for it to qualify
-            as a marker.
-        subset_control_vals: `None` or `bool` (default: `None`)
-            Whether to subset the top ranked control values. When `subset_control_vals = None`, this is subset to be
-            `True` when layer is not related to either `velocity` related or `acceleration` or `curvature` related
-            layers and `False` otherwise. When layer is not related to either `velocity` related or `acceleration` or
-            `curvature` related layers used, the control values will be sorted by absolute values.
-
-    Returns
-    -------
-        Returns an updated `~anndata.AnnData` with a new property `cluster_markers` in the .uns attribute, which includes
-        a concated pandas DataFrame of the differential expression analysis result for all groups and a dictionary where keys are
-        cluster numbers and values are lists of marker genes for the corresponding clusters.
+    Args:
+        adata: an AnnData object.
+        group: the column key/name that identifies the grouping information (for example, clusters that correspond to
+            different cell types or different time points) of cells. This will be used for calculating group-specific
+            genes.
+        genes: the list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
+            all genes will be used. Defaults to None.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        exp_frac_thresh: the minimum percentage of cells with expression for a gene to proceed differential expression
+            test. If `layer` is not `velocity` related (i.e. `velocity_S`), `exp_frac_thresh` by default is set to be
+            0.1, otherwise 0. Defaults to None.
+        log2_fc_thresh: the minimal threshold of log2 fold change for a gene to proceed differential expression test. If
+            `layer` is not `velocity` related (i.e. `velocity_S`), `log2_fc_thresh` by default is set to be 1, otherwise
+            0. Defaults to None.
+        qval_thresh: the minimal threshold of qval to be considered as significant genes. Defaults to 0.05.
+        de_frequency: minimum number of clusters against a gene should be significantly differentially expressed for it
+            to qualify as a marker. Defaults to 1.
+        subset_control_vals: whether to subset the top ranked control values. When `subset_control_vals = None`, this is
+            subset to be `True` when layer is not related to either `velocity` related or `acceleration` or `curvature`
+            related layers and `False` otherwise. When layer is not related to either `velocity` related or
+            `acceleration` or `curvature` related layers used, the control values will be sorted by absolute values.
+            Defaults to None.
+
+    Raises:
+        ValueError: gene list does not overlap with genes in adata.
+        ValueError: `group` is invalid.
+        ValueError: .obs[group] does not contain enough number of groups.
+
+    Returns:
+        An updated `~anndata.AnnData` with a new property `cluster_markers` in the .uns attribute, which includes a
+        concated pandas DataFrame of the differential expression analysis result for all groups and a dictionary where
+        keys are cluster numbers and values are lists of marker genes for the corresponding clusters.
     """
+
     if layer is None or not (layer.startswith("velocity") or layer in ["acceleration", "curvature"]):
         exp_frac_thresh = 0.1 if exp_frac_thresh is None else exp_frac_thresh
         log2_fc_thresh = 1 if log2_fc_thresh is None else log2_fc_thresh
         subset_control_vals = True if subset_control_vals is None else subset_control_vals
     else:
         exp_frac_thresh = 0 if exp_frac_thresh is None else exp_frac_thresh
         log2_fc_thresh = None
@@ -290,74 +294,68 @@
 
     adata.uns["cluster_markers"] = {"deg_table": de_table, "de_genes": de_genes}
 
     return adata
 
 
 def two_groups_degs(
-    adata,
-    genes,
-    layer,
-    group,
-    test_group,
-    control_groups,
-    X_data=None,
-    exp_frac_thresh=None,
-    log2_fc_thresh=None,
-    qval_thresh=0.05,
-    subset_control_vals=None,
-):
-    """Find marker genes between two groups of cells based on gene expression or velocity values as specified by the layer.
-
-    Tests each gene for differential expression between cells in one group to cells from another groups via Mann-Whitney U
-    test. It also calculates the fraction of cells with non-zero expression, log 2 fold changes as well as the specificity
-    (calculated as 1 - Jessen-Shannon distance between the distribution of percentage of cells with expression across all
-    groups to the hypothetical perfect distribution in which only the current group of cells have expression). In addition,
-    Rank-biserial correlation (rbc) and qval are calculated. The rank biserial correlation is used to assess the relationship
-    between a dichotomous categorical variable and an ordinal variable. The rank biserial test is very similar to the
-    non-parametric Mann-Whitney U test that is used to compare two independent groups on an ordinal variable. Mann-Whitney
-    U tests are preferable to rank biserial correlations when comparing independent groups. Rank biserial correlations can
-    only be used with dichotomous (two levels) categorical variables. qval is calculated using Benjamini-Hochberg adjustment.
-
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        genes: `list` or None (default: `None`)
-            The list of genes that will be used to subset the data for dimension reduction and clustering. If `None`, all
-            genes will be used.
-        layer: `str` or None (default: `None`)
-            The layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is used.
-        group: `str` or None (default: `None`)
-            The column key/name that identifies the grouping information (for example, clusters that correspond to
-            different cell types or different time points) of cells. This will be used for calculating group-specific genes.
-        test_group: `str` or None (default: `None`)
-            The group name from `group` for which markers has to be found.
-        control_groups: `list`
-            The list of group name(s) from `group` for which markers has to be tested against.
-        X_data: `np.ndarray` (default: `None`)
-            The user supplied data that will be used for marker gene detection directly.
-        exp_frac_thresh: `float` (default: None)
-            The minimum percentage of cells with expression for a gene to proceed differential expression test. If `layer`
-            is not `velocity` related (i.e. `velocity_S`), `exp_frac_thresh` by default is set to be 0.1, otherwise 0.
-        log2_fc_thresh: `float` (default: None)
-            The minimal threshold of log2 fold change for a gene to proceed differential expression test. If `layer` is
-            not `velocity` related (i.e. `velocity_S`), `log2_fc_thresh` by default is set to be 1, otherwise 0.
-        qval_thresh: `float` (default: 0.05)
-            The maximal threshold of qval to be considered as significant genes.
-        subset_control_vals: `None` or `bool` (default: `None`)
-            Whether to subset the top ranked control values. When `subset_control_vals = None`, this is subset to be
-            `True` when layer is not related to either `velocity` related or `acceleration` or `curvature` related
-            layers and `False` otherwise. When layer is not related to either `velocity` related or `acceleration` or
-            `curvature` related layers used, the control values will be sorted by absolute values.
+    adata: AnnData,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    group: Optional[str] = None,
+    test_group: Optional[str] = None,
+    control_groups: List[str] = None,
+    X_data: Optional[np.ndarray] = None,
+    exp_frac_thresh: Optional[float] = None,
+    log2_fc_thresh: Optional[str] = None,
+    qval_thresh: float = 0.05,
+    subset_control_vals: Optional[bool] = None,
+) -> pd.DataFrame:
+    """Find marker genes between two groups of cells based on gene expression or velocity as specified by the layer.
+
+    Tests each gene for differential expression between cells in one group to cells from another groups via Mann-Whitney
+    U test. It also calculates the fraction of cells with non-zero expression, log 2 fold changes as well as the
+    specificity (calculated as 1 - Jessen-Shannon distance between the distribution of percentage of cells with
+    expression across all groups to the hypothetical perfect distribution in which only the current group of cells have
+    expression). In addition, Rank-biserial correlation (rbc) and qval are calculated. The rank biserial correlation is
+    used to assess the relationship between a dichotomous categorical variable and an ordinal variable. The rank
+    biserial test is very similar to the non-parametric Mann-Whitney U test that is used to compare two independent
+    groups on an ordinal variable. Mann-Whitney U tests are preferable to rank biserial correlations when comparing
+    independent groups. Rank biserial correlations can only be used with dichotomous (two levels) categorical variables.
+    qval is calculated using Benjamini-Hochberg adjustment.
+
+    Args:
+        adata: an AnnData object.
+        genes: the list of genes that will be used to subset the data for dimension reduction and clustering. If `None`,
+            all genes will be used.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used.
+        group: the column key/name that identifies the grouping information (for example, clusters that correspond to
+            different cell types or different time points) of cells. This will be used for calculating group-specific
+            genes.
+        test_group: the group name from `group` for which markers has to be found.
+        control_groups: the list of group name(s) from `group` for which markers has to be tested against.
+        X_data: the user supplied data that will be used for marker gene detection directly. Defaults to None.
+        exp_frac_thresh: the minimum percentage of cells with expression for a gene to proceed differential expression
+            test. If `layer` is not `velocity` related (i.e. `velocity_S`), `exp_frac_thresh` by default is set to be
+            0.1, otherwise 0. Defaults to None.
+        log2_fc_thresh: the minimal threshold of log2 fold change for a gene to proceed differential expression test. If
+            `layer` is not `velocity` related (i.e. `velocity_S`), `log2_fc_thresh` by default is set to be 1, otherwise
+            0. Defaults to None.
+        qval_thresh: the maximal threshold of qval to be considered as significant genes. Defaults to 0.05.
+        subset_control_vals: whether to subset the top ranked control values. When `subset_control_vals = None`, this is
+            subset to be `True` when layer is not related to either `velocity` related or `acceleration` or `curvature`
+            related layers and `False` otherwise. When layer is not related to either `velocity` related or
+            `acceleration` or `curvature` related layers used, the control values will be sorted by absolute values.
+            Defaults to None.
 
+    Raises:
+        ValueError: `X_data` is provided but `genes` does not correspond to its columns.
 
-    Returns
-    -------
+    Returns:
         A pandas DataFrame of the differential expression analysis result between the two groups.
     """
 
     if layer is None or not (layer.startswith("velocity") or layer in ["acceleration", "curvature"]):
         exp_frac_thresh = 0.1 if exp_frac_thresh is None else exp_frac_thresh
         log2_fc_thresh = 1 if log2_fc_thresh is None else log2_fc_thresh
         subset_control_vals = True if subset_control_vals is None else subset_control_vals
@@ -497,60 +495,52 @@
     de = de[out_order].sort_values(by="qval")
     res = de[(de.qval < qval_thresh)].reset_index().drop(columns=["index"])
 
     return res
 
 
 def top_n_markers(
-    adata,
-    with_moran_i=False,
-    group_by="test_group",
-    sort_by="specificity",
-    sort_order="decreasing",
-    top_n_genes=5,
-    exp_frac_thresh=0.1,
-    log2_fc_thresh=None,
-    qval_thresh=0.05,
-    specificity_thresh=0.3,
-    only_gene_list=False,
-    display=True,
-):
+    adata: AnnData,
+    with_moran_i: bool = False,
+    group_by: str = "test_group",
+    sort_by: Union[str, List[str]] = "specificity",
+    sort_order: Literal["increasing", "decreasing"] = "decreasing",
+    top_n_genes: int = 5,
+    exp_frac_thresh: float = 0.1,
+    log2_fc_thresh: Optional[float] = None,
+    qval_thresh: float = 0.05,
+    specificity_thresh: float = 0.3,
+    only_gene_list: bool = False,
+    display: bool = True,
+) -> Union[List[List[str]], pd.DataFrame]:
     """Filter cluster deg (Moran's I test) results and retrieve top markers for each cluster.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        with_moran_i: `bool` (default: `False`)
-            Whether or not to include Moran's I test results for selecting top marker genes.
-        group_by: `str` or `list` (default: `test_group`)
-            Column name or names to group by.
-        sort_by: `str` or `list`
-            Column name or names to sort by.
-        sort_order: `str` (default: `decreasing`)
-            Whether to sort the data frame with `increasing` or `decreasing` order.
-        top_n_genes: `int`
-            The number of top sorted markers.
-        exp_frac_thresh: `float` (default: 0.1)
-            The minimum percentage of cells with expression for a gene to proceed selection of top markers.
-        log2_fc_thresh: `None` or `float` (default: None)
-            The minimal threshold of log2 fold change for a gene to proceed selection of top markers. Applicable to none
-            `velocity`, `acceleration` or `curvature` layers based DEGs.
-        qval_thresh: `float` (default: 0.05)
-            The maximal threshold of qval to be considered as top markers.
-        only_gene_list: `bool`
-            Whether to only return the gene list for each cluster.
-        display: `bool`
-            Whether to print the data frame for the top marker genes after the filtering.
-
-    Returns
-    -------
-        A data frame that stores the top marker for each group or just a list for those markers, depending on
-        whether `only_gene_list` is set to be True. In addition, it will display the data frame depending on whether
-        `display` is set to be True.
+    Args:
+        adata: an AnnData object.
+        with_moran_i: whether to include Moran's I test results for selecting top marker genes. Defaults to False.
+        group_by: column name or names to group by. Defaults to "test_group".
+        sort_by: column name or names to sort by. Defaults to "specificity".
+        sort_order: whether to sort the data frame with `increasing` or `decreasing` order. Defaults to "decreasing".
+        top_n_genes: the number of top sorted markers. Defaults to 5.
+        exp_frac_thresh: the minimum percentage of cells with expression for a gene to proceed selection of top markers.
+            Defaults to 0.1.
+        log2_fc_thresh: the minimal threshold of log2 fold change for a gene to proceed selection of top markers.
+            Applicable to none `velocity`, `acceleration` or `curvature` layers based DEGs. Defaults to None.
+        qval_thresh: the maximal threshold of qval to be considered as top markers. Defaults to 0.05.
+        specificity_thresh: the minimum threshold of specificity to be considered as top markers. Defaults to 0.3.
+        only_gene_list: whether to only return the gene list for each cluster. Defaults to False.
+        display: whether to print the data frame for the top marker genes after the filtering. Defaults to True.
+
+    Raises:
+        ValueError: threshold too extreme that no genes passed the filter.
+
+    Returns:
+        If `only_gene_list` is false, a data frame that stores the top marker for each group would be returned.
+        Otherwise, a list containing lists of genes in each cluster would be returned. In addition, it will display the
+        data frame depending on whether `display` is set to be True.
     """
 
     if "cluster_markers" not in adata.uns.keys():
         main_warning(
             f"No info of cluster markers stored in your adata. "
             f"Running `find_group_markers` with default parameters."
         )
@@ -620,91 +610,88 @@
             de_genes[i] = top_n_df[top_n_df[group_by] == i].loc[:, "gene"].to_list()
         return de_genes
     else:
         return top_n_df
 
 
 def glm_degs(
-    adata,
-    X_data=None,
-    genes=None,
-    layer=None,
-    fullModelFormulaStr="~cr(integral_time, df=3)",
-    reducedModelFormulaStr="~1",
-    family="NB2",
-):
+    adata: AnnData,
+    X_data: Optional[np.ndarray] = None,
+    genes: Optional[List[str]] = None,
+    layer: Optional[str] = None,
+    fullModelFormulaStr: str = "~cr(integral_time, df=3)",
+    reducedModelFormulaStr: str = "~1",
+    family: Literal["NB2"] = "NB2",
+) -> None:
     """Differential genes expression tests using generalized linear regressions.
 
+    The results would be stored in the adata's .uns["glm_degs"] annotation and the update is inplace.
+
     Tests each gene for differential expression as a function of integral time (the time estimated via the reconstructed
     vector field function) or pseudotime using generalized additive models with natural spline basis. This function can
-    also use other covariates as specified in the full (i.e `~clusters`) and reduced model formula to identify differentially
-    expression genes across different categories, group, etc.
+    also use other covariates as specified in the full (i.e `~clusters`) and reduced model formula to identify
+    differentially expression genes across different categories, group, etc.
 
     glm_degs relies on statsmodels package and is adapted from the `differentialGeneTest` function in Monocle. Note that
-    glm_degs supports performing deg analysis for any layer or normalized data in your adata object. That is you can either
-    use the total, new, unspliced or velocity, etc. for the differential expression analysis.
+    glm_degs supports performing deg analysis for any layer or normalized data in your adata object. That is you can
+    either use the total, new, unspliced or velocity, etc. for the differential expression analysis.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        X_data: `np.ndarray` (default: `None`)
-            The user supplied data that will be used for differential expression analysis directly.
-        genes: `list` or None (default: `None`)
-            The list of genes that will be used to subset the data for differential expression analysis. If `None`, all
-            genes will be used.
-        layer: `str` or None (default: `None`)
-            The layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is used.
-        fullModelFormulaStr: `str` (default: `~cr(time, df=3)`)
-            A formula string specifying the full model in differential expression tests (i.e. likelihood ratio tests) for
-            each gene/feature.
-        reducedModelFormulaStr: `str` (default: `~1`)
-            A formula string specifying the reduced model in differential expression tests (i.e. likelihood ratio tests)
-            for each gene/feature.
-        family: `str` (default: `NB2`)
-            The distribution family used for the expression responses in statsmodels. Currently always uses `NB2` and this
-            is ignored. NB model requires us to define a parameter $\alpha$ which it uses to express the variance in terms
-            of the mean as follows: variance = mean + $\alpha$ mean^p. When $p=2$, it corresponds to the NB2 model. In order
-            to obtain the correct parameter $\alpha$ (sm.genmod.families.family.NegativeBinomial(link=None, alpha=1.0), by
-            default it is 1), we use the auxiliary OLS regression without a constant from Messrs Cameron and Trivedi. More
-            details can be found here: https://towardsdatascience.com/negative-binomial-regression-f99031bb25b4.
-
-    Returns
-    -------
-        Returns an updated `~anndata.AnnData` with a new key `glm_degs` in the .uns attribute, storing the differential
-        expression test results after the GLM test.
+    Args:
+        adata: an AnnData object.
+        X_data: the user supplied data that will be used for differential expression analysis directly. Defaults to None.
+        genes: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        layer: the layer that will be used to retrieve data for dimension reduction and clustering. If `None`, .X is
+            used. Defaults to None.
+        fullModelFormulaStr: a formula string specifying the full model in differential expression tests (i.e.
+            likelihood ratio tests) for each gene/feature. Defaults to "~cr(integral_time, df=3)".
+        reducedModelFormulaStr: a formula string specifying the reduced model in differential expression tests (i.e.
+            likelihood ratio tests) for each gene/feature. Defaults to "~1".
+        family: the distribution family used for the expression responses in statsmodels. Currently always uses `NB2`
+            and this is ignored. NB model requires us to define a parameter alpha which it uses to express the
+            variance in terms of the mean as follows: variance = mean + alpha mean^p. When p=2, it corresponds to
+            the NB2 model. In order to obtain the correct parameter alpha (sm.genmod.families.family.NegativeBinomial
+            (link=None, alpha=1.0), by default it is 1), we use the auxiliary OLS regression without a constant from
+            Messrs Cameron and Trivedi. More details can be found here:
+            https://towardsdatascience.com/negative-binomial-regression-f99031bb25b4. Defaults to "NB2".
+
+    Raises:
+        ValueError: `X_data` is provided but `genes` does not correspond to its columns.
+        Exception: factors from the model formula `fullModelFormulaStr` invalid.
     """
 
     if X_data is None:
         genes, X_data = fetch_X_data(adata, genes, layer)
     else:
         if genes is None or len(genes) != X_data.shape[1]:
             raise ValueError(
                 f"When providing X_data, a list of genes name that corresponds to the columns of X_data "
                 f"must be provided"
             )
+
+    norm_method_key = "X_norm_method" if layer is None or layer == "X" else "layers_norm_method"
     if layer is None:
         if issparse(X_data):
             X_data.data = (
                 2**X_data.data - 1
-                if adata.uns["pp"]["norm_method"] == "log2"
+                if adata.uns["pp"][norm_method_key] == "log2"
                 else np.exp(X_data.data) - 1
-                if adata.uns["pp"]["norm_method"] == "log"
-                else Freeman_Tukey(X_data.data + 1, inverse=True)
-                if adata.uns["pp"]["norm_method"] == "Freeman_Tukey"
+                if adata.uns["pp"][norm_method_key] == "log"
+                else _Freeman_Tukey(X_data.data + 1, inverse=True)
+                if adata.uns["pp"][norm_method_key] == "Freeman_Tukey"
                 else X_data.data
             )
         else:
             X_data = (
                 2**X_data - 1
-                if adata.uns["pp"]["norm_method"] == "log2"
+                if adata.uns["pp"][norm_method_key] == "log2"
                 else np.exp(X_data) - 1
-                if adata.uns["pp"]["norm_method"] == "log"
-                else Freeman_Tukey(X_data, inverse=True)
-                if adata.uns["pp"]["norm_method"] == "Freeman_Tukey"
+                if adata.uns["pp"][norm_method_key] == "log"
+                else _Freeman_Tukey(X_data, inverse=True)
+                if adata.uns["pp"][norm_method_key] == "Freeman_Tukey"
                 else X_data
             )
 
     factors = get_all_variables(fullModelFormulaStr)
     factors = ["Pseudotime" if i == "cr(Pseudotime, df=3)" else i for i in factors]
     if len(set(factors).difference(adata.obs.columns)) == 0:
         df_factors = adata.obs[factors]
@@ -725,18 +712,33 @@
 
     deg_df["qval"] = multipletests(deg_df["pval"], method="fdr_bh")[1]
 
     adata.uns["glm_degs"] = deg_df
 
 
 def diff_test_helper(
-    data,
-    fullModelFormulaStr="~cr(time, df=3)",
-    reducedModelFormulaStr="~1",
-):
+    data: pd.DataFrame,
+    fullModelFormulaStr: str = "~cr(time, df=3)",
+    reducedModelFormulaStr: str = "~1",
+) -> Union[Tuple[Literal["fail"], Literal["NB2"], Literal[1]], Tuple[Literal["ok"], Literal["NB2"], np.ndarray],]:
+    """A helper function to generate required data fields for differential gene expression test.
+
+    Args:
+        data: the original dataframe containing expression data.
+        fullModelFormulaStr: a formula string specifying the full model in differential expression tests (i.e.
+            likelihood ratio tests) for each gene/feature. Defaults to "~cr(integral_time, df=3)".
+        reducedModelFormulaStr: a formula string specifying the reduced model in differential expression tests (i.e.
+            likelihood ratio tests) for each gene/feature. Defaults to "~1".
+    Returns:
+        A tuple [parseResult, family, pval], where `parseResult` should be "ok" or "fail", showing whether the provided
+        dataframe is successfully parsed or not. `family` is the distribution family used for the expression responses
+        in statsmodels, currently only "NB2" is supported. `pval` is the survival probability (1-cumulative probability)
+        to observe the likelihood ratio for the constrained model to be true. If parsing dataframe failed, this value is
+        set to be 1.
+    """
     # Dividing data into train and validation datasets
     transformed_x = dmatrix(fullModelFormulaStr, data, return_type="dataframe")
     transformed_x_null = dmatrix(reducedModelFormulaStr, data, return_type="dataframe")
 
     expression = data["expression"]
 
     try:
@@ -756,27 +758,46 @@
     except:
         return ("fail", "NB2", 1)
 
     pval = lrt(nb2_full, nb2_null)
     return ("ok", "NB2", pval)
 
 
-def get_all_variables(formula):
+def get_all_variables(formula: str) -> List[str]:
+    """A helper function to get all variable names for a formula string.
+
+    Args:
+        formula: the formula string specifying the model in differential expression data.
+
+    Returns:
+        A list of variable names.
+    """
     md = patsy.ModelDesc.from_formula(formula)
     termlist = md.rhs_termlist + md.lhs_termlist
 
     factors = []
     for term in termlist:
         for factor in term.factors:
             factors.append(factor.name())
 
     return factors
 
 
-def lrt(full, restr):
+def lrt(full: GLMResultsWrapper, restr: GLMResultsWrapper) -> np.float64:
+    """Perform likelihood-ratio test on the full model and constrained model.
+
+    Args:
+        full: the regression model without constraint.
+        restr: the regression model after constraint.
+
+    Returns:
+        The survival probability (1-cumulative probability) to observe the likelihood ratio for the constrained
+        model to be true.
+    """
+
     llf_full = full.llf
     llf_restr = restr.llf
     df_full = full.df_resid
     df_restr = restr.df_resid
     lrdf = df_restr - df_full
     lrstat = -2 * (llf_restr - llf_full)
     lr_pvalue = stats.chi2.sf(lrstat, df=lrdf)
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/metric_velocity.py` & `dynamo-release-1.3.0/dynamo/tools/metric_velocity.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,81 +1,84 @@
+from typing import Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
 import numpy as np
 import pandas as pd
+from anndata import AnnData
 from scipy.sparse import csr_matrix, issparse
 from sklearn.neighbors import NearestNeighbors
 from tqdm import tqdm
 
+from ..configuration import DKM
 from ..dynamo_logger import LoggerManager, main_critical, main_info, main_warning
 from .connectivity import (
     adj_to_knn,
     check_and_recompute_neighbors,
     mnn_from_list,
     umap_conn_indices_dist_embedding,
 )
 from .utils import einsum_correlation, fetch_X_data, get_finite_inds, inverse_norm
 
 
 def cell_wise_confidence(
-    adata,
-    X_data=None,
-    V_data=None,
-    ekey="M_s",
-    vkey="velocity_S",
-    neighbors_from_basis=False,
-    method="jaccard",
-):
+    adata: AnnData,
+    X_data: Union[np.ndarray, csr_matrix, None] = None,
+    V_data: Union[np.ndarray, csr_matrix, None] = None,
+    ekey: str = "M_s",
+    vkey: str = "velocity_S",
+    neighbors_from_basis: bool = False,
+    method: Literal["cosine", "consensus", "correlation", "jaccard", "hybrid", "divergence"] = "jaccard",
+) -> AnnData:
     """Calculate the cell-wise velocity confidence metric.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        X_data: 'np.ndarray' or `sp.csr_matrix` or None (optional, default `None`)
-            The expression states of single cells (or expression states in reduced dimension, like pca, of single cells)
-        V_data: 'np.ndarray' or `sp.csr_matrix` or None (optional, default `None`)
-            The RNA velocity of single cells (or velocity estimates projected to reduced dimension, like pca, of single
-            cells). Note that X, V_mat need to have the exact dimensionalities.
-        ekey: `str` (optional, default `M_s`)
-            The dictionary key that corresponds to the gene expression in the layer attribute. By default, it is the
-            smoothed expression `M_s`.
-        vkey: 'str' (optional, default `velocity_S`)
-            The dictionary key that corresponds to the estimated velocity values in layers attribute.
-        neighbors_from_basis: `bool` (optional, default `False`)
-            Whether to construct nearest neighbors from low dimensional space as defined by the `basis`, instead of using
-            that calculated during UMAP process.
-        method: `str` (optional, default `jaccard`)
-            Which method will be used for calculating the cell wise velocity confidence metric.
-            By default it uses
-            `jaccard` index, which measures how well each velocity vector meets the geometric constraints defined by the
+    Args:
+        adata: an AnnData object.
+        X_data: the expression states of single cells (or expression states in reduced dimension, like pca, of single
+            cells). Defaults to None.
+        V_data: the RNA velocity of single cells (or velocity estimates projected to reduced dimension, like pca, of
+            single cells). Note that X, V_mat need to have the exact dimensionalities. Defaults to None.
+        ekey: the dictionary key that corresponds to the gene expression in the layer attribute. By default, it is the
+            smoothed expression `M_s`. Defaults to "M_s".
+        vkey: the dictionary key that corresponds to the estimated velocity values in layers attribute. Defaults to
+            "velocity_S".
+        neighbors_from_basis: whether to construct nearest neighbors from low dimensional space as defined by the
+            `basis`, instead of using that calculated during UMAP process. Defaults to False.
+        method: which method will be used for calculating the cell wise velocity confidence metric. Defaults to
+            "jaccard", which measures how well each velocity vector meets the geometric constraints defined by the
             local neighborhood structure. Jaccard index is calculated as the fraction of the number of the intersected
             set of nearest neighbors from each cell at current expression state (X) and that from the future expression
-            state (X + V) over the number of the union of these two sets. The `cosine` or `correlation` method is similar
-            to that used by scVelo (https://github.com/theislab/scvelo).
+            state (X + V) over the number of the union of these two sets. The `cosine` or `correlation` method is
+            similar to that used by scVelo (https://github.com/theislab/scvelo).
 
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            Returns an updated `~anndata.AnnData` with `.obs.confidence` as the cell-wise velocity confidence.
+    Raises:
+        NotImplementedError: `method` is invalid.
+
+    Returns:
+        An updated AnnData object with `.obs.confidence` as the cell-wise velocity confidence.
     """
 
     if method in ["cosine", "consensus", "correlation"]:
         if "indices" not in adata.uns["neighbors"].keys():
             adata.uns["neighbors"]["indices"], _ = adj_to_knn(
                 adata.obsp["connectivities"], n_neighbors=adata.uns["neighbors"]["params"]["n_neighbors"]
             )
 
     if ekey == "X":
         X, V = (
-            adata.X if X_data is None else X_data,
+            adata.X if X_data is None else X_data,  ##### Check! inverse_norm for adata.X?
             adata.layers[vkey] if V_data is None else V_data,
         )
-        norm_method = adata.uns["pp"]["norm_method"].copy()
-        adata.uns["pp"]["norm_method"] = "log1p"
+        norm_method = adata.uns["pp"]["layers_norm_method"].copy()
+        adata.uns["pp"]["layers_norm_method"] = "log1p"
         X = inverse_norm(adata, X) if X_data is None else X_data
-        adata.uns["pp"]["norm_method"] = norm_method
+        adata.uns["pp"]["layers_norm_method"] = norm_method
     else:
         X, V = (
             adata.layers[ekey] if X_data is None else X_data,
             adata.layers[vkey] if V_data is None else V_data,
         )
         X = inverse_norm(adata, X) if X_data is None else X_data
 
@@ -108,15 +111,15 @@
         col = nbrs_idx[:, 1:].flatten()
         X_neighbors = csr_matrix(
             (np.repeat(1, len(col)), (row, col)),
             shape=(adata.n_obs, adata.n_obs),
         )
 
     n_neigh = n_neigh[0] if type(n_neigh) == np.ndarray else n_neigh
-    n_pca_components = adata.obsm["X"].shape[1]
+    n_pca_components = adata.obsm[DKM.X_PCA].shape[1]
 
     finite_inds = get_finite_inds(V, 0)
     X, V = X[:, finite_inds], V[:, finite_inds]
     if method == "jaccard":
         jac, _, _ = jaccard(X, V, n_pca_components, n_neigh, X_neighbors)
         confidence = jac
 
@@ -186,24 +189,43 @@
                 )
             )
 
     elif method == "divergence":
         pass
 
     else:
-        raise Exception(
+        raise NotImplementedError(
             "The input {} method for cell-wise velocity confidence calculation is not implemented" " yet".format(method)
         )
 
     adata.obs[method + "_velocity_confidence"] = confidence
 
     return adata
 
 
-def jaccard(X, V, n_pca_components, n_neigh, X_neighbors):
+def jaccard(
+    X: np.ndarray, V: np.ndarray, n_pca_components: int, n_neigh: int, X_neighbors: csr_matrix
+) -> Tuple[np.ndarray, csr_matrix, np.ndarray]:
+    """Calculate cell-wise confidence matrix with Jaccard method.
+
+    This method measures how well each velocity vector meets the geometric constraints defined by the local neighborhood structure. Jaccard index is calculated as the fraction of the number of the intersected set of nearest neighbors
+    from each cell at current expression state (X) and that from the future expression state (X + V) over the number of
+    the union of these two sets.
+
+    Args:
+        X: the expression states of single cells (or expression states in reduced dimension, like pca, of single cells).
+        V: the RNA velocity of single cells (or velocity estimates projected to reduced dimension, like pca, of single
+            cells). Note that X, V_mat need to have the exact dimensionalities.
+        n_pca_components: the number of PCA components of the expression data.
+        n_neigh: the number of neighbors to be found.
+        X_neighbors: the neighbor matrix.
+
+    Returns:
+        The cell wise velocity confidence metric.
+    """
     from sklearn.decomposition import TruncatedSVD
 
     transformer = TruncatedSVD(n_components=n_pca_components + 1, random_state=0)
     Xt = X + V
     if issparse(Xt):
         Xt.data[Xt.data < 0] = 0
         Xt.data = np.log2(Xt.data + 1)
@@ -221,36 +243,49 @@
     intersect_ = mnn_from_list([X_neighbors_, V_neighbors_]) > 0
 
     jaccard = (intersect_.sum(1) / union_.sum(1)).A1 if issparse(X) else intersect_.sum(1) / union_.sum(1)
 
     return jaccard, intersect_, union_
 
 
-def consensus(x, y):
+def consensus(x: np.ndarray, y: np.ndarray) -> np.ndarray:
+    """Calculate the consensus with expression matrix and velocity matrix.
+
+    Args:
+        x: expression matrix (genes x cells).
+        y: velocity vectors y_i for gene i.
+
+    Returns:
+        The consensus matrix.
+    """
     x_norm, y_norm = np.linalg.norm(x), np.linalg.norm(y)
     consensus = (
         einsum_correlation(x[None, :], y, type="cosine")[0, 0] * np.min([x_norm, y_norm]) / np.max([x_norm, y_norm])
     )
 
     return consensus
 
 
 def gene_wise_confidence(
-    adata,
-    group,
-    lineage_dict=None,
-    genes=None,
-    ekey="M_s",
-    vkey="velocity_S",
-    X_data=None,
-    V_data=None,
-    V_threshold=1,
-):
+    adata: AnnData,
+    group: str,
+    lineage_dict: Optional[Dict[str, str]] = None,
+    genes: Optional[List[str]] = None,
+    ekey: str = "M_s",
+    vkey: str = "velocity_S",
+    X_data: Optional[np.ndarray] = None,
+    V_data: Optional[np.ndarray] = None,
+    V_threshold: float = 1,
+) -> None:
     """Diagnostic measure to identify genes contributed to "wrong" directionality of the vector flow.
 
+    The results would be stored in a new `gene_wise_confidence` key in .uns, which contains gene-wise confidence score
+    in each cell state. `.var` will also be updated with `avg_prog_confidence` and `avg_mature_confidence` key which
+    correspond to the average gene wise confidence in the progenitor state or the mature cell state.
+
     In some scenarios, you may find unexpected "wrong vector back-flow" from your dynamo analyses, in order to diagnose
     those cases, we can identify those genes showing up in the wrong phase portrait position. Then we may remove those
     identified genes to "correct" velocity vectors. This requires us to give some priors about what progenitor and
     terminal cell types are. The rationale behind this basically boils down to understanding the following two
     scenarios:
 
     1). if the progenitor’s expression is low, starting from time point 0, cells should start to increase expression.
@@ -271,52 +306,44 @@
     a velocity confidence measure.
 
     Note that, this heuristic method requires you provide meaningful `progenitors_groups` and `mature_cells_groups`. In
     particular, the progenitor groups should in principle have cell going out (transcriptomically) while mature groups
     should end up in a different expression state and there are intermediate cells going to the dead end cells in the
     each terminal group (or most terminal groups).
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object
-        group: `str`
-            The column key/name that identifies the cell state grouping information of cells. This will be used for
+    Args:
+        adata: an AnnData object.
+        group: the column key/name that identifies the cell state grouping information of cells. This will be used for
             calculating gene-wise confidence score in each cell state.
-        lineage_dict: `dict`
-            A dictionary describes lineage priors. Keys corresponds to the group name from `group` that corresponding
-            to the state of one progenitor type while values correspond to the group names from `group` that
-            corresponding to the states of one or multiple terminal cell states. The best practice for determining
+        lineage_dict: a dictionary describes lineage priors. Keys corresponds to the group name from `group` that
+            corresponding to the state of one progenitor type while values correspond to the group names from `group`
+            that corresponding to the states of one or multiple terminal cell states. The best practice for determining
             terminal cell states are those fully functional cells instead of intermediate cell states. Note that in
             python a dictionary key cannot be a list, so if you have two progenitor types converge into one terminal
-            cell state, you need to create two records each with the same terminal cell as value but different progenitor
-            as the key. Value can be either a string for one cell group or a list of string for multiple cell groups.
-        genes: `list` or None (default: `None`)
-            The list of genes that will be used to gene-wise confidence score calculation. If `None`, all genes that go
-            through velocity estimation will be used.
-        ekey: `str` or None (default: `M_s`)
-            The layer that will be used to retrieve data for identifying the gene is in induction or repression phase at
-            each cell state. If `None`, .X is used.
-        vkey: `str` or None (default: `velocity_S`)
-            The layer that will be used to retrieve velocity data for calculating gene-wise confidence. If `None`,
-            `velocity_S` is used.
-        X_data: `np.ndarray` (default: `None`)
-            The user supplied data that will be used for identifying the gene is in induction or repression phase at
-            each cell state directly
-        V_data: `np.ndarray` (default: `None`)
-            The user supplied data that will be used for calculating gene-wise confidence directly.
-        V_threshold: `float` (default: `1`)
-            The threshold of velocity to calculate the gene wise confidence.
-
-    Returns
-    -------
-        An updated adata object with a new `gene_wise_confidence` key in .uns, which contains gene-wise confidence score
-        in each cell state. .var will also be updated with `avg_prog_confidence` and `avg_mature_confidence` key which
-        correspond to the average gene wise confidence in the progenitor state or the mature cell state.
+            cell state, you need to create two records each with the same terminal cell as value but different
+            progenitor as the key. Value can be either a string for one cell group or a list of string for multiple cell
+            groups. Defaults to None.
+        genes: the list of genes that will be used to gene-wise confidence score calculation. If `None`, all genes that
+            go through velocity estimation will be used. Defaults to None.
+        ekey: the layer that will be used to retrieve data for identifying the gene is in induction or repression phase
+            at each cell state. If `None`, .X is used. Defaults to "M_s".
+        vkey: the layer that will be used to retrieve velocity data for calculating gene-wise confidence. If `None`,
+            `velocity_S` is used. Defaults to "velocity_S".
+        X_data: the user supplied data that will be used for identifying the gene is in induction or repression phase at
+            each cell state directly. Defaults to None.
+        V_data: the user supplied data that will be used for calculating gene-wise confidence directly. Defaults to None.
+        V_threshold: the threshold of velocity to calculate the gene wise confidence. Defaults to 1.
+
+    Raises:
+        ValueError: `X_data` is provided but `genes` does not correspond to its columns.
+        ValueError: `X_data` is provided but `genes` does not correspond to its columns.
+        Exception: progenitor cell extracted from lineage_dict is not in `adata.obs[group]`.
+        Exception: terminal cell extracted from lineage_dict is not in `adata.obs[group]`.
     """
+
     logger = LoggerManager.gen_logger("gene_wise_confidence")
 
     if X_data is None:
         genes, X_data = fetch_X_data(adata, genes, ekey)
     else:
         if genes is None or len(genes) != X_data.shape[1]:
             raise ValueError(
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/moments.py` & `dynamo-release-1.3.0/dynamo/tools/moments.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,82 +1,77 @@
 import warnings
-from typing import Union
+from typing import Callable, List, Optional, Tuple, Union
 
-import anndata
 import numpy as np
+from anndata import AnnData
 from scipy.sparse import csr_matrix, diags, issparse, lil_matrix
 from tqdm import tqdm
 
 from ..configuration import DKM, DynamoAdataKeyManager
 from ..dynamo_logger import LoggerManager
-from ..preprocessing.utils import pca_monocle
+from ..preprocessing.normalization import normalize_mat_monocle, sz_util
+from ..preprocessing.pca import pca
 from ..utils import copy_adata
 from .connectivity import mnn, normalize_knn_graph, umap_conn_indices_dist_embedding
 from .utils import elem_prod, get_mapper, inverse_norm
 
 
 # ---------------------------------------------------------------------------------------------------
 # use for calculating moments for stochastic model:
 def moments(
-    adata: anndata.AnnData,
-    X_data: np.ndarray = None,
-    genes: Union[list, None] = None,
-    group: Union[str, None] = None,
-    conn: Union[csr_matrix, None] = None,
+    adata: AnnData,
+    X_data: Optional[np.ndarray] = None,
+    genes: Optional[list] = None,
+    group: Optional[str] = None,
+    conn: Optional[csr_matrix] = None,
     use_gaussian_kernel: bool = False,
     normalize: bool = True,
     use_mnn: bool = False,
-    layers: str = "all",
+    layers: Union[List[str], str] = "all",
     n_pca_components: int = 30,
     n_neighbors: int = 30,
     copy: bool = False,
-) -> Union[anndata.AnnData, None]:
-    """Calculate kNN based first and second moments (including uncentered covariance) for
-     different layers of data.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object.
-        X_data: `np.ndarray` (default: `None`)
-            The user supplied data that will be used for constructing the nearest neighbor graph directly.
-        genes: `np.array` (default: `None`)
-            The one-dimensional numpy array of the genes that you want to perform pca analysis (if adata.obsm['X'] is
+) -> Optional[AnnData]:
+    """Calculate kNN based first and second moments (including uncentered covariance) for different layers of data.
+
+    Args:
+        adata: an AnnData object.
+        X_data: the user supplied data that will be used for constructing the nearest neighbor graph directly. Defaults
+            to None.
+        genes: the one-dimensional numpy array of the genes that you want to perform pca analysis (if adata.obsm['X'] is
              not available). `X` keyname (instead of `X_pca`) was used to enable you use a different set of genes for
              flexible connectivity graph construction. If `None`, by default it will select genes based `use_for_pca`
-             key in .var attributes if it exists otherwise it will also all genes stored in adata.X
-        group: `str` or None (default: `None`)
-            The column key/name that identifies the grouping information (for example, clusters that correspond to
+             key in .var attributes if it exists otherwise it will also all genes stored in adata.X. Defaults to None.
+        group: the column key/name that identifies the grouping information (for example, clusters that correspond to
             different cell types or different time points) of cells. This will be used to compute kNN graph for each
-            group (i.e cell-type/time-point). This is important, for example, we don't want cells from different
-            labeling time points to be mixed when performing the kNN graph for calculating the moments.
-        conn: csr_matrix or None (default: `None`)
-            The connectivity graph that will be used for moment calculations.
-        use_gaussian_kernel: `bool` (default: `True`)
-            Whether to normalize the kNN graph via a Guasian kernel.
-        normalize: `bool` (default: `True`)
-            Whether to normalize the connectivity matrix so that each row sums up to 1. When `use_gaussian_kernel` is
-            False, this will be reset to be False because we will already normalize the connectivity matrix matrix by
-            dividing each row the total number of connections.
-        use_mnn: `bool` (default: `False`)
-            Whether to use mutual kNN across different layers as for the moment calculation.
-        layers: `str` or a list of str (default: `str`)
-            The layers that will be used for calculating the moments.
-        n_pca_components: `int` (default: `30`)
-            The number of pca components to use for constructing nearest neighbor graph and calculating 1/2-st moments.
-        n_neighbors: `int` (default: `30`)
-            The number of neighbors for constructing nearest neighbor graph used to calculate 1/2-st moments.
-
-    Returns
-    -------
-         adata: :class:`Union[AnnData, None]`
-            If `copy` is set to False, `annData` object is updated with with calculated first/second moments (including
-                uncentered covariance)
-            If `copy` is set to True, a deep copy of the original `adata` object is returned.
+            group (i.e. cell-type/time-point). This is important, for example, we don't want cells from different
+            labeling time points to be mixed when performing the kNN graph for calculating the moments. Defaults to
+            None.
+        conn: the connectivity graph that will be used for moment calculations. Defaults to None.
+        use_gaussian_kernel: whether to normalize the kNN graph via a Guasian kernel. Defaults to False.
+        normalize: whether to normalize the connectivity matrix so that each row sums up to 1. When
+            `use_gaussian_kernel` is False, this will be reset to be False because we will already normalize the
+            connectivity matrix by dividing each row the total number of connections. Defaults to True.
+        use_mnn: whether to use mutual kNN across different layers as for the moment calculation. Defaults to False.
+        layers: the layers that will be used for calculating the moments. Defaults to "all".
+        n_pca_components: the number of pca components to use for constructing nearest neighbor graph and calculating
+            1/2-st moments. Defaults to 30.
+        n_neighbors: the number of pca components to use for constructing nearest neighbor graph and calculating 1/2-st
+            moments. Defaults to 30.
+        copy: whether to return a new updated AnnData object or update inplace. Defaults to False.
+
+    Raises:
+        Exception: `group` is invalid.
+        ValueError: `conn` is invalid. It should be a square array with dimension equal to the cell number.
+
+    Returns:
+        The updated AnnData object if `copy` is true. Otherwise, the AnnData object passed in would be updated inplace
+        and None would be returned.
     """
+
     logger = LoggerManager.gen_logger("dynamo-moments")
     logger.info("calculating first/second moments...", indent_level=1)
     logger.log_time()
 
     adata = copy_adata(adata) if copy else adata
 
     mapper = get_mapper()
@@ -100,41 +95,37 @@
                     save_all_to_adata=False,
                 )
             conn = adata.uns["mnn"]
         else:
             if X_data is not None:
                 X = X_data
             else:
-                if "X" not in adata.obsm.keys():
+                if DKM.X_PCA not in adata.obsm.keys():
                     if not any([i.startswith("X_") for i in adata.layers.keys()]):
-                        from ..preprocessing.preprocess import recipe_monocle
+                        from ..preprocessing import Preprocessor
 
                         genes_to_use = adata.var_names[genes] if genes.dtype == "bool" else genes
-                        recipe_monocle(
-                            adata,
-                            genes_to_use=genes_to_use,
-                            num_dim=n_pca_components,
-                        )
-                        adata.obsm["X"] = adata.obsm["X_pca"]
+                        preprocessor = Preprocessor(force_gene_list=genes_to_use)
+                        preprocessor.preprocess_adata(adata, recipe="monocle")
                     else:
                         CM = adata.X if genes is None else adata[:, genes].X
                         cm_genesums = CM.sum(axis=0)
                         valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
                         valid_ind = np.array(valid_ind).flatten()
                         CM = CM[:, valid_ind]
-                        adata, fit, _ = pca_monocle(
+                        adata, fit, _ = pca(
                             adata,
                             CM,
                             n_pca_components=n_pca_components,
                             return_all=True,
                         )
 
                         adata.uns["explained_variance_ratio_"] = fit.explained_variance_ratio_[1:]
 
-                X = adata.obsm["X"][:, :n_pca_components]
+                X = adata.obsm[DKM.X_PCA][:, :n_pca_components]
 
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore")
                 if group is None:
                     (kNN, knn_indices, knn_dists, _,) = umap_conn_indices_dist_embedding(
                         X,
                         n_neighbors=np.min((n_neighbors, adata.n_obs - 1)),
@@ -186,33 +177,45 @@
         layer
         for layer in layers
         if layer.startswith("X_") and (not layer.endswith("matrix") and not layer.endswith("ambiguous"))
     ]
     layers.sort(reverse=True)  # ensure we get M_us, M_tn, etc (instead of M_su or M_nt).
     for i, layer in enumerate(layers):
         layer_x = adata.layers[layer].copy()
-        layer_x_group = np.where([layer in x for x in [only_splicing, only_labeling, splicing_and_labeling]])[0][0]
-        layer_x = inverse_norm(adata, layer_x)
+        matched_x_group_indices = np.where([layer in x for x in [only_splicing, only_labeling, splicing_and_labeling]])
+        if len(matched_x_group_indices[0]) == 0:
+            logger.warning(
+                f"layer {layer} is not in any of the {only_splicing, only_labeling, splicing_and_labeling} groups, skipping..."
+            )
+            continue
+        layer_x_group = matched_x_group_indices[0][0]
 
         if mapper[layer] not in adata.layers.keys():
             adata.layers[mapper[layer]], conn = (
                 calc_1nd_moment(layer_x, conn, normalize_W=normalize)
                 if use_gaussian_kernel
                 else (conn.dot(layer_x), conn)
             )
         for layer2 in layers[i:]:
+            matched_y_group_indices = np.where(
+                [layer2 in x for x in [only_splicing, only_labeling, splicing_and_labeling]]
+            )
+            if len(matched_y_group_indices[0]) == 0:
+                logger.warning(
+                    f"layer {layer2} is not in any of the {only_splicing, only_labeling, splicing_and_labeling} groups, skipping..."
+                )
+                continue
             layer_y = adata.layers[layer2].copy()
 
-            layer_y_group = np.where([layer2 in x for x in [only_splicing, only_labeling, splicing_and_labeling]])[0][0]
+            layer_y_group = matched_y_group_indices[0][0]
             # don't calculate 2 moments among uu, ul, su, sl -
             # they should be time-dependent moments and
             # those calculations are model specific
             if (layer_x_group != layer_y_group) or layer_x_group == 2:
                 continue
-            layer_y = inverse_norm(adata, layer_y)
 
             if mapper[layer2] not in adata.layers.keys():
                 adata.layers[mapper[layer2]], conn = (
                     calc_1nd_moment(layer_y, conn, normalize_W=normalize)
                     if use_gaussian_kernel
                     else (conn.dot(layer_y), conn)
                 )
@@ -229,44 +232,34 @@
 
     if copy:
         return adata
     return None
 
 
 def time_moment(
-    adata,
-    tkey,
-    has_splicing,
-    has_labeling=True,
-    t_label_keys=None,
-):
-    """Calculate time based first and second moments (including uncentered covariance) for
-     different layers of data.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object.
-        tkey: `str` or None (default: None)
-            The column key for the time label of cells in .obs. Used for either "ss" or "kinetic" model.
-            mode  with labeled data.
-        has_splicing: `bool`
-            Whether the data has splicing information.
-        has_labeling: `bool` (default: True)
-            Whether the data has labeling information.
-        t_label_keys: `str`, `list` or None (default: None)
-            The column key(s) for the labeling time label of cells in .obs. Used for either "ss" or "kinetic" model.
-            Not used for now and `tkey` is implicitly assumed as `t_label_key` (however, `tkey` should just be the time
-            of the experiment).
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An updated AnnData object with calculated first/second moments (including uncentered covariance) for
-             each time point for each layer included.
+    adata: AnnData,
+    tkey: Optional[str],
+    has_splicing: bool,
+    has_labeling: bool = True,
+    t_label_keys: Union[List[str], str, None] = None,
+) -> AnnData:
+    """Calculate time based first and second moments (including uncentered covariance) for different layers of data.
+
+    Args:
+        adata: an AnnData object.
+        tkey: The column key for the time label of cells in .obs. Used for either "ss" or "kinetic" model.
+        has_splicing: whether the data has splicing information.
+        has_labeling: whether the data has labeling information. Defaults to True.
+        t_label_keys: (not used for now) The column key(s) for the labeling time label of cells in .obs. Used for either
+            "ss" or "kinetic" model. `tkey` is implicitly assumed as `t_label_key` (however, `tkey` should just be the
+            time of the experiment). Defaults to None.
+
+    Returns:
+        An updated AnnData object with calculated first/second moments (including uncentered covariance) for each time
+        point for each layer included.
     """
 
     if has_labeling:
         if has_splicing:
             layers = ["uu", "ul", "su", "sl"]
         else:
             layers = ["new", "total"]
@@ -280,27 +273,43 @@
     adata.varm["v_t"] = v
 
     return adata
 
 
 # ---------------------------------------------------------------------------------------------------
 # use for kinetic assumption
-def get_layer_pair(layer):
+def get_layer_pair(layer: str) -> Optional[str]:
+    """Get the layer in pair for the input layer.
+
+    Args:
+        layer: the key for the input layer.
+
+    Returns:
+        The key for corresponding layer in pair.
+    """
     pair = {
         "new": "total",
         "total": "new",
         "X_new": "X_total",
         "X_total": "X_new",
         "M_t": "M_n",
         "M_n": "M_t",
     }
     return pair[layer] if layer in pair.keys() else None
 
 
-def get_layer_group(layer):
+def get_layer_group(layer: str) -> Optional[str]:
+    """Get the layer group in pair for the input layer group.
+
+    Args:
+        layer: the key for the input layer group.
+
+    Returns:
+        The key for corresponding layer group in pair.
+    """
     group = {
         "uu": "ul",
         "ul": "uu",
         "su": "sl",
         "sl": "su",
         "X_uu": "X_ul",
         "X_ul": "X_uu",
@@ -311,24 +320,40 @@
         "M_su": "M_sl",
         "M_sl": "M_su",
     }
     return group[layer] if layer in group.keys() else None
 
 
 def prepare_data_deterministic(
-    adata,
-    genes,
-    time,
-    layers,
-    use_total_layers=True,
-    total_layers=["X_ul", "X_sl", "X_uu", "X_su"],
-    log=False,
-    return_ntr=False,
-):
-    from ..preprocessing.utils import normalize_mat_monocle, sz_util
+    adata: AnnData,
+    genes: List[str],
+    time: np.ndarray,
+    layers: List[str],
+    use_total_layers: bool = True,
+    total_layers: List[str] = ["X_ul", "X_sl", "X_uu", "X_su"],
+    log: bool = False,
+    return_ntr: bool = False,
+) -> Tuple[List[np.ndarray], List[np.ndarray], List[Union[np.ndarray, csr_matrix]]]:
+    """Prepare the data for kinetic calculation based on deterministic model.
+
+    Args:
+        adata: an AnnData object.
+        genes: the genes to be estimated.
+        time: the array containing time stamp.
+        layers: the layer keys in adata object to be processed.
+        use_total_layers: whether to use total layers embedded in the AnnData object. Defaults to True.
+        total_layers: the layer(s) that can be summed up to get the total mRNA. Defaults to ["X_ul", "X_sl", "X_uu",
+            "X_su"].
+        log: whether to perform log1p (i.e. log(1+x)) on result data. Defaults to False.
+        return_ntr: whether to deal with new/total ratio. Defaults to False.
+
+    Returns:
+        A tuple [m, v, raw], where `m` is the first momentum, `v` is the second momentum, and `raw` is the normalized
+        expression data.
+    """
 
     if return_ntr:
         use_total_layers = True
     if use_total_layers:
         if "total_Size_Factor" not in adata.obs.keys():
             # total_layers = ["uu", "ul", "su", "sl"] if 'uu' in adata.layers.keys() else ['total']
             tot_sfs, _ = sz_util(
@@ -344,15 +369,15 @@
         sfs_x, sfs_y = tot_sfs[:, None], tot_sfs[:, None]
 
     m = [None] * len(layers)
     v = [None] * len(layers)
     raw = [None] * len(layers)
     for i, layer in enumerate(layers):
         if layer in ["X_total", "total", "M_t"]:
-            if (layer == "X_total" and adata.uns["pp"]["norm_method"] is None) or layer == "M_t":
+            if (layer == "X_total" and adata.uns["pp"]["layers_norm_method"] is None) or layer == "M_t":
                 x_layer = adata[:, genes].layers[layer]
                 if return_ntr:
                     T_genes = adata[:, genes].layers[get_layer_pair(layer)]
                     T_genes = T_genes.A if issparse(T_genes) else T_genes
                     x_layer = x_layer / (T_genes + 1e-5)
                 else:
                     x_layer = x_layer - adata[:, genes].layers[get_layer_pair(layer)]
@@ -420,15 +445,15 @@
                     T_genes = adata[:, genes].layers[t_layer_key]
                     T_genes = T_genes.A if issparse(T_genes) else T_genes
                     x_layer = (x_layer - y_layer) / (T_genes + 1e-5)
                 else:
                     x_layer = x_layer - y_layer
 
         else:
-            if (layer == ["X_new"] and adata.uns["pp"]["norm_method"] is None) or layer == "M_n":
+            if (layer == ["X_new"] and adata.uns["pp"]["layers_norm_method"] is None) or layer == "M_n":
                 total_layer = "X_total" if layer == ["X_new"] else "M_t"
 
                 if return_ntr:
                     T_genes = adata[:, genes].layers[total_layer]
                     T_genes = T_genes.A if issparse(T_genes) else T_genes
                     x_layer = adata[:, genes].layers[layer] / (T_genes + 1e-5)
                 else:
@@ -477,27 +502,43 @@
         m[i], v[i], _ = calc_12_mom_labeling(x_layer.T, time)
         raw[i] = x_layer
 
     return m, v, raw  # each list element corresponds to a layer
 
 
 def prepare_data_has_splicing(
-    adata,
-    genes,
-    time,
-    layer_u,
-    layer_s,
-    use_total_layers=True,
-    total_layers=["X_ul", "X_sl", "X_uu", "X_su"],
-    total_layer="X_total",
-    return_cov=True,
-    return_ntr=False,
-):
-    """Prepare data when assumption is kinetic and data has splicing"""
-    from ..preprocessing.utils import normalize_mat_monocle, sz_util
+    adata: AnnData,
+    genes: List[str],
+    time: np.ndarray,
+    layer_u: str,
+    layer_s: str,
+    use_total_layers: bool = True,
+    total_layers: List[str] = ["X_ul", "X_sl", "X_uu", "X_su"],
+    total_layer: str = "X_total",
+    return_cov: bool = True,
+    return_ntr: bool = False,
+) -> Tuple[List[np.ndarray], List[np.ndarray]]:
+    """Prepare data when assumption is kinetic and data has splicing.
+
+    Args:
+        adata: an AnnData object.
+        genes: the genes to be estimated.
+        time: the array containing time stamps.
+        layer_u: the layer key for unspliced data.
+        layer_s: the layer key for spliced data.
+        use_total_layers: whether to use total layers embedded in the AnnData object. Defaults to True.
+        total_layers: the layer(s) that can be summed up to get the total mRNA. Defaults to ["X_ul", "X_sl", "X_uu",
+            "X_su"].
+        total_layer: the layer key for the precalculated total mRNA data. Defaults to "X_total".
+        return_cov: whether to calculate the covariance between spliced and unspliced data. Defaults to True.
+        return_ntr: whether to return the new to total ratio or original expression data. Defaults to False.
+
+    Returns:
+        A tuple [res, raw] where `res` is the calculated momentum data and `raw` is the normalized expression data.
+    """
 
     res = [0] * len(genes)
     raw = [0] * len(genes)
 
     U, S = (
         adata[:, genes].layers[layer_u] if layer_u == "M_ul" else None,
         adata[:, genes].layers[layer_s] if layer_s == "M_sl" else None,
@@ -616,25 +657,41 @@
         res[i] = x
         raw[i] = np.vstack((u, s))
 
     return res, raw
 
 
 def prepare_data_no_splicing(
-    adata,
-    genes,
-    time,
-    layer,
-    use_total_layers=True,
-    total_layer="X_total",
-    return_old=False,
-    return_ntr=False,
-):
-    """Prepare data when assumption is kinetic and data has no splicing"""
-    from ..preprocessing.utils import normalize_mat_monocle, sz_util
+    adata: AnnData,
+    genes: List[str],
+    time: np.ndarray,
+    layer: str,
+    use_total_layers: bool = True,
+    total_layer: str = "X_total",
+    return_old: bool = False,
+    return_ntr: bool = False,
+) -> Tuple[List[np.ndarray], List[np.ndarray]]:
+    """Prepare the data when assumption is kinetic and data has no splicing.
+
+    Args:
+        adata: an AnnData object.
+        genes: the genes to be estimated.
+        time: the array containing time stamps.
+        layer: the layer containing the expression data.
+        use_total_layers: whether to use the total data embedded in the AnnData object. Defaults to True.
+        total_layer: the layer key for the precalculated total mRNA data. Defaults to "X_total".
+        return_old: whether to return the old expression data together or the newly expressed gene data only. Defaults
+            to False.
+        return_ntr: whether to return the new to total ratio or the original expression data. Defaults to False.
+
+    Returns:
+        A tuple [res, raw] where `res` is the calculated momentum data and `raw` is the normalized expression data.
+    """
+
+    from ..preprocessing.normalization import normalize_mat_monocle, sz_util
 
     res = [0] * len(genes)
     raw = [0] * len(genes)
 
     U, T = (
         adata[:, genes].layers[layer] if layer == "M_n" else None,
         adata[:, genes].layers[total_layer] if total_layer == "M_t" else None,
@@ -711,30 +768,49 @@
         res[i] = np.vstack([ut, uut])
         raw[i] = np.vstack([u, t - u]) if return_old else u
 
     return res, raw
 
 
 def prepare_data_mix_has_splicing(
-    adata,
-    genes,
-    time,
-    layer_u="X_uu",
-    layer_s="X_su",
-    layer_ul="X_ul",
-    layer_sl="X_sl",
-    use_total_layers=True,
-    total_layers=["X_ul", "X_sl", "X_uu", "X_su"],
-    mix_model_indices=None,
-):
+    adata: AnnData,
+    genes: List[str],
+    time: np.ndarray,
+    layer_u: str = "X_uu",
+    layer_s: str = "X_su",
+    layer_ul: str = "X_ul",
+    layer_sl: str = "X_sl",
+    use_total_layers: bool = True,
+    total_layers: List[str] = ["X_ul", "X_sl", "X_uu", "X_su"],
+    mix_model_indices: Optional[List[int]] = None,
+) -> Tuple[List[np.ndarray], List[np.ndarray]]:
     """Prepare data for mixture modeling when assumption is kinetic and data has splicing.
-    Note that the mix_model_indices is indexed on 10 total species, which can be used to specify
-    the data required for different mixture models.
+
+    Note that the mix_model_indices is indexed on 10 total species, which can be used to specify the data required for
+    different mixture models.
+
+    Args:
+        adata: an AnnData object.
+        genes: the genes to be estimated.
+        time: the array containing time stamps.
+        layer_u: the layer key for unspliced mRNA count data. Defaults to "X_uu".
+        layer_s: the layer key for spliced mRNA count data. Defaults to "X_su".
+        layer_ul: the layer key for unspliced, labeled mRNA count data. Defaults to "X_ul".
+        layer_sl: the layer key for spliced, labeled mRNA count data. Defaults to "X_sl".
+        use_total_layers: whether to use total layers embedded in the AnnData object. Defaults to True.
+        total_layers: the layer(s) that can be summed up to get the total mRNA. Defaults to ["X_ul", "X_sl", "X_uu",
+            "X_su"].
+        mix_model_indices: the indices for data required by the mixture model. If None, all data would be returned.
+            Defaults to None.
+
+    Returns:
+        A tuple [res, raw] where `res` is the calculated momentum data and `raw` is the normalized expression data.
     """
-    from ..preprocessing.utils import normalize_mat_monocle, sz_util
+
+    from ..preprocessing.normalization import normalize_mat_monocle, sz_util
 
     res = [0] * len(genes)
     raw = [0] * len(genes)
 
     U, S = (
         adata[:, genes].layers[layer_u] if layer_u == "M_uu" else None,
         adata[:, genes].layers[layer_s] if layer_u == "M_su" else None,
@@ -851,28 +927,43 @@
         res[i] = x
         raw[i] = np.vstack((ul, sl, u, s))
 
     return res, raw
 
 
 def prepare_data_mix_no_splicing(
-    adata,
-    genes,
-    time,
-    layer_n,
-    layer_t,
-    use_total_layers=True,
-    total_layer="X_total",
-    mix_model_indices=None,
-):
+    adata: AnnData,
+    genes: List[str],
+    time: np.ndarray,
+    layer_n: str,
+    layer_t: str,
+    use_total_layers: bool = True,
+    total_layer: bool = "X_total",
+    mix_model_indices: Optional[List[int]] = None,
+) -> Tuple[List[np.ndarray], List[np.ndarray]]:
     """Prepare data for mixture modeling when assumption is kinetic and data has NO splicing.
+
     Note that the mix_model_indices is indexed on 4 total species, which can be used to specify
     the data required for different mixture models.
+
+    Args:
+        adata: an AnnData object.
+        genes: the genes to be estimated.
+        time: the array containing time stamps.
+        layer_n: the layer key for new mRNA count.
+        layer_t: the layer key for total mRNA count.
+        use_total_layers: whether to use total layers embedded in the AnnData object. Defaults to True.
+        total_layer: the layer key for the precalculated total mRNA data. Defaults to "X_total".
+        mix_model_indices: the indices for data required by the mixture model. If None, all data would be returned. Defaults to None.
+
+    Returns:
+        A tuple [res, raw] where `res` is the calculated momentum data and `raw` is the normalized expression data.
     """
-    from ..preprocessing.utils import normalize_mat_monocle, sz_util
+
+    from ..preprocessing.normalization import normalize_mat_monocle, sz_util
 
     res = [0] * len(genes)
     raw = [0] * len(genes)
 
     N, T = (
         adata[:, genes].layers[layer_n] if layer_n == "M_n" else None,
         adata[:, genes].layers[layer_t] if layer_t == "M_t" else None,
@@ -956,26 +1047,61 @@
     return res, raw
 
 
 # ---------------------------------------------------------------------------------------------------
 # moment related:
 
 
-def stratify(arr, strata):
+def stratify(arr: np.ndarray, strata: np.ndarray) -> List[np.ndarray]:
+    """Stratify the given array with the given reference strata.
+
+    Args:
+        arr: The array to be stratified.
+        strata: The reference strata vector.
+
+    Returns:
+        A list containing the strata from the array, with each element of the list to be the components with line index
+        corresponding to the reference strata vector's unique elements' index.
+    """
+
     s = np.unique(strata)
     return [arr[strata == s[i]] for i in range(len(s))]
 
 
-def strat_mom(arr, strata, fcn_mom):
+def strat_mom(arr: Union[np.ndarray, csr_matrix], strata: np.ndarray, fcn_mom: Callable) -> np.ndarray:
+    """Stratify the mRNA expression data and calculate its momentum.
+
+    Args:
+        arr: the mRNA expression data.
+        strata: the time stamp array used to stratify `arr`.
+        fcn_mom: the function used to calculate the momentum.
+
+    Returns:
+        The momentum for each stratum.
+    """
+
     arr = arr.A if issparse(arr) else arr
     x = stratify(arr, strata)
     return np.array([fcn_mom(y) for y in x])
 
 
-def calc_mom_all_genes(T, adata, fcn_mom):
+def calc_mom_all_genes(
+    T: np.ndarray, adata: AnnData, fcn_mom: Callable
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
+    """Calculate momentum for all genes in an AnnData object.
+
+    Args:
+        T: the time stamp array.
+        adata: an AnnData object.
+        fcn_mom: the function used to calculate momentum.
+
+    Returns:
+        A tuple (Mn, Mo, Mt, Mr), where `Mn` is momentum calculated from labeled (new) mRNA count, `Mo` is from
+        unlabeled (old) mRNA count, `Mt` is from total mRNA count, and `Mr` is from new to total ratio.
+    """
     ng = adata.var.shape[0]
     nT = len(np.unique(T))
     Mn = np.zeros((ng, nT))
     Mo = np.zeros((ng, nT))
     Mt = np.zeros((ng, nT))
     Mr = np.zeros((ng, nT))
     for g in tqdm(range(ng), desc="calculating 1/2 moments"):
@@ -1006,30 +1132,58 @@
     if center:
         mX = calc_1nd_moment(X, W, False) if mX is None else mX
         mY = calc_1nd_moment(Y, W, False) if mY is None else mY
         XY = XY - np.multiply(mX, mY)
     return XY
 
 
-def gaussian_kernel(X, nbr_idx, sigma, k=None, dists=None):
+def gaussian_kernel(
+    X: np.ndarray, nbr_idx: np.ndarray, sigma: int, k: Optional[int] = None, dists: Optional[np.ndarray] = None
+) -> csr_matrix:
+    """Normalize connectivity map with Gaussian kernel.
+
+    Args:
+        X: the mRNA expression data.
+        nbr_idx: the indices of nearest neighbors of each cell.
+        sigma: the standard deviation for gaussian model.
+        k: the number of nearest neighbors to be considered. Defaults to None.
+        dists: the distances to the n_neighbors closest points in knn graph. Defaults to None.
+
+    Returns:
+        The normalized connectivity map.
+    """
     n = X.shape[0]
     if dists is None:
         dists = []
         for i in range(n):
             d = X[nbr_idx[i][:k]] - X[i]
             dists.append(np.sum(elem_prod(d, d), 1).flatten())
     W = lil_matrix((n, n))
     s2_inv = 1 / (2 * sigma**2)
     for i in range(n):
         W[i, nbr_idx[i][:k]] = np.exp(-s2_inv * dists[i][:k] ** 2)
 
     return csr_matrix(W)
 
 
-def calc_12_mom_labeling(data, t, calculate_2_mom=True):
+def calc_12_mom_labeling(
+    data: np.ndarray, t: np.ndarray, calculate_2_mom: bool = True
+) -> Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:
+    """Calculate 1st and 2nd momentum for given data.
+
+    Args:
+        data: the normalized mRNA expression data.
+        t: the time stamp array.
+        calculate_2_mom: whether to calculate 2nd momentum. Defaults to True.
+
+    Returns:
+        A tuple (m, [v], t_uniq) where `m` is the first momentum, `v` is the second momentum which would be returned only
+        if `calculate_2_mom` is true, and `t_uniq` is the unique time stamps.
+    """
+
     t_uniq = np.unique(t)
 
     m = np.zeros((data.shape[0], len(t_uniq)))
     if calculate_2_mom:
         v = np.zeros((data.shape[0], len(t_uniq)))
 
     for i in range(data.shape[0]):
@@ -1039,27 +1193,61 @@
         m[i] = strat_mom(data_, t, np.nanmean)
         if calculate_2_mom:
             v[i] = strat_mom(data_, t, np.nanvar)
 
     return (m, v, t_uniq) if calculate_2_mom else (m, t_uniq)
 
 
-def calc_1nd_moment(X, W, normalize_W=True):
+def calc_1nd_moment(
+    X: np.ndarray, W: np.ndarray, normalize_W: bool = True
+) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
+    """Calculate first moment for the layers.
+
+    Args:
+        X: the layer to calculate the moment.
+        W: the connectivity graph that will be used for moment calculations.
+        normalize_W: whether to normalize W before calculation. Defaults to True.
+
+    Returns:
+        The first moment of the layer.
+    """
     if normalize_W:
         if type(W) == np.ndarray:
             d = np.sum(W, 1).flatten()
         else:
             d = np.sum(W, 1).A.flatten()
         W = diags(1 / d) @ W if issparse(W) else np.diag(1 / d) @ W
         return W @ X, W
     else:
         return W @ X
 
 
-def calc_2nd_moment(X, Y, W, normalize_W=True, center=False, mX=None, mY=None):
+def calc_2nd_moment(
+    X: np.ndarray,
+    Y: np.ndarray,
+    W: np.ndarray,
+    normalize_W: bool = True,
+    center: bool = False,
+    mX: np.ndarray = None,
+    mY: np.ndarray = None,
+) -> np.ndarray:
+    """Calculate the 2nd moment for the layers.
+
+    Args:
+        X: the first layer to be used.
+        Y: the second layer to be used.
+        W: the connectivity graph that will be used for moment calculations.
+        normalize_W: whether to normalize W before calculation. Defaults to True.
+        center: whether to correct the center. Defaults to False.
+        mX: the moment matrix to correct the center. Defaults to None.
+        mY: the moment matrix to correct the center. Defaults to None.
+
+    Returns:
+        The second moment of the layers.
+    """
     if normalize_W:
         if type(W) == np.ndarray:
             d = np.sum(W, 1).flatten()
         else:
             d = W.sum(1).A.flatten()
         W = diags(1 / d) @ W if issparse(W) else np.diag(1 / d) @ W
 
@@ -1071,15 +1259,15 @@
         XY = XY - elem_prod(mX, mY)
 
     return XY
 
 
 # ---------------------------------------------------------------------------------------------------
 # old moment estimation code
-class MomData(anndata.AnnData):
+class MomData(AnnData):
     """deprecated"""
 
     def __init__(self, adata, time_key="Time", has_nan=False):
         # self.data = adata
         self.__dict__ = adata.__dict__
         # calculate first and second moments from data
         self.times = np.array(self.obs[time_key].values, dtype=float)
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/multiomics.py` & `dynamo-release-1.3.0/dynamo/tools/multiomics.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,60 +1,47 @@
 import anndata
-import numpy as np
+import pandas as pd
 
 # 1. concatenate RNA/protein data
 # 2. filter gene/protein for velocity
 # 3. use steady state assumption to calculate protein velocity
 # 4. use the PRL paper to estimate the parameters
 
 
-def AddAssay(adata, data, key, slot="obsm"):
-    """Add a new data as a key to the specified slot
+def AddAssay(adata: anndata.AnnData, data: pd.DataFrame, key: str, slot: str = "obsm") -> anndata.AnnData:
+    """Add a new data as a key to the specified slot.
 
-    Parameters
-    ----------
-        adata: :AnnData
-            AnnData object
-        data: `pd.DataFrame`
-            The data (in pandas DataFrame format) that will be added to adata.
-        key: `str`
-            The key name to be used for the new data.
-        slot: `str`
-            The slot of adata to store the new data.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An updated anndata object that are updated with a new data as a key to the specified slot.
+    Args:
+        adata: an AnnData object.
+        data: the data (in pandas DataFrame format) that will be added to adata.
+        key: the key name to be used for the new data.
+        slot: the slot of adata to store the new data. Defaults to "obsm".
+
+    Returns:
+        An updated anndata object that are updated with a new data as a key to the specified slot.
     """
 
     if slot == "uns":
         adata.uns[key] = data.loc[adata.obs.index, set(adata.var.index).intersection(data.columns)]
     elif slot == "obsm":
         adata.obsm[key] = data.loc[adata.obs.index, set(adata.var.index).intersection(data.columns)]
 
     return adata
 
 
-def getAssay(adata, key, slot="obsm"):
-    """Retrieve a key named data from the specified slot
+def getAssay(adata: anndata.AnnData, key: str, slot: str = "obsm") -> pd.DataFrame:
+    """Retrieve a key named data from the specified slot.
+
+    Args:
+        adata: an AnnData object.
+        key: the key name of the data to be retrieved. .
+        slot: the slot of adata to be retrieved from. Defaults to "obsm".
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object
-        key: `str`
-            The key name to be used for the new data.
-        slot: `str`
-            The slot of adata to store the new data.
-
-    Returns
-    -------
-        data: `pd.DataFrame`
-            The data (in pandas DataFrame format) that will be retrieved from adata.
+    Returns:
+        The data (in pd.DataFrame) that will be retrieved from adata.
     """
 
     if slot == "uns":
         data = adata.uns[key]
     elif slot == "obsm":
         data = adata.obsm[key]
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/pseudotime.py` & `dynamo-release-1.3.0/dynamo/tools/pseudotime.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,20 @@
+from typing import Optional
+
+import anndata
 import numpy as np
 
 from .DDRTree_py import DDRTree
 from .utils import log1p_
 
 
-def Pseudotime(adata, layer="X", basis=None, method="DDRTree", **kwargs):
+def Pseudotime(
+    adata: anndata.AnnData, layer: str = "X", basis: Optional[str] = None, method: str = "DDRTree", **kwargs
+) -> anndata.AnnData:
+
     """
 
     Parameters
     ----------
     adata
     layer
     method
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/pseudotime_velocity.py` & `dynamo-release-1.3.0/dynamo/tools/pseudotime_velocity.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,35 @@
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
+from typing import Union
+
 import anndata
 import numpy as np
 from scipy.sparse import csr_matrix, diags, issparse
 
 from ..dynamo_logger import LoggerManager
 from .connectivity import adj_to_knn, knn_to_adj
 from .graph_operators import build_graph, gradop
 from .utils import projection_with_transition_matrix
 
 
-def gradient(E, f, tol=1e-5):
+def gradient(E: Union[csr_matrix, np.ndarray], f: np.ndarray, tol: float = 1e-5) -> csr_matrix:
+    """Calculate the graph's gradient.
+
+    Args:
+        E: the adjacency matrix of the graph.
+        f: the pseudotime matrix.
+        tol: the tolerance of considering a value to be non-zero. Defaults to 1e-5.
+
+    Returns:
+        The gradient of the graph.
+    """
     if issparse(E):
         row, col = E.nonzero()
         val = E.data
     else:
         row, col = np.where(E != 0)
         val = E[E != 0]
 
@@ -26,31 +43,59 @@
     valid_ind = G_val != 0
     G = csr_matrix((G_val[valid_ind], (G_i[valid_ind], G_j[valid_ind])), shape=E.shape)
     G.eliminate_zeros()
 
     return G
 
 
-def laplacian(E, convention="graph"):
+def laplacian(E: Union[csr_matrix, np.ndarray], convention: Literal["graph", "diffusion"] = "graph") -> csr_matrix:
+    """Calculate the laplacian of the given graph (here the adjacency matrix).
+
+    Args:
+        E: the adjacency matrix.
+        convention: the convention of results. Could be either "graph" or "diffusion". If "diffusion" is specified, the
+            negative of graph laplacian would be returned. Defaults to "graph".
+
+    Returns:
+        The laplacian matrix.
+
+    Raises:
+        NotImplementedError: invalid `convention`.
+    """
     if issparse(E):
         A = E.copy()
         A.data = np.ones_like(A.data)
         L = diags(A.sum(0).A1, 0) - A
     else:
         A = np.sign(E)
         L = np.diag(np.sum(A, 0)) - A
-    if convention == "diffusion":
+    if convention == "graph":
+        pass
+    elif convention == "diffusion":
         L = -L
+    else:
+        raise NotImplementedError("The convention is not implemented. ")
 
     L = csr_matrix(L)
 
     return L
 
 
-def pseudotime_transition(E, pseudotime, laplace_weight=10):
+def pseudotime_transition(E: np.ndarray, pseudotime: np.ndarray, laplace_weight: float = 10) -> csr_matrix:
+    """Calculate the transition graph with pseudotime gradient.
+
+    Args:
+        E: the adjacency matrix.
+        pseudotime: the pseudo time value matrix.
+        laplace_weight: the weight of adding laplacian to gradient during calculation of transition graph. Defaults to
+            10.
+
+    Returns:
+        The pseudo-based transition matrix.
+    """
     grad = gradient(E, pseudotime)
     lap = laplacian(E, convention="diffusion")
     T = grad + laplace_weight * lap
     return T
 
 
 def pseudotime_velocity(
@@ -58,63 +103,55 @@
     pseudotime: str = "pseudotime",
     basis: str = "umap",
     adj_key: str = "distances",
     ekey: str = "M_s",
     vkey: str = "velocity_S",
     add_tkey: str = "pseudotime_transition_matrix",
     add_ukey: str = "M_u_pseudo",
-    method: str = "hodge",
+    method: Literal["hodge", "naive", "gradient"] = "hodge",
     dynamics_info: bool = False,
     unspliced_RNA: bool = False,
-):
+) -> None:
     """Embrace RNA velocity and velocity vector field analysis for pseudotime.
 
+    The AnnData object will be updated, inplace, with low-dimensional velocity, pseudotime based transition matrix as
+    well as the pseudotime based RNA velocity.
+
     When you don't have unspliced/spliced RNA but still want to utilize the velocity/vector field and downstream
     differential geometry analysis, we can use `pseudotime_velocity` to convert pseudotime to RNA velocity. Essentially
     this function computes the gradient of pseudotime and use that to calculate a transition graph (a directed weighted
     graph) between each cell and use that to learn either the velocity on low dimensional embedding as well as the
     gene-wise RNA velocity.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-                an Annodata object.
-        pseudotime: str (default, `pseudotime`)
-            The key in the adata.obs that corresponds to the pseudotime values.
-        basis: str (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be `X_spliced_umap`
-            or `X_total_umap`, etc.
-        adj_key: str (default, `distances`)
-            The dictionary key that corresponds to the adjacency matrix in `.obsp` attribute. If method is `gradient`,
-            the weight will be ignored; while it is `exponent` the weight will be used.
-        ekey: str or None (optional, default `M_s`)
-            The dictionary key that corresponds to the gene expression in the layer attribute. This will be used to
-            calculate RNA velocity.
-        vkey: str or None (optional, default `velocity_S`)
-            The dictionary key that will be used to save the estimated velocity values in the layers attribute.
-        add_tkey: str (default: `pseudotime_transition_matrix`)
-            The dictionary key that will be used to keep the pseudotime-based transition matrix.
-        add_ukey: str (default: `M_u_pseudo`)
-            The dictionary key that will be used to save the estimated "unspliced mRNA". Since we assume gamma is 0, we
-            thus have M_u_pseudo essentially the estimated high dimensional velocity vector.
-        method: str (default: `hodge`)
-            Which pseudotime to vector field method to be used. There are three different methods, `hodge`, `naive`,
-            `gradient`. By default the `hodge` method will be used.
-        dynamics_info: bool (default: `False`)
-            Whether to add dynamics info (a dictionary (with `dynamics` key to the .uns) to your adata object
-            which is required for downstream velocity and vector field analysis.
-        unspliced_RNA: bool (default: `False`)
-            Whether to add a unspliced layer to your adata object which is required for downstream velocity and
-            vector field analysis.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            An new or updated anndata object, based on copy parameter, that are updated with low-dimensional velocity,
-            pseudotime based transition matrix as well as the pseudotime based RNA velocity.
+    Args:
+        adata: an AnnData object.
+        pseudotime: the key in the adata.obs that corresponds to the pseudotime values. Defaults to "pseudotime".
+        basis: the dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be
+            `X_spliced_umap` or `X_total_umap`, etc. Defaults to "umap".
+        adj_key: the dictionary key that corresponds to the adjacency matrix in `.obsp` attribute. If method is
+            `gradient`, the weight will be ignored; while it is `exponent` the weight will be used. Defaults to
+            "distances".
+        ekey: the dictionary key that corresponds to the gene expression in the layer attribute. This will be used to
+            calculate RNA velocity. Defaults to "M_s".
+        vkey: the dictionary key that will be used to save the estimated velocity values in the layers attribute.
+            Defaults to "velocity_S".
+        add_tkey: the dictionary key that will be used to keep the pseudotime-based transition matrix. Defaults to
+            "pseudotime_transition_matrix".
+        add_ukey: the dictionary key that will be used to save the estimated "unspliced mRNA". Since we assume gamma is
+            0, we thus have M_u_pseudo essentially the estimated high dimensional velocity vector. Defaults to
+            "M_u_pseudo".
+        method: which pseudotime to vector field method to be used. There are three different methods, `hodge`, `naive`,
+            `gradient`. By default the `hodge` method will be used. Defaults to "hodge".
+        dynamics_info: whether to add dynamics info (a dictionary (with `dynamics` key to the .uns) to your adata object
+            which is required for downstream velocity and vector field analysis. Defaults to False.
+        unspliced_RNA: whether to add a unspliced layer to your adata object which is required for downstream velocity
+            and vector field analysis. Defaults to False.
+
+    Raises:
+        Exception: `method` is invalid.
     """
 
     logger = LoggerManager.get_main_logger()
     logger.info(
         "Embrace RNA velocity and velocity vector field analysis for pseudotime...",
     )
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/recipes.py` & `dynamo-release-1.3.0/dynamo/tools/recipes.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,96 +1,91 @@
+from typing import Any, Dict, Optional
+
 import numpy as np
+from anndata import AnnData
 
 from ..configuration import DynamoAdataConfig
-from ..preprocessing.utils import pca_monocle
+from ..preprocessing.pca import pca
+from ..preprocessing.utils import (
+    del_raw_layers,
+    detect_experiment_datatype,
+    reset_adata_X,
+)
 from .cell_velocities import cell_velocities
 from .connectivity import neighbors, normalize_knn_graph
 from .dimension_reduction import reduceDimension
 from .dynamics import dynamics
 from .moments import moments
 from .utils import set_transition_genes
 
 # add recipe_csc_data()
 
 
 def recipe_kin_data(
-    adata,
-    tkey=None,
-    reset_X=True,
-    X_total_layers=False,
-    splicing_total_layers=False,
-    n_top_genes=1000,
-    keep_filtered_cells=None,
-    keep_filtered_genes=None,
-    keep_raw_layers=None,
-    del_2nd_moments=None,
-    ekey="M_t",
-    vkey="velocity_T",
-    basis="umap",
-    rm_kwargs={},
-):
+    adata: AnnData,
+    tkey: Optional[str] = None,
+    reset_X: bool = True,
+    X_total_layers: bool = False,
+    splicing_total_layers: bool = False,
+    n_top_genes: int = 1000,
+    keep_filtered_cells: Optional[bool] = None,
+    keep_filtered_genes: Optional[bool] = None,
+    keep_raw_layers: Optional[bool] = None,
+    del_2nd_moments: Optional[bool] = None,
+    ekey: str = "M_t",
+    vkey: str = "velocity_T",
+    basis: str = "umap",
+    rm_kwargs: Dict["str", Any] = {},
+) -> AnnData:
     """An analysis recipe that properly pre-processes different layers for an kinetics experiment with both labeling and
     splicing or only labeling data.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that stores data for the the kinetics experiment, must include `uu, ul, su, sl` four
+    Args:
+        adata: an AnnData object that stores data for the kinetics experiment, must include `uu, ul, su, sl` four
             different layers.
-        tkey: `str` or None (default: None)
-            The column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
+        tkey: the column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
             support for conventional scRNA-seq data). Note that `tkey` will be saved to adata.uns['pp']['tkey'] and used
             in `dyn.tl.dynamics` in which when `group` is None, `tkey` will also be used for calculating  1st/2st moment
-            or covariance. We recommend to use hour as the unit of `time`.
-        reset_X: bool (default: `False`)
-            Whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
+            or covariance. We recommend to use hour as the unit of `time`. Defaults to None.
+        reset_X: whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
             critical functionality of dynamo is about visualizing RNA velocity vector flows which requires proper data
             into which the high dimensional RNA velocity vectors will be projected.
             (1) For `kinetics` experiment, we recommend the use of `total` layer as `adata.X`;
             (2) For `degradation/conventional` experiment scRNA-seq, we recommend using `splicing` layer as `adata.X`.
-            Set `reset_X` to `True` to set those default values if you are not sure.
-        splicing_total_layers: bool (default `False`)
-            Whether to also normalize spliced / unspliced layers by size factor from total RNA. Paramter to
-            `recipe_monocle` function.
-        X_total_layers: bool (default `False`)
-            Whether to also normalize adata.X by size factor from total RNA. Paramter to `recipe_monocle` function.
-        n_top_genes: `int` (default: `1000`)
-            How many top genes based on scoring method (specified by sort_by) will be selected as feature genes.
-            Arguments required by the `recipe_monocle` function.
-        keep_filtered_cells: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_filtered_genes: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_raw_layers: `bool` (default: `False`)
-            Whether to keep layers with raw measurements in the returned adata object. Used in `recipe_monocle`.
-       del_2nd_moments: `bool` (default: `None`)
-            Whether to remove second moments or covariances. Default it is `None` rgument used for `dynamics` function.
-         tkey: `str` (default: `time`)
-            The column key for the time label of cells in .obs. Used for  the "kinetic" model.
-            mode  with labeled data. When `group` is None, `tkey` will also be used for calculating  1st/2st moment or
-            covariance. `{tkey}` column must exist in your adata object and indicates the labeling time period.
-            Parameters required for `dynamics` function.
-        ekey: str or None (optional, default None)
-            The dictionary key that corresponds to the gene expression in the layer attribute. By default, ekey and vkey
-            will be automatically detected from the adata object. Parameters required by `cell_velocities`.
-        vkey: str or None (optional, default None)
-            The dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
-            required by `cell_velocities`
-        basis: int (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be `X_spliced_umap`
-            or `X_total_umap`, etc. Parameters required by `cell_velocities`
-        rm_kwargs: `dict` or None (default: `None`)
-            Other Parameters passed into the pp.recipe_monocle function.
+            Set `reset_X` to `True` to set those default values if you are not sure. Defaults to True.
+        X_total_layers: whether to also normalize adata.X by size factor from total RNA. Parameter to `recipe_monocle`
+            function. Defaults to False.
+        splicing_total_layers: whether to also normalize spliced / unspliced layers by size factor from total RNA.
+            Parameter to `recipe_monocle` function. Defaults to False.
+        n_top_genes: the number of top genes based on scoring method (specified by sort_by) will be selected as feature
+            genes. Arguments required by the `recipe_monocle` function. Defaults to 1000.
+        keep_filtered_cells: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_filtered_genes: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_raw_layers: whether to keep layers with raw measurements in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        del_2nd_moments: whether to remove second moments or covariances. Argument used for `dynamics` function. If
+            None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        ekey: the dictionary key that corresponds to the gene expression in the layer attribute. ekey and vkey will be
+            automatically detected from the adata object. Parameters required by `cell_velocities`. Defaults to "M_t".
+        vkey: the dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
+            required by `cell_velocities` Defaults to "velocity_T".
+        basis: the dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be
+            `X_spliced_umap` or `X_total_umap`, etc. Parameters required by `cell_velocities`. Defaults to "umap".
+        rm_kwargs: other kwargs passed into the pp.recipe_monocle function. Defaults to {}.
 
-    Returns
-    -------
+    Raises:
+        Exception: the recipe is only applicable to kinetics experiment datasets with labeling data.
+
+    Returns:
         An updated adata object that went through a proper and typical time-resolved RNA velocity analysis.
     """
-    from ..preprocessing import recipe_monocle
-    from ..preprocessing.utils import detect_experiment_datatype, pca_monocle
+
+    from ..preprocessing import Preprocessor
 
     keep_filtered_cells = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_cells, DynamoAdataConfig.RECIPE_KEEP_FILTERED_CELLS_KEY
     )
     keep_filtered_genes = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_genes, DynamoAdataConfig.RECIPE_KEEP_FILTERED_GENES_KEY
     )
@@ -111,168 +106,143 @@
     if not has_labeling:
         raise Exception(
             "This recipe is only applicable to kinetics experiment datasets that have "
             "labeling data (at least either with `'uu', 'ul', 'su', 'sl'` or `'new', 'total'` "
             "layers."
         )
 
+    # Preprocessing
+    preprocessor = Preprocessor(cell_cycle_score_enable=True)
+    preprocessor.config_monocle_recipe(adata, n_top_genes=n_top_genes)
+    preprocessor.size_factor_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+        }
+    )
+    preprocessor.normalize_by_cells_function_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+            "keep_filtered": keep_filtered_genes,
+            "total_szfactor": "total_Size_Factor",
+        }
+    )
+    preprocessor.filter_cells_by_outliers_kwargs["keep_filtered"] = keep_filtered_cells
+    preprocessor.select_genes_kwargs["keep_filtered"] = keep_filtered_genes
+
+    if reset_X:
+        reset_adata_X(adata, experiment_type="kin", has_labeling=has_labeling, has_splicing=has_splicing)
+    preprocessor.preprocess_adata_monocle(adata=adata, tkey=tkey, experiment_type="kin")
+    if not keep_raw_layers:
+        del_raw_layers(adata)
+
     if has_splicing and has_labeling:
         # new, total (and uu, ul, su, sl if existed) layers will be normalized with size factor calculated with total
         # layers spliced / unspliced layers will be normalized independently.
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="kin",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
+
         tkey = adata.uns["pp"]["tkey"]
         # first calculate moments for labeling data relevant layers using total based connectivity graph
         moments(adata, group=tkey, layers=layers)
 
         # then we want to calculate moments for spliced and unspliced layers based on connectivity graph from spliced
         # data.
         # first get X_spliced based pca embedding
         CM = np.log1p(adata[:, adata.var.use_for_pca].layers["X_spliced"].A)
         cm_genesums = CM.sum(axis=0)
         valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
         valid_ind = np.array(valid_ind).flatten()
 
-        pca_monocle(adata, CM[:, valid_ind], pca_key="X_spliced_pca")
+        pca(adata, CM[:, valid_ind], pca_key="X_spliced_pca")
         # then get neighbors graph based on X_spliced_pca
         neighbors(adata, X_data=adata.obsm["X_spliced_pca"], layer="X_spliced")
         # then normalize neighbors graph so that each row sums up to be 1
         conn = normalize_knn_graph(adata.obsp["connectivities"] > 0)
         # then calculate moments for spliced related layers using spliced based connectivity graph
         moments(adata, conn=conn, layers=["X_spliced", "X_unspliced"])
         # then perform kinetic estimations with properly preprocessed layers for either the labeling or the splicing
         # data
-        dynamics(
-            adata,
-            model="deterministic",
-            est_method="twostep",
-            del_2nd_moments=del_2nd_moments,
-        )
+        dynamics(adata, model="deterministic", est_method="twostep", del_2nd_moments=del_2nd_moments)
         # then perform dimension reduction
         reduceDimension(adata, reduction_method=basis)
         # lastly, project RNA velocity to low dimensional embedding.
         cell_velocities(adata, enforce=True, vkey=vkey, ekey=ekey, basis=basis)
     else:
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="kin",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
-        dynamics(
-            adata,
-            model="deterministic",
-            est_method="twostep",
-            del_2nd_moments=del_2nd_moments,
-        )
+        dynamics(adata, model="deterministic", est_method="twostep", del_2nd_moments=del_2nd_moments)
         reduceDimension(adata, reduction_method=basis)
         cell_velocities(adata, basis=basis)
 
     return adata
 
 
 def recipe_deg_data(
-    adata,
-    tkey=None,
-    reset_X=True,
-    X_total_layers=False,
-    splicing_total_layers=False,
-    n_top_genes=1000,
-    keep_filtered_cells=None,
-    keep_filtered_genes=None,
-    keep_raw_layers=None,
-    del_2nd_moments=True,
-    fraction_for_deg=False,
-    ekey="M_s",
-    vkey="velocity_S",
-    basis="umap",
-    rm_kwargs={},
+    adata: AnnData,
+    tkey: Optional[str] = None,
+    reset_X: bool = True,
+    X_total_layers: bool = False,
+    splicing_total_layers: bool = False,
+    n_top_genes: int = 1000,
+    keep_filtered_cells: Optional[bool] = None,
+    keep_filtered_genes: Optional[bool] = None,
+    keep_raw_layers: Optional[bool] = None,
+    del_2nd_moments: Optional[bool] = True,
+    fraction_for_deg: bool = False,
+    ekey: str = "M_s",
+    vkey: str = "velocity_S",
+    basis: str = "umap",
+    rm_kwargs: Dict[str, Any] = {},
 ):
     """An analysis recipe that properly pre-processes different layers for a degradation experiment with both
-    labeling and splicing data or only labeling . Functions need to be updated.
+    labeling and splicing data or only labeling. Functions need to be updated.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that stores data for the the kinetics experiment, must include `uu, ul, su, sl` four
+    Args:
+        adata: an AnnData object that stores data for the kinetics experiment, must include `uu, ul, su, sl` four
             different layers.
-        tkey: `str` or None (default: None)
-            The column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
+        tkey: the column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
             support for conventional scRNA-seq data). Note that `tkey` will be saved to adata.uns['pp']['tkey'] and used
             in `dyn.tl.dynamics` in which when `group` is None, `tkey` will also be used for calculating  1st/2st moment
-            or covariance. We recommend to use hour as the unit of `time`.
-        reset_X: bool (default: `False`)
-            Whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
+            or covariance. We recommend to use hour as the unit of `time`. Defaults to None.
+        reset_X: whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
             critical functionality of dynamo is about visualizing RNA velocity vector flows which requires proper data
             into which the high dimensional RNA velocity vectors will be projected.
             (1) For `kinetics` experiment, we recommend the use of `total` layer as `adata.X`;
             (2) For `degradation/conventional` experiment scRNA-seq, we recommend using `splicing` layer as `adata.X`.
-            Set `reset_X` to `True` to set those default values if you are not sure.
-        splicing_total_layers: bool (default `False`)
-            Whether to also normalize spliced / unspliced layers by size factor from total RNA. Paramter to
-            `recipe_monocle` function.
-        X_total_layers: bool (default `False`)
-            Whether to also normalize adata.X by size factor from total RNA. Paramter to `recipe_monocle` function.
-        n_top_genes: `int` (default: `1000`)
-            How many top genes based on scoring method (specified by sort_by) will be selected as feature genes.
-            Arguments required by the `recipe_monocle` function.
-        keep_filtered_cells: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_filtered_genes: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_raw_layers: `bool` (default: `False`)
-            Whether to keep layers with raw measurements in the returned adata object. Used in `recipe_monocle`.
-       del_2nd_moments: `bool` (default: `None`)
-            Whether to remove second moments or covariances. Default it is `None` rgument used for `dynamics` function.
-         fraction_for_deg: `bool` (default: `False`)
-            Whether to use the fraction of labeled RNA instead of the raw labeled RNA to estimate the degradation parameter.
-        tkey: `str` (default: `time`)
-            The column key for the time label of cells in .obs. Used for  the "kinetic" model.
-            mode  with labeled data. When `group` is None, `tkey` will also be used for calculating  1st/2st moment or
-            covariance. `{tkey}` column must exist in your adata object and indicates the labeling time period.
-            Parameters required for `dynamics` function.
-        ekey: str or None (optional, default None)
-            The dictionary key that corresponds to the gene expression in the layer attribute. By default, ekey and vkey
-            will be automatically detected from the adata object. Parameters required by `cell_velocities`.
-        vkey: str or None (optional, default None)
-            The dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
-            required by `cell_velocities`
-        basis: int (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be `X_spliced_umap`
-            or `X_total_umap`, etc. Parameters required by `cell_velocities`
-        rm_kwargs: `dict` or None (default: `None`)
-            Other Parameters passed into the pp.recipe_monocle function.
+            Set `reset_X` to `True` to set those default values if you are not sure. Defaults to True.
+        X_total_layers: whether to also normalize adata.X by size factor from total RNA. Parameter to `recipe_monocle`
+            function. Defaults to False.
+        splicing_total_layers: whether to also normalize spliced / unspliced layers by size factor from total RNA.
+            Parameter to `recipe_monocle` function. Defaults to False.
+        n_top_genes: the number of top genes based on scoring method (specified by sort_by) will be selected as feature
+            genes. Arguments required by the `recipe_monocle` function. Defaults to 1000.
+        keep_filtered_cells: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_filtered_genes: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_raw_layers: whether to keep layers with raw measurements in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        del_2nd_moments: whether to remove second moments or covariances. Argument used for `dynamics` function. If
+            None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        fraction_for_deg: whether to use the fraction of labeled RNA instead of the raw labeled RNA to estimate the
+            degradation parameter. Defaults to False.
+        ekey: the dictionary key that corresponds to the gene expression in the layer attribute. ekey and vkey will be
+            automatically detected from the adata object. Parameters required by `cell_velocities`. Defaults to "M_s".
+        vkey: the dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
+            required by `cell_velocities` Defaults to "velocity_S".
+        basis: the dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be
+            `X_spliced_umap` or `X_total_umap`, etc. Parameters required by `cell_velocities`. Defaults to "umap".
+        rm_kwargs: other kwargs passed into the pp.recipe_monocle function. Defaults to {}.
+
+    Raises:
+        Exception: the recipe is only applicable to kinetics experiment datasets with labeling data.
 
-    Returns
-    -------
+    Returns:
         An updated adata object that went through a proper and typical time-resolved RNA velocity analysis.
     """
 
-    from ..preprocessing import recipe_monocle
-    from ..preprocessing.utils import detect_experiment_datatype, pca_monocle
+    from ..preprocessing import Preprocessor
 
     keep_filtered_cells = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_cells, DynamoAdataConfig.RECIPE_KEEP_FILTERED_CELLS_KEY
     )
     keep_filtered_genes = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_genes, DynamoAdataConfig.RECIPE_KEEP_FILTERED_GENES_KEY
     )
@@ -290,43 +260,53 @@
     if not has_labeling:
         raise Exception(
             "This recipe is only applicable to kinetics experiment datasets that have "
             "labeling data (at least either with `'uu', 'ul', 'su', 'sl'` or `'new', 'total'` "
             "layers."
         )
 
+    preprocessor = Preprocessor()
+    preprocessor.config_monocle_recipe(adata, n_top_genes=n_top_genes)
+    preprocessor.size_factor_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+        }
+    )
+    preprocessor.normalize_by_cells_function_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+            "keep_filtered": keep_filtered_genes,
+            "total_szfactor": "total_Size_Factor",
+        }
+    )
+    preprocessor.filter_cells_by_outliers_kwargs["keep_filtered"] = keep_filtered_cells
+    preprocessor.select_genes_kwargs["keep_filtered"] = keep_filtered_genes
+
+    if reset_X:
+        reset_adata_X(adata, experiment_type="deg", has_labeling=has_labeling, has_splicing=has_splicing)
+    preprocessor.preprocess_adata_monocle(adata=adata, tkey=tkey, experiment_type="deg")
+    if not keep_raw_layers:
+        del_raw_layers(adata)
+
     if has_splicing and has_labeling:
         # new, total (and uu, ul, su, sl if existed) layers will be normalized with size factor calculated with total
         # layers spliced / unspliced layers will be normalized independently.
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="deg",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
-
         tkey = adata.uns["pp"]["tkey"]
         # first calculate moments for spliced related layers using spliced based connectivity graph
         moments(adata, layers=["X_spliced", "X_unspliced"])
 
         # then calculate moments for labeling data relevant layers using total based connectivity graph
         # first get X_total based pca embedding
         CM = np.log1p(adata[:, adata.var.use_for_pca].layers["X_total"].A)
         cm_genesums = CM.sum(axis=0)
         valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
         valid_ind = np.array(valid_ind).flatten()
-        pca_monocle(adata, CM[:, valid_ind], pca_key="X_total_pca")
+        pca(adata, CM[:, valid_ind], pca_key="X_total_pca")
         # then get neighbors graph based on X_spliced_pca
         neighbors(adata, X_data=adata.obsm["X_total_pca"], layer="X_total")
         # then normalize neighbors graph so that each row sums up to be 1
         conn = normalize_knn_graph(adata.obsp["connectivities"] > 0)
         moments(adata, conn=conn, group=tkey, layers=layers)
 
         # then perform kinetic estimations with properly preprocessed layers for either the labeling or the splicing
@@ -351,114 +331,82 @@
                 enforce=True,
                 vkey=vkey,
                 ekey=ekey,
                 basis=basis,
             )
 
     else:
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="deg",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
-        dynamics(
-            adata,
-            model="deterministic",
-            del_2nd_moments=del_2nd_moments,
-            fraction_for_deg=fraction_for_deg,
-        )
+        dynamics(adata, model="deterministic", del_2nd_moments=del_2nd_moments, fraction_for_deg=fraction_for_deg)
         reduceDimension(adata, reduction_method=basis)
 
     return adata
 
 
 def recipe_mix_kin_deg_data(
-    adata,
-    tkey=None,
-    reset_X=True,
-    X_total_layers=False,
-    splicing_total_layers=False,
-    n_top_genes=1000,
-    keep_filtered_cells=None,
-    keep_filtered_genes=None,
-    keep_raw_layers=None,
-    del_2nd_moments=None,
-    ekey="M_t",
-    vkey="velocity_T",
-    basis="umap",
-    rm_kwargs={},
+    adata: AnnData,
+    tkey: Optional[str] = None,
+    reset_X: bool = True,
+    X_total_layers: bool = False,
+    splicing_total_layers: bool = False,
+    n_top_genes: int = 1000,
+    keep_filtered_cells: Optional[bool] = None,
+    keep_filtered_genes: Optional[bool] = None,
+    keep_raw_layers: Optional[bool] = None,
+    del_2nd_moments: Optional[bool] = None,
+    ekey: str = "M_t",
+    vkey: str = "velocity_T",
+    basis: str = "umap",
+    rm_kwargs: Dict[str, Any] = {},
 ):
-    """An analysis recipe that properly pre-processes different layers for an mixture kinetics and degradation
+    """An analysis recipe that properly pre-processes different layers for a mixture kinetics and degradation
     experiment with both labeling and splicing or only labeling data.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that stores data for the the kinetics experiment, must include `uu, ul, su, sl` four
+    Args:
+        adata: an AnnData object that stores data for the kinetics experiment, must include `uu, ul, su, sl` four
             different layers.
-        tkey: `str` or None (default: None)
-            The column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
+        tkey: the column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
             support for conventional scRNA-seq data). Note that `tkey` will be saved to adata.uns['pp']['tkey'] and used
             in `dyn.tl.dynamics` in which when `group` is None, `tkey` will also be used for calculating  1st/2st moment
-            or covariance. We recommend to use hour as the unit of `time`.
-        reset_X: bool (default: `False`)
-            Whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
+            or covariance. We recommend to use hour as the unit of `time`. Defaults to None.
+        reset_X: whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
             critical functionality of dynamo is about visualizing RNA velocity vector flows which requires proper data
             into which the high dimensional RNA velocity vectors will be projected.
             (1) For `kinetics` experiment, we recommend the use of `total` layer as `adata.X`;
             (2) For `degradation/conventional` experiment scRNA-seq, we recommend using `splicing` layer as `adata.X`.
-            Set `reset_X` to `True` to set those default values if you are not sure.
-        splicing_total_layers: bool (default `False`)
-            Whether to also normalize spliced / unspliced layers by size factor from total RNA. Paramter to
-            `recipe_monocle` function.
-        X_total_layers: bool (default `False`)
-            Whether to also normalize adata.X by size factor from total RNA. Paramter to `recipe_monocle` function.
-        n_top_genes: `int` (default: `1000`)
-            How many top genes based on scoring method (specified by sort_by) will be selected as feature genes.
-            Arguments required by the `recipe_monocle` function.
-        keep_filtered_cells: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_filtered_genes: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_raw_layers: `bool` (default: `False`)
-            Whether to keep layers with raw measurements in the returned adata object. Used in `recipe_monocle`.
-       del_2nd_moments: `bool` (default: `None`)
-            Whether to remove second moments or covariances. Default it is `None` rgument used for `dynamics` function.
-         tkey: `str` (default: `time`)
-            The column key for the time label of cells in .obs. Used for  the "kinetic" model.
-            mode  with labeled data. When `group` is None, `tkey` will also be used for calculating  1st/2st moment or
-            covariance. `{tkey}` column must exist in your adata object and indicates the labeling time period.
-            Parameters required for `dynamics` function.
-        ekey: str or None (optional, default None)
-            The dictionary key that corresponds to the gene expression in the layer attribute. By default, ekey and vkey
-            will be automatically detected from the adata object. Parameters required by `cell_velocities`.
-        vkey: str or None (optional, default None)
-            The dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
-            required by `cell_velocities`
-        basis: int (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be `X_spliced_umap`
-            or `X_total_umap`, etc. Parameters required by `cell_velocities`
-        rm_kwargs: `dict` or None (default: `None`)
-            Other Parameters passed into the pp.recipe_monocle function.
+            Set `reset_X` to `True` to set those default values if you are not sure. Defaults to True.
+        X_total_layers: whether to also normalize adata.X by size factor from total RNA. Parameter to `recipe_monocle`
+            function. Defaults to False.
+        splicing_total_layers: whether to also normalize spliced / unspliced layers by size factor from total RNA.
+            Parameter to `recipe_monocle` function. Defaults to False.
+        n_top_genes: the number of top genes based on scoring method (specified by sort_by) will be selected as feature
+            genes. Arguments required by the `recipe_monocle` function. Defaults to 1000.
+        keep_filtered_cells: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_filtered_genes: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_raw_layers: whether to keep layers with raw measurements in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        del_2nd_moments: whether to remove second moments or covariances. Argument used for `dynamics` function. If
+            None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        ekey: the dictionary key that corresponds to the gene expression in the layer attribute. ekey and vkey will be
+            automatically detected from the adata object. Parameters required by `cell_velocities`. Defaults to "M_t".
+        vkey: the dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
+            required by `cell_velocities` Defaults to "velocity_T".
+        basis: the dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be
+            `X_spliced_umap` or `X_total_umap`, etc. Parameters required by `cell_velocities`. Defaults to "umap".
+        rm_kwargs: other kwargs passed into the pp.recipe_monocle function. Defaults to {}.
+
+    Raises:
+        Exception: the recipe is only applicable to kinetics experiment datasets with labeling data.
 
-    Returns
-    -------
+    Returns:
         An updated adata object that went through a proper and typical time-resolved RNA velocity analysis.
     """
-    from ..preprocessing import recipe_monocle
-    from ..preprocessing.utils import detect_experiment_datatype, pca_monocle
+
+    from ..preprocessing import Preprocessor
 
     keep_filtered_cells = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_cells, DynamoAdataConfig.RECIPE_KEEP_FILTERED_CELLS_KEY
     )
     keep_filtered_genes = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_genes, DynamoAdataConfig.RECIPE_KEEP_FILTERED_GENES_KEY
     )
@@ -479,44 +427,56 @@
     if not has_labeling:
         raise Exception(
             "This recipe is only applicable to kinetics experiment datasets that have "
             "labeling data (at least either with `'uu', 'ul', 'su', 'sl'` or `'new', 'total'` "
             "layers."
         )
 
+    # Preprocessing
+    preprocessor = Preprocessor()
+    preprocessor.config_monocle_recipe(adata, n_top_genes=n_top_genes)
+    preprocessor.size_factor_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+        }
+    )
+    preprocessor.normalize_by_cells_function_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+            "keep_filtered": keep_filtered_genes,
+            "total_szfactor": "total_Size_Factor",
+        }
+    )
+    preprocessor.filter_cells_by_outliers_kwargs["keep_filtered"] = keep_filtered_cells
+    preprocessor.select_genes_kwargs["keep_filtered"] = keep_filtered_genes
+
+    if reset_X:
+        reset_adata_X(adata, experiment_type="mix_pulse_chase", has_labeling=has_labeling, has_splicing=has_splicing)
+    preprocessor.preprocess_adata_monocle(adata=adata, tkey=tkey, experiment_type="mix_pulse_chase")
+    if not keep_raw_layers:
+        del_raw_layers(adata)
+
     if has_splicing and has_labeling:
         # new, total (and uu, ul, su, sl if existed) layers will be normalized with size factor calculated with total
         # layers spliced / unspliced layers will be normalized independently.
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="mix_pulse_chase",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
         tkey = adata.uns["pp"]["tkey"]
         # first calculate moments for labeling data relevant layers using total based connectivity graph
         moments(adata, group=tkey, layers=layers)
 
         # then we want to calculate moments for spliced and unspliced layers based on connectivity graph from spliced
         # data.
         # first get X_spliced based pca embedding
         CM = np.log1p(adata[:, adata.var.use_for_pca].layers["X_spliced"].A)
         cm_genesums = CM.sum(axis=0)
         valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
         valid_ind = np.array(valid_ind).flatten()
 
-        pca_monocle(adata, CM[:, valid_ind], pca_key="X_spliced_pca")
+        pca(adata, CM[:, valid_ind], pca_key="X_spliced_pca")
         # then get neighbors graph based on X_spliced_pca
         neighbors(adata, X_data=adata.obsm["X_spliced_pca"], layer="X_spliced")
         # then normalize neighbors graph so that each row sums up to be 1
         conn = normalize_knn_graph(adata.obsp["connectivities"] > 0)
         # then calculate moments for spliced related layers using spliced based connectivity graph
         moments(adata, conn=conn, layers=["X_spliced", "X_unspliced"])
         # then perform kinetic estimations with properly preprocessed layers for either the labeling or the splicing
@@ -528,119 +488,92 @@
             del_2nd_moments=del_2nd_moments,
         )
         # then perform dimension reduction
         reduceDimension(adata, reduction_method=basis)
         # lastly, project RNA velocity to low dimensional embedding.
         cell_velocities(adata, enforce=True, vkey=vkey, ekey=ekey, basis=basis)
     else:
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="mix_pulse_chase",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
         dynamics(
             adata,
             model="deterministic",
             est_method="twostep",
             del_2nd_moments=del_2nd_moments,
         )
         reduceDimension(adata, reduction_method=basis)
         cell_velocities(adata, enforce=True, vkey=vkey, ekey=ekey, basis=basis)
 
     return adata
 
 
 # support using just spliced/unspliced/new/total 4 layers, as well as uu, ul, su, sl layers
 def recipe_one_shot_data(
-    adata,
-    tkey=None,
-    reset_X=True,
-    X_total_layers=False,
-    splicing_total_layers=False,
-    n_top_genes=1000,
-    keep_filtered_cells=None,
-    keep_filtered_genes=None,
-    keep_raw_layers=None,
-    one_shot_method="sci-fate",
-    del_2nd_moments=None,
-    ekey="M_t",
-    vkey="velocity_T",
-    basis="umap",
-    rm_kwargs={},
-):
-    """An analysis recipe that properly pre-processes different layers for an one-shot experiment with both labeling and
+    adata: AnnData,
+    tkey: Optional[str] = None,
+    reset_X: bool = True,
+    X_total_layers: bool = False,
+    splicing_total_layers: bool = False,
+    n_top_genes: int = 1000,
+    keep_filtered_cells: Optional[bool] = None,
+    keep_filtered_genes: Optional[bool] = None,
+    keep_raw_layers: Optional[bool] = None,
+    one_shot_method: str = "sci-fate",
+    del_2nd_moments: Optional[bool] = None,
+    ekey: str = "M_t",
+    vkey: str = "velocity_T",
+    basis: str = "umap",
+    rm_kwargs: Dict[str, Any] = {},
+) -> AnnData:
+    """An analysis recipe that properly pre-processes different layers for a one-shot experiment with both labeling and
     splicing data.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that stores data for the the kinetics experiment, must include `uu, ul, su, sl` four
+    Args:
+        adata: AnnData object that stores data for the kinetics experiment, must include `uu, ul, su, sl` four
             different layers.
-        tkey: `str` or None (default: None)
-            The column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
+        tkey: the column key for the labeling time  of cells in .obs. Used for labeling based scRNA-seq data (will also
             support for conventional scRNA-seq data). Note that `tkey` will be saved to adata.uns['pp']['tkey'] and used
             in `dyn.tl.dynamics` in which when `group` is None, `tkey` will also be used for calculating  1st/2st moment
-            or covariance. We recommend to use hour as the unit of `time`.
-        reset_X: bool (default: `False`)
-            Whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
+            or covariance. We recommend to use hour as the unit of `time`. Defaults to None.
+        reset_X: whether do you want to let dynamo reset `adata.X` data based on layers stored in your experiment. One
             critical functionality of dynamo is about visualizing RNA velocity vector flows which requires proper data
             into which the high dimensional RNA velocity vectors will be projected.
             (1) For `kinetics` experiment, we recommend the use of `total` layer as `adata.X`;
             (2) For `degradation/conventional` experiment scRNA-seq, we recommend using `splicing` layer as `adata.X`.
-            Set `reset_X` to `True` to set those default values if you are not sure.
-        splicing_total_layers: bool (default `False`)
-            Whether to also normalize spliced / unspliced layers by size factor from total RNA. Paramter to
-            `recipe_monocle` function.
-        X_total_layers: bool (default `False`)
-            Whether to also normalize adata.X by size factor from total RNA. Paramter to `recipe_monocle` function.
-        n_top_genes: `int` (default: `1000`)
-            How many top genes based on scoring method (specified by sort_by) will be selected as feature genes.
-            Arguments required by the `recipe_monocle` function.
-        keep_filtered_cells: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_filtered_genes: `bool` (default: `False`)
-            Whether to keep genes that don't pass the filtering in the returned adata object. Used in `recipe_monocle`.
-        keep_raw_layers: `bool` (default: `False`)
-            Whether to keep layers with raw measurements in the returned adata object. Used in `recipe_monocle`.
-        one_shot_method: `str` (default: `sci-fate`)
-            The method to use for calculate the absolute labeling and splicing velocity for the one-shot data of use.
-        del_2nd_moments: `bool` (default: `None`)
-            Whether to remove second moments or covariances. Default it is `None` rgument used for `dynamics` function.
-        tkey: `str` (default: `time`)
-            The column key for the time label of cells in .obs. Used for  the "kinetic" model.
-            mode  with labeled data. When `group` is None, `tkey` will also be used for calculating  1st/2st moment or
-            covariance. `{tkey}` column must exist in your adata object and indicates the labeling time period.
-            Parameters required for `dynamics` function.
-        ekey: str or None (optional, default None)
-            The dictionary key that corresponds to the gene expression in the layer attribute. By default, ekey and vkey
-            will be automatically detected from the adata object. Parameters required by `cell_velocities`.
-        vkey: str or None (optional, default None)
-            The dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
-            required by `cell_velocities`
-        basis: int (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be `X_spliced_umap`
-            or `X_total_umap`, etc. Parameters required by `cell_velocities`
-        rm_kwargs: `dict` or None (default: `None`)
-            Other Parameters passed into the pp.recipe_monocle function.
+            Set `reset_X` to `True` to set those default values if you are not sure. Defaults to True.
+        X_total_layers: whether to also normalize adata.X by size factor from total RNA. Parameter to `recipe_monocle`
+            function. Defaults to False.
+        splicing_total_layers: whether to also normalize spliced / unspliced layers by size factor from total RNA.
+            Parameter to `recipe_monocle` function. Defaults to False.
+        n_top_genes: the number of top genes based on scoring method (specified by sort_by) will be selected as feature
+            genes. Arguments required by the `recipe_monocle` function. Defaults to 1000.
+        keep_filtered_cells: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_filtered_genes: whether to keep genes that don't pass the filtering in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        keep_raw_layers: whether to keep layers with raw measurements in the returned adata object. Used in
+            `recipe_monocle`. If None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        one_shot_method: the method to use for calculate the absolute labeling and splicing velocity for the one-shot
+            data of use. Defaults to "sci-fate".
+        del_2nd_moments: whether to remove second moments or covariances. Argument used for `dynamics` function. If
+            None, would be set according to `DynamoAdataConfig`. Defaults to None.
+        ekey: the dictionary key that corresponds to the gene expression in the layer attribute. ekey and vkey will be
+            automatically detected from the adata object. Parameters required by `cell_velocities`. Defaults to "M_t".
+        vkey: the dictionary key that corresponds to the estimated velocity values in the layers attribute. Parameters
+            required by `cell_velocities` Defaults to "velocity_T".
+        basis: the dictionary key that corresponds to the reduced dimension in `.obsm` attribute. Can be
+            `X_spliced_umap` or `X_total_umap`, etc. Parameters required by `cell_velocities`. Defaults to "umap".
+        rm_kwargs: other kwargs passed into the pp.recipe_monocle function. Defaults to {}.
 
-    Returns
-    -------
+    Raises:
+        Exception: the recipe is only applicable to kinetics experiment datasets with labeling data.
+
+    Returns:
         An updated adata object that went through a proper and typical time-resolved RNA velocity analysis.
     """
-    from ..preprocessing import recipe_monocle
-    from ..preprocessing.utils import detect_experiment_datatype, pca_monocle
+
+    from ..preprocessing import Preprocessor
 
     keep_filtered_cells = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_cells, DynamoAdataConfig.RECIPE_KEEP_FILTERED_CELLS_KEY
     )
     keep_filtered_genes = DynamoAdataConfig.use_default_var_if_none(
         keep_filtered_genes, DynamoAdataConfig.RECIPE_KEEP_FILTERED_GENES_KEY
     )
@@ -661,44 +594,56 @@
     if not has_labeling:
         raise Exception(
             "This recipe is only applicable to kinetics experiment datasets that have "
             "labeling data (at least either with `'uu', 'ul', 'su', 'sl'` or `'new', 'total'` "
             "layers."
         )
 
+    # Preprocessing
+    preprocessor = Preprocessor()
+    preprocessor.config_monocle_recipe(adata, n_top_genes=n_top_genes)
+    preprocessor.size_factor_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+        }
+    )
+    preprocessor.normalize_by_cells_function_kwargs.update(
+        {
+            "X_total_layers": X_total_layers,
+            "splicing_total_layers": splicing_total_layers,
+            "keep_filtered": keep_filtered_genes,
+            "total_szfactor": "total_Size_Factor",
+        }
+    )
+    preprocessor.filter_cells_by_outliers_kwargs["keep_filtered"] = keep_filtered_cells
+    preprocessor.select_genes_kwargs["keep_filtered"] = keep_filtered_genes
+
+    if reset_X:
+        reset_adata_X(adata, experiment_type="one-shot", has_labeling=has_labeling, has_splicing=has_splicing)
+    preprocessor.preprocess_adata_monocle(adata=adata, tkey=tkey, experiment_type="one-shot")
+    if not keep_raw_layers:
+        del_raw_layers(adata)
+
     if has_splicing and has_labeling:
         # new, total (and uu, ul, su, sl if existed) layers will be normalized with size factor calculated with total
         # layers spliced / unspliced layers will be normalized independently.
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="one-shot",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
         tkey = adata.uns["pp"]["tkey"]
         # first calculate moments for labeling data relevant layers using total based connectivity graph
         moments(adata, group=tkey, layers=layers)
 
         # then we want to calculate moments for spliced and unspliced layers based on connectivity graph from spliced
         # data.
         # first get X_spliced based pca embedding
         CM = np.log1p(adata[:, adata.var.use_for_pca].layers["X_spliced"].A)
         cm_genesums = CM.sum(axis=0)
         valid_ind = np.logical_and(np.isfinite(cm_genesums), cm_genesums != 0)
         valid_ind = np.array(valid_ind).flatten()
 
-        pca_monocle(adata, CM[:, valid_ind], pca_key="X_spliced_pca")
+        pca(adata, CM[:, valid_ind], pca_key="X_spliced_pca")
         # then get neighbors graph based on X_spliced_pca
         neighbors(adata, X_data=adata.obsm["X_spliced_pca"], layer="X_spliced")
         # then normalize neighbors graph so that each row sums up to be 1
         conn = normalize_knn_graph(adata.obsp["connectivities"] > 0)
         # then calculate moments for spliced related layers using spliced based connectivity graph
         moments(adata, conn=conn, layers=["X_spliced", "X_unspliced"])
         # then perform kinetic estimations with properly preprocessed layers for either the labeling or the splicing
@@ -710,72 +655,55 @@
             del_2nd_moments=del_2nd_moments,
         )
         # then perform dimension reduction
         reduceDimension(adata, reduction_method=basis)
         # lastly, project RNA velocity to low dimensional embedding.
         cell_velocities(adata, enforce=True, vkey=vkey, ekey=ekey, basis=basis)
     else:
-        recipe_monocle(
-            adata,
-            tkey=tkey,
-            experiment_type="one-shot",
-            reset_X=reset_X,
-            X_total_layers=X_total_layers,
-            splicing_total_layers=splicing_total_layers,
-            n_top_genes=n_top_genes,
-            total_layers=True,
-            keep_filtered_cells=keep_filtered_cells,
-            keep_filtered_genes=keep_filtered_genes,
-            keep_raw_layers=keep_raw_layers,
-            **rm_kwargs,
-        )
         dynamics(
             adata,
             model="deterministic",
             one_shot_method=one_shot_method,
             del_2nd_moments=del_2nd_moments,
         )
         reduceDimension(adata, reduction_method=basis)
         cell_velocities(adata, enforce=True, vkey=vkey, ekey=ekey, basis=basis)
 
     return adata
 
 
 def velocity_N(
-    adata,
-    group=None,
-    recalculate_pca=True,
-    recalculate_umap=True,
-    del_2nd_moments=None,
-):
-    """use new RNA based pca, umap, for velocity calculation and projection for kinetics or one-shot experiment.
+    adata: AnnData,
+    group: Optional[str] = None,
+    recalculate_pca: bool = True,
+    recalculate_umap: bool = True,
+    del_2nd_moments: Optional[bool] = None,
+) -> None:
+    """Use new RNA based pca, umap, for velocity calculation and projection for kinetics or one-shot experiment.
+
+    The AnnData object will be updated inplace with the low dimensional (umap or pca) velocity projections with the new
+    RNA or pca based RNA velocities.
 
     Note that currently velocity_N function only considers labeling data and removes splicing data if they exist.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that stores data for the the kinetics or one-shot experiment, must include `X_new, X_total`
-            layers.
-        group: `str` or None (default: None)
-            The cell group that will be used to calculate velocity in each separate group. This is useful if your data
-            comes from different labeling condition, etc.
-        recalculate_pca: `bool` (default: True)
-            Whether to recalculate pca with the new RNA data. If setting to be False, you need to make sure the pca is
-            already generated via new RNA.
-        recalculate_umap: `bool` (default: True)
-            Whether to recalculate umap with the new RNA data. If setting to be False, you need to make sure the umap is
-            already generated via new RNA.
-        del_2nd_moments: `None` or `bool`
-            Whether to remove second moments or covariances. Default it is `None` rgument used for `dynamics` function.
-
-    Returns
-    -------
-        Nothing but the adata object is updated with the low dimensional (umap or pca) velocity projections with the
-        new RNA or pca based RNA velocities.
+    Args:
+        adata: AnnData object that stores data for the the kinetics or one-shot experiment, must include `X_new`,
+            `X_total` layers.
+        group: the cell group that will be used to calculate velocity in each separate group. This is useful if your
+            data comes from different labeling condition, etc. Defaults to None.
+        recalculate_pca: whether to recalculate pca with the new RNA data. If setting to be False, you need to make sure
+            the pca is already generated via new RNA. Defaults to True.
+        recalculate_umap: whether to recalculate umap with the new RNA data. If setting to be False, you need to make
+            sure the umap is already generated via new RNA. Defaults to True.
+        del_2nd_moments: whether to remove second moments or covariances. If None, would be set according to
+            `DynamoAdataConfig`. Defaults to None.
+
+    Raises:
+        Exception: `X_new` or `X_total` layer unavailable.
+        Exception: experiment type is not supported.
     """
 
     del_2nd_moments = DynamoAdataConfig.use_default_var_if_none(
         del_2nd_moments, DynamoAdataConfig.RECIPE_DEL_2ND_MOMENTS_KEY
     )
 
     var_columns = adata.var.columns
@@ -846,15 +774,15 @@
                 "gamma_k",
             ]:
                 if i + j in var_columns:
                     del adata.var[i + j]
 
     # now let us first run pca with new RNA
     if recalculate_pca:
-        pca_monocle(adata, np.log1p(adata[:, adata.var.use_for_pca].layers["X_new"]), pca_key="X_pca")
+        pca(adata, np.log1p(adata[:, adata.var.use_for_pca].layers["X_new"]), pca_key="X_pca")
 
     # if there are unspliced / spliced data, delete them for now:
     for i in ["spliced", "unspliced", "X_spliced", "X_unspliced"]:
         if i in layer_keys:
             del adata.layers[i]
 
     # now redo the RNA velocity analysis with moments generated with pca space of new RNA
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/sampling.py` & `dynamo-release-1.3.0/dynamo/tools/sampling.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,32 +1,64 @@
-from typing import Callable, Union
+from typing import List, Optional, Union
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import numpy as np
 from scipy.cluster.vq import kmeans2
 from sklearn.neighbors import NearestNeighbors
 
 from ..dynamo_logger import LoggerManager
 from .utils import nearest_neighbors, timeit
 
 
 class TRNET:
-    def __init__(self, n_nodes, X, seed=19491001):
+    """Class for topology representing network sampling.
+
+    Attributes:
+        n_nodes: the number of nodes in the graph.
+        n_dim: the dimensions of the array to be sub-sampled.
+        X: coordinates associated to each element in original array to be sub-sampled.
+        seed: the randomization seed.
+        W: the sample graph.
+    """
+
+    def __init__(self, n_nodes: int, X: np.ndarray, seed: int = 19491001) -> None:
+        """Initialize the TRNET object.
+
+        Args:
+            n_nodes: the number of nodes in the graph.
+            X: coordinates associated to each element in original array to be sub-sampled.
+            seed: the randomization seed. Defaults to 19491001.
+        """
+
         self.n_nodes = n_nodes
         self.n_dims = X.shape[1]
         self.X = X
         self.seed = seed
         self.W = self.draw_sample(self.n_nodes)  # initialize the positions of nodes
 
-    def draw_sample(self, n_samples):
+    def draw_sample(self, n_samples: int) -> np.ndarray:
+        """Initialize the positions of nodes.
+
+        Args:
+            n_samples: the number of nodes.
+
+        Returns:
+            The initial positions of nodes.
+        """
+
         np.random.seed(self.seed)
 
         idx = np.random.randint(0, self.X.shape[0], n_samples)
         return self.X[idx]
 
-    def runOnce(self, p, l, ep, c):
+    def runOnce(self, p: np.ndarray, l: float, ep: float, c: float) -> None:
         # calc the squared distances ||w - p||^2
         D = p - self.W
         sD = np.zeros(self.n_nodes)
         for i in range(self.n_nodes):
             sD[i] = D[i].dot(D[i])
 
         # calc the closeness rank k's
@@ -40,27 +72,39 @@
         else:
             # move nodes whose displacements are larger than the cutoff to accelerate the calculation
             kc = -l * np.log(c / ep)
             idx = K < kc
             K = K[:, None]
             self.W[idx, :] += ep * np.exp(-K[idx] / l) * D[idx, :]
 
-    def run(self, tmax=200, li=0.2, lf=0.01, ei=0.3, ef=0.05, c=0):
+    def run(
+        self, tmax: int = 200, li: float = 0.2, lf: float = 0.01, ei: float = 0.3, ef: float = 0.05, c: float = 0
+    ) -> None:
         tmax = int(tmax * self.n_nodes)
         li = li * self.n_nodes
         P = self.draw_sample(tmax)
         for t in LoggerManager.progress_logger(range(1, tmax + 1), progress_name="Running TRN"):
             # calc the parameters
             tt = t / tmax
             l = li * np.power(lf / li, tt)
             ep = ei * np.power(ef / ei, tt)
             # run once
             self.runOnce(P[t - 1], l, ep, c)
 
-    def run_n_pause(self, k0, k, tmax=200, li=0.2, lf=0.01, ei=0.3, ef=0.05, c=0):
+    def run_n_pause(
+        self,
+        k0: int,
+        k: int,
+        tmax: float = 200,
+        li: float = 0.2,
+        lf: float = 0.01,
+        ei: float = 0.3,
+        ef: float = 0.05,
+        c: int = 0,
+    ) -> None:
         tmax = int(tmax * self.n_nodes)
         li = li * self.n_nodes
         P = self.draw_sample(tmax)
         for t in range(k0, k + 1):
             # calc the parameters
             tt = t / tmax
             l = li * np.power(lf / li, tt)
@@ -69,15 +113,27 @@
             self.runOnce(P[t - 1], l, ep, c)
 
             if t % 1000 == 0:
                 print(str(t) + " steps have been run")
 
 
 @timeit
-def trn(X, n, return_index=True, seed=19491001, **kwargs):
+def trn(X: np.ndarray, n: int, return_index: bool = True, seed: int = 19491001, **kwargs) -> np.ndarray:
+    """Sample method based on topology representing network.
+
+    Args:
+        X: coordinates associated to each element in original array to be sub-sampled.
+        n: the number of samples.
+        return_index: whether to return the indices of the sub-sampled array or the sample graph. Defaults to
+            True.
+        seed: the randomization seed. Defaults to 19491001.
+
+    Returns:
+        The sample graph or the indices of the sub-sampled array.
+    """
     trnet = TRNET(n, X, seed)
     trnet.run(**kwargs)
     if not return_index:
         return trnet.W
     else:
         if X.shape[0] > 200000 and X.shape[1] > 2:
             from pynndescent import NNDescent
@@ -94,52 +150,66 @@
             alg = "ball_tree" if X.shape[1] > 10 else "kd_tree"
             nbrs = NearestNeighbors(n_neighbors=1, algorithm=alg, n_jobs=-1).fit(X)
             _, idx = nbrs.kneighbors(trnet.W)
 
         return idx[:, 0]
 
 
-def sample_by_velocity(V, n, seed=19491001):
+def sample_by_velocity(V: np.ndarray, n: int, seed: int = 19491001) -> np.ndarray:
+    """Sample method based on velocity.
+
+    Args:
+        V: Velocity associated with each element in the sample array.
+        n: the number of samples.
+        seed: the randomization seed. Defaults to 19491001.
+
+    Returns:
+        The sample data array.
+    """
     np.random.seed(seed)
     tmp_V = np.linalg.norm(V, axis=1)
     p = tmp_V / np.sum(tmp_V)
     idx = np.random.choice(np.arange(len(V)), size=n, p=p, replace=False)
     return idx
 
 
-def sample_by_kmeans(X, n, return_index=False):
+def sample_by_kmeans(X: np.ndarray, n: int, return_index: bool = False) -> Optional[np.ndarray]:
+    """Sample method based on kmeans.
+
+    Args:
+        X: coordinates associated to each element in `arr`.
+        n: the number of samples.
+        return_index: whether to return the sample indices. Defaults to False.
+
+    Returns:
+        The sample index array if `return_index` is True.
+    """
     C, _ = kmeans2(X, n)
     nbrs = nearest_neighbors(C, X, k=1).flatten()
 
     if return_index:
         return nbrs
     else:
         X[nbrs]
 
 
-def lhsclassic(n_samples: int, n_dim: int, bounds=None, seed=19491001):
-    """
-    Latin Hypercube Sampling method implemented from PyDOE.
+def lhsclassic(
+    n_samples: int, n_dim: int, bounds: Union[np.ndarray, List[List[float]]] = None, seed: int = 19491001
+) -> np.ndarray:
+    """Latin Hypercube Sampling method implemented from PyDOE.
+
+    Args:
+        n_samples: the number of samples to be generated.
+        n_dim: the number of data dimensions.
+        bounds: n_dim-by-2 matrix where each row specifies the lower and upper bound for the corresponding dimension. If
+            None, it is assumed to be (0, 1) for every dimension. Defaults to None.
+        seed: the randomization seed. Defaults to 19491001.
 
-    Parameters
-    ----------
-        n_samples: int
-            Number of samples to be generated.
-        n_dim: int
-            Number of data dimensions.
-        bounds: None, list, or :class:`~numpy.ndarray`
-            n_dim-by-2 matrix where each row specifies the lower and upper bound for the corresponding dimension.
-            If None, it is assumed to be (0, 1) for every dimension.
-        seed: int
-            Randomization seed.
-
-    Returns
-    -------
-        H: :class:`~numpy.ndarray`
-            The sampled data array (n_samples-by-n_dim).
+    Returns:
+        The sampled data array.
     """
 
     # Generate the intervals
     np.random.seed(seed)
     cut = np.linspace(0, 1, n_samples + 1)
 
     # Fill points uniformly in each interval
@@ -163,47 +233,42 @@
 
     return H
 
 
 def sample(
     arr: Union[list, np.ndarray],
     n: int,
-    method: str = "random",
-    X: Union[np.ndarray, None] = None,
-    V: Union[np.ndarray, None] = None,
+    method: Literal["random", "velocity", "trn", "kmeans"] = "random",
+    X: Optional[np.ndarray] = None,
+    V: Optional[np.ndarray] = None,
     seed: int = 19491001,
     **kwargs,
-):
-    """
-    A collection of various sampling methods.
+) -> np.ndarray:
+    """A collection of various sampling methods.
 
-    Parameters
-    ----------
-        arr: list or :class:`~numpy.ndarray`
-            The array to be subsampled.
-        n: int
-            The number of samples.
-        method: str
-            Sampling method:
+    Args:
+        arr: the array to be sub-sampled.
+        n: the number of samples.
+        method: the method to be used.
             "random": randomly choosing `n` elements from `arr`;
             "velocity": Higher the velocity, higher the chance to be sampled;
             "trn": Topology Representing Network based sampling;
             "kmeans": `n` points that are closest to the kmeans centroids on `X` are chosen.
-        X: None or :class:`~numpy.ndarray`
-            Coordinates associated to each element in `arr`
-        V: None or :class:`~numpy.ndarray`
-            Velocity associated to each element in `arr`
-        seed: int
-            randomization seed
-
-    Returns
-    -------
-        sub_arr: :class:`~numpy.ndarray`
-            The sampled data array.
+            Defaults to "random".
+        X: coordinates associated to each element in `arr`. Defaults to None.
+        V: velocity associated to each element in `arr`. Defaults to None.
+        seed: the randomization seed. Defaults to 19491001.
+
+    Raises:
+        NotImplementedError: `method` is invalid.
+
+    Returns:
+        The sampled data array.
     """
+
     if method == "random":
         np.random.seed(seed)
         sub_arr = np.random.choice(arr, size=n, replace=False)
     elif method == "velocity" and V is not None:
         sub_arr = arr[sample_by_velocity(V=V, n=n, seed=seed, **kwargs)]
     elif method == "trn" and X is not None:
         sub_arr = arr[trn(X=X, n=n, return_index=True, seed=seed, **kwargs)]
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/utils.py` & `dynamo-release-1.3.0/dynamo/tools/utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,21 @@
 import itertools
 import time
 import warnings
 from inspect import signature
-from typing import Union
+from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+
+try:
+    from typing import Literal
+except:
+    from typing_extensions import Literal
 
 import numpy as np
+import numpy.typing as npt
 import pandas as pd
-import scipy.sparse as sp
 from anndata._core.anndata import AnnData
 from anndata._core.views import ArrayView
 from scipy import interpolate
 from scipy import sparse as sp
 from scipy import stats
 from scipy.integrate import odeint
 from scipy.linalg.blas import dgemm
@@ -26,21 +31,30 @@
     main_exception,
     main_info,
     main_info_insert_adata,
     main_info_verbose_timeit,
     main_tqdm,
     main_warning,
 )
-from ..preprocessing.utils import Freeman_Tukey
+from ..preprocessing.transform import _Freeman_Tukey
 from ..utils import areinstance, isarray
 
 
 # ---------------------------------------------------------------------------------------------------
 # others
-def get_mapper(smoothed=True):
+def get_mapper(smoothed: bool = True) -> Dict[str, str]:
+    """Return the mapper for layers depending on whether the data is smoothed.
+
+    Args:
+        smoothed: whether the data is smoothed. Defaults to True.
+
+    Returns:
+        The mapper dictionary for layers.
+    """
+
     mapper = {
         "X_spliced": "M_s" if smoothed else "X_spliced",
         "X_unspliced": "M_u" if smoothed else "X_unspliced",
         "X_new": "M_n" if smoothed else "X_new",
         "X_old": "M_o" if smoothed else "X_old",
         "X_total": "M_t" if smoothed else "X_total",
         "X_uu": "M_uu" if smoothed else "X_uu",
@@ -49,98 +63,170 @@
         "X_sl": "M_sl" if smoothed else "X_sl",
         "X_protein": "M_p" if smoothed else "X_protein",
         "X": "X" if smoothed else "X",
     }
     return mapper
 
 
-def get_mapper_inverse(smoothed=True):
+def get_mapper_inverse(smoothed: bool = True) -> Dict[str, str]:
+    """Return the inverse mapper for layers depending on whether the data is smoothed.
+
+    Args:
+        smoothed: whether the data is smoothed. Defaults to True.
+
+    Returns:
+        The inverse mapper dictionary for layers.
+    """
+
     mapper = get_mapper(smoothed)
 
     return dict([(v, k) for k, v in mapper.items()])
 
 
-def get_finite_inds(X, ax=0):
+def get_finite_inds(X: Union[np.ndarray, sp.csr_matrix], ax: int = 0) -> np.ndarray:
+    """Find the indices of finite elements in an array.
+
+    Args:
+        X: the matrix to be inspected.
+        ax: the axis for indexing. Defaults to 0.
+
+    Returns:
+        The indices of finite elements.
+    """
+
     finite_inds = np.isfinite(X.sum(ax).A1) if sp.issparse(X) else np.isfinite(X.sum(ax))
 
     return finite_inds
 
 
-def get_pd_row_column_idx(df, queries, type="column"):
-    """Find the numeric indices of multiple index/column matches with a vectorized solution using np.searchsorted
-    method. adapted from:
+def get_pd_row_column_idx(
+    df: pd.DataFrame, queries: List[str], type: Literal["column", "row"] = "column"
+) -> np.ndarray:
+    """Find the numeric indices of multiple index/column matches with a vectorized solution using np.searchsorted.
+
+    The function is adapted from:
     https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas
 
-    Parameters
-    ----------
-        df: `pd.DataFrame`
-            Pandas dataframe that will be used for finding indices.
-        queries: `list`
-            List of strings, corresponding to either column names or index of the `df` that will be used for finding
-            indices.
-        type: `{"column", "row:}` (default: "row")
-            The type of the queries / search, either `column` (list of queries are from column names) or "row" (list of
-            queries are from index names).
-
-    Returns
-    -------
-        Indices: `np.ndarray`
-            One dimensional array for the numeric indices that corresponds to the matches of the queries.
+    Args:
+        df: the dataframe to be inspected.
+        queries: a list of either column names or index of the dataframe that will be used for finding indices.
+        type: the type of the queries/search, either `column` (list of queries are from column names) or "row" (list of
+            queries are from index names). Defaults to "column".
+
+    Returns:
+        An one dimensional array for the numeric indices that corresponds to the matches of the queries.
     """
 
     names = df.columns.values if type == "column" else df.index.values if type == "row" else None
     sidx = np.argsort(names)
     Indices = sidx[np.searchsorted(names, queries, sorter=sidx)]
 
     return Indices
 
 
-def update_dict(dict1, dict2):
+def update_dict(dict1: dict, dict2: dict) -> dict:
+    """Update the values of dict 1 with the values of dict 2. The keys of dict 1 would not be modified.
+
+    Args:
+        dict1: the dict to be updated.
+        dict2: the dict to provide new values.
+
+    Returns:
+        The updated dict.
+    """
     dict1.update((k, dict2[k]) for k in dict1.keys() & dict2.keys())
 
     return dict1
 
 
-def update_n_merge_dict(dict1, dict2):
+def update_n_merge_dict(dict1: dict, dict2: dict) -> dict:
+    """Merge two dictionaries.
+
+    For overlapping keys, the values in dict 2 would replace values in dict 1.
+
+    Args:
+        dict1: the dict to be merged into and overwritten.
+        dict2: the dict to be merged.
+
+    Returns:
+        The updated dict.
+    """
+
     dict = {
         **dict1,
         **dict2,
     }  # dict1.update((k, dict2[k]) for k in dict1.keys() | dict2.keys())
 
     return dict
 
 
-def subset_dict_with_key_list(dict, list):
+def subset_dict_with_key_list(dict: dict, list: list) -> dict:
+    """Subset the dict with keys provided.
+
+    Args:
+        dict: the dict to be subset.
+        list: the keys that should be left in dict.
+
+    Returns:
+        The subset dict.
+    """
+
     return {key: value for key, value in dict.items() if key in list}
 
 
-def nearest_neighbors(coord, coords, k=5):
+def nearest_neighbors(coord: np.ndarray, coords: Union[np.ndarray, sp.csr_matrix], k: int = 5) -> np.ndarray:
+    """Find the nearest neighbors in a given space for a given point.
+
+    Args:
+        coord: the point for which nearest neighbors are searched.
+        coords: the space to search neighbors.
+        k: the number of neighbors to be searched. Defaults to 5.
+
+    Returns:
+        The indices of the nearest neighbors.
+    """
+
     nbrs = NearestNeighbors(n_neighbors=k, algorithm="ball_tree").fit(coords)
     _, neighs = nbrs.kneighbors(np.atleast_2d(coord))
     return neighs
 
 
 def k_nearest_neighbors(
-    X,
+    X: np.ndarray,
     k: int,
     exclude_self: bool = True,
     knn_dim: int = 10,
     pynn_num: int = int(2e5),
     pynn_dim: int = 2,
     pynn_rand_state: int = 19491001,
     n_jobs: int = -1,
     return_nbrs: bool = False,
-):
-    """Compute k nearest neighbors on X
+) -> Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, NearestNeighbors]]:
+    """Compute k nearest neighbors for a given space.
 
-    Parameters
-    ----------
-    return_nbrs:
-        returns a neighbor object if this arg is true, by default False. A neighbor object is from k_nearest_neighbors and may be from NNDescent (pynndescent) or NearestNeighbors.
+    Args:
+        X: the space to find nearest neighbors on.
+        k: the number of neighbors to be found, excluding the point itself.
+        exclude_self: whether to exclude the point itself from the result. Defaults to True.
+        knn_dim: the lowest threshold of dimensions of data to use `ball_tree` algorithm. If dimensions of the data is
+            smaller than this value, `kd_tree` algorithm would be used. Defaults to 10.
+        pynn_num: the lowest threshold of features to use NNDescent package. If number of features less than/equal to
+            this value, `sklearn` package would be used. Defaults to int(2e5).
+        pynn_dim: the lowest threshold of dimensions to use NNDescent package. If number of features less than/equal to
+            this value, `sklearn` package would be used. Defaults to 2.
+        pynn_rand_state: the random seed for NNDescent calculation. Defaults to 19491001.
+        n_jobs: number of parallel jobs for NNDescent. -1 means all cores would be used. Defaults to -1.
+        return_nbrs: whether to return the fitted nearest neighbor object. Defaults to False.
+
+    Returns:
+        A tuple (nbrs_idx, dists, [nbrs]), where nbrs_idx contains the indices of nearest neighbors found for each
+        point and dists contains the distances between neighbors and the point. nbrs is the fitted nearest neighbor
+        object and it would be returned only if `return_nbrs` is True.
     """
+
     n, d = np.atleast_2d(X).shape
     if n > int(pynn_num) and d > pynn_dim:
         from pynndescent import NNDescent
 
         nbrs = NNDescent(
             X,
             metric="euclidean",
@@ -159,29 +245,45 @@
         nbrs_idx = nbrs_idx[:, 1:]
         dists = dists[:, 1:]
     if return_nbrs:
         return nbrs_idx, dists, nbrs
     return nbrs_idx, dists
 
 
-def nbrs_to_dists(X, nbrs_idx):
+def nbrs_to_dists(X: np.ndarray, nbrs_idx: np.ndarray) -> List[np.ndarray]:
+    """Calculate the distances between neighbors of a given space.
+
+    Args:
+        X: the space to find nearest neighbors on.
+        nbrs_idx: the indices of nearest neighbors found for each point.
+
+    Returns:
+        The distances between neighbors and the point.
+    """
+
     dists = []
     n = X.shape[0]
     for i in range(n):
         d = X[nbrs_idx[i]] - X[i]
         d = np.linalg.norm(d, axis=1)
         dists.append(d)
     return dists
 
 
-def symmetrize_symmetric_matrix(W):
+def symmetrize_symmetric_matrix(W: Union[np.ndarray, sp.csr_matrix]) -> sp.csr_matrix:
     """
-    symmetrize a supposedly symmetric matrix W, so that W_ij == Wji strictly.
-    returns a csr sparse matrix.
+    Symmetrize a supposedly symmetric matrix W, so that W_ij == Wji strictly.
+
+    Args:
+        W: the matrix supposed to be symmetric.
+
+    Returns:
+        The matrix that is now strictly symmetric.
     """
+
     if not sp.issparse(W):
         W = sp.csr_matrix(W)
 
     _row_inds, _col_inds = W.nonzero()
     _data = W.data.copy()
 
     row_inds = np.hstack((_row_inds, _col_inds))
@@ -189,15 +291,35 @@
     data = np.hstack((_data, _data))
 
     I = np.unique(np.vstack((row_inds, col_inds)).T, axis=0, return_index=True)[1]
     W = sp.csr_matrix((data[I], (row_inds[I], col_inds[I])))
     return W
 
 
-def create_layer(adata, data, layer_key=None, genes=None, cells=None, **kwargs):
+def create_layer(
+    adata: AnnData,
+    data: np.ndarray,
+    layer_key: Optional[str] = None,
+    genes: Optional[np.ndarray] = None,
+    cells: Optional[np.ndarray] = None,
+    **kwargs,
+) -> Optional[np.ndarray]:
+    """Create a new layer with data supplied.
+
+    Args:
+        adata: an AnnData object to insert the layer into.
+        data: the main data of the new layer.
+        layer_key: the key of the layer when gets inserted into the adata object. If None, the layer would be returned.
+            Defaults to None.
+        genes: the genes for the provided data. If None, genes from the adata object would be used. Defaults to None.
+        cells: the cells for the provided data. If None, cells from the adata object would be used. Defaults to None.
+
+    Returns:
+        If the layer key is provided, nothing would be returned. Otherwise, the layer itself would be returned.
+    """
     all_genes = adata.var.index
     if genes is None:
         genes = all_genes
     elif areinstance(genes, np.bool_) or areinstance(genes, bool):
         genes = all_genes[genes]
 
     if cells is None:
@@ -214,33 +336,33 @@
     if layer_key is not None:
         main_info_insert_adata(layer_key, adata_attr="layers")
         adata.layers[layer_key] = new
     else:
         return new
 
 
-def index_gene(adata, arr, genes):
+def index_gene(adata: AnnData, arr: np.ndarray, genes: List[str]) -> np.ndarray:
     """A lightweight method for indexing adata arrays by genes.
-    it is memory efficient especially when `.uns` contains large data.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object
-        arr: :class:`~numpy.ndarray`
-            The array to be indexed.
+    The function is designed to have good memory efficiency especially when `.uns` contains large data.
+
+    Args:
+        adata: an AnnData object.
+        arr: the array to be indexed.
             If 1d, the length of the array has to be equal to `adata.n_vars`.
             If 2d, the second dimension of the array has to be equal to `adata.n_vars`.
-        genes: list
-            A list of gene names or boolean flags for indexing.
+        genes: a list of gene names or boolean flags for indexing.
+
+    Raises:
+        ValueError: gene in `genes` not found in adata.
+        Exception: the lengths of arr does not match the number of genes.
+        Exception: the dimension of arr does not match the number of genes.
 
-    Returns
-    -------
-        :class:`~numpy.ndarray`
-            The indexed array.
+    Returns:
+        The indexed array.
     """
 
     if areinstance(genes, [bool, np.bool_]):
         mask = np.array(genes)
     else:
         all_genes = adata.var_names
         # Note: this mask here is in fact an indices vector!
@@ -259,28 +381,28 @@
     else:
         if arr.shape[1] != adata.n_vars:
             raise Exception("The dimension of the input array does not match the number of genes.")
         else:
             return arr[:, mask]
 
 
-def reserve_minimal_genes_by_gamma_r2(
-    adata: AnnData, var_store_key: str, minimal_gene_num: Union[int, None] = 50
-) -> Union[list, pd.Series, np.array]:
-    """When the sum of `adata.var[var_store_key]` is less than `minimal_gene_num`, select the `minimal_gene_num` genes and save to (update) adata.var[var_store_key].
-
-    Parameters
-    ----------
-        adata:
-        var_store_key:
-        minimal_gene_num: int, optional
-            by default 50
-    Returns
-    -------
-        The data stored in `adata.var[var_store_key]`.
+def reserve_minimal_genes_by_gamma_r2(adata: AnnData, var_store_key: str, minimal_gene_num: int = 50) -> pd.DataFrame:
+    """Select given number of minimal genes.
+
+    Args:
+        adata: an AnnData object.
+        var_store_key: the key in adata.var for the gene count data.
+        minimal_gene_num: the number of minimal genes to select. Defaults to 50.
+
+    Raises:
+        ValueError: `adata.var[var_store_key]` invalid.
+        ValueError: `adata.var[var_store_key]` does not have enough genes with non-nan values.
+
+    Returns:
+        The minimal gene data.
     """
 
     # already satisfy the requirement
     if var_store_key in adata.var.columns and adata.var[var_store_key].sum() >= minimal_gene_num:
         return adata.var[var_store_key]
 
     if var_store_key not in adata.var.columns:
@@ -292,43 +414,49 @@
 
     argsort_result = np.argsort(-np.abs(gamma_r2_not_na))
     adata.var[var_store_key] = False
     adata.var[var_store_key][argsort_result[:minimal_gene_num]] = True
     return adata.var[var_store_key]
 
 
-def select_cell(adata, grp_keys, grps, presel=None, mode="union", output_format="index"):
-    """
-    Select cells based on `grp_keys` in .obs
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object
-        grp_keys: str or list
-            The key(s) in `.obs` to be used for selecting cells.
-            If list, each element is a key in .obs that corresponds to an element in `grps`.
-        grps: str or list
-            The value(s) in `.obs[grp_keys]` to be used for selecting cells.
-            If list, each element is a value that corresponds to an element in `grp_keys`.
-        presel: None, list, or :class:`~numpy.ndarray`
-            An array of indices or mask of pre-selected cells. It will be combined with selected cells specified by
-            `grp_keys` and `grps` according to `mode`.
-        mode: str
+def select_cell(
+    adata: AnnData,
+    grp_keys: Union[str, List[str]],
+    grps: Union[str, List[str]],
+    presel: Optional[np.ndarray] = None,
+    mode: Literal["union", "intersection"] = "union",
+    output_format: Literal["mask", "index"] = "index",
+) -> np.ndarray:
+    """Select cells based on `grep_keys` in .obs.
+
+    Args:
+        adata: an AnnData object.
+        grp_keys: the key(s) in `.obs` to be used for selecting cells. If a list, each element is a key in .obs that
+            corresponds to an element in `grps`.
+        grps: the value(s) in `.obs[grp_keys]` to be used for selecting cells. If a list, each element is a value that
+            corresponds to an element in `grp_keys`.
+        presel: an array of indices or mask of pre-selected cells. It will be combined with selected cells specified by
+            `grp_keys` and `grps` according to `mode`. Defaults to None.
+        mode: the mode to select cells.
             "union" - the selected cells are the union of the groups specified in `grp_keys` and `grps`;
             "intersection" - the selected cells are the intersection of the groups specified in `grp_keys` and `grps`.
-        output_format: str
+            Defaults to "union".
+        output_format: whether to output a mask of selection or selected items' indices.
             "index" - returns a list of indices of selected cells;
-            "mask" - returns an array of booleans.
+            "mask" - returns an array of booleans. Defaults to "index".
+
+    Raises:
+        NotImplementedError: `mode` is invalid.
+        Exception: `grp_keys` has key that is not in .obs.
+        NotImplementedError: `output_format` is invalid.
 
-    Returns
-    -------
-        list or :class:`~numpy.ndarray`
-            The cell index or mask array.
+    Returns:
+        A mask of selection or selected items' indices.
     """
+
     if type(grp_keys) is str:
         grp_keys = [grp_keys]
     if not isarray(grps):
         grps = [grps]
 
     if len(grp_keys) == 1 and len(grps) > 1:
         grp_keys = np.repeat(grp_keys, len(grps))
@@ -369,85 +497,121 @@
         pass
     else:
         raise NotImplementedError(f"The output format `{output_format}` is not supported.")
 
     return cell_idx
 
 
-def flatten(arr):
+def flatten(arr: Union[pd.Series, sp.csr_matrix, np.ndarray]) -> np.ndarray:
+    """Flatten the given array-like object.
+
+    Args:
+        arr: the array-like object to be flattened.
+
+    Returns:
+        The flatten result.
+    """
     if type(arr) == pd.core.series.Series:
         ret = arr.values.flatten()
     elif sp.issparse(arr):
         ret = arr.A.flatten()
     else:
         ret = arr.flatten()
     return ret
 
 
-def closest_cell(coord, cells):
+def closest_cell(coord: np.ndarray, cells: np.ndarray) -> int:
+    """Find the closest cell to the specified coord.
+
+    Args:
+        coord: the target coordination.
+        cells: an array containing cells.
+
+    Returns:
+        The column index of the cell that closest to the coordination specified.
+    """
     cells = np.asarray(cells)
     dist_2 = np.sum((cells - coord) ** 2, axis=1)
 
     return np.argmin(dist_2)
 
 
-def elem_prod(X, Y):
+def elem_prod(
+    X: Union[np.ndarray, sp.csr_matrix], Y: Union[np.ndarray, sp.csr_matrix]
+) -> Union[np.ndarray, sp.csr_matrix]:
+    """Calculate element-wise production between 2 arrays.
+
+    Args:
+        X: the first array.
+        Y: the second array.
+
+    Returns:
+        The resulted array.
+    """
     if sp.issparse(X):
         return X.multiply(Y)
     elif sp.issparse(Y):
         return Y.multiply(X)
     else:
         return np.multiply(X, Y)
 
 
-def norm(x, **kwargs):
-    """calculate the norm of an array or matrix"""
+def norm(x: Union[sp.csr_matrix, np.ndarray], **kwargs) -> np.ndarray:
+    """Calculate the norm of an array or matrix
+
+    Args:
+        x: the array.
+        kwargs: other kwargs passed to `sp.linalg.norm` or `np.linalg.norm`.
+    """
     if sp.issparse(x):
         return sp.linalg.norm(x, **kwargs)
     else:
         return np.linalg.norm(x, **kwargs)
 
 
-def cell_norm(adata, key, prefix_store=None, **norm_kwargs):
-    """Calculate the norm of vector for each cell.
+def cell_norm(adata: AnnData, key: str, prefix_store: Optional[str] = None, **norm_kwargs) -> np.ndarray:
+    """Calculate the norm of vectors of each cell.
+
+    Args:
+        adata: an AnnData object that contains the reconstructed vector field function in the `uns` attribute.
+        key: the key of the vectors stored in either .obsm or .layers.
+        prefix_store: the prefix used in the key for storing the returned in adata.obs. Defaults to None.
+
+    Raises:
+        ValueError: `key` not found in .obsm or .layers.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the reconstructed vector field function in the `uns` attribute.
-        key: str
-            The key of the vectors stored in either .obsm or .layers.
-        prefix_store: str or None (default: None)
-            The prefix used in the key for storing the returned in adata.obs.
-            If None, the returned is not stored.
-        norm_kwargs:
-            Keyword arguments passed to norm functions.
-
-    Returns
-    -------
-        norm: :class:`~numpy.ndarray`
-            Norms of vectors.
+    Returns:
+        The norms of the vectors.
     """
+
     if key in adata.obsm.keys():
         X = adata.obsm[key]
     elif key in adata.layers.keys():
         X = adata.layers[key]
     else:
         raise ValueError("The key is not found in adata.obsm and adata.layers!")
 
     ret = norm(X, axis=1, **norm_kwargs)
 
     if prefix_store is not None:
         adata.obs[prefix_store + "_" + key] = ret
     return ret
 
 
-def einsum_correlation(X, Y_i, type="pearson"):
-    """calculate pearson or cosine correlation between X (genes/pcs/embeddings x cells) and the velocity vectors Y_i
-    for gene i"""
+def einsum_correlation(X: np.ndarray, Y_i: np.ndarray, type: str = "pearson") -> np.ndarray:
+    """Calculate pearson or cosine correlation between gene expression data and the velocity vectors.
+
+    Args:
+        X: the gene expression data (genes x cells).
+        Y_i: the velocity vector.
+        type: the type of correlation to be calculated. Defaults to "pearson".
+
+    Returns:
+        The correlation matrix.
+    """
 
     if type == "pearson":
         X -= X.mean(axis=1)[:, None]
         Y_i -= np.nanmean(Y_i)
     elif type == "cosine":
         X, Y_i = X, Y_i
     elif type == "spearman":
@@ -466,17 +630,23 @@
             corr = np.zeros(X_norm.shape[0])
         else:
             corr = np.einsum("ij, j", X, Y_i) / (X_norm * Y_norm)[None, :]
 
     return corr
 
 
-def form_triu_matrix(arr):
+def form_triu_matrix(arr: np.ndarray) -> np.ndarray:
     """
     Construct upper triangle matrix from an 1d array.
+
+    Args:
+        arr: the array used to generate the upper triangle matrix.
+
+    Returns:
+        The generated upper triangle matrix.
     """
     n = int(np.ceil((np.sqrt(1 + 8 * len(arr)) - 1) * 0.5))
     M = np.zeros((n, n))
     c = 0
     for i in range(n):
         for j in range(n):
             if j >= i:
@@ -484,42 +654,46 @@
                     M[i, j] = arr[c]
                     c += 1
                 else:
                     break
     return M
 
 
-def index_condensed_matrix(n, i, j):
-    """
-    Return the index of an element in a condensed n-by-n square matrix
-    by the row index i and column index j of the square form.
+def index_condensed_matrix(n: int, i: int, j: int) -> int:
+    """Return the index of an element in a condensed n-by-n square matrix by the row index i and column index j of the
+    square form.
+
+    Args:
+        n: size of the square form.
+        i: row index of the element in the square form.
+        j: column index of the element in the square form.
 
-    Arguments
-    ---------
-        n: int
-            Size of the squareform.
-        i: int
-            Row index of the element in the squareform.
-        j: int
-            Column index of the element in the the squareform.
-
-    Returns
-    -------
-        k: int
-            The index of the element in the condensed matrix.
+    Returns:
+        The index of the element in the condensed matrix.
     """
+
     if i == j:
         main_warning("Diagonal elements (i=j) are not stored in condensed matrices.")
         return None
     elif i > j:
         i, j = j, i
     return int(i * (n - (i + 3) * 0.5) + j - 1)
 
 
-def condensed_idx_to_squareform_idx(arr_len, i):
+def condensed_idx_to_squareform_idx(arr_len: int, i: int) -> Tuple[int, int]:
+    """Return the row index i and column index j of the square matrix by giving the index of an element in the matrix's
+    condensed form.
+
+    Args:
+        arr_len: the size of the array in condensed form.
+        i: the index of the element in the condensed array.
+
+    Returns:
+        A tuple (x, y) of the row and column index of the element in sqaure form of the matrix.
+    """
     n = int((1 + np.sqrt(1 + 8 * arr_len)) / 2)
 
     def fr(x):
         return int(x * (n - (x + 3) * 0.5) + n - 1)
 
     for x in range(n):
         d = fr(x) - (i + 1)
@@ -543,20 +717,41 @@
 
 
 def var2m2(var, m1):
     m2 = var + elem_prod(m1, m1)
     return m2
 
 
-def gaussian_1d(x, mu=0, sigma=1):
+def gaussian_1d(x: npt.ArrayLike, mu: float = 0, sigma: float = 1) -> npt.ArrayLike:
+    """Calculate the probability density at x with given mean and standard deviation.
+
+    Args:
+        x: the x to calculate probability density. If x is an array, the probability density would be calculated
+        element-wisely.
+        mu: the mean of the distribution. Defaults to 0.
+        sigma: the standard deviation of the distribution. Defaults to 1.
+
+    Returns:
+        The probability density of the distribution at x.
+    """
+
     y = (x - mu) / sigma
     return np.exp(-y * y / 2) / np.sqrt(2 * np.pi) / sigma
 
 
-def timeit(method):
+def timeit(method: Callable) -> Callable:
+    """Wrap a Callable that if its kwargs contains "timeit" = True, measures how much time the function takes to finish.
+
+    Args:
+        method: the Callable to be measured.
+
+    Returns:
+        The wrapped Callable.
+    """
+
     def timed(*args, **kw):
         ti = kw.pop("timeit", False)
         if ti:
             ts = time.time()
             result = method(*args, **kw)
             te = time.time()
             main_info_verbose_timeit("Time elapsed for %r: %.4f s" % (method.__name__, (te - ts)))
@@ -625,125 +820,203 @@
     w = (w.T / w_mass).T
 
     V[np.isnan(V)] = 0
     V_grid = np.einsum("ijk, ij -> ik", V[neighs[:, :k]], w)
     return X_grid, V_grid
 
 
-def argsort_mat(mat, order=1):
+def argsort_mat(mat: np.ndarray, order: Literal[-1, 1] = 1) -> List[Tuple[int, int]]:
+    """Sort a 2D array and return the index of sorted elements.
+
+    Args:
+        mat: the 2D array to be sort.
+        order: sort the array ascending if set to 1 and descending if set to -1. Defaults to 1.
+
+    Returns:
+        A list containing 2D index of sorted elements.
+    """
+
     isort = np.argsort(mat, axis=None)[::order]
     index = np.zeros((len(isort), 2), dtype=int)
     index[:, 0] = isort // mat.shape[1]
     index[:, 1] = isort % mat.shape[1]
     return [(index[i, 0], index[i, 1]) for i in range(len(isort))]
 
 
-def list_top_genes(arr, gene_names, n_top_genes=30, order=-1, return_sorted_array=False):
+def list_top_genes(
+    arr: np.ndarray,
+    gene_names: np.ndarray,
+    n_top_genes: int = 30,
+    order: Literal[1, -1] = -1,
+    return_sorted_array: bool = False,
+) -> Union[Tuple[np.ndarray, np.ndarray], np.ndarray]:
+    """List top genes in a set gene data.
+
+    Args:
+        arr: an 1D array containing expression value of a set of genes.
+        gene_names: the gene name corresponding to each value in `arr`.
+        n_top_genes: the number of top genes to be selected. Defaults to 30.
+        order: coule be 1 or -1. If set to 1, most expressed genes are selected; otherwise, least expressed genes are
+            selected. Defaults to -1.
+        return_sorted_array: whether to return the sorted expression array together with sorted gene names. Defaults to
+            False.
+
+    Returns:
+        The names of the sorted genes in the specified order. If `return_sorted_array` is set to True, the sorted
+        expression array would also be returned.
+    """
     imax = np.argsort(arr)[::order]
     if return_sorted_array:
         return gene_names[imax][:n_top_genes], arr[imax][:n_top_genes]
     else:
         return gene_names[imax][:n_top_genes]
 
 
-def list_top_interactions(mat, row_names, column_names, order=-1):
+def list_top_interactions(
+    mat: np.ndarray, row_names: np.ndarray, column_names: np.ndarray, order: Literal[1, -1] = -1
+) -> Tuple[List[List[str]], np.ndarray]:
+    """Sort a 2D array with raw and column names in specified order.
+
+    Args:
+        mat: the array to be sorted.
+        row_names: the name for each row of `mat`.
+        column_names: the name for each column of `mat`.
+        order: coule be 1 or -1. If set to 1, sort ascending. Otherwise, sort descending. Defaults to -1.
+
+    Returns:
+        A tuple (ints, sorted_mat) where `ints` is a sorted list whose elements are pairs of row name and column name
+        corresponding to the element in the mat. `sorted_mat` is a list containing the sorted values of the mat.
+    """
     imax = argsort_mat(mat, order=order)
     ints = []
     sorted_mat = []
     for im in imax:
         ints.append([row_names[im[0]], column_names[im[1]]])
         sorted_mat.append(mat[im])
     return ints, np.array(sorted_mat)
 
 
-def table_top_genes(arrs, item_names, gene_names, return_df=True, output_values=False, **kwargs):
+def table_top_genes(
+    arrs: np.ndarray,
+    item_names: str,
+    gene_names: np.ndarray,
+    return_df: bool = True,
+    output_values: bool = False,
+    **kwargs,
+) -> Union[pd.DataFrame, dict]:
+    """Sort gene expressions for multiple items (cells) and save the result in a dict or a DataFrame.
+
+    Args:
+        arrs: a 2D array with each row corresponding to an item and each column corresponding to a gene.
+        item_names: the names of items corresponding to the rows of `arrs`.
+        gene_names: the names of genes corresponding to the columns of `arrs`.
+        return_df: whether to return the result in DataFrame or dict. Defaults to True.
+        output_values: whether to return the genes expression value together with sorted gene names. Defaults to False.
+
+    Returns:
+        A DataFrame or a dict containing sorted genes for each item.
+    """
     table = {}
     for i, item in enumerate(item_names):
         if output_values:
             table[item], table[item + "_values"] = list_top_genes(
                 arrs[i], gene_names, return_sorted_array=True, **kwargs
             )
         else:
             table[item] = list_top_genes(arrs[i], gene_names, **kwargs)
     if return_df:
         return pd.DataFrame(data=table)
     else:
         return table
 
 
-def table_rank_dict(rank_dict, n_top_genes=30, order=1, output_values=False):
-    """
-    Generate a pandas.Dataframe from a rank dictionary. A rank dictionary is a
-    dictionary of gene names and values, based on which the genes are sorted,
-    for each group of cells.
-
-    Arguments
-    ---------
-        rank_dict: dict
-            The rank dictionary.
-        n_top_genes: int (default 30)
-            The number of top genes put into the Dataframe.
-        order: int (default 1)
-            The order of picking the top genes from the rank dictionary.
-            1: ascending, -1: descending.
-        output_values: bool (default False)
-            Whether or not output the values along with gene names.
-    Returns
-    -------
-        :class:'~pandas.DataFrame'
-            The table of top genes of each group.
+def table_rank_dict(
+    rank_dict: dict, n_top_genes: int = 30, order: int = 1, output_values: bool = False
+) -> pd.DataFrame:
+    """Generate a pandas.Dataframe from a rank dictionary. A rank dictionary is a dictionary of gene names and values,
+    based on which the genes are sorted, for each group of cells.
+
+    Args:
+        rank_dict: the rank dictionary.
+        n_top_genes: the number of top genes put into the Dataframe. Defaults to 30.
+        order: the order of picking the top genes from the rank dictionary.
+            1: ascending, -1: descending. Defaults to 1.
+        output_values: whether output the values along with gene names. Defaults to False.
+
+    Returns:
+        The table of top genes of each group.
     """
+
     data = {}
     for g, r in rank_dict.items():
         d = [k for k in r.keys()][::order]
         data[g] = d[:n_top_genes]
         if output_values:
             dd = [v for v in r.values()][::order]
             data[g + "_values"] = dd[:n_top_genes]
     return pd.DataFrame(data=data)
 
 
 # ---------------------------------------------------------------------------------------------------
 # data transformation related:
-def log1p_(adata, X_data):
-    if "norm_method" not in adata.uns["pp"].keys():
+def log1p_(adata: AnnData, X_data: np.ndarray) -> np.ndarray:
+    """Perform log(1+x) X_data if adata.uns["pp"]["layers_norm_method"] is None.
+
+    Args:
+        adata: the AnnData that has been preprocessed.
+        X_data: the data to perform log1p on.
+
+    Returns:
+        The log1p result data if "layers_norm_method" in adata is None; otherwise, X_data would be returned unchanged.
+    """
+    if "layers_norm_method" not in adata.uns["pp"].keys():
         return X_data
     else:
-        if adata.uns["pp"]["norm_method"] is None:
+        if adata.uns["pp"]["layers_norm_method"] is None:
             if sp.issparse(X_data):
                 X_data.data = np.log1p(X_data.data)
             else:
                 X_data = np.log1p(X_data)
 
         return X_data
 
 
-def inverse_norm(adata, layer_x):
+def inverse_norm(adata: AnnData, layer_x: Union[np.ndarray, sp.csr_matrix]) -> np.ndarray:
+    """Perform inverse normalization on the given data. The normalization method is stored in adata after preprocessing.
+
+    Args:
+        adata: an AnnData object that has been preprocessed.
+        layer_x: the data to perform inverse normalization on.
+
+    Returns:
+        The inverse normalized data.
+    """
+
     if sp.issparse(layer_x):
         layer_x.data = (
             np.expm1(layer_x.data)
-            if adata.uns["pp"]["norm_method"] == "log1p"
+            if adata.uns["pp"]["layers_norm_method"] == "log1p"
             else 2**layer_x.data - 1
-            if adata.uns["pp"]["norm_method"] == "log2"
+            if adata.uns["pp"]["layers_norm_method"] == "log2"
             else np.exp(layer_x.data) - 1
-            if adata.uns["pp"]["norm_method"] == "log"
-            else Freeman_Tukey(layer_x.data + 1, inverse=True)
-            if adata.uns["pp"]["norm_method"] == "Freeman_Tukey"
+            if adata.uns["pp"]["layers_norm_method"] == "log"
+            else _Freeman_Tukey(layer_x.data + 1, inverse=True)
+            if adata.uns["pp"]["layers_norm_method"] == "Freeman_Tukey"
             else layer_x.data
         )
     else:
         layer_x = (
             np.expm1(layer_x)
-            if adata.uns["pp"]["norm_method"] == "log1p"
+            if adata.uns["pp"]["layers_norm_method"] == "log1p"
             else 2**layer_x - 1
-            if adata.uns["pp"]["norm_method"] == "log2"
+            if adata.uns["pp"]["layers_norm_method"] == "log2"
             else np.exp(layer_x) - 1
-            if adata.uns["pp"]["norm_method"] == "log"
-            else Freeman_Tukey(layer_x, inverse=True)
-            if adata.uns["pp"]["norm_method"] == "Freeman_Tukey"
+            if adata.uns["pp"]["layers_norm_method"] == "log"
+            else _Freeman_Tukey(layer_x, inverse=True)
+            if adata.uns["pp"]["layers_norm_method"] == "Freeman_Tukey"
             else layer_x
         )
 
     return layer_x
 
 
 # ---------------------------------------------------------------------------------------------------
@@ -793,33 +1066,62 @@
 
 def compute_velocity_labeling_B(B, alpha, R):
     return alpha - elem_prod(B, R.T).T
 
 
 # ---------------------------------------------------------------------------------------------------
 # dynamics related:
-def remove_2nd_moments(adata):
+def remove_2nd_moments(adata: AnnData) -> None:
+    """Delete layers of 2nd moments.
+
+    Args:
+        adata: the AnnData object from which 2nd moment layers are deleted.
+    """
     layers = list(adata.layers.keys())
     for layer in layers:
         if layer.startswith("M_") and len(layer) == 4:
             del adata.layers[layer]
 
 
-def get_valid_bools(adata, filter_gene_mode):
+def get_valid_bools(adata: AnnData, filter_gene_mode: Literal["final", "basic", "no"]) -> np.ndarray:
+    """Get a boolean array showing the gene passing the filter specified.
+
+    Args:
+        adata: an AnnData object.
+        filter_gene_mode: the gene filter. Could be one of "final", "basic", and "no".
+
+    Raises:
+        NotImplementedError: invalid `filter_gene_mode`.
+
+    Returns:
+        A boolean array showing the gene passing the filter specified.
+    """
     if filter_gene_mode == "final":
         valid_ind = adata.var.use_for_pca.values
     elif filter_gene_mode == "basic":
         valid_ind = adata.var.pass_basic_filter.values
     elif filter_gene_mode == "no":
         valid_ind = np.repeat([True], adata.shape[1])
-
+    else:
+        raise NotImplementedError("Invalid filter_gene_mode. ")
     return valid_ind
 
 
-def log_unnormalized_data(raw, log_unnormalized):
+def log_unnormalized_data(
+    raw: Union[np.ndarray, sp.csr_matrix], log_unnormalized: bool
+) -> Union[np.ndarray, sp.csr_matrix]:
+    """Perform log1p on unnormalized data.
+
+    Args:
+        raw: the matrix to be operated on.
+        log_unnormalized: whether the matrix is unnormalized and log1p should be performed.
+
+    Returns:
+        Updated matrix with log1p if it is unormalized; otherwise, return original `raw`.
+    """
     if sp.issparse(raw):
         raw.data = np.log(raw.data + 1) if log_unnormalized else raw.data
     else:
         raw = np.log1p(raw) if log_unnormalized else raw
 
     return raw
 
@@ -1457,34 +1759,62 @@
 
             X_data = log1p_(adata, X_data)
 
     return genes, X_data
 
 
 class AnnDataPredicate(object):
-    def __init__(self, key, value, op="=="):
-        """
-        Predicate class for item selection for anndata
+    """The predicate class for item selection for anndata.
 
-        Parameters
-        ----------
-            key: str
-                The key in the dictionary (specified as `data` in `.check()`) that will be used for selection.
-            value: any
-                The value that will be used based on `op` to select items.
-            op: str
-                operators for selection:
+    Attributes:
+        key: The key in the AnnData object (specified as `data` in `.check()`) that will be used for selection.
+        value: The value that will be used based on `op` to select items.
+        op: operators for selection.
+    """
+
+    def __init__(
+        self,
+        key: str,
+        value: Any,
+        op: Literal[
+            "==",
+            "!=",
+            ">",
+            "<",
+            ">=",
+            "<=",
+        ] = "==",
+    ) -> None:
+        """Initialize an AnnDataPredicate object.
+
+        Args:
+            key: the key in the AnnData object (specified as `data` in `.check()`) that will be used for selection.
+            value: the value that will be used based on `op` to select items.
+            op: operators for selection:
                 '==' - equal to `value`; '!=' - unequal to; '>' - greater than; '<' - smaller than;
-                '>=' - greater than or equal to; '<=' - less than or equal to.
+                '>=' - greater than or equal to; '<=' - less than or equal to. Defaults to "==".
         """
+
         self.key = key
         self.value = value
         self.op = op
 
-    def check(self, data):
+    def check(self, data: AnnData) -> np.ndarray:
+        """Filter out the elements in data[AnnDataPredicate.key] that fullfill the requirement specified as
+        AnnDataPredicate's `value` and `op` attr.
+
+        Args:
+            data: the AnnData object to be tested.
+
+        Raises:
+            NotImplementedError: invalid `op`.
+
+        Returns:
+            A boolean array with `True` at positions where the element pass the check.
+        """
         if self.op == "==":
             return data[self.key] == self.value
         elif self.op == "!=":
             return data[self.key] != self.value
         elif self.op == ">":
             return data[self.key] > self.value
         elif self.op == "<":
@@ -1493,20 +1823,46 @@
             return data[self.key] >= self.value
         elif self.op == "<=":
             return data[self.key] <= self.value
         else:
             raise NotImplementedError(f"Unidentified operator {self.op}!")
 
     def __or__(self, other):
+        """Combine requirement from another AnnDataPredicate object and return an AnnDataPredicates that set True on
+        elements that pass at least one requirement from the AnnDataPredicate objects.
+
+        Args:
+            other (AnnDataPredicate): another AnnDataPredicate object containing requirement for "or" test.
+
+        Returns:
+            PredicateUnion: the updated Predicates object.
+        """
         return PredicateUnion(self, other)
 
     def __and__(self, other):
+        """Combine requirement from another AnnDataPredicate object and return an AnnDataPredicates that set True on
+        elements that pass all requirements from the AnnDataPredicate objects.
+
+        Args:
+            other (AnnDataPredicate): another AnnDataPredicate object containing requirement for "and" test.
+
+        Returns:
+            PredicateIntersection: the updated Predicates object.
+        """
         return PredicateIntersection(self, other)
 
     def __invert__(self):
+        """Inverse the current requirement.
+
+        Raises:
+            NotImplementedError: invalid `op`.
+
+        Returns:
+            AnnDataPredicate: the updated Predicate object.
+        """
         if self.op == "==":
             op = "!="
         elif self.op == "!=":
             op = "=="
         elif self.op == ">":
             op = "<="
         elif self.op == "<":
@@ -1518,79 +1874,221 @@
         else:
             raise NotImplementedError(f"Unidentified operator {self.op}!")
 
         return AnnDataPredicate(self.key, self.value, op)
 
 
 class AlwaysTrue(AnnDataPredicate):
-    def __init__(self, key=None):
+    """A class inherited from AnnDataPredicate. Will return true for all elements under any requirement.
+
+    Attributes:
+        key: the key in the AnnData object (specified as `data` in `.check()`) that will be used for selection.
+    """
+
+    def __init__(self, key: Optional[str] = None) -> None:
+        """Initialize an AlwaysTrue object.
+
+        Args:
+            key: the key in the AnnData object (specified as `data` in `.check()`) that will be used for selection. If
+                None, the first layer of the AnnData for comparison would be used. Defaults to None.
+        """
         self.key = key
 
-    def check(self, data):
+    def check(self, data: AnnData) -> np.ndarray:
+        """Return an all-true boolean array with the shape of the layer.
+
+        Args:
+            data: the AnnData object to be tested.
+
+        Returns:
+            An all-true boolean array with the shape of the layer.
+        """
         key = self.key if self.key is not None else data.keys()[0]
         return np.ones(len(data[key]), dtype=bool)
 
     def __invert__(self):
+        """Inverse the AlwaysTrue object to AlwaysFalse object.
+
+        Returns:
+            An AlwaysFalse object.
+        """
         return AlwaysFalse(key=self.key)
 
 
 class AlwaysFalse(AnnDataPredicate):
+    """A class inherited from AnnDataPredicate. Will return false for all elements under any requirement.
+
+    Attributes:
+        key: the key in the AnnData object (specified as `data` in `.check()`) that will be used for selection.
+    """
+
     def __init__(self, key=None):
+        """Initialize an AlwaysTrue object.
+
+        Args:
+            key: the key in the AnnData object (specified as `data` in `.check()`) that will be used for selection. If
+                None, the first layer of the AnnData for comparison would be used. Defaults to None.
+        """
         self.key = key
 
     def check(self, data):
+        """Return an all-false boolean array with the shape of the layer.
+
+        Args:
+            data: the AnnData object to be tested.
+
+        Returns:
+            An all-false boolean array with the shape of the layer.
+        """
         key = self.key if self.key is not None else data.keys()[0]
         return np.zeros(len(data[key]), dtype=bool)
 
     def __invert__(self):
+        """Inverse the AlwaysTrue object to AlwaysTrue object.
+
+        Returns:
+            An AlwaysTrue object.
+        """
         return AlwaysTrue(key=self.key)
 
 
 class AnnDataPredicates(object):
-    def __init__(self, *predicates):
+    """A set of multiple AnnDataPredicate objects.
+
+    Attributions:
+        predicates (List[AnnDataPredicate]): a tuple containing multiple AnnDataPredicate objects.
+    """
+
+    def __init__(self, *predicates) -> None:
+        """Initialize an AnnDataPredicates object.
+
+        Args:
+            *predicates: one or more AnnDataPredicate objects.
+        """
         self.predicates = predicates
 
     def binop(self, other, op):
+        """Bind two Predicate(s) objects together with given operation type.
+
+        Args:
+            other: another Predicate(s) object to bind with.
+            op: how to bind the requirement of Predicates (Union or Intersection).
+
+        Raises:
+            NotImplementedError: `other` is not an Predcate(s) instance.
+
+        Returns:
+            AnnDataPredicates: a new Predicates object with requirement binded.
+        """
         if isinstance(other, AnnDataPredicate):
             return op(*self.predicates, other)
         elif isinstance(other, AnnDataPredicates):
             return op(*self.predicates, *other)
         else:
             raise NotImplementedError(f"Unidentified predicate type {type(other)}!")
 
 
 class PredicateUnion(AnnDataPredicates):
-    def check(self, data):
+    """Inherited from AnnDataPredicates. If at least 1 requirement from all predicates is fulfilled, the data would pass
+    the check.
+    """
+
+    def check(self, data: AnnData) -> np.ndarray:
+        """Check whether the data could fulfill at least 1 requirement by all Predicates.
+
+        Args:
+            data (AnnData): an AnnData object.
+
+        Returns:
+            np.ndarray: A boolean array with `True` at positions where the element can pass at least one check.
+        """
         ret = None
         for pred in self.predicates:
             ret = np.logical_or(ret, pred.check(data)) if ret is not None else pred.check(data)
         return ret
 
     def __or__(self, other):
+        """Bind with other Predicate(s) with union.
+
+        Args:
+            other: another Predicate(s) object to bind with.
+
+        Returns:
+            PredicateUnion: the binded predicates.
+        """
         return self.binop(other, PredicateUnion)
 
     def __and__(self, other):
+        """Bind with other Predicate(s) with intersection.
+
+        Args:
+            other: another Predicate(s) object to bind with.
+
+        Returns:
+            PredicateIntersection: the binded predicates.
+        """
         return self.binop(other, PredicateIntersection)
 
 
 class PredicateIntersection(AnnDataPredicates):
+    """Inherited from AnnDataPredicates. If all requirements from all predicates are fulfilled, the data would pass
+    the check.
+    """
+
     def check(self, data):
+        """Check whether the data could fulfill all requirements by all Predicates.
+
+        Args:
+            data (AnnData): an AnnData object.
+
+        Returns:
+            np.ndarray: A boolean array with `True` at positions where the element can pass all checks.
+        """
         ret = None
         for pred in self.predicates:
             ret = np.logical_and(ret, pred.check(data)) if ret is not None else pred.check(data)
         return ret
 
     def __or__(self, other):
+        """Bind with other Predicate(s) with union.
+
+        Args:
+            other: another Predicate(s) object to bind with.
+
+        Returns:
+            PredicateUnion: the binded predicates.
+        """
         return self.binop(other, PredicateUnion)
 
     def __and__(self, other):
+        """Bind with other Predicate(s) with intersection.
+
+        Args:
+            other: another Predicate(s) object to bind with.
+
+        Returns:
+            PredicateIntersection: the binded predicates.
+        """
         return self.binop(other, PredicateIntersection)
 
 
-def select(array, pred=AlwaysTrue(), output_format="mask"):
+def select(
+    array: np.ndarray, pred: AnnDataPredicate = AlwaysTrue(), output_format: Literal["mask", "index"] = "mask"
+) -> np.ndarray:
+    """Select part of the array based on the condition provided by the used.
+
+    Args:
+        array: the original data to be selected from.
+        pred: the condition provided by the user. Defaults to AlwaysTrue().
+        output_format: whether to output a mask of selection or selected items' indices. Defaults to "mask".
+
+    Returns:
+        A mask of selection or selected items' indices.
+    """
+
     ret = pred.check(array)
     if output_format == "mask":
         pass
     elif output_format == "index":
         ret = np.where(ret)[0]
     return ret
 
@@ -1618,15 +2116,15 @@
 
     return 1 - SS_res / SS_tot
 
 
 def norm_loglikelihood(x, mu, sig):
     """Calculate log-likelihood for the data."""
     err = (x - mu) / sig
-    ll = -len(err) / 2 * np.log(2 * np.pi) - np.sum(np.log(sig)) - 0.5 * err.dot(err.T)
+    ll = -len(err) / 2 * np.log(2 * np.pi) - 0.5 * len(err) * np.log(sig**2) - 0.5 * err.dot(err.T)
     return np.sum(ll, 0)
 
 
 def calc_norm_loglikelihood(X, Y, k, f=lambda X, k: np.einsum("ij,i -> ij", X, k)):
     """calculate log likelihood based on normal distribution. X, Y: n_species (mu, sigma) x n_obs"""
     if X.ndim == 1:
         X = X[None]
@@ -1958,15 +2456,17 @@
 
     return ekey, vkey, layer
 
 
 # ---------------------------------------------------------------------------------------------------
 # cell velocities related
 def get_neighbor_indices(adjacency_list, source_idx, n_order_neighbors=2, max_neighbors_num=None):
-    """returns a list (np.array) of `n_order_neighbors` neighbor indices of source_idx. If `max_neighbors_num` is set and the n order neighbors of `source_idx` is larger than `max_neighbors_num`, a list of neighbors will be randomly chosen and returned."""
+    """returns a list (np.array) of `n_order_neighbors` neighbor indices of source_idx. If `max_neighbors_num` is set
+    and the n order neighbors of `source_idx` is larger than `max_neighbors_num`, a list of neighbors will be randomly
+    chosen and returned."""
     _indices = [source_idx]
     for _ in range(n_order_neighbors):
         _indices = np.append(_indices, adjacency_list[_indices])
         if np.isnan(_indices).any():
             _indices = _indices[~np.isnan(_indices)]
     _indices = np.unique(_indices)
     if max_neighbors_num is not None and len(_indices) > max_neighbors_num:
@@ -2026,41 +2526,34 @@
 #  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 #  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 #  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 #  SOFTWARE.
 #  https://gist.github.com/aldro61/5889795
 
 
-def linear_least_squares(a, b, residuals=False):
-    """
-    Return the least-squares solution to a linear matrix equation.
-    Solves the equation `a x = b` by computing a vector `x` that
-    minimizes the Euclidean 2-norm `|| b - a x ||^2`.  The equation may
-    be under-, well-, or over- determined (i.e., the number of
-    linearly independent rows of `a` can be less than, equal to, or
-    greater than its number of linearly independent columns).  If `a`
-    is square and of full rank, then `x` (but for round-off error) is
-    the "exact" solution of the equation.
-    Parameters
-    ----------
-    a : (M, N) array_like
-        "Coefficient" matrix.
-    b : (M,) array_like
-        Ordinate or "dependent variable" values.
-    residuals : bool
-        Compute the residuals associated with the least-squares solution
-    Returns
-    -------
-    x : (M,) ndarray
-        Least-squares solution. The shape of `x` depends on the shape of
-        `b`.
-    residuals : int (Optional)
-        Sums of residuals; squared Euclidean 2-norm for each column in
-        ``b - a*x``.
+def linear_least_squares(
+    a: npt.ArrayLike, b: npt.ArrayLike, residuals: bool = False
+) -> Union[np.ndarray, Tuple[np.ndarray, float]]:
+    """Return the least-squares solution to a linear matrix equation.
+
+    Solves the equation `a x = b` by computing a vector `x` that minimizes the Euclidean 2-norm `|| b - a x ||^2`.
+    The equation may be under-, well-, or over- determined (i.e., the number of linearly independent rows of `a` can be
+    less than, equal to, or greater than its number of linearly independent columns).  If `a` is square and of full
+    rank, then `x` (but for round-off error) is the "exact" solution of the equation.
+
+    Args:
+        a: the coefficient matrix.
+        b: the ordinate or "dependent variable" values.
+        residuals: whether to compute the residuals associated with the least-squares solution. Defaults to False.
+
+    Returns:
+        The least-squares solution. If `residuals` is True, the sum of residuals (squared Euclidean 2-norm for each
+        column in ``b - a*x``) would also be returned.
     """
+
     if type(a) != np.ndarray or not a.flags["C_CONTIGUOUS"]:
         main_warning(
             "Matrix a is not a C-contiguous numpy array. The solver will create a copy, which will result"
             + " in increased memory usage."
         )
 
     a = np.asarray(a, order="c")
@@ -2232,34 +2725,34 @@
         t_linspace = np.arange(0, t_end + step_size, step_size)
 
     return t_linspace
 
 
 # ---------------------------------------------------------------------------------------------------
 # spatial related
-def compute_smallest_distance(coords: list, leaf_size: int = 40, sample_num=None, use_unique_coords=True) -> float:
-    """Compute and return smallest distance. A wrapper for sklearn API
+def compute_smallest_distance(
+    coords: np.ndarray, leaf_size: int = 40, sample_num: Optional[int] = None, use_unique_coords: bool = True
+) -> float:
+    """Compute and return smallest distance.
+
+    This function is a wrapper for sklearn API.
+
+    Args:
+        coords: NxM matrix. N is the number of data points and M is the dimension of each point's feature.
+        leaf_size: the leaf size parameter for building Kd-tree. Defaults to 40.
+        sample_num: the number of cells to be sampled. Defaults to None.
+        use_unique_coords: whether to remove duplicate coordinates. Defaults to True.
 
-    Parameters
-    ----------
-        coords:
-            NxM matrix. N is the number of data points and M is the dimension of each point's feature.
-        leaf_size : int, optional
-            Leaf size parameter for building Kd-tree, by default 40.
-        sample_num:
-            The number of cells to be sampled.
-        use_unique_coords:
-            Whether to remove duplicate coordinates
-
-    Returns
-    -------
-        min_dist: float
-            the minimum distance between points
+    Raises:
+        ValueError: the dimension of coords is not 2x2
 
+    Returns:
+        The minimum distance between points.
     """
+
     if len(coords.shape) != 2:
         raise ValueError("Coordinates should be a NxM array.")
     if use_unique_coords:
         main_info("using unique coordinates for computing smallest distance")
         coords = [tuple(coord) for coord in coords]
         coords = np.array(list(set(coords)))
     # use cKDTree which is implmented in C++ and is much faster than KDTree
@@ -2289,44 +2782,40 @@
 def apply_args_and_kwargs(fn, args, kwargs):
     return fn(*args, **kwargs)
 
 
 # ---------------------------------------------------------------------------------------------------
 # ranking related
 def get_rank_array(
-    adata,
-    arr_key,
-    genes=None,
-    abs=False,
-    dtype=None,
-):
-    """Get the data array that will be used for gene-wise or cell-wise ranking
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains the array to be sorted in `.var` or `.layer`.
-        arr_key: str or :class:`~numpy.ndarray`
-            The key of the to-be-ranked array stored in `.var` or or `.layer`.
-            If the array is found in `.var`, the `groups` argument will be ignored.
-            If a numpy array is passed, it is used as the array to be ranked and must
+    adata: AnnData,
+    arr_key: Union[str, np.ndarray],
+    genes: Optional[List[str]] = None,
+    abs: bool = False,
+    dtype: Optional[np.dtype] = None,
+) -> Tuple[List[str], np.ndarray]:
+    """Get the data array that will be used for gene-wise or cell-wise ranking.
+
+    Args:
+        adata: annData object that contains the array to be sorted in `.var` or `.layer`.
+        arr_key: the key of the to-be-ranked array stored in `.var` or or `.layer`. If the array is found in `.var`, the
+            `groups` argument will be ignored. If a numpy array is passed, it is used as the array to be ranked and must
             be either an 1d array of length `.n_var`, or a `.n_obs`-by-`.n_var` 2d array.
-        groups: str or None (default: None)
-            Cell groups used to group the array.
-        genes: list or None (default: None)
-            The gene list that speed will be ranked. If provided, they must overlap the dynamics genes.
-        abs: bool (default: False)
-            When pooling the values in the array (see below), whether to take the absolute values.
-        fcn_pool: callable (default: numpy.mean(x, axis=0))
-            The function used to pool values in the to-be-ranked array if the array is 2d.
-
-    Returns
-    -------
-        arr: `~numpy.ndarray`
-            An array that stores information required for gene/cell-wise ranking.
+        genes: the gene list that speed will be ranked. If provided, they must overlap the dynamics genes. Defaults to
+            None.
+        abs: when pooling the values in the array (see below), whether to take the absolute values. Defaults to False.
+        dtype: the dtype the the result array would be formated into. Defaults to None.
+
+    Raises:
+        TypeError: invalid `arr_key`.
+        ValueError: invalid `genes`.
+        Exception: invalid `arr_key`.
+
+    Returns:
+        A tuple (genes, arr) where `genes` is a list containing genes be ranked and `arr` is an array that stores
+        information required for gene/cell-wise ranking.
     """
 
     dynamics_genes = (
         adata.var.use_for_dynamics if "use_for_dynamics" in adata.var.keys() else np.ones(adata.n_vars, dtype=bool)
     )
     if genes is not None:
         if type(genes) is str:
@@ -2370,15 +2859,17 @@
         arr = np.abs(arr)
 
     return genes, arr
 
 
 # ---------------------------------------------------------------------------------------------------
 # projection related
-def projection_with_transition_matrix(T, X_embedding, correct_density=True, norm_dist=True):
+def projection_with_transition_matrix(
+    T: Union[np.ndarray, sp.csr_matrix], X_embedding: np.ndarray, correct_density: bool = True, norm_dist: bool = True
+) -> np.ndarray:
     n = T.shape[0]
     delta_X = np.zeros((n, X_embedding.shape[1]))
 
     if not sp.issparse(T):
         T = sp.csr_matrix(T)
 
     with warnings.catch_warnings():
@@ -2395,7 +2886,21 @@
                 diff_emb[np.isnan(diff_emb)] = 0
             T_i = T[i].data
             delta_X[i] = T_i.dot(diff_emb)
             if correct_density:
                 delta_X[i] -= T_i.mean() * diff_emb.sum(0)
 
     return delta_X
+
+def density_corrected_transition_matrix(T):
+    '''
+        Returns the transition matrix with density correction from T
+    '''
+    T = sp.csr_matrix(T, copy=True)
+
+    for i in range(T.shape[0]):
+        idx = T[i].indices
+        T_i = T[i].data
+        T_i -= T_i.mean()
+        T[i, idx] = T_i
+
+    return T
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/utils_markers.py` & `dynamo-release-1.3.0/dynamo/tools/utils_markers.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,41 +1,41 @@
 import numpy as np
 
 # ---------------------------------------------------------------------------------------------------
 # specificity related
 
 
-def specificity(percentage, perfect_specificity):
+def specificity(percentage: np.ndarray, perfect_specificity: np.ndarray) -> float:
     """Calculate specificity"""
 
     spec = 1 - JSdistVec(makeprobsvec(percentage), perfect_specificity)
 
     return spec
 
 
-def makeprobsvec(p):
+def makeprobsvec(p: np.ndarray) -> np.ndarray:
     """Calculate the probability matrix for a relative abundance matrix"""
 
     phat = p / np.sum(p)
     phat[np.isnan((phat))] = 0
 
     return phat
 
 
-def shannon_entropy(p):
+def shannon_entropy(p: np.ndarray) -> float:
     """Calculate the Shannon entropy based on the probability vector"""
 
     if np.min(p) < 0 or np.sum(p) <= 0:
         return np.inf
     p_norm = p[p > 0] / np.sum(p)
 
     return -np.sum(np.log(p_norm) * p_norm)
 
 
-def JSdistVec(p, q):
+def JSdistVec(p: np.ndarray, q: np.ndarray) -> float:
     """Calculate the Jessen-Shannon distance for two probability distribution"""
 
     Jsdiv = shannon_entropy((p + q) / 2) - (shannon_entropy(p) + shannon_entropy(q)) / 2
     if np.isinf(Jsdiv):
         Jsdiv = 1
     if Jsdiv < 0:
         Jsdiv = 0
@@ -44,16 +44,16 @@
     return JSdist
 
 
 # ---------------------------------------------------------------------------------------------------
 # differential gene expression test related
 
 
-def fdr(p_vals):
-    """calculate fdr_bh (Benjamini/Hochberg (non-negative))"""
+def fdr(p_vals: np.ndarray) -> np.ndarray:
+    """Calculate fdr_bh (Benjamini/Hochberg (non-negative))"""
     from scipy.stats import rankdata
 
     ranked_p_values = rankdata(p_vals)
     fdr = p_vals * len(p_vals) / ranked_p_values
     fdr[fdr > 1] = 1
 
     return fdr
```

### Comparing `dynamo-release-1.2.0/dynamo/tools/utils_moments_deprecated.py` & `dynamo-release-1.3.0/dynamo/tools/utils_moments_deprecated.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/tools/velocyto_scvelo.py` & `dynamo-release-1.3.0/dynamo/tools/velocyto_scvelo.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,41 +1,40 @@
 # functions to run velocyto and scvelo
 # from .moments import *
+from typing import List, Optional
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+
 import anndata
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
-
-# import velocyto as vcy
-# import scvelo as scv
 from scipy.sparse import csr_matrix
 
+from ..configuration import DKM
 
-def vlm_to_adata(vlm, n_comps=30, basis="umap", trans_mats=None, cells_ixs=None):
-    """Conversion function from the velocyto world to the dynamo world.
-    Code original from scSLAM-seq repository
-
-    Parameters
-    ----------
-        vlm: VelocytoLoom Object
-            The VelocytoLoom object that will be converted into adata.
-        n_comps: `int` (default: 30)
-            The number of pc components that will be stored.
-        basis: `str` (default: `umap`)
-            The embedding that will be used to store the vlm.ts attribute. Note that velocyto doesn't usually use
-            umap as embedding although `umap` as set as default for the convenience of dynamo itself.
-        trans_mats: None or dict
-            A dict of all relevant transition matrices
-        cell_ixs: list of int
-            These are the indices of the subsampled cells
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            AnnData object
+
+def vlm_to_adata(
+    vlm, n_comps: int = 30, basis: str = "umap", trans_mats: Optional[dict] = None, cells_ixs: List[int] = None
+) -> anndata.AnnData:
+    """Conversion function from the velocyto world to the dynamo world. Code original from scSLAM-seq repository.
+
+    Args:
+        vlm (VelocytoLoom): the VelocytoLoom object that will be converted into adata.
+        n_comps: the number of pc components that will be stored. Defaults to 30.
+        basis: the embedding that will be used to store the vlm.ts attribute. Note that velocyto doesn't usually use
+            umap as embedding although `umap` as set as default for the convenience of dynamo itself. Defaults to "umap".
+        trans_mats: a dict of all relevant transition matrices. Defaults to None.
+        cells_ixs: these are the indices of the subsampled cells. Defaults to None.
+
+    Returns:
+        The updated AnnData object.
     """
 
     from collections import OrderedDict
 
     # set obs, var
     obs, var = pd.DataFrame(vlm.ca), pd.DataFrame(vlm.ra)
     if "CellID" in obs.keys():
@@ -87,15 +86,15 @@
     if hasattr(vlm, "Sx_sz") and not hasattr(vlm, "Sx"):
         layers["M_s"] = csr_matrix(vlm.Sx_sz).T
     if hasattr(vlm, "Ux_sz") and hasattr(vlm, "Ux"):
         layers["M_u"] = csr_matrix(vlm.Ux_sz).T
 
     # set obsm
     obsm = {}
-    obsm["X"] = vlm.pcs[:, : min(n_comps, vlm.pcs.shape[1])]
+    obsm[DKM.X_PCA] = vlm.pcs[:, : min(n_comps, vlm.pcs.shape[1])]
     # set basis and velocity on the basis
     if basis is not None:
         obsm["X_" + basis] = vlm.ts
         obsm["velocity_" + basis] = vlm.delta_embedding
 
     # set transition matrix:
     uns = {}
@@ -123,15 +122,16 @@
     uns["pp"] = {
         "has_splicing": True,
         "has_labeling": False,
         "splicing_labeling": False,
         "has_protein": False,
         "tkey": None,
         "experiment_type": "conventional",
-        "norm_method": None,
+        "X_norm_method": None,
+        "layers_norm_method": None,
     }
 
     uns["dynamics"] = {
         "filter_gene_mode": None,
         "t": None,
         "group": None,
         "X_data": None,
@@ -159,20 +159,32 @@
 
     # create an anndata object with Dynamo characteristics.
     dyn_adata = anndata.AnnData(X=X, obs=obs, obsp=obsp, obsm=obsm, var=var, layers=layers, uns=uns)
 
     return dyn_adata
 
 
-def converter(data_in, from_type="adata", to_type="vlm", dir="."):
-    """
-    convert adata to loom object
-    - we may save_fig to a temp directory automatically
-    - we may write a on-the-fly converter which doesn't involve saving and reading files
+def converter(
+    data_in, from_type: Literal["adata", "vlm"] = "adata", to_type: Literal["adata", "vlm"] = "vlm", dir: str = "."
+):
+    """Convert adata to loom object or vice versa.
+
+    Args:
+        data_in (Union[vcy.VelocytoLoom, anndata.AnnData]): the object to be converted.
+        from_type: the type of data_in. Defaults to "adata".
+        to_type: convert to which type. Defaults to "vlm".
+        dir: the path to save the loom file. Defaults to ".".
+
+    Raises:
+        ImportError: velocyto not installed.
+
+    Returns:
+        the converted object.
     """
+
     try:
         import velocyto as vcy
     except ImportError:
         raise ImportError("You need to install the package `velocyto`." "install velocyto via `pip install velocyto`")
 
     if from_type == "adata":
         if to_type == "vlm":
@@ -196,19 +208,26 @@
         return colors20[np.mod(x, 20)]
 
     data_out.colorandum = colormap_fun([1] * data_out.S.shape[1])
 
     return data_out
 
 
-def run_velocyto(adata):
-    """
+def run_velocyto(adata: anndata.AnnData) -> anndata.AnnData:
+    """Run velocyto over the AnnData object.
+
     1. convert adata to vlm data
     2. set up PCA, UMAP, etc.
     3. estimate the gamma parameter
+
+    Args:
+        adata: an AnnData object.
+
+    Returns:
+        The updated AnnData object.
     """
     vlm = converter(adata)
 
     # U_norm: log2(U_sz + pcount)
     # vlm.U_sz: norm_factor * U
     # S_norm: log2(S_sz + pcount)
     # vlm.S_sz norm_factor * S
@@ -237,20 +256,31 @@
     # predict future state after dt
     vlm.calculate_shift()  # assumption = 'constant_velocity'
     vlm.extrapolate_cell_at_t()  # delta_t = 1.
 
     return vlm
 
 
-def run_scvelo(adata):
-    """
-    1. set up PCA, UMAP, etc.
-    2. estimate gamma and all other parameters
-    3. return results (adata.var['velocity_gamma'])
+def run_scvelo(adata: anndata.AnnData) -> anndata.AnnData:
+    """Run Scvelo over the AnnData.
+
+    1. Set up PCA, UMAP, etc.
+    2. Estimate gamma and all other parameters
+    3. Return results (adata.var['velocity_gamma'])
+
+    Args:
+        adata: an AnnData object.
+
+    Raises:
+        ImportError: scvelo not installed.
+
+    Returns:
+        The updated AnnData object.
     """
+
     try:
         import scvelo as scv
     except ImportError:
         raise ImportError("You need to install the package `scvelo`." "install scvelo via `pip install scvelo`")
 
     # scv.pp.filter_and_normalize(adata, min_counts=2, min_counts_u=1, n_top_genes=3000)
     scv.pp.moments(adata)  # , n_pcs = 12, n_neighbors = 15, mode = 'distances'
@@ -258,15 +288,24 @@
     scv.tl.velocity_graph(adata)
 
     # how to fit other parameters, beta, etc.?
 
     return adata
 
 
-def mean_var_by_time(X, Time):
+def mean_var_by_time(X: np.ndarray, Time: np.ndarray) -> np.ndarray:
+    """Group the data based on time and find the group's mean and var.
+
+    Args:
+        X: the data to be grouped.
+        Time: the corresponding time.
+
+    Returns:
+        The mean and var of each group.
+    """
     import pandas as pd
 
     exp_data = pd.DataFrame(X)
     exp_data["Time"] = Time
 
     mean_val = exp_data.groupby(["Time"]).mean()
     var_val = exp_data.groupby(["Time"]).var()
```

### Comparing `dynamo-release-1.2.0/dynamo/utils.py` & `dynamo-release-1.3.0/dynamo/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """General utility functions
 """
+import anndata
 import numpy as np
 import scipy.sparse as sp
-from anndata import AnnData
 
 from .dynamo_logger import LoggerManager
 
 
 def isarray(arr):
     """
     Check if a variable is an array. Essentially the variable has the attribute 'len'
@@ -34,15 +34,15 @@
         if ret is None:
             ret = [isinstance(a, dt) for a in arr]
         else:
             ret = np.logical_or(ret, [isinstance(a, dt) for a in arr])
     return logic_func(ret)
 
 
-def copy_adata(adata: AnnData, logger=None) -> AnnData:
+def copy_adata(adata: anndata.AnnData, logger=None) -> anndata.AnnData:
     """wrapper for deep copy adata and log copy operation since it is memory intensive.
 
     Parameters
     ----------
     adata :
          An adata object that will be deep copied.
     logger : [bool], optional
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/Ao.py` & `dynamo-release-1.3.0/dynamo/vectorfield/Ao.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,24 +1,25 @@
+from typing import Callable, List, Optional, Tuple, Union
+
 import numpy as np
-import scipy.sparse as sp
 from scipy.optimize import least_squares
-from tqdm import tqdm
+import tqdm
 
 from ..tools.utils import condensed_idx_to_squareform_idx, squareform, timeit
 
 # from scPotential import show_landscape
 
 
-def f_left(X, F):
+def f_left(X: np.ndarray, F: np.ndarray) -> np.ndarray:
     """An auxiliary function for fast computation of F.X - (F.X)^T"""
     R = F.dot(X)
     return R - R.T
 
 
-def f_left_jac(q, F):
+def f_left_jac(q: np.ndarray, F: np.ndarray) -> np.ndarray:
     """
     Analytical Jacobian of f(Q) = F.Q - (F.Q)^T, where Q is
     an anti-symmetric matrix s.t. Q^T = -Q.
     """
     J = np.zeros((np.prod(F.shape), len(q)))
     for i in range(len(q)):
         jac = np.zeros(F.shape)
@@ -28,36 +29,35 @@
         jac[b, :] -= F[:, a]
         jac[a, :] -= -F[:, b]
         J[:, i] = jac.flatten()
     return J
 
 
 @timeit
-def solveQ(D, F, q0=None, debug=False, precompute_jac=True, **kwargs):
+def solveQ(
+    D: np.ndarray,
+    F: np.ndarray,
+    q0: Optional[np.ndarray] = None,
+    debug: bool = False,
+    precompute_jac: bool = True,
+    **kwargs
+) -> Union[Tuple[np.ndarray, np.ndarray, np.ndarray, float], Tuple[np.ndarray, np.ndarray]]:
     """Function to solve for the anti-symmetric Q matrix in the equation:
         F.Q - (F.Q)^T = F.D - (F.D)^T
     using least squares.
 
-    Parameters
-    ----------
-        D: :class:`~numpy.ndarray`
-            A symmetric diffusion matrix.
-        F: :class:`~numpy.ndarray`
-            Jacobian of the vector field function evaluated at a particular point.
-        debug: bool
-            Whether additional info of the solution is returned.
-        precompute_jac: bool
-            Whether the analytical Jacobian is precomputed for the optimizer.
-
-    Returns
-    -------
-        Q: :class:`~numpy.ndarray`
-            The solved anti-symmetric Q matrix.
-        C: :class:`~numpy.ndarray`
-            The right-hand side of the equation to be solved.
+    Args:
+        D: A symmetric diffusion matrix.
+        F: Jacobian of the vector field function evaluated at a particular point.
+        q0: Initial guess of the solution Q
+        debug: Whether additional info of the solution is returned.
+        precompute_jac: Whether the analytical Jacobian is precomputed for the optimizer.
+
+    Returns:
+        The solved anti-symmetric Q matrix and the value of the right-hand side of the equation to be solved, optionally along with the value of the left-hand side of the equation and the cost of the solution if debug is True.
     """
 
     n = D.shape[0]
     m = int(n * (n - 1) / 2)
     C = f_left(D, F)
     f_obj = lambda q: (f_left(squareform(q, True), F) - C).flatten()
     q0 = np.ones(m, dtype=float) if q0 is None else q0
@@ -71,56 +71,48 @@
     if debug:
         C_left = f_left(Q, F)
         return Q, C, C_left, sol.cost
     else:
         return Q, C
 
 
-def Ao_pot_map(vecFunc, X, D=None, **kwargs):
+def Ao_pot_map(
+    vecFunc: Callable, X: np.ndarray, fjac: Optional[Callable] = None, D: Optional[np.ndarray] = None, **kwargs
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List, List, List]:
     """Mapping potential landscape with the algorithm developed by Ao method.
     References: Potential in stochastic differential equations: novel construction. Journal of physics A: mathematical and
         general, Ao Ping, 2004
 
-    Parameters
-    ----------
-        vecFunc: `function`
-            The vector field function
-        X: :class:`~numpy.ndarray`
-            A n_cell x n_dim matrix of coordinates where the potential function is evaluated.
-        D: None or :class:`~numpy.ndarray`
-            Diffusion matrix. It must be a square matrix with size corresponds to the number of columns (features) in the X matrix.
-
-    Returns
-    -------
-        X: :class:`~numpy.ndarray`
-            A matrix storing the x-coordinates on the two-dimesional grid.
-        U: :class:`~numpy.ndarray`
-            A matrix storing the potential value at each position.
-        P: :class:`~numpy.ndarray`
-            Steady state distribution or the Boltzmann-Gibbs distribution for the state variable.
-        vecMat: list
-            List velocity vector at each position from X.
-        S: list
-            List of constant symmetric and semi-positive matrix or friction matrix, corresponding to the divergence part,
+    Args:
+        vecFunc: The vector field function.
+        X: A (n_cell x n_dim) matrix of coordinates where the potential function is evaluated.
+        fjac: function that returns the Jacobian of the vector field function evaluated at a particular point.
+        D: Diffusion matrix. It must be a square matrix with size corresponds to the number of columns (features) in the X matrix.
+
+    Returns:
+        X: A matrix storing the x-coordinates on the two-dimesional grid.
+        U: A matrix storing the potential value at each position.
+        P: Steady state distribution or the Boltzmann-Gibbs distribution for the state variable.
+        vecMat: List velocity vector at each position from X.
+        S: List of constant symmetric and semi-positive matrix or friction (dissipative) matrix, corresponding to the divergence part,
             at each position from X.
-        A: list
-            List of constant antisymmetric matrix or transverse matrix, corresponding to the curl part, at each position
+        A: List of constant antisymmetric matrix or transverse (non-dissipative) matrix, corresponding to the curl part, at each position
             from X.
     """
 
     import numdifftools as nda
 
     nobs, ndim = X.shape
     D = 0.1 * np.eye(ndim) if D is None else D
     U = np.zeros((nobs, 1))
     vecMat, S, A = [None] * nobs, [None] * nobs, [None] * nobs
 
     for i in range(nobs):
         X_s = X[i, :]
-        F = nda.Jacobian(vecFunc)(X_s)
+        F = nda.Jacobian(vecFunc)(X_s) if fjac is None else fjac(X_s)
         Q, _ = solveQ(D, F, **kwargs)
         H = np.linalg.inv(D + Q).dot(F)
         U[i] = -0.5 * X_s.dot(H).dot(X_s)
 
         vecMat[i] = vecFunc(X_s)
         S[i], A[i] = (
             (np.linalg.inv(D + Q) + np.linalg.inv((D + Q).T)) / 2,
@@ -128,15 +120,14 @@
         )
 
     P = np.exp(-U)
     P = P / np.sum(P)
 
     return X, U, P, vecMat, S, A
 
-
 def Ao_pot_map_jac(fjac, X, D=None, **kwargs):
     nobs, ndim = X.shape
     if D is None:
         D = 0.1 * np.eye(ndim)
     elif np.isscalar(D):
         D = D * np.eye(ndim)
     U = np.zeros((nobs, 1))
@@ -146,8 +137,8 @@
     for i in tqdm(range(nobs), "Calc Ao potential"):
         X_s = X[i, :]
         F = fjac(X_s)
         Q, _ = solveQ(D, F, q0=q0, **kwargs)
         H = np.linalg.inv(D + Q).dot(F)
         U[i] = -0.5 * X_s.dot(H).dot(X_s)
 
-    return U.flatten()
+    return U.flatten()
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/Bhattacharya.py` & `dynamo-release-1.3.0/dynamo/vectorfield/Bhattacharya.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,57 +1,50 @@
+from typing import Callable, Tuple, Union
+
 import numpy as np
 
 # from scipy.integrate import odeint
 from scipy.interpolate import griddata
 
 
-def path_integral(VecFnc, x_lim, y_lim, xyGridSpacing, dt=1e-2, tol=1e-2, numTimeSteps=1400):
+def path_integral(
+    VecFnc: Callable,
+    x_lim: np.ndarray,
+    y_lim: np.ndarray,
+    xyGridSpacing: Union[int, float],
+    dt: float = 1e-2,
+    tol: float = 1e-2,
+    numTimeSteps: int = 1400,
+) -> Tuple[
+    int, np.ndarray, np.ndarray, np.ndarray, int, int, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray
+]:
     """A deterministic map of Waddington’s epigenetic landscape for cell fate specification
     Sudin Bhattacharya, Qiang Zhang and Melvin E. Andersen
 
-    Parameters
-    ----------
-    VecFnc
-    x_lim: `list`
-        Lower or upper limit of x-axis.
-    y_lim: `list`
-        Lower or upper limit of y-axis
-    xyGridSpacing: `float`
-        Grid spacing for "starting points" for each "path" on the pot. surface
-    dt: `float`
-        Time step for the path integral.
-    tol: `float` (default: 1.0e-2)
-        Tolerance to test for convergence.
-    numTimeSteps: `int`
-        A high-enough number for convergence with given dt.
-
-    Returns
-    -------
-    numAttractors: `int`
-        Number of attractors identified by the path integral approach.
-    attractors_num_X_Y: `numpy.ndarray`
-        Attractor number and the corresponding x, y coordinates.
-    sepx_old_new_pathNum: `numpy.ndarray`
-        The IDs of the two attractors for each separaxis per row.
-    numPaths_att `numpy.ndarray`
-        Number of paths per attractor
-    numPaths: `int`
-        Total Number of paths for defined grid spacing.
-    numTimeSteps: `int`
-        A high-enough number for convergence with given dt.
-    pot_path: `numpy.ndarray` (dimension: numPaths x numTimeSteps)
-        Potential along the path.
-    path_tag: `numpy.ndarray` (dimension: numPaths x 1)
-        Tag for given path (to denote basin of attraction).
-    attractors_pot: `numpy.ndarray`
-        Potential value of each identified attractors by the path integral approach.
-    x_path: `numpy.ndarray`
-        x-coord. along path.
-    y_path: `numpy.ndarray`
-        y-coord. along path.
+    Args:
+        VecFnc: The function of vector field that takes in x, y and returns the velocity at that point.
+        x_lim: Lower or upper limit of x-axis.
+        y_lim: Lower or upper limit of y-axis
+        xyGridSpacing: Grid spacing for "starting points" for each "path" on the potential surface
+        dt: Time step for the path integral.
+        tol: Tolerance to test for convergence.
+        numTimeSteps: A high-enough number for convergence with given dt.
+
+    Returns:
+        numAttractors: Number of attractors identified by the path integral approach.
+        attractors_num_X_Y: Attractor number and the corresponding x, y coordinates.
+        sepx_old_new_pathNum: The IDs of the two attractors for each separaxis per row.
+        numPaths_att: Number of paths per attractor
+        numPaths: Total Number of paths for defined grid spacing.
+        numTimeSteps: A high-enough number for convergence with given dt.
+        pot_path: Potential along the path. (dimension: numPaths x numTimeSteps)
+        path_tag: Tag for given path (to denote basin of attraction). (dimension: numPaths x 1)
+        attractors_pot: Potential value of each identified attractors by the path integral approach.
+        x_path: x-coord along path.
+        y_path: y-coord along path.
     """
 
     # -- First, generate potential surface from deterministic rate equations –
 
     # Define grid spacing for "starting points" for each "path" on the pot. surface
 
     # Define grid spacing for "starting points" for each "path" on the pot. surface
@@ -275,15 +268,15 @@
 
                     # update no. of paths for current attractor
                     # (path tag already updated at start of path-counter loop)
                     tag = path_tag[path_counter]
                     numPaths_att[tag - 1] = numPaths_att[tag - 1] + 1
 
             # increment "path counter"
-            path_counter = path_counter + 1
+            path_counter += 1
 
     return (
         attractors_num_X_Y,
         sepx_old_new_pathNum,
         numPaths_att,
         num_attractors,
         numPaths,
@@ -293,76 +286,62 @@
         attractors_pot,
         x_path,
         y_path,
     )
 
 
 def alignment(
-    numPaths,
-    numTimeSteps,
-    pot_path,
-    path_tag,
-    attractors_pot,
-    x_path,
-    y_path,
-    grid=100,
-    interpolation_method="linear",
+    numPaths: int,
+    numTimeSteps: int,
+    pot_path: np.ndarray,
+    path_tag: np.ndarray,
+    attractors_pot: np.ndarray,
+    x_path: np.ndarray,
+    y_path: np.ndarray,
+    grid: int = 100,
+    interpolation_method: str = "linear",
 ):
     """Align potential values so all path-potentials end up at same global min and then generate potential surface with
     interpolation on a grid.
 
-    Parameters
-    ----------
-    numPaths: `int`
-        Total Number of paths for defined grid spacing.
-    numTimeSteps: `int`
-        A high-enough number for convergence with given dt.
-    pot_path: `numpy.ndarray` (dimension: numPaths x numTimeSteps)
-        Potential along the path.
-    path_tag: `numpy.ndarray` (dimension: numPaths x 1)
-        Tag for given path (to denote basin of attraction).
-    attractors_pot: `numpy.ndarray`
-        Potential value of each identified attractors by the path integral approach.
-    x_path: `numpy.ndarray`
-        x-coord. along path.
-    y_path: `numpy.ndarray`
-        y-coord. along path.
-    grid: `int`
-        No. of grid lines in x- and y- directions
-    interpolation_method: `string`
-        Method of interpolation in griddata function. One of
-
-        ``nearest``
-          return the value at the data point closest to
-          the point of interpolation.  See `NearestNDInterpolator` for
-          more details.
-
-        ``linear``
-          tessellate the input point set to n-dimensional
-          simplices, and interpolate linearly on each simplex.  See
-          `LinearNDInterpolator` for more details.
-
-        ``cubic`` (1-D)
-          return the value determined from a cubic
-          spline.
-
-        ``cubic`` (2-D)
-          return the value determined from a
-          piecewise cubic, continuously differentiable (C1), and
-          approximately curvature-minimizing polynomial surface. See
-          `CloughTocher2DInterpolator` for more details.
-
-    Returns
-    -------
-    Xgrid: `numpy.ndarray`
-        x-coordinates of the Grid produced from the meshgrid function.
-    Ygrid: `numpy.ndarray`
-            y-coordinates of the Grid produced from the meshgrid function.
-    Zgrid: `numpy.ndarray`
-            z-coordinates or potential at each of the x/y coordinate.
+    Args:
+        numPaths: Total Number of paths for defined grid spacing.
+        numTimeSteps: A high-enough number for convergence with given dt.
+        pot_path: Potential along the path. (dimension: numPaths x numTimeSteps)
+        path_tag: Tag for given path (to denote basin of attraction). (dimension: numPaths x 1)
+        attractors_pot: Potential value of each identified attractors by the path integral approach.
+        x_path: x-coord. along path.
+        y_path: y-coord. along path.
+        grid: No. of grid lines in x- and y- directions
+        interpolation_method: Method of interpolation in griddata function. One of
+
+            ``nearest``
+            return the value at the data point closest to
+            the point of interpolation.  See `NearestNDInterpolator` for
+            more details.
+
+            ``linear``
+            tessellate the input point set to n-dimensional
+            simplices, and interpolate linearly on each simplex.  See
+            `LinearNDInterpolator` for more details.
+
+            ``cubic`` (1-D)
+            return the value determined from a cubic
+            spline.
+
+            ``cubic`` (2-D)
+            return the value determined from a
+            piecewise cubic, continuously differentiable (C1), and
+            approximately curvature-minimizing polynomial surface. See
+            `CloughTocher2DInterpolator` for more details.
+
+    Returns:
+        Xgrid: x-coordinates of the Grid produced from the meshgrid function.
+        Ygrid: y-coordinates of the Grid produced from the meshgrid function.
+        Zgrid: z-coordinates or potential at each of the x/y coordinate.
     """
 
     # -- need 1-D "lists" (vectors) to plot all x,y, Pot values along paths --
     list_size = numPaths * numTimeSteps
 
     x_p_list = np.zeros((list_size, 1))
     y_p_list = np.zeros((list_size, 1))
@@ -406,7 +385,10 @@
         method=interpolation_method,
     )
     Zgrid = Zgrid.reshape(Xgrid.shape)
 
     # print('Ran surface grid-interpolation okay!\n')
 
     return Xgrid, Ygrid, Zgrid
+
+
+# %%
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/Wang.py` & `dynamo-release-1.3.0/dynamo/vectorfield/Wang.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,33 +1,28 @@
+from typing import Callable, Tuple
+
 import numpy as np
 from scipy import optimize
+from scipy.optimize import OptimizeResult
 
 
-def Wang_action(X_input, F, D, dim, N, lamada_=1):
+def Wang_action(X_input: np.ndarray, F: Callable, D: float, dim: int, N: int, lamada_: float = 1) -> float:
     """Calculate action by path integral by Wang's method.
     Quantifying the Waddington landscape and biological paths for development and differentiation. Jin Wang, Kun Zhang,
     Li Xu, and Erkang Wang, PNAS, 2011
 
-    Parameters
-    ----------
-        X_input: `numpy.ndarray`
-            The initial guess of the least action path. Default is a straight line connecting the starting and end path.
-        F: `Function`
-            The reconstructed vector field function. This is assumed to be time-independent.
-        D: `float`
-            The diffusion constant. Note that this can be a space-dependent matrix.
-        dim: `int`
-            The feature numbers of the input data.
-        N: `int`
-            Number of waypoints along the least action path.
-        lamada_: `float`
-            Regularization parameter
+    Args:
+        X_input: The initial guess of the least action path. Default is a straight line connecting the starting and end path.
+        F: The reconstructed vector field function. This is assumed to be time-independent.
+        D: The diffusion constant. Note that this can be a space-dependent matrix.
+        dim: The feature numbers of the input data.
+        N: Number of waypoints along the least action path.
+        lamada_: Regularization parameter
 
-    Returns
-    -------
+    Returns:
         The action function calculated by the Hamilton-Jacobian method.
     """
 
     X_input = X_input.reshape((int(dim), -1)) if len(X_input.shape) == 1 else X_input
 
     delta, delta_l = delta_delta_l(X_input)
 
@@ -52,74 +47,57 @@
     import numdifftools as nda
 
     V_jacobina = nda.Jacobian(F)
 
     return V_jacobina(X)
 
 
-def V(F, D, X):
+def V(F: Callable, D: float, X: np.ndarray) -> np.ndarray:
     """Calculate V
 
-    Parameters
-    ----------
+    Args:
         F: `Function`
             The reconstructed vector field function
         D: `float`
             The diffusion constant
         X: `nummpy.ndarray`
             The input coordinates corresponding to the cell states.
 
-    Returns
-    -------
+    Returns:
         Returns V
     """
 
     V = 1 / (4 * D) * np.sum(F(X) ** 2) + 1 / 2 * np.trace(V_jacobina(F, X))
 
     return V
 
 
-def delta_delta_l(X_input):
-    """Calculate delta_L
-
-    Parameters
-    ----------
-    X_input: `numpy.ndarray`
-
-    Returns
-    -------
-    Return delta_L
-    """
+def delta_delta_l(X_input) -> Tuple[np.ndarray, float]:
+    """Calculate delta_L"""
 
     delta = np.diff(X_input, 1, 1)
     delta_l = np.sqrt(np.sum(delta**2, 0))
 
     return delta, delta_l
 
 
-def Wang_LAP(F, n_points, point_start, point_end, D=0.1, lambda_=1):
+def Wang_LAP(
+    F: Callable, n_points: int, point_start: np.ndarray, point_end: np.ndarray, D: float = 0.1, lambda_: float = 1
+) -> OptimizeResult:
     """Calculating least action path based methods from Jin Wang and colleagues (http://www.pnas.org/cgi/doi/10.1073/pnas.1017017108)
 
-    Parameters
-    ----------
-        F: `Function`
-            The reconstructed vector field function
-        n_points: 'int'
-            The number of points along the least action path.
-        point_start: 'np.ndarray'
-            The matrix for storing the coordinates (gene expression configuration) of the start point (initial cell state).
-        point_end: 'np.ndarray'
-            The matrix for storing the coordinates (gene expression configuration) of the end point (terminal cell state).
-        D: `float`
-            The diffusion constant. Note that this can be a space-dependent matrix.
-        lamada_: `float`
-            Regularization parameter
+    Args:
+        F: The reconstructed vector field function
+        n_points: The number of points along the least action path.
+        point_start: The matrix for storing the coordinates (gene expression configuration) of the start point (initial cell state).
+        point_end: The matrix for storing the coordinates (gene expression configuration) of the end point (terminal cell state).
+        D: The diffusion constant. Note that this can be a space-dependent matrix.
+        lamada_: Regularization parameter
 
-    Returns
-    -------
+    Returns:
         The least action path and the action way of the inferred path.
     """
     initpath = point_start.dot(np.ones((1, n_points + 1))) + (point_end - point_start).dot(
         np.linspace(0, 1, n_points + 1, endpoint=True).reshape(1, -1)
     )
 
     dim, N = initpath.shape
@@ -129,69 +107,57 @@
         x0=initpath,
         minimizer_kwargs={"args": (F, D, dim, N, lambda_)},
     )
 
     return res
 
 
-def transition_rate(X_input, F, D=0.1, lambda_=1):
+def transition_rate(X_input: np.ndarray, F: Callable, D: float = 0.1, lambda_: float = 1) -> np.ndarray:
     """Calculate the rate to convert from one cell state to another cell state by taking the optimal path.
 
      In the small noise limit (D -> 0) the Wentzell-Freidlin theory states that the transition rate from one basin to
      another one to a leading order is related to the minimum action corresponding zero energy path (Eeff = 0) connecting
      the starting fixed point and the saddle point x_{sd} by k \approx exp(−S0 (x_{sd})). To take into account that for finite
      noise, the actual optimal path bypasses the saddle point, in Eqn. 2 of the main text a transition rate is
      actually estimated by the action of the whole path connecting the two fixed points, giving that the portion of the path
      following the vector field contributes zero action. Here we have neglected some pre-exponential factor (see Eq. 5.24 of
      reference [15]), which is expected to be on the order of 1 [12]. (Reference: Epigenetic state network approach for
      describing cell phenotypic transitions. Ping Wang, Chaoming Song, Hang Zhang, Zhanghan Wu, Xiao-Jun Tian and Jianhua Xing)
 
-    Parameters
-    ----------
-        X_input: `numpy.ndarray`
-            The initial guess of the least action path. Default is a straight line connecting the starting and end path.
-        F: `Function`
-            The reconstructed vector field function
-        D: `float`
-            The diffusion constant. Note that this can be a space-dependent matrix.
-        lamada_: `float`
-            Regularization parameter
+    Args:
+        X_input: The initial guess of the least action path. Default is a straight line connecting the starting and end path.
+        F: The reconstructed vector field function
+        D: The diffusion constant. Note that this can be a space-dependent matrix.
+        lamada_: Regularization parameter
 
-    Returns
-    -------
+    Returns:
         The transition to convert from one cell state to another.
     """
 
     res = Wang_LAP(X_input, F, D=D, lambda_=lambda_)
     r = np.exp(-res)
 
     return r
 
 
-def MFPT(X_input, F, D=0.1, lambda_=1):
+def MFPT(X_input: np.ndarray, F: Callable, D: float = 0.1, lambda_: float = 1) -> float:
     """Calculate the MFPT (mean first passage time) to convert from one cell state to another cell state by taking the optimal path.
 
      The mean first-passage time (MFPT) defines an average timescale for a stochastic event to first occur. The MFPT maps
      a multi-step kinetic process to a coarse-grained timescale for reaching a final state, having started at some initial
      state. The inverse of the MFPT is an effective rate of the overall reaction. (reference: Mean First-Passage Times in Biology
      Nicholas F. Polizzi,a Michael J. Therien,b and David N. Beratan)
 
-    Parameters
-    ----------
-        X_input: `numpy.ndarray`
-            The initial guess of the least action path. Default is a straight line connecting the starting and end path.
-        F: `Function`
-            The reconstructed vector field function
-        D: `float`
-            The diffusion constant. Note that this can be a space-dependent matrix.
-        lamada_: `float`
-            Regularization parameter
+    Args:
+        X_input: The initial guess of the least action path. Default is a straight line connecting the starting and end path.
+        F: The reconstructed vector field function
+        D: The diffusion constant. Note that this can be a space-dependent matrix.
+        lamada_: Regularization parameter
 
-    Returns
-    -------
+    Returns:
         The transition to convert from one cell state to another.
     """
 
     r = transition_rate(X_input, F, D=D, lambda_=lambda_)
     t = 1 / r
 
     return t
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/__init__.py` & `dynamo-release-1.3.0/dynamo/vectorfield/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,14 +4,27 @@
 from .Ao import Ao_pot_map, solveQ
 from .Bhattacharya import alignment, path_integral
 from .cell_vectors import cell_accelerations, cell_curvatures
 
 # vector field clustering related:
 from .clustering import cluster_field, streamline_clusters
 from .networks import adj_list_to_matrix, build_network_per_cluster
+from .rank_vf import (
+    aggregateRegEffs,
+    rank_acceleration_genes,
+    rank_cells,
+    rank_curvature_genes,
+    rank_divergence_genes,
+    rank_expression_genes,
+    rank_genes,
+    rank_jacobian_genes,
+    rank_s_divergence_genes,
+    rank_sensitivity_genes,
+    rank_velocity_genes,
+)
 
 # potential related
 from .scPotential import (  # , vector_field_function
     DiffusionMatrix,
     IntGrad,
     Pot,
     Potential,
@@ -36,31 +49,20 @@
     VectorField2D,
     assign_fixedpoints,
     topography,
 )
 from .utils import get_jacobian, parse_int_df, vector_field_function
 from .vector_calculus import (
     acceleration,
-    aggregateRegEffs,
     curl,
     curvature,
     divergence,
     hessian,
     jacobian,
     laplacian,
-    rank_acceleration_genes,
-    rank_cells,
-    rank_curvature_genes,
-    rank_divergence_genes,
-    rank_expression_genes,
-    rank_genes,
-    rank_jacobian_genes,
-    rank_s_divergence_genes,
-    rank_sensitivity_genes,
-    rank_velocity_genes,
     sensitivity,
     speed,
     torsion,
     velocities,
 )
 
 # vfGraph operation related:
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/cell_vectors.py` & `dynamo-release-1.3.0/dynamo/vectorfield/cell_vectors.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,25 @@
+from typing import Dict
+
+from anndata import AnnData
+
 from ..tools.cell_velocities import cell_velocities
 from .topography import VectorField
 from .vector_calculus import acceleration, curvature
 
 
 def cell_accelerations(
-    adata, vf_basis="pca", basis="umap", enforce=True, preserve_len=True, other_kernels_dict={}, **kwargs
-):
+    adata: AnnData,
+    vf_basis: str = "pca",
+    basis: str = "umap",
+    enforce: bool = True,
+    preserve_len: bool = True,
+    other_kernels_dict: Dict = {},
+    **kwargs
+) -> None:
     """Compute RNA acceleration field via reconstructed vector field and project it to low dimensional embeddings.
 
     In classical physics, including fluidics and aerodynamics, velocity and acceleration vector fields are used as
     fundamental tools to describe motion or external force of objects, respectively. In analogy, RNA velocity or
     accelerations estimated from single cells can be regarded as samples in the velocity (La Manno et al. 2018) or
     acceleration vector field (Gorin, Svensson, and Pachter 2019). In general, a vector field can be defined as a
     vector-valued function f that maps any points (or cells’ expression state) x in a domain Ω with D dimension (or the
@@ -28,44 +38,35 @@
     regions of the gene expression state space under a steady flow field. Another more intuitive way to visualize the
     structure of vector field is the so called line integral convolution method or LIC (Cabral and Leedom 1993), which
     works by adding random black-and-white paint sources on the vector field and letting the flowing particle on the
     vector field picking up some texture to ensure the same streamline having similar intensity. Although we have not
     provides such functionalities in dynamo, with vector field that changes over time, similar methods, for example,
     streakline, pathline, timeline, etc. can be used to visualize the evolution of single cell or cell populations.
 
-    Arguments
-    ---------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        vf_basis: 'int' (optional, default `pca`)
-            The dictionary key that corresponds to the low dimensional embedding where the vector field function
+    Args:
+        adata: an Anndata object.
+        vf_basis: The dictionary key that corresponds to the low dimensional embedding where the vector field function
             reconstructed.
-        basis: 'int' (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute.
-        enforce: `bool` (default: `False`)
-            Whether to enforce 1) redefining use_for_transition column in obs attribute;
+        basis: The dictionary key that corresponds to the reduced dimension in `.obsm` attribute.
+        enforce: Whether to enforce 1) redefining use_for_transition column in obs attribute;
                                2) recalculation of transition matrix.
-        preserve_len: `bool` (default: `True`)
-            Whether to preserve the length of high dimension vector length. When set to be True, the length  of low
+        preserve_len: Whether to preserve the length of high dimension vector length. When set to be True, the length  of low
             dimension projected vector will be proportionally scaled to that of the high dimensional vector. Note that
-            when preserve_len is set to be True, the acceleration field may seem to be messy (although the magnitude will
-            be reflected) while the trend of acceleration when `preserve_len` is `True` is more clearer but will lose
+            when `preserve_len` is set to be `True`, the acceleration field may seem to be messy (although the magnitude will
+            be reflected) while the trend of acceleration when `preserve_len` is `False` is clearer but will lose
             information of acceleration magnitude. This is because the acceleration is not directly related to the
             distance of cells in the low embedding space; thus the acceleration direction can be better preserved than
             the magnitude. On the other hand, velocity is more relevant to the distance in low embedding space, so
             preserving magnitude and direction of velocity vector in low dimension can be more easily achieved.
-        other_kernels_dict: `dict` (default: `{}`)
-            A dictionary of paramters that will be passed to the cosine/correlation kernel.
+        other_kernels_dict: A dictionary of paramters that will be passed to the cosine/correlation kernel.
 
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            Returns an updated `~anndata.AnnData` with transition_matrix and projected embedding of high dimension
+    Returns:
+        adata: Returns an updated `~anndata.AnnData` with transition_matrix and projected embedding of high dimension
             acceleration vectors in the existing embeddings of current cell state, calculated using either the Itô
-            kernel method (default) or the diffusion approximation or the method from (La Manno et al. 2018).
+            kernel method (default), the diffusion approximation, or the method from (La Manno et al. 2018).
     """
 
     if "velocity_" + vf_basis not in adata.obsm.keys():
         cell_velocities(adata, basis=vf_basis)
 
     if "VecFld_" + vf_basis not in adata.uns_keys():
         VectorField(adata, basis=vf_basis)
@@ -95,16 +96,22 @@
             preserve_len=preserve_len,
             other_kernels_dict=other_kernels_dict,
             **kwargs
         )
 
 
 def cell_curvatures(
-    adata, vf_basis="pca", basis="umap", enforce=True, preserve_len=True, other_kernels_dict={}, **kwargs
-):
+    adata: AnnData,
+    vf_basis: str = "pca",
+    basis: str = "umap",
+    enforce: bool = True,
+    preserve_len: bool = True,
+    other_kernels_dict: Dict = {},
+    **kwargs
+) -> None:
     """Compute RNA curvature field via reconstructed vector field and project it to low dimensional embeddings.
 
     In classical physics, including fluidics and aerodynamics, velocity and acceleration vector fields are used as
     fundamental tools to describe motion or external force of objects, respectively. In analogy, RNA velocity or
     accelerations estimated from single cells can be regarded as samples in the velocity (La Manno et al. 2018) or
     acceleration vector field (Gorin, Svensson, and Pachter 2019). In general, a vector field can be defined as a
     vector-valued function f that maps any points (or cells’ expression state) x in a domain Ω with D dimension (or the
@@ -123,42 +130,33 @@
     regions of the gene expression state space under a steady flow field. Another more intuitive way to visualize the
     structure of vector field is the so called line integral convolution method or LIC (Cabral and Leedom 1993), which
     works by adding random black-and-white paint sources on the vector field and letting the flowing particle on the
     vector field picking up some texture to ensure the same streamline having similar intensity. Although we have not
     provides such functionalities in dynamo, with vector field that changes over time, similar methods, for example,
     streakline, pathline, timeline, etc. can be used to visualize the evolution of single cell or cell populations.
 
-    Arguments
-    ---------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        vf_basis: 'int' (optional, default `pca`)
-            The dictionary key that corresponds to the low dimensional embedding where the vector field function
+    Args:
+        adata: an AnnData object.
+        vf_basis: The dictionary key that corresponds to the low dimensional embedding where the vector field function
             reconstructed.
-        basis: 'int' (optional, default `umap`)
-            The dictionary key that corresponds to the reduced dimension in `.obsm` attribute.
-        enforce: `bool` (default: `False`)
-            Whether to enforce 1) redefining use_for_transition column in obs attribute;
+        basis: The dictionary key that corresponds to the reduced dimension in `.obsm` attribute.
+        enforce: Whether to enforce 1) redefining use_for_transition column in obs attribute;
                                2) recalculation of transition matrix.
-        preserve_len: `bool` (default: `True`)
-            Whether to preserve the length of high dimension vector length. When set to be True, the length  of low
+        preserve_len: Whether to preserve the length of high dimension vector length. When set to be True, the length  of low
             dimension projected vector will be proportionally scaled to that of the high dimensional vector. Note that
             when preserve_len is set to be True, the acceleration field may seem to be messy (although the magnitude will
             be reflected) while the trend of acceleration when `preserve_len` is `True` is more clearer but will lose
             information of acceleration magnitude. This is because the acceleration is not directly related to the
             distance of cells in the low embedding space; thus the acceleration direction can be better preserved than
             the magnitude. On the other hand, velocity is more relevant to the distance in low embedding space, so
             preserving magnitude and direction of velocity vector in low dimension can be more easily achieved.
-        other_kernels_dict: `dict` (default: `{}`)
-            A dictionary of paramters that will be passed to the cosine/correlation kernel.
+        other_kernels_dict: A dictionary of paramters that will be passed to the cosine/correlation kernel.
 
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            Returns an updated `~anndata.AnnData` with transition_matrix and projected embedding of high dimension
+    Returns:
+        adata: Returns an updated `~anndata.AnnData` with transition_matrix and projected embedding of high dimension
             curvature vectors in the existing embeddings of current cell state, calculated using either the Itô kernel
             method (default) or the diffusion approximation or the method from (La Manno et al. 2018).
     """
 
     if "velocity_" + vf_basis not in adata.obsm.keys():
         cell_velocities(adata, basis=vf_basis)
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/clustering.py` & `dynamo-release-1.3.0/dynamo/vectorfield/clustering.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,41 +1,42 @@
-from typing import Union
+from typing import List, Optional, Union
 
 import numpy as np
 import pandas as pd
 from anndata import AnnData
 from scipy.sparse import csr_matrix
 from scipy.stats import mode
 from sklearn.neighbors import NearestNeighbors
 
 from ..dynamo_logger import main_info
-from ..preprocessing.utils import pca_monocle
+from ..preprocessing.pca import pca
 from ..tools.clustering import hdbscan, infomap, leiden, louvain
 from ..tools.Markov import (
     grid_velocity_filter,
     prepare_velocity_grid_data,
     velocity_on_grid,
 )
 from ..utils import LoggerManager, copy_adata
 from .scVectorField import SvcVectorField
 from .utils import vecfld_from_adata
 
 
 def cluster_field(
-    adata,
-    basis="pca",
-    features=["speed", "potential", "divergence", "acceleration", "curvature", "curl"],
-    add_embedding_basis=True,
-    embedding_basis=None,
-    normalize=False,
-    method="leiden",
-    cores=1,
-    copy=False,
+    adata: AnnData,
+    basis: str = "pca",
+    features: List = ["speed", "potential", "divergence", "acceleration", "curvature", "curl"],
+    add_embedding_basis: bool = True,
+    embedding_basis: Optional[str] = None,
+    normalize: bool = False,
+    method: str = "leiden",
+    cores: int = 1,
+    copy: bool = False,
+    resolution: float = 1.0,
     **kwargs,
-):
+) -> Optional[AnnData]:
     """Cluster cells based on vector field features.
 
     We would like to see whether the vector field can be used to better define cell state/types. This can be accessed
     via characterizing critical points (attractor/saddle/repressor, etc.) and characteristic curves (nullcline,
     separatrix). However, the calculation of those is not easy, for example, a strict definition of an attractor is
     states where velocity is 0 and the eigenvalue of the jacobian matrix at that point is all negative. Under this
     strict definition, we may sometimes find the attractors are very far away from our sampled cell states which makes
@@ -43,68 +44,50 @@
     velocity projection. This is not unexpected as the vector field we learned is defined via a set of basis functions
     based on gaussian kernels and thus it is hard to satisfy that strict definition.
 
     Fortunately, we can handle this better with the help of a different set of ideas. Instead of using critical points
     by the classical dynamic system methods, we can use some machine learning approaches that are based on extracting
     geometric features of streamline to "cluster vector field space" for define cell states/type. This requires
     calculating, potential (ordered pseudotime), speed, curliness, divergence, acceleration, curvature, etc. Thanks to
-    the fact that we can analytically calculate Jacobian matrix matrix, those quantities of the vector field function
+    the fact that we can analytically calculate the Jacobian matrix, those quantities of the vector field function
     can be conveniently and efficiently calculated.
 
-    Parameters
-    ----------
-    adata: :class:`~anndata.AnnData`.
-        adata object that includes both newly synthesized and total gene expression of cells. Alternatively,
-        the object should include both unspliced and spliced gene expression of cells.
-    basis: `str` or None (default: `None`)
-        The space that will be used for calculating vector field features. Valid names includes, for example, `pca`,
-        `umap`, etc.
-    embedding_basis: `str` or None (default: `None`)
-        The embedding basis that will be combined with the vector field feature space for clustering.
-    normalize: `bool` (default: `False`)
-        Whether to mean center and scale the feature across all cells.
-    method: `str` (default: `leiden`)
-        The method that will be used for clustering, one of `{'kmeans'', 'hdbscan', 'louvain', 'leiden'}`. If `louvain`
-        or `leiden` used, you need to have `cdlib` installed.
-    cores: `int` (default: 1)
-        The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a
-        :obj:`joblib.parallel_backend` context.
-        ``-1`` means using all processors.
-    copy:
-        Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments.
-    kwargs:
-        Any additional arguments that will be passed to either kmeans, hdbscan, louvain or leiden clustering algorithms.
+    Args:
+        adata: adata object that includes both newly synthesized and total gene expression of cells. Alternatively,
+            the object should include both unspliced and spliced gene expression of cells.
+        basis: The space that will be used for calculating vector field features. Valid names includes, for example, `pca`,
+            `umap`, etc.
+        features: features have to be selected from ['speed', 'potential', 'divergence', 'acceleration', 'curvature', 'curl']
+        add_embedding_basis: Whether to add the embedding basis to the feature space for clustering.
+        embedding_basis: The embedding basis that will be combined with the vector field feature space for clustering.
+        normalize: Whether to mean center and scale the feature across all cells.
+        method: The method that will be used for clustering, one of `{'kmeans'', 'hdbscan', 'louvain', 'leiden'}`. If `louvain`
+            or `leiden` used, you need to have `cdlib` installed.
+        cores: The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a
+            :obj:`joblib.parallel_backend` context.
+            ``-1`` means using all processors.
+        copy: Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments.
+        resolution: Clustering resolution, higher values yield more fine-grained clusters.
+        kwargs: Any additional arguments that will be passed to either kmeans, hdbscan, louvain or leiden clustering algorithms.
 
-    Returns
-    -------
+    Returns:
+        Either updates `adata` or directly returns a new `adata` object if `copy` is `True`.
 
     """
 
     logger = LoggerManager.gen_logger("dynamo-cluster_field")
     logger.log_time()
     adata = copy_adata(adata) if copy else adata
 
-    if method in ["louvain", "leiden"]:
-        try:
-            from cdlib import algorithms
-
-            "leiden" in dir(algorithms)
-
-        except ImportError:
-            raise ImportError(
-                "You need to install the excellent package `cdlib` if you want to use louvain or leiden "
-                "for clustering."
-            )
-
     features = list(
         set(features).intersection(["speed", "potential", "divergence", "acceleration", "curvature", "curl"])
     )
     if len(features) < 1:
         raise ValueError(
-            "features has to be selected from ['speed', 'potential', 'divergence', 'acceleration', "
+            "features have to be selected from ['speed', 'potential', 'divergence', 'acceleration', "
             f"'curvature', 'curl']. your feature is {features}"
         )
 
     feature_key = [
         "speed_" + basis,
         basis + "_ddhodge_potential",
         "divergence_" + basis,
@@ -188,31 +171,19 @@
         graph = csr_matrix(
             (np.repeat(1, len(col)), (row, col)),
             shape=(adata.n_obs, adata.n_obs),
         )
         adata.obsp["vf_feature_knn"] = graph
 
         if method == "leiden":
-            leiden(
-                adata,
-                adj_matrix_key="vf_feature_knn",
-                result_key="field_leiden",
-            )
+            leiden(adata, resolution=resolution, adj_matrix_key="vf_feature_knn", result_key="field_leiden", **kwargs)
         elif method == "louvain":
-            louvain(
-                adata,
-                adj_matrix_key="vf_feature_knn",
-                result_key="field_louvain",
-            )
+            louvain(adata, resolution=resolution, adj_matrix_key="vf_feature_knn", result_key="field_louvain", **kwargs)
         elif method == "infomap":
-            infomap(
-                adata,
-                adj_matrix_key="vf_feature_knn",
-                result_key="field_infomap",
-            )
+            infomap(adata, adj_matrix_key="vf_feature_knn", result_key="field_infomap", **kwargs)
 
     logger.finish_progress(progress_name="clustering_field")
 
     if copy:
         return adata
     return None
 
@@ -226,48 +197,55 @@
     density: float = 5,
     curvature_method: int = 1,
     feature_bins: int = 10,
     clustering_method: str = "leiden",
     assign_fixedpoints: bool = False,
     reversed_fixedpoints: bool = False,
     **kwargs,
-):
-    """
-
-    Parameters
-    ----------
-    adata
-    basis
-    features
-    method
-    xy_grid_nums
-    density
-    curvature_method
-    feature_bins
-    clustering_method
-
-    Returns
-    -------
-
+) -> None:
+    """Cluster 2D streamlines based on vector field features. Initialize a grid over the state space and compute the
+    flow of data through the grid using plt.streamplot with a given density. For each point individual streamline,
+    computes the vector field 'features' of interest and stores the data via histograms. Add fixed points and
+    "reversed fixed points" (sources of the streamlines) to the feature data dataframe based on the
+    'assigned_fixedpoints' and 'reversed_fixedpoints' args. Finally, then cluster the streamlines based on these
+    features using the given 'clustering_method'.
+
+    Args:
+        adata: An AnnData object representing the network to be analyzed.
+        basis: The basis to use for creating the vector field, either "umap" or "tsne". Defaults to "umap".
+        features: A list of features to calculate for each point in the vector field. Defaults to ["speed", "divergence", "acceleration", "curvature", "curl"].
+        method: The method to use for calculating the flow of data through the grid, either "sparsevfc" or "gaussian". Defaults to "sparsevfc".
+        xy_grid_nums: The number of points to use in the x and y dimensions of the grid. Defaults to [50, 50].
+        density: The density of the grid. Defaults to 5.
+        curvature_method: The method to use for calculating curvature. Defaults to 1.
+        feature_bins: The number of bins to use for discretizing the data. Defaults to 10.
+        clustering_method: The method to use for clustering the data into modules, either "louvain" or "leiden". Defaults to "leiden".
+        assign_fixedpoints: A boolean indicating whether to assign fixed points to the data. Defaults to False.
+        reversed_fixedpoints: A boolean indicating whether to reverse the fixed points assignment. Defaults to False.
+
+    Raises:
+        ImportError: If the "cdlib" package is not installed and the "louvain" or "leiden" clustering method is specified.
+        ValueError: If an invalid method is specified for calculating the flow of data through the grid.
+        ValueError: If an invalid method is specified for clustering the data into modules.
+
+    Returns:
+        None, but updates the `adata` object with the following fields of the `adata.uns["streamline_clusters_" + basis]`
+            -  "feature_df"
+            - "segments"
+            - "X_pca"
+            - "clustering_method"
+            - "distances"
+            - "connectivities"
+            - "clusters"
+            - "fixed_point"
+            - "rev_fixed_point"
     """
 
     import matplotlib.pyplot as plt
 
-    if method in ["louvain", "leiden"]:
-        try:
-            from cdlib import algorithms
-
-            "leiden" in dir(algorithms)
-
-        except ImportError:
-            raise ImportError(
-                "You need to install the excellent package `cdlib` if you want to use louvain or leiden "
-                "for clustering."
-            )
-
     vf_dict, func = vecfld_from_adata(adata, basis=basis)
     grid_kwargs_dict = {
         "density": None,
         "smooth": None,
         "n_neighbors": None,
         "min_mass": None,
         "autoscale": False,
@@ -367,15 +345,15 @@
     line_len = []
     feature_df = np.zeros((len(line_list), len(features) * bins))
 
     for key, values in line_list.items():
         line_len.append(values.shape[0])
         tmp = None
         if has_acc:
-            acceleration_val, acceleration_vec = vector_field_class.compute_acceleration(values)
+            acceleration_val, _ = vector_field_class.compute_acceleration(values)
             acc_dict[key] = acceleration_val
 
             _, acc_hist = np.histogram(acceleration_val, bins=(bins - 1), density=True)
             if tmp is None:
                 tmp = acc_hist
         if has_curv:
             curvature_val_1 = vector_field_class.compute_curvature(values, formula=1)[0]
@@ -419,15 +397,15 @@
             else:
                 tmp = np.hstack((tmp, curl_hist))
 
         feature_df[key, :] = tmp
 
     # clustering
     feature_adata = AnnData(feature_df)
-    pca_monocle(feature_adata, X_data=feature_df, pca_key="X_pca")
+    pca(feature_adata, X_data=feature_df, pca_key="X_pca")
     if clustering_method == "louvain":
         louvain(feature_adata, obsm_key="X_pca")
     elif clustering_method == "leiden":
         leiden(feature_adata, obsm_key="X_pca")
     elif clustering_method == "infomap":
         infomap(feature_adata, obsm_key="X_pca")
     elif method in ["hdbscan", "kmeans"]:
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/networks.py` & `dynamo-release-1.3.0/dynamo/vectorfield/networks.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,39 +1,36 @@
+from typing import Dict, List, Optional, Union
+
+import networkx as nx
 import numpy as np
 import pandas as pd
+from anndata import AnnData
 
 from ..dynamo_logger import main_debug, main_info, main_tqdm
-from .vector_calculus import rank_jacobian_genes
+from .rank_vf import rank_jacobian_genes
 
 
 def get_interaction_in_cluster(
-    rank_df_dict,
-    group,
-    genes,
-    n_top_genes=100,
-    rank_regulators=False,
-    negative_values=False,
-):
+    rank_df_dict: Dict[str, pd.DataFrame],
+    group: str,
+    genes: List,
+    n_top_genes: int = 100,
+    rank_regulators: bool = False,
+    negative_values: bool = False,
+) -> pd.DataFrame:
     """Retrieve interactions among input genes given the ranking dataframe.
 
-    Parameters
-    ----------
-        rank_df_dict: `dict` of `pandas.DataFrame`
-            The dictionary of pandas data frame storing the gene ranking information for each cluster.
-        group: `str`
-            The group name that points to the key for the rank_df.
-        genes: `list`
-            The list of input genes, from which the network will be constructed.
-        n_top_genes: `int`
-            Number of top genes that will be selected from to build the network.
-        rank_regulators
-            Whether the input dictionary is about ranking top regulators of each gene per cluster.
+    Args:
+        rank_df_dict: The dictionary of pandas data frame storing the gene ranking information for each cluster.
+        group: The group name that points to the key for the rank_df.
+        genes: The list of input genes, from which the network will be constructed.
+        n_top_genes: Number of top genes that will be selected from to build the network.
+        rank_regulators: Whether the input dictionary is about ranking top regulators of each gene per cluster.
 
-    Returns
-    -------
+    Returns:
         A dataframe of interactions between input genes for the specified group of cells based on ranking information
         of Jacobian analysis. It has `regulator`, `target` and `weight` three columns.
 
     """
 
     subset_rank_df = rank_df_dict[group].head(n_top_genes)
     if negative_values:
@@ -74,49 +71,40 @@
 
                 edges = tmp if edges is None else pd.concat((edges, tmp), axis=0)
 
     return edges
 
 
 def build_network_per_cluster(
-    adata,
-    cluster,
-    cluster_names=None,
-    full_reg_rank=None,
-    full_eff_rank=None,
-    genes=None,
-    n_top_genes=100,
-    abs=False,
-):
+    adata: AnnData,
+    cluster: str,
+    cluster_names: Optional[str] = None,
+    full_reg_rank: Optional[Dict] = None,
+    full_eff_rank: Optional[Dict] = None,
+    genes: Optional[List] = None,
+    n_top_genes: int = 100,
+    abs: bool = False,
+) -> Dict[str, pd.DataFrame]:
     """Build a cluster specific network between input genes based on ranking information of Jacobian analysis.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`.
-            AnnData object, must at least have gene-wise Jacobian matrix calculated for each or selected cell.
-        cluster: `str`
-            The group key that points to the columns of `adata.obs`.
-        cluster_names: `str` or `list` (default: `None`)
-            The groups whose networks will be constructed, must overlap with names in adata.obs and / or keys from the
+    Args:
+        adata: AnnData object, must at least have gene-wise Jacobian matrix calculated for each or selected cell.
+        cluster: The group key that points to the columns of `adata.obs`.
+        cluster_names: The groups whose networks will be constructed, must overlap with names in adata.obs and / or keys from the
             ranking dictionaries.
-        full_reg_rank: `dict` (default: `None`)
-            The dictionary stores the regulator ranking information per cluster based on cell-wise Jacobian matrix. If
+        full_reg_rank: The dictionary stores the regulator ranking information per cluster based on cell-wise Jacobian matrix. If
             None, we will call `rank_jacobian_genes(adata, groups=cluster, mode='full reg', abs=True,
             output_values=True)` to first obtain this dictionary.
-        full_eff_rank (default: `None`)
-            The dictionary stores the effector ranking information per cluster based on cell-wise Jacobian matrix. If
+        full_eff_rank: The dictionary stores the effector ranking information per cluster based on cell-wise Jacobian matrix. If
             None, we will call `rank_jacobian_genes(adata, , groups=cluster, mode='full eff', abs=True,
             output_values=True)` to first obtain this dictionary.
-        genes: `list` (default: `None`)
-            The list of input genes, from which the network will be constructed.
-        n_top_genes: `int` (default: `100`)
-            Number of top genes that will be selected from to build the network.
+        genes: The list of input genes, from which the network will be constructed.
+        n_top_genes: Number of top genes that will be selected from to build the network.
 
-    Returns
-    -------
+    Returns:
         A dictionary of dataframe of interactions between input genes for each group of cells based on ranking
         information of Jacobian analysis. Each composite dataframe has `regulator`, `target` and `weight` three columns.
     """
 
     genes = np.unique(genes)
     if full_reg_rank is None:
         full_reg_rank = rank_jacobian_genes(
@@ -178,31 +166,27 @@
             edges_list[c] = eff_df
         elif len(reg_valid_genes) > 0 and len(eff_valid_genes) > 0:
             edges_list[c] = pd.concat((reg_df, eff_df), axis=0)
 
     return edges_list
 
 
-def adj_list_to_matrix(adj_list, only_one_edge=False, clr=False, graph=False):
+def adj_list_to_matrix(
+    adj_list: pd.DataFrame, only_one_edge: bool = False, clr: bool = False, graph: bool = False
+) -> Union[pd.DataFrame, nx.Graph]:
     """Convert a pandas adjacency list (with regulator, target, weight columns) to a processed adjacency matrix (or
     network).
 
-    Parameters
-    ----------
-    adj_list: `pandas.DataFrae`
-        A pandas adjacency dataframe with regulator, target, weight columns for representing a network graph.
-    only_one_edge: `bool`
-        Whether or not to only keep the edges with higher weight for any two gene pair.
-    clr: `bool`
-        Whether to post-process the direct network via the context likelihood relatedness.
-    graph: `bool`
-        Whether a direct, weighted graph based on networkx should be returned.
+    Args:
+        adj_list: A pandas adjacency dataframe with regulator, target, weight columns for representing a network graph.
+        only_one_edge: Whether or not to only keep the edges with higher weight for any two gene pair.
+        clr: Whether to post-process the direct network via the context likelihood relatedness.
+        graph: Whether a direct, weighted graph based on networkx should be returned.
 
-    Returns
-    -------
+    Returns:
         A pandas adjacency matrix or a direct, weighted graph constructed via networkx.
     """
 
     uniq_genes = list(set(adj_list.regulator) | set(adj_list.target))
 
     adj_matrix = pd.DataFrame(0, index=uniq_genes, columns=uniq_genes)
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/scPotential.py` & `dynamo-release-1.3.0/dynamo/vectorfield/scPotential.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 from warnings import warn
 
+from anndata._core.anndata import AnnData
 import numpy as np
 import scipy as sp
 import scipy.optimize
+from typing import Callable, List, Optional, Tuple, Union
 
 from ..tools.sampling import lhsclassic
 from .Ao import Ao_pot_map
 from .Bhattacharya import alignment, path_integral
 from .topography import FixedPoints
 from .utils import is_outside_domain
 from .Wang import Wang_action, Wang_LAP
@@ -15,25 +17,45 @@
 # from autograd import grad, jacobian # calculate gradient and jacobian
 
 
 # the LAP method should be rewritten in TensorFlow/PyTorch using optimization with SGD
 
 
 def search_fixed_points(
-    func,
-    domain,
-    x0,
-    x0_method="lhs",
-    reverse=False,
-    return_x0=False,
-    fval_tol=1e-8,
-    remove_outliers=True,
-    ignore_fsolve_err=False,
+    func: Callable,
+    domain: np.ndarray,
+    x0: np.ndarray,
+    x0_method: str = "lhs",
+    reverse: bool = False,
+    return_x0: bool = False,
+    fval_tol: float = 1e-8,
+    remove_outliers: bool = True,
+    ignore_fsolve_err: bool = False,
     **fsolve_kwargs
-):
+) -> Union[FixedPoints, Tuple[FixedPoints, np.ndarray]]:
+    """Search the fixed points of (learned) vector field function in a given domain.
+
+    The initial points are sampled by given methods. Then the function uses the fsolve function
+    from SciPy to find the fixed points and Numdifftools to compute the Jacobian matrix of the function.
+
+    Args:
+        func: The function of the (learned) vector field function that are required to fixed points for.
+        domain: The domain to search in.
+        x0: The initial point to start with.
+        x0_method: The method to sample initial points.
+        reverse: Whether to reverse the sign (direction) of vector field (VF).
+        return_x0: Whether to return the initial points used in the search.
+        fval_tol: The tolerance for the function value at the fixed points.
+        remove_outliers: Whether to remove the outliers.
+        ignore_fsolve_err: Whether to ignore the fsolve error.
+
+    Returns:
+        The fixed points found with their Jacobian matrix of the function. The sampled initial points
+        will be returned as well if return_x0 == True.
+    """
     import numdifftools as nda
 
     func_ = (lambda x: -func(x)) if reverse else func
     k = domain.shape[1]
 
     if np.isscalar(x0):
         n = x0
@@ -82,52 +104,41 @@
     if return_x0:
         return fp, x0
     else:
         return fp
 
 
 def gen_fixed_points(
-    func,
-    auto_func,
-    dim_range,
-    RandNum,
-    EqNum,
-    reverse=False,
-    grid_num=50,
-    x_ini=None,
-):
+    func: Callable,
+    auto_func: Optional[np.ndarray],
+    dim_range: List,
+    RandNum: int,
+    EqNum: int,
+    reverse: bool = False,
+    grid_num: int = 50,
+    x_ini: Optional[np.ndarray] = None,
+) -> Tuple[np.ndarray, np.ndarray]:
     """Calculate the fixed points of (learned) vector field function . Classify the fixed points into classes of stable and saddle points
     based on the eigenvalue of the Jacobian on the point.
 
-    Arguments
-    ---------
-        func: 'function'
-            The function of the (learned) vector field function that are required to fixed points for
-        auto_func: 'np.ndarray' (not used)
-            The function that is written with autograd of the same ODE equations that is used to calculate the Jacobian matrix.
-            If auto_func is set to be None, Jacobian is calculated through the fjac, r returned from fsolve.
-        dim_range: 'list'
-            The range of variables in the ODE equations
-        RandNum: 'int'
-            The number of random initial points to sample
-        EqNum: 'int'
-            The number of equations (dimension) of the system
-        reverse: `bool`
-            Whether to reverse the sign (direction) of vector field (VF).
-        grid_num: `int` (default: 50)
-            The number of grids on each dimension, only used when the EqNum is 2 and x_ini is None.
-        x_ini: 'np.ndarray'
-            The user provided initial points that is used to find the fixed points
-
-    Returns
-    -------
-    stable: 'np.ndarray'
-        A matrix consists of the coordinates of the stable steady state
-    saddle: 'np.ndarray'
-        A matrix consists of the coordinates of the unstable steady state
+    Args:
+        func: The function of the (learned) vector field function that are required to fixed points for
+        auto_func: The function that is written with autograd of the same ODE equations
+            that is used to calculate the Jacobian matrix. If auto_func is set to be None,
+            Jacobian is calculated through the fjac, r returned from fsolve.
+        dim_range: The range of variables in the ODE equations.
+        RandNum: The number of random initial points to sample.
+        EqNum: The number of equations (dimension) of the system.
+        reverse: Whether to reverse the sign (direction) of vector field (VF).
+        grid_num: The number of grids on each dimension, only used when the EqNum is 2 and x_ini is None.
+        x_ini: The user provided initial points that is used to find the fixed points
+
+    Returns:
+        stable: A matrix consists of the coordinates of the stable steady state
+        saddle: A matrix consists of the coordinates of the unstable steady state
 
     """
     import numdifftools as nda
 
     if reverse is True:
         func_ = lambda x: -func(x)
     else:
@@ -228,34 +239,31 @@
                 SaddleNum = SaddleNum + 1
 
     stable, saddle = StablePoint[:StableNum, :].T, SaddlePoint[:SaddleNum, :].T
 
     return stable, saddle
 
 
-def gen_gradient(dim, N, Function, DiffusionMatrix):
+def gen_gradient(
+    dim: int,
+    N: int,
+    Function: Callable,
+    DiffusionMatrix: Callable,
+) -> Tuple[np.ndarray, np.ndarray]:
     """Calculate the gradient of the (learned) vector field function for the least action path (LAP) symbolically
 
-    Arguments
-    ---------
-        dim: 'int'
-            The number of dimension of the system
-        N: 'int'
-            The number of the points on the discretized path of the LAP
-        Function: 'function'
-            The function of the (learned) vector field function that is needed to calculate the Jacobian matrix
-        DiffusionMatrix: Python function
-            The function that returns the diffusion matrix which can be variable (for example, gene) dependent
-
-    Returns
-    -------
-    ret: 'np.ndarray'
-        The symbolic function that calculates the gradient of the LAP based on the Jacobian of the vector field function
-    V: 'np.ndarray'
-        A matrix consists of the coordinates of the unstable steady state
+    Args:
+        dim: The number of dimension of the system.
+        N: The number of the points on the discretized path of the LAP.
+        Function: The function of the (learned) vector field function that is needed to calculate the Jacobian matrix.
+        DiffusionMatrix: The function that returns the diffusion matrix which can be variable (e.g. gene) dependent
+
+    Returns:
+        ret: The symbolic function that calculates the gradient of the LAP based on the Jacobian of the vector field function.
+        V: A matrix consists of the coordinates of the unstable steady state.
     """
 
     from StringFunction import StringFunction
     from sympy import Identity, Matrix, MatrixSymbol, simplify, symbols
 
     N = N + 1
     X = MatrixSymbol("x", dim, N)
@@ -303,91 +311,82 @@
 
 
 ##################################################
 # rewrite gen_gradient with autograd or TF
 ##################################################
 
 
-def IntGrad(points, Function, DiffusionMatrix, dt):
+def IntGrad(
+    points: np.ndarray,
+    Function: Callable,
+    DiffusionMatrix: Callable,
+    dt: float,
+) -> np.ndarray:
     """Calculate the action of the path based on the (reconstructed) vector field function and diffusion matrix (Eq. 18)
 
-    Arguments
-    ---------
-        points: :class:`~numpy.ndarray`
-            The sampled points in the state space used to calculate the action.
-        Function: function
-            The (learned) vector field function.
-        DiffusionMatrix: function
-            The function that returns diffusion matrix which can be dependent on the variables (for example, genes)
-        dt: 'float'
-            The time interval used in calculating action
+    Arg:
+        points: The sampled points in the state space used to calculate the action.
+        Function: The (learned) vector field function.
+        DiffusionMatrix: The function that returns diffusion matrix which can be dependent on the variables (for example, genes).
+        dt: The time interval used in calculating action.
 
-    Returns
-    -------
-    integral: 'np.ndarray'
-        The action calculated based on the input path, the vector field function and the diffusion matrix.
+    Returns:
+        integral: The action calculated based on the input path, the vector field function and the diffusion matrix.
     """
 
     integral = 0
     for k in np.arange(1, points.shape[1]):
         Tmp = (points[:, k] - points[:, k - 1]).reshape((-1, 1)) / dt - Function(points[:, k - 1]).reshape((-1, 1))
         integral = (
             integral + (Tmp.T).dot(np.linalg.matrix_power(DiffusionMatrix(points[:, k - 1]), -1)).dot(Tmp) * dt
         )  # part of the Eq. 18 in Sci. Rep. paper
     integral = integral / 4
 
     return integral[0, 0]
 
 
-def DiffusionMatrix(x):
+def DiffusionMatrix(x: np.ndarray) -> np.ndarray:
     """Diffusion matrix can be variable dependent
 
-    Arguments
-    ---------
-        x: :class:`~numpy.ndarray`
-            The matrix of sampled points (cells) in the (gene expression) state space. A
+    Args:
+        x: The matrix of sampled points (cells) in the (gene expression) state space. A
 
-    Returns
-    -------
-    out: 'np.ndarray'
-        The diffusion matrix. By default, it is a diagonal matrix.
+    Returns:
+        out: The diffusion matrix. By default, it is a diagonal matrix.
     """
     out = np.zeros((x.shape[0], x.shape[0]))
     np.fill_diagonal(out, 1)
 
     return out
 
 
-def action(n_points, tmax, point_start, point_end, boundary, Function, DiffusionMatrix):
+def action(
+    n_points: int,
+    tmax: int,
+    point_start: np.ndarray,
+    point_end: np.ndarray,
+    boundary: np.ndarray,
+    Function: Callable,
+    DiffusionMatrix: Callable,
+) -> Tuple[np.ndarray, np.ndarray]:
     """It calculates the minimized action value given an initial path, ODE, and diffusion matrix. The minimization is
     realized by scipy.optimize.Bounds function in python (without using the gradient of the action function).
 
-    Arguments
-    ---------
-        n_points: 'int'
-            The number of points along the least action path.
-        tmax: 'int'
-            The value at maximum t.
-        point_start: :class:`~numpy.ndarray`
-            The matrix for storing the coordinates (gene expression configuration) of the start point (initial cell state).
-        point_end: :class:`~numpy.ndarray`
-            The matrix for storing the coordinates (gene expression configuration) of the end point (terminal cell state).
-        boundary: :class:`~numpy.ndarray`
-            Not used.
-        Function: function
-            The (reconstructed) vector field function.
-        DiffusionMatrix: function
-            The function that returns the diffusion matrix which can variable (for example, gene) dependent.
-
-    Returns
-    -------
-    fval: :class:`~numpy.ndarray`
-        The action value for the learned least action path.
-    output_path: :class:`~numpy.ndarray`
-        The least action path learned
+    Args:
+        n_points: The number of points along the least action path.
+        tmax: The value at maximum t.
+        point_start: The matrix for storing the coordinates (gene expression configuration) of the start point (initial cell state).
+        point_end: The matrix for storing the coordinates (gene expression configuration) of the end point (terminal cell state).
+        boundary: Not used.
+        Function: The (reconstructed) vector field function.
+        DiffusionMatrix: The function that returns the diffusion matrix which can variable (for example, gene) dependent.
+
+    Returns:
+        fval: The action value for the learned least action path.
+        output_path: The least action path learned.
     """
 
     dim = point_end.shape[0]  # genes x cells
     dt = tmax / n_points
     lambda_f = lambda x: IntGrad(
         np.hstack((point_start, x.reshape((dim, -1)), point_end)),
         Function,
@@ -415,15 +414,20 @@
         res["fun"],
         np.hstack((point_start, res["x"].reshape((2, -1)), point_end)),
     )
 
     return fval, output_path
 
 
-def Potential(adata, DiffMat=None, method="Ao", **kwargs):
+def Potential(
+    adata: AnnData,
+    DiffMat: Optional[Callable] = None,
+    method: str = "Ao",
+    **kwargs
+) -> AnnData:
     """Function to map out the pseudo-potential landscape.
 
     Although it is appealing to define “potential” for biological systems as it is intuitive and familiar from other
     fields, it is well-known that the definition of a potential function in open biological systems is controversial
     (Ping Ao 2009). In the conservative system, the negative gradient of potential function is relevant to the velocity
     vector by ma = −Δψ (where m, a, are the mass and acceleration of the object, respectively). However, a biological
     system is massless, open and nonconservative, thus methods that directly learn potential function assuming a gradient
@@ -433,72 +437,64 @@
     this very goal (Xing 2010; Wang et al. 2011; J. X. Zhou et al. 2012; Qian 2013; P. Zhou and Li 2016). Bhattacharya
     and others also recently provided a numeric algorithm to approximate the potential landscape.
 
     This function implements the Ao, Bhattacharya method and Ying method and will also support other methods shortly.
 
     Parameters
     ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains embedding and velocity data
-        method: `str` (default: `Ao`)
-            Method to map the potential landscape.
+        adata: AnnData object that contains embedding and velocity data.
+        DiffMat: The function which returns the diffusion matrix which can variable (for example, gene) dependent.
+        method: Method to map the potential landscape.
 
     Returns
     -------
-        adata: :class:`~anndata.AnnData`
-            `AnnData` object that is updated with the `Pot` dictionary in the `uns` attribute.
+        adata: `AnnData` object that is updated with the `Pot` dictionary in the `uns` attribute.
 
     """
 
     Function = adata.uns["VecFld"]
     DiffMat = DiffusionMatrix if DiffMat is None else DiffMat
     pot = Pot(Function, DiffMat, **kwargs)
     pot.fit(method=method)
 
     return adata
 
 
 class Pot:
     def __init__(
         self,
-        Function=None,
-        DiffMat=None,
-        boundary=None,
-        n_points=25,
-        fixed_point_only=False,
-        find_fixed_points=False,
-        refpoint=None,
-        stable=None,
-        saddle=None,
+        Function: Callable = None,
+        DiffMat: Callable = None,
+        boundary: List = None,
+        n_points: int = 25,
+        fixed_point_only: bool = False,
+        find_fixed_points: bool = False,
+        refpoint: Optional[np.ndarray] = None,
+        stable: Optional[np.ndarray] = None,
+        saddle: Optional[np.ndarray] = None,
     ):
         """It implements the least action method to calculate the potential values of fixed points for a given SDE (stochastic
         differential equation) model. The function requires the vector field function and a diffusion matrix. This code is based
         on the MATLAB code from Ruoshi Yuan and Ying Tang. Potential landscape of high dimensional nonlinear stochastic dynamics with
         large noise. Y Tang, R Yuan, G Wang, X Zhu, P Ao - Scientific reports, 2017
 
-        Arguments
-        ---------
-            Function: 'function'
-                The (reconstructed) vector field function.
-            DiffMat: 'function'
-                The function that returns the diffusion matrix which can variable (for example, gene) dependent.
-            boundary: 'list'
-                The range of variables (genes).
-            n_points: 'int'
-                The number of points along the least action path.
-            fixed_point_only: 'bool'
-                The logic flag to determine whether only the potential for fixed point or entire space should be mapped.
-            find_fixed_points: 'bool'
-                The logic flag to determine whether only the gen_fixed_points function should be run to identify fixed points.
-            refpoint: 'np.ndarray'
-                The reference point to define the potential.
-            stable: 'np.ndarray'
-                The matrix for storing the coordinates (gene expression configuration) of the stable fixed point (characteristic state of a particular cell type).
-            saddle: 'np.ndarray'
-                The matrix for storing the coordinates (gene expression configuration) of the unstable fixed point (characteristic state of cells prime to bifurcation).
+        Args:
+            Function: The (reconstructed) vector field function.
+            DiffMat: The function that returns the diffusion matrix which can variable (for example, gene) dependent.
+            boundary: The range of variables (genes).
+            n_points: The number of points along the least action path.
+            fixed_point_only: The logic flag to determine whether only the potential
+                for fixed point or entire space should be mapped.
+            find_fixed_points: The logic flag to determine whether only the gen_fixed_points function
+                should be run to identify fixed points.
+            refpoint: The reference point to define the potential.
+            stable: The matrix for storing the coordinates (gene expression configuration)
+                of the stable fixed point (characteristic state of a particular cell type).
+            saddle: The matrix for storing the coordinates (gene expression configuration)
+                of the unstable fixed point (characteristic state of cells prime to bifurcation).
         """
 
         self.VecFld = {
             "Function": Function,
             "DiffusionMatrix": DiffMat,
         }  # should we use annadata here?
 
@@ -510,24 +506,24 @@
             "refpoint": refpoint,
             "stable": stable,
             "saddle": saddle,
         }
 
     def fit(
         self,
-        adata,
-        x_lim,
-        y_lim,
-        basis="umap",
-        method="Ao",
-        xyGridSpacing=2,
-        dt=1e-2,
-        tol=1e-2,
-        numTimeSteps=1400,
-    ):
+        adata: AnnData,
+        x_lim: List,
+        y_lim: List,
+        basis: str = "umap",
+        method: str = "Ao",
+        xyGridSpacing: int = 2,
+        dt: float = 1e-2,
+        tol: float= 1e-2,
+        numTimeSteps: int =1400,
+    ) -> AnnData:
         """Function to map out the pseudo-potential landscape.
 
         Although it is appealing to define “potential” for biological systems as it is intuitive and familiar from other
         fields, it is well-known that the definition of a potential function in open biological systems is controversial
         (Ping Ao 2009). In the conservative system, the negative gradient of potential function is relevant to the velocity
         vector by ma = −Δψ (where m, a, are the mass and acceleration of the object, respectively). However, a biological
         system is massless, open and nonconservative, thus methods that directly learn potential function assuming a gradient
@@ -535,44 +531,37 @@
         equations into either the gradient or the dissipative part and uses the gradient part to define a physical equivalent
         of potential in biological systems (P. Ao 2004). Later, various theoretical studies have been conducted towards
         this very goal (Xing 2010; Wang et al. 2011; J. X. Zhou et al. 2012; Qian 2013; P. Zhou and Li 2016). Bhattacharya
         and others also recently provided a numeric algorithm to approximate the potential landscape.
 
         This function implements the Ao, Bhattacharya method and Ying method and will also support other methods shortly.
 
-        Arguments
-        ---------
-            adata: :class:`~anndata.AnnData`
-                AnnData object that contains U_grid and V_grid data
-            x_lim: `list`
-                Lower or upper limit of x-axis.
-            y_lim: `list`
-                Lower or upper limit of y-axis
-            basis: `str` (default: umap)
-                The dimension reduction method to use.
-            method: 'string' (default: Bhattacharya)
-                Method used to map the pseudo-potential landscape. By default, it is Bhattacharya (A deterministic map of
+        Args:
+            adata: AnnData object that contains U_grid and V_grid data.
+            x_lim: Lower or upper limit of x-axis.
+            y_lim: Lower or upper limit of y-axis.
+            basis: The dimension reduction method to use.
+            method: Method used to map the pseudo-potential landscape. By default, it is Bhattacharya (A deterministic map of
                 Waddington’s epigenetic landscape for cell fate specification. Sudin Bhattacharya, Qiang Zhang and Melvin
                 E. Andersen). Other methods will be supported include: Tang (), Ping (), Wang (), Zhou ().
-
-        Returns
-        -------
-        if Bhattacharya is used:
-            Xgrid: 'np.ndarray'
-                The X grid to visualize "potential surface"
-            Ygrid: 'np.ndarray'
-                The Y grid to visualize "potential surface"
-            Zgrid: 'np.ndarray'
-                The interpolate potential corresponding to the X,Y grids.
-
-        if Tang method is used:
-        retmat: 'np.ndarray'
-            The action value for the learned least action path.
-        LAP: 'np.ndarray'
-            The least action path learned
+            xyGridSpacing: Grid spacing for "starting points" for each "path" on the potential surface
+            dt: Time step for the path integral.
+            tol: Tolerance to test for convergence.
+            numTimeSteps: A high-enough number for convergence with given dt.
+
+        Returns:
+            The AnnData object updated with the following values:
+                if Bhattacharya is used:
+                    Xgrid: The X grid to visualize "potential surface"
+                    Ygrid: The Y grid to visualize "potential surface"
+                    Zgrid: The interpolate potential corresponding to the X,Y grids.
+
+                if Tang method is used:
+                    retmat: The action value for the learned least action path.
+                    LAP: The least action path learned
         """
 
         if method == "Ao":
             X = adata.obsm["X_" + basis]
             X, U, P, vecMat, S, A = Ao_pot_map(self.VecFld["Function"], X, D=self.VecFld["DiffusionMatrix"])
 
             adata.uns["grid_Pot_" + basis] = {
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/scVectorField.py` & `dynamo-release-1.3.0/dynamo/vectorfield/topography.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1285 +1,1281 @@
-import functools
-import itertools
+# create by Yan Zhang, minor adjusted by Xiaojie Qiu
+import datetime
+import os
 import warnings
-from multiprocessing.dummy import Pool as ThreadPool
-from typing import Callable, Union
+from typing import Callable, List, Optional, Tuple, Union
 
+import anndata
 import numpy as np
-import numpy.matlib
 import scipy.sparse as sp
-from numpy import format_float_scientific as scinot
-from scipy.linalg import lstsq
-from scipy.spatial.distance import pdist
+from anndata import AnnData
+from scipy.integrate import odeint
+from scipy.linalg import eig
+from scipy.optimize import fsolve
 from sklearn.neighbors import NearestNeighbors
 
 from ..dynamo_logger import LoggerManager, main_info, main_warning
-from ..simulation.ODE import jacobian_bifur2genes, ode_bifur2genes
-from ..tools.sampling import lhsclassic, sample, sample_by_velocity
-from ..tools.utils import (
-    index_condensed_matrix,
-    linear_least_squares,
-    nearest_neighbors,
-    starmap_with_kwargs,
-    timeit,
-    update_dict,
-    update_n_merge_dict,
-)
+from ..tools.utils import gaussian_1d, inverse_norm, nearest_neighbors, update_dict
+from ..utils import copy_adata
+from .FixedPoints import FixedPoints
+from .scVectorField import BaseVectorField, SvcVectorField
 from .utils import (
-    FixedPoints,
-    Hessian_rkhs_gaussian,
-    Jacobian_kovf,
-    Jacobian_numerical,
-    Jacobian_rkhs_gaussian,
-    Jacobian_rkhs_gaussian_parallel,
-    Laplacian,
-    compute_acceleration,
-    compute_curl,
-    compute_curvature,
-    compute_divergence,
-    compute_sensitivity,
-    compute_torsion,
-    con_K,
-    con_K_div_cur_free,
+    VecFldDict,
+    angle,
+    dynode_vector_field_function,
     find_fixed_points,
+    is_outside,
     remove_redundant_points,
     vecfld_from_adata,
     vector_field_function,
-    vector_transformation,
 )
 
 
-def norm(X, V, T, fix_velocity=True):
-    """Normalizes the X, Y (X + V) matrix to have zero means and unit covariance.
-        We use the mean of X, Y's center (mean) and scale parameters (standard deviation) to normalize T.
-
-    Arguments
-    ---------
-        X: :class:`~numpy.ndarray`
-            Current state. This corresponds to, for example, the spliced transcriptomic state.
-        V: :class:`~numpy.ndarray`
-            Velocity estimates in delta t. This corresponds to, for example, the inferred spliced transcriptomic
-            velocity estimated calculated by dynamo or velocyto, scvelo.
-        T: :class:`~numpy.ndarray`
-            Current state on a grid which is often used to visualize the vector field. This corresponds to, for example,
-            the spliced transcriptomic state.
-        fix_velocity: bool (default: `True`)
-            Whether to fix velocity and don't transform it.
-
-    Returns
-    -------
-        A tuple of updated X, V, T and norm_dict which includes the mean and scale values for original X, V data used
-        in normalization.
-    """
-
-    Y = X + V
-    n, m = X.shape[0], V.shape[0]
-
-    xm = np.mean(X, 0)
-    ym = np.mean(Y, 0)
+def pac_onestep(x0: np.ndarray, func: Callable, v0: np.ndarray, ds: float = 0.01):
+    """One step of the predictor-corrector method
 
-    x, y, t = (
-        X - xm[None, :],
-        Y - ym[None, :],
-        T - (1 / 2 * (xm[None, :] + ym[None, :])) if T is not None else None,
-    )
+    Args:
+        x0: current value
+        func: function to be integrated
+        v0: tangent predictor
+        ds: step size, Defaults to 0.01.
 
-    xscale, yscale = (
-        np.sqrt(np.sum(np.sum(x**2, 1)) / n),
-        np.sqrt(np.sum(np.sum(y**2, 1)) / m),
-    )
+    Returns:
+        x1: next value
+    """
+    x01 = x0 + v0 * ds
 
-    X, Y, T = x / xscale, y / yscale, t / (1 / 2 * (xscale + yscale)) if T is not None else None
+    def F(x):
+        return np.array([func(x), (x - x0).dot(v0) - ds])
 
-    X, V, T = X, V if fix_velocity else Y - X, T
-    norm_dict = {"xm": xm, "ym": ym, "xscale": xscale, "yscale": yscale, "fix_velocity": fix_velocity}
+    x1 = fsolve(F, x01)
+    return x1
 
-    return X, V, T, norm_dict
 
+def continuation(
+    x0: np.ndarray,
+    func: Callable,
+    s_max: float,
+    ds: float = 0.01,
+    v0: Optional[np.ndarray] = None,
+    param_axis: int = 0,
+    param_direction: int = 1,
+) -> np.ndarray:
+    """Continually integrate the ODE `func` from x0
+
+    Args:
+        x0: initial value
+        func: function to be integrated
+        s_max: maximum integration length
+        ds: step size, Defaults to 0.01.
+        v0: initial tangent vector, Defaults to None.
+        param_axis: axis of the parameter, Defaults to 0.
+        param_direction: direction of the parameter, Defaults to 1.
 
-def bandwidth_rule_of_thumb(X, return_sigma=False):
-    """
-    This function computes a rule-of-thumb bandwidth for a Gaussian kernel based on:
-    https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator
+    Returns:
+        np.ndarray of values along the curve
     """
-    sig = sig = np.sqrt(np.mean(np.diag(np.cov(X.T))))
-    h = 1.06 * sig / (len(X) ** (-1 / 5))
-    if return_sigma:
-        return h, sig
+    ret = [x0]
+    if v0 is None:  # initialize tangent predictor
+        v = np.zeros_like(x0)
+        v[param_axis] = param_direction
     else:
-        return h
-
+        v = v0
+    s = 0
+    while s <= s_max:
+        x1 = ret[-1]
+        x = pac_onestep(x1, func, v, ds)
+        ret.append(x)
+        s += ds
+
+        # compute tangent predictor
+        v = x - x1
+        v /= np.linalg.norm(v)
+    return np.array(ret)
+
+
+def clip_curves(
+    curves: Union[List[List], List[np.ndarray]], domain: np.ndarray, tol_discont=None
+) -> Union[List[List], List[np.ndarray]]:
+    """Clip curves to the domain
+
+    Args:
+        curves: list of curves
+        domain: domain of the curves of dimension n x 2
+        tol_discont: tolerance for discontinuity, Defaults to None.
 
-def bandwidth_selector(X):
+    Returns:
+        list of clipped curves joined together
     """
-    This function computes an empirical bandwidth for a Gaussian kernel.
-    """
-    n, m = X.shape
-    if n > 200000 and m > 2:
-        from pynndescent import NNDescent
-
-        nbrs = NNDescent(
-            X,
-            metric="euclidean",
-            n_neighbors=max(2, int(0.2 * n)),
-            n_jobs=-1,
-            random_state=19491001,
-        )
-        _, distances = nbrs.query(X, k=max(2, int(0.2 * n)))
-    else:
-        alg = "ball_tree" if X.shape[1] > 10 else "kd_tree"
-        nbrs = NearestNeighbors(n_neighbors=max(2, int(0.2 * n)), algorithm=alg, n_jobs=-1).fit(X)
-        distances, _ = nbrs.kneighbors(X)
-
-    d = np.mean(distances[:, 1:]) / 1.5
-    return np.sqrt(2) * d
-
-
-def denorm(VecFld, X_old, V_old, norm_dict):
-    """Denormalize data back to the original scale.
-
-    Parameters
-    ----------
-        VecFld:  `dict`
-            The dictionary that stores the information for the reconstructed vector field function.
-        X_old: `np.ndarray`
-            The original data for current state.
-        V_old: `np.ndarray`
-            The original velocity data.
-        norm_dict: `dict`
-            norm_dict to the class which includes the mean and scale values for X, Y used in normalizing the data.
-
-    Returns
-    -------
-        An updated VecFld function that includes denormalized X, Y, X_ctrl, grid, grid_V, V and the norm_dict key.
-    """
-
-    Y_old = X_old + V_old
-    X, Y, V, xm, ym, x_scale, y_scale, fix_velocity = (
-        VecFld["X"],
-        VecFld["Y"],
-        VecFld["V"],
-        norm_dict["xm"],
-        norm_dict["ym"],
-        norm_dict["xscale"],
-        norm_dict["yscale"],
-        norm_dict["fix_velocity"],
-    )
-    grid, grid_V = VecFld["grid"], VecFld["grid_V"]
-    xy_m, xy_scale = (xm + ym) / 2, (x_scale + y_scale) / 2
+    ret = []
+    for cur in curves:
+        clip_away = np.zeros(len(cur), dtype=bool)
+        for i, p in enumerate(cur):
+            for j in range(len(domain)):
+                if p[j] < domain[j][0] or p[j] > domain[j][1]:
+                    clip_away[i] = True
+                    break
+            if tol_discont is not None and i > 0:
+                d = np.linalg.norm(p - cur[i - 1])
+                if d > tol_discont:
+                    clip_away[i] = True
+        # clip curve and assemble
+        i_start = 0
+        while i_start < len(cur) - 1:
+            if not clip_away[i_start]:
+                for i_end in range(i_start, len(cur)):
+                    if clip_away[i_end]:
+                        break
+                # a tiny bit of the end could be chopped off
+                ret.append(cur[i_start:i_end])
+                i_start = i_end
+            else:
+                i_start += 1
+    return ret
 
-    VecFld["X"] = X_old
-    VecFld["Y"] = Y_old
-    # VecFld["X_ctrl"] = X * x_scale + np.matlib.tile(xm, [X.shape[0], 1])
-    VecFld["grid"] = grid * xy_scale + np.matlib.tile(xy_m, [grid.shape[0], 1]) if grid is not None else None
-    VecFld["grid_V"] = (
-        (grid + grid_V) * xy_scale + np.matlib.tile(xy_m, [grid_V.shape[0], 1]) - grid if grid_V is not None else None
-    )
-    VecFld["V"] = V if fix_velocity else (V + X) * y_scale + np.matlib.tile(ym, [V.shape[0], 1]) - X_old
-    VecFld["norm_dict"] = norm_dict
 
-    return VecFld
+def compute_nullclines_2d(
+    X0: Union[List, np.ndarray],
+    fdx: Callable,
+    fdy: Callable,
+    x_range: List,
+    y_range: List,
+    s_max: Optional[float] = None,
+    ds: Optional[float] = None,
+) -> Tuple[List]:
+    """Compute nullclines of a 2D vector field. Nullclines are curves along which vector field is zero in either the x or y direction.
+
+    Args:
+        X0: initial value
+        fdx: differential equation for x
+        fdy: differential equation for y
+        x_range: range of x
+        y_range: range of y
+        s_max: maximum integration length, Defaults to None.
+        ds: step size, Defaults to None.
 
+    Returns:
+        Tuple of nullclines in x and y
+    """
+    if s_max is None:
+        s_max = 5 * ((x_range[1] - x_range[0]) + (y_range[1] - y_range[0]))
+    if ds is None:
+        ds = s_max / 1e3
+
+    NCx = []
+    NCy = []
+    for x0 in X0:
+        # initialize tangent predictor
+        theta = np.random.rand() * 2 * np.pi
+        v0 = [np.cos(theta), np.sin(theta)]
+        v0 /= np.linalg.norm(v0)
+        # nullcline continuation
+        NCx.append(continuation(x0, fdx, s_max, ds, v0=v0))
+        NCx.append(continuation(x0, fdx, s_max, ds, v0=-v0))
+        NCy.append(continuation(x0, fdy, s_max, ds, v0=v0))
+        NCy.append(continuation(x0, fdy, s_max, ds, v0=-v0))
+    NCx = clip_curves(NCx, [x_range, y_range], ds * 10)
+    NCy = clip_curves(NCy, [x_range, y_range], ds * 10)
+    return NCx, NCy
+
+
+def compute_separatrices(
+    Xss: np.ndarray,
+    Js: np.ndarray,
+    func: Callable,
+    x_range: List,
+    y_range: List,
+    t: int = 50,
+    n_sample: int = 500,
+    eps: float = 1e-6,
+) -> List:
+    """Compute separatrix based on jacobians at points in `Xss`
+
+    Args:
+        Xss: list of steady states
+        Js: list of jacobians at steady states
+        func: function to be integrated
+        x_range: range of x
+        y_range: range of y
+        t: integration time, Defaults to 50.
+        n_sample: number of samples, Defaults to 500.
+        eps: tolerance for discontinuity, Defaults to 1e-6.
 
-@timeit
-def lstsq_solver(lhs, rhs, method="drouin"):
-    if method == "scipy":
-        C = lstsq(lhs, rhs)[0]
-    elif method == "drouin":
-        C = linear_least_squares(lhs, rhs)
-    else:
-        main_warning("Invalid linear least squares solver. Use Drouin's method instead.")
-        C = linear_least_squares(lhs, rhs)
-    return C
-
-
-def get_P(Y, V, sigma2, gamma, a, div_cur_free_kernels=False):
-    """GET_P estimates the posterior probability and part of the energy.
-
-    Arguments
-    ---------
-        Y: 'np.ndarray'
-            Velocities from the data.
-        V: 'np.ndarray'
-            The estimated velocity: V=f(X), f being the vector field function.
-        sigma2: 'float'
-            sigma2 is defined as sum(sum((Y - V)**2)) / (N * D)
-        gamma: 'float'
-            Percentage of inliers in the samples. This is an inital value for EM iteration, and it is not important.
-        a: 'float'
-            Paramerter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is a.
-        div_cur_free_kernels: `bool` (default: False)
-            A logic flag to determine whether the divergence-free or curl-free kernels will be used for learning the
-            vector field.
-
-    Returns
-    -------
-    P: 'np.ndarray'
-        Posterior probability, related to equation 27.
-    E: `np.ndarray'
-        Energy, related to equation 26.
+    Returns:
+        list of separatrices
+    """
+    ret = []
+    for i, x in enumerate(Xss):
+        print(x)
+        J = Js[i]
+        w, v = eig(J)
+        I_stable = np.where(np.real(w) < 0)[0]
+        print(I_stable)
+        for j in I_stable:  # I_unstable
+            u = np.real(v[j])
+            u = u / np.linalg.norm(u)
+            print("u=%f, %f" % (u[0], u[1]))
+
+            # Parameters for building separatrix
+            T = np.linspace(0, t, n_sample)
+            # all_sep_a, all_sep_b = None, None
+            # Build upper right branch of separatrix
+            ab_upper = odeint(lambda x, _: -func(x), x + eps * u, T)
+            # Build lower left branch of separatrix
+            ab_lower = odeint(lambda x, _: -func(x), x - eps * u, T)
+
+            sep = np.vstack((ab_lower[::-1], ab_upper))
+            ret.append(sep)
+    ret = clip_curves(ret, [x_range, y_range])
+    return ret
+
+
+def set_test_points_on_curve(curve: List[np.ndarray], interval: float) -> np.ndarray:
+    """Generates an np.ndarray of test points that are spaced out by `interval` distance
+
+    Args:
+        curve: list of points
+        interval: distance for separation
 
+    Returns:
+        np.ndarray of test points
     """
+    P = [curve[0]]
+    dist = 0
+    for i in range(1, len(curve)):
+        dist += np.linalg.norm(curve[i] - curve[i - 1])
+        if dist >= interval:
+            P.append(curve[i])
+            dist = 0
+    return np.array(P)
+
+
+def find_intersection_2d(curve1: List[np.ndarray], curve2: List[np.ndarray], tol_redundant: float = 1e-4) -> np.ndarray:
+    """Compute intersections between curve 1 and curve2
+
+    Args:
+        curve1: list of points
+        curve2: list of points
+        tol_redundant: Defaults to 1e-4.
 
-    if div_cur_free_kernels:
-        Y = Y.reshape((2, int(Y.shape[0] / 2)), order="F").T
-        V = V.reshape((2, int(V.shape[0] / 2)), order="F").T
-
-    D = Y.shape[1]
-    temp1 = np.exp(-np.sum((Y - V) ** 2, 1) / (2 * sigma2))
-    temp2 = (2 * np.pi * sigma2) ** (D / 2) * (1 - gamma) / (gamma * a)
-    temp1[temp1 == 0] = np.min(temp1[temp1 != 0])
-    P = temp1 / (temp1 + temp2)
-    E = P.T.dot(np.sum((Y - V) ** 2, 1)) / (2 * sigma2) + np.sum(P) * np.log(sigma2) * D / 2
-
-    return (P[:, None], E) if P.ndim == 1 else (P, E)
-
-
-@timeit
-def graphize_vecfld(
-    func,
-    X,
-    nbrs_idx=None,
-    dist=None,
-    k=30,
-    distance_free=True,
-    n_int_steps=20,
-    cores=1,
-):
-    n, d = X.shape
+    Returns:
+        np.ndarray of intersection points between curve1 and curve2
+    """
+    P = []
+    for i in range(len(curve1) - 1):
+        for j in range(len(curve2) - 1):
+            p1 = curve1[i]
+            p2 = curve1[i + 1]
+            p3 = curve2[j]
+            p4 = curve2[j + 1]
+            denom = np.linalg.det([p1 - p2, p3 - p4])
+            if denom != 0:
+                t = np.linalg.det([p1 - p3, p3 - p4]) / denom
+                u = -np.linalg.det([p1 - p2, p1 - p3]) / denom
+                if t >= 0 and t <= 1 and u >= 0 and u <= 1:
+                    P.append(p1 + t * (p2 - p1))
+    if tol_redundant is not None:
+        remove_redundant_points(P, tol=tol_redundant)
+    return np.array(P)
+
+
+def find_fixed_points_nullcline(
+    func: Callable,
+    NCx: List[List[np.ndarray]],
+    NCy: List[List[np.ndarray]],
+    sample_interval: float = 0.5,
+    tol_redundant: float = 1e-4,
+) -> Tuple[np.ndarray, np.ndarray]:
+    """Find fixed points by computing the intersections of x and y nullclines using `find_intersection_2d` and passing these intersection points as samppling points to `find_fixed_points`.
+
+    Args:
+        func: Callable passed to `find_fixed_points` along with the intersection points of the two nullclines
+        NCx: List of x nullcline
+        NCy: List of y nullcline
+        sample_interval: Interval for sampling test points along x and y nullclines. Defaults to 0.5.
+        tol_redundant: Defaults to 1e-4.
 
-    nbrs = None
-    if nbrs_idx is None:
-        if X.shape[0] > 200000 and X.shape[1] > 2:
-            from pynndescent import NNDescent
+    Returns:
+        A tuple with solutions for where func(x) = 0 and the Jacobian matrix
+    """
+    test_Px = []
+    for i in range(len(NCx)):
+        test_Px.append(set_test_points_on_curve(NCx[i], sample_interval))
+
+    test_Py = []
+    for i in range(len(NCy)):
+        test_Py.append(set_test_points_on_curve(NCy[i], sample_interval))
+
+    int_P = []
+    for i in range(len(test_Px)):
+        for j in range(len(test_Py)):
+            p = find_intersection_2d(test_Px[i], test_Py[j], tol_redundant)
+            for k in range(len(p)):
+                int_P.append(p[k])
+    int_P = np.array(int_P)
+    P, J, _ = find_fixed_points(int_P, func, tol_redundant=tol_redundant)
+    return P, J
+
+
+def calc_fft(x):
+    out = np.fft.rfft(x)
+    n = len(x)
+    xFFT = abs(out) / n * 2
+    freq = np.arange(int(n / 2)) / n
+    return xFFT[: int(n / 2)], freq
 
-            nbrs = NNDescent(
-                X,
-                metric="euclidean",
-                n_neighbors=k + 1,
-                n_jobs=-1,
-                random_state=19491001,
-            )
-            nbrs_idx, dist = nbrs.query(X, k=k + 1)
-        else:
-            alg = "ball_tree" if X.shape[1] > 10 else "kd_tree"
-            nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm=alg, n_jobs=-1).fit(X)
-            dist, nbrs_idx = nbrs.kneighbors(X)
 
-    if dist is None and not distance_free:
-        D = pdist(X)
-    else:
-        D = None
+def dup_osc_idx(x: np.ndarray, n_dom: int = 3, tol: float = 0.05):
+    """
+    Find the index of the end of the first division in an array where the oscillatory patterns of two consecutive divisions are similar within a given tolerance.
 
-    V = sp.csr_matrix((n, n))
-    if cores == 1:
-        for i, idx in enumerate(LoggerManager.progress_logger(nbrs_idx, progress_name="graphize_vecfld")):
-            V += construct_v(X, i, idx, n_int_steps, func, distance_free, dist, D, n)
+    Args:
+        x: An array-like object containing the data to be analyzed.
+        n_dom: An integer specifying the number of divisions to make in the array. Defaults to 3.
+        tol: A float specifying the tolerance for considering the oscillatory patterns of two divisions to be similar. Defaults to 0.05.
 
+    Returns:
+        A tuple containing the index of the end of the first division and the difference between the FFTs of the two divisions. If the oscillatory patterns of the two divisions are not similar within the given tolerance, returns (None, None).
+    """
+    l_int = int(np.floor(len(x) / n_dom))
+    ind_a, ind_b = np.arange((n_dom - 2) * l_int, (n_dom - 1) * l_int), np.arange((n_dom - 1) * l_int, n_dom * l_int)
+    y1 = x[ind_a]
+    y2 = x[ind_b]
+
+    def calc_fft_k(x):
+        ret = []
+        for k in range(x.shape[1]):
+            xFFT, _ = calc_fft(x[:, k])
+            ret.append(xFFT[1:])
+        return np.hstack(ret)
+
+    try:
+        xFFt1 = calc_fft_k(y1)
+        xFFt2 = calc_fft_k(y2)
+    except ValueError:
+        print("calc_fft_k run failed...")
+        return None, None
+
+    diff = np.linalg.norm(xFFt1 - xFFt2) / len(xFFt1)
+    if diff <= tol:
+        idx = (n_dom - 1) * l_int
     else:
-        pool = ThreadPool(cores)
-        res = pool.starmap(
-            construct_v,
-            zip(
-                itertools.repeat(X),
-                np.arange(len(nbrs_idx)),
-                nbrs_idx,
-                itertools.repeat(n_int_steps),
-                itertools.repeat(func),
-                itertools.repeat(distance_free),
-                itertools.repeat(dist),
-                itertools.repeat(D),
-                itertools.repeat(n),
-            ),
-        )
-        pool.close()
-        pool.join()
-        V = functools.reduce((lambda a, b: a + b), res)
-    return V, nbrs
-
-
-def construct_v(X, i, idx, n_int_steps, func, distance_free, dist, D, n):
-    """helper function for parallism"""
-
-    V = sp.csr_matrix((n, n))
-    x = X[i].A if sp.issparse(X) else X[i]
-    Y = X[idx[1:]].A if sp.issparse(X) else X[idx[1:]]
-    for j, y in enumerate(Y):
-        pts = np.linspace(x, y, n_int_steps)
-        v = func(pts)
-
-        lxy = np.linalg.norm(y - x)
-        if lxy > 0:
-            u = (y - x) / np.linalg.norm(y - x)
-        else:
-            u = y - x
-        v = np.mean(v.dot(u))
-        if not distance_free:
-            if dist is None:
-                d = D[index_condensed_matrix(n, i, idx[j + 1])]
-            else:
-                d = dist[i][j + 1]
-            v *= d
-        V[i, idx[j + 1]] = v
-        V[idx[j + 1], i] = -v
-
-    return V
-
-
-def SparseVFC(
-    X: np.ndarray,
-    Y: np.ndarray,
-    Grid: np.ndarray,
-    M: int = 100,
-    a: float = 5,
-    beta: float = None,
-    ecr: float = 1e-5,
-    gamma: float = 0.9,
-    lambda_: float = 3,
-    minP: float = 1e-5,
-    MaxIter: int = 500,
-    theta: float = 0.75,
-    div_cur_free_kernels: bool = False,
-    velocity_based_sampling: bool = True,
-    sigma: float = 0.8,
-    eta: float = 0.5,
-    seed=0,
-    lstsq_method: str = "drouin",
-    verbose: int = 1,
-) -> dict:
-    """Apply sparseVFC (vector field consensus) algorithm to learn a functional form of the vector field from random
-    samples with outlier on the entire space robustly and efficiently. (Ma, Jiayi, etc. al, Pattern Recognition, 2013)
-
-    Arguments
-    ---------
-        X: 'np.ndarray'
-            Current state. This corresponds to, for example, the spliced transcriptomic state.
-        Y: 'np.ndarray'
-            Velocity estimates in delta t. This corresponds to, for example, the inferred spliced transcriptomic
-            velocity or total RNA velocity based on metabolic labeling data estimated calculated by dynamo.
-        Grid: 'np.ndarray'
-            Current state on a grid which is often used to visualize the vector field. This corresponds to, for example,
-            the spliced transcriptomic state or total RNA state.
-        M: 'int' (default: 100)
-            The number of basis functions to approximate the vector field.
-        a: 'float' (default: 10)
-            Parameter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is `a`.
-        beta: 'float' (default: 0.1)
-            Parameter of Gaussian Kernel, k(x, y) = exp(-beta*||x-y||^2).
-            If None, a rule-of-thumb bandwidth will be computed automatically.
-        ecr: 'float' (default: 1e-5)
-            The minimum limitation of energy change rate in the iteration process.
-        gamma: 'float' (default: 0.9)
-            Percentage of inliers in the samples. This is an initial value for EM iteration, and it is not important.
-        lambda_: 'float' (default: 3)
-            Represents the trade-off between the goodness of data fit and regularization. Larger Lambda_ put more
-            weights on regularization.
-        minP: 'float' (default: 1e-5)
-            The posterior probability Matrix P may be singular for matrix inversion. We set the minimum value of P as
-            minP.
-        MaxIter: 'int' (default: 500)
-            Maximum iteration times.
-        theta: 'float' (default: 0.75)
-            Define how could be an inlier. If the posterior probability of a sample is an inlier is larger than theta,
-            then it is regarded as an inlier.
-        div_cur_free_kernels: `bool` (default: False)
-            A logic flag to determine whether the divergence-free or curl-free kernels will be used for learning the
-            vector field.
-        sigma: 'int' (default: `0.8`)
-            Bandwidth parameter.
-        eta: 'int' (default: `0.5`)
-            Combination coefficient for the divergence-free or the curl-free kernels.
-        seed : int or 1-d array_like, optional (default: `0`)
-            Seed for RandomState. Must be convertible to 32 bit unsigned integers. Used in sampling control points.
-            Default is to be 0 for ensure consistency between different runs.
-        lstsq_method: 'str' (default: `drouin`)
-           The name of the linear least square solver, can be either 'scipy` or `douin`.
-        verbose: `int` (default: `1`)
-            The level of printing running information.
-
-    Returns
-    -------
-    VecFld: 'dict'
-        A dictionary which contains:
-            X: Current state.
-            valid_ind: The indices of cells that have finite velocity values.
-            X_ctrl: Sample control points of current state.
-            ctrl_idx: Indices for the sampled control points.
-            Y: Velocity estimates in delta t.
-            beta: Parameter of the Gaussian Kernel for the kernel matrix (Gram matrix).
-            V: Prediction of velocity of X.
-            C: Finite set of the coefficients for the
-            P: Posterior probability Matrix of inliers.
-            VFCIndex: Indexes of inliers found by sparseVFC.
-            sigma2: Energy change rate.
-            grid: Grid of current state.
-            grid_V: Prediction of velocity of the grid.
-            iteration: Number of the last iteration.
-            tecr_vec: Vector of relative energy changes rate comparing to previous step.
-            E_traj: Vector of energy at each iteration,
-        where V = f(X), P is the posterior probability and VFCIndex is the indexes of inliers found by sparseVFC.
-        Note that V = `con_K(Grid, X_ctrl, beta).dot(C)` gives the prediction of velocity on Grid (but can also be any
-        point in the gene expression state space).
+        idx = None
+    return idx, diff
 
-    """
-    logger = LoggerManager.gen_logger("SparseVFC")
-    temp_logger = LoggerManager.get_temp_timer_logger()
-    logger.info("[SparseVFC] begins...")
-    logger.log_time()
 
-    need_utility_time_measure = verbose > 1
-    X_ori, Y_ori = X.copy(), Y.copy()
-    valid_ind = np.where(np.isfinite(Y.sum(1)))[0]
-    X, Y = X[valid_ind], Y[valid_ind]
-    N, D = Y.shape
-    grid_U = None
-
-    # Construct kernel matrix K
-    tmp_X, uid = np.unique(X, axis=0, return_index=True)  # return unique rows
-    M = min(M, tmp_X.shape[0])
-    if velocity_based_sampling:
-        logger.info("Sampling control points based on data velocity magnitude...")
-        idx = sample_by_velocity(Y[uid], M, seed=seed)
-    else:
-        idx = np.random.RandomState(seed=seed).permutation(tmp_X.shape[0])  # rand select some initial points
-        idx = idx[range(M)]
-    ctrl_pts = tmp_X[idx, :]
-
-    if beta is None:
-        h = bandwidth_selector(ctrl_pts)
-        beta = 1 / h**2
-
-    K = (
-        con_K(ctrl_pts, ctrl_pts, beta, timeit=need_utility_time_measure)
-        if div_cur_free_kernels is False
-        else con_K_div_cur_free(ctrl_pts, ctrl_pts, sigma, eta, timeit=need_utility_time_measure)[0]
-    )
-    U = (
-        con_K(X, ctrl_pts, beta, timeit=need_utility_time_measure)
-        if div_cur_free_kernels is False
-        else con_K_div_cur_free(X, ctrl_pts, sigma, eta, timeit=need_utility_time_measure)[0]
-    )
-    if Grid is not None:
-        grid_U = (
-            con_K(Grid, ctrl_pts, beta, timeit=need_utility_time_measure)
-            if div_cur_free_kernels is False
-            else con_K_div_cur_free(Grid, ctrl_pts, sigma, eta, timeit=need_utility_time_measure)[0]
-        )
-    M = ctrl_pts.shape[0] * D if div_cur_free_kernels else ctrl_pts.shape[0]
+def dup_osc_idx_iter(x: np.ndarray, max_iter: int = 5, **kwargs) -> Tuple[int, np.ndarray]:
+    """
+    Find the index of the end of the first division in an array where the oscillatory patterns of two consecutive divisions are similar within a given tolerance, using iterative search.
 
-    if div_cur_free_kernels:
-        X = X.flatten()[:, None]
-        Y = Y.flatten()[:, None]
-
-    # Initialization
-    V = X.copy() if div_cur_free_kernels else np.zeros((N, D))
-    C = np.zeros((M, 1)) if div_cur_free_kernels else np.zeros((M, D))
-    i, tecr, E = 0, 1, 1
-    # test this
-    sigma2 = sum(sum((Y - X) ** 2)) / (N * D) if div_cur_free_kernels else sum(sum((Y - V) ** 2)) / (N * D)
-    sigma2 = 1e-7 if sigma2 < 1e-8 else sigma2
-    tecr_vec = np.ones(MaxIter) * np.nan
-    E_vec = np.ones(MaxIter) * np.nan
-    P = None
-    while i < MaxIter and tecr > ecr and sigma2 > 1e-8:
-        # E_step
-        E_old = E
-        P, E = get_P(Y, V, sigma2, gamma, a, div_cur_free_kernels)
-
-        E = E + lambda_ / 2 * np.trace(C.T.dot(K).dot(C))
-        E_vec[i] = E
-        tecr = abs((E - E_old) / E)
-        tecr_vec[i] = tecr
-
-        # logger.report_progress(count=i, total=MaxIter, progress_name="E-step iteration")
-        if need_utility_time_measure:
-            logger.info(
-                "iterate: %d, gamma: %.3f, energy change rate: %s, sigma2=%s"
-                % (i, gamma, scinot(tecr, 3), scinot(sigma2, 3))
-            )
+    Args:
+        x: An array-like object containing the data to be analyzed.
+        max_iter: An integer specifying the maximum number of iterations to perform. Defaults to 5.
 
-        # M-step. Solve linear system for C.
-        temp_logger.log_time()
-        P = np.maximum(P, minP)
-        if div_cur_free_kernels:
-            P = np.kron(P, np.ones((int(U.shape[0] / P.shape[0]), 1)))  # np.kron(P, np.ones((D, 1)))
-            lhs = (U.T * np.matlib.tile(P.T, [M, 1])).dot(U) + lambda_ * sigma2 * K
-            rhs = (U.T * np.matlib.tile(P.T, [M, 1])).dot(Y)
+    Returns:
+        A tuple containing the index of the end of the first division and an array of differences between the FFTs of consecutive divisions. If the oscillatory patterns of the two divisions are not similar within the given tolerance after the maximum number of iterations, returns the index and array from the final iteration.
+    """
+    stop = False
+    idx = len(x)
+    j = 0
+    D = []
+    while not stop:
+        i, d = dup_osc_idx(x[:idx], **kwargs)
+        D.append(d)
+        if i is None:
+            stop = True
         else:
-            UP = U.T * numpy.matlib.repmat(P.T, M, 1)
-            lhs = UP.dot(U) + lambda_ * sigma2 * K
-            rhs = UP.dot(Y)
-        if need_utility_time_measure:
-            temp_logger.finish_progress(progress_name="computing lhs and rhs")
-        temp_logger.log_time()
-
-        C = lstsq_solver(lhs, rhs, method=lstsq_method, timeit=need_utility_time_measure)
-
-        # Update V and sigma**2
-        V = U.dot(C)
-        Sp = sum(P) / 2 if div_cur_free_kernels else sum(P)
-        sigma2 = (sum(P.T.dot(np.sum((Y - V) ** 2, 1))) / np.dot(Sp, D))[0]
-
-        # Update gamma
-        numcorr = len(np.where(P > theta)[0])
-        gamma = numcorr / X.shape[0]
-
-        if gamma > 0.95:
-            gamma = 0.95
-        elif gamma < 0.05:
-            gamma = 0.05
-
-        i += 1
-    if i == 0 and not (tecr > ecr and sigma2 > 1e-8):
-        raise Exception(
-            "please check your input parameters, "
-            f"tecr: {tecr}, ecr {ecr} and sigma2 {sigma2},"
-            f"tecr must larger than ecr and sigma2 must larger than 1e-8"
-        )
+            idx = i
+        j += 1
+        if j >= max_iter or idx == 0:
+            stop = True
+    D = np.array(D)
+    return idx, D
 
-    grid_V = None
-    if Grid is not None:
-        grid_V = np.dot(grid_U, C)
-
-    VecFld = {
-        "X": X_ori,
-        "valid_ind": valid_ind,
-        "X_ctrl": ctrl_pts,
-        "ctrl_idx": idx,
-        "Y": Y_ori,
-        "beta": beta,
-        "V": V.reshape((N, D)) if div_cur_free_kernels else V,
-        "C": C,
-        "P": P,
-        "VFCIndex": np.where(P > theta)[0],
-        "sigma2": sigma2,
-        "grid": Grid,
-        "grid_V": grid_V,
-        "iteration": i - 1,
-        "tecr_traj": tecr_vec[:i],
-        "E_traj": E_vec[:i],
-    }
-    if div_cur_free_kernels:
-        VecFld["div_cur_free_kernels"], VecFld["sigma"], VecFld["eta"] = (
-            True,
-            sigma,
-            eta,
-        )
-        temp_logger.log_time()
-        (
-            _,
-            VecFld["df_kernel"],
-            VecFld["cf_kernel"],
-        ) = con_K_div_cur_free(X, ctrl_pts, sigma, eta, timeit=need_utility_time_measure)
-        temp_logger.finish_progress(progress_name="con_K_div_cur_free")
 
-    logger.finish_progress(progress_name="SparseVFC")
-    return VecFld
+# TODO: This should be inherited from the BaseVectorField/DifferentiatiableVectorField class,
+#       and BifurcationTwoGenes should be inherited from this class.
+class VectorField2D:
+    """
+    The VectorField2D class is a class that represents a 2D vector field, which is a type of mathematical object that assigns a 2D vector to each point in a 2D space. This vector field can be defined using a function that returns the vector at each point, or by separate functions for the x and y components of the vector.
 
+    The class also has several methods for finding fixed points (points where the vector is zero) in the vector field, as well as for querying the fixed points that have been found. The `find_fixed_points_by_sampling` method uses sampling to find fixed points within a specified range in the x and y dimensions. It does this by generating a set of random or Latin Hypercube Sampled (LHS) points within the specified range, and then using the `find_fixed_points` function to find the fixed points that are closest to these points. The `find_fixed_points function` uses an iterative method to find fixed points, starting from an initial guess and using the Jacobian matrix at each point to update the guess until the fixed point is found to within a certain tolerance.
+
+    The `get_Xss_confidence` method estimates the confidence of the fixed points by computing the mean distance of each fixed point to its nearest
+    neighbors in the data used to define the vector field. It returns an array of confidence values for each fixed point, with higher values indicating higher confidence.
+    """
 
-class BaseVectorField:
     def __init__(
         self,
-        X=None,
-        V=None,
-        Grid=None,
-        *args,
-        **kwargs,
+        func: Callable,
+        func_vx: Optional[Callable] = None,
+        func_vy: Optional[Callable] = None,
+        X_data: Optional[np.ndarray] = None,
     ):
-        self.data = {"X": X, "V": V, "Grid": Grid}
-        self.vf_dict = kwargs.pop("vf_dict", {})
-        self.func = kwargs.pop("func", None)
-        self.fixed_points = kwargs.pop("fixed_points", None)
-        super().__init__(**kwargs)
-
-    def construct_graph(self, X=None, **kwargs):
-        X = self.data["X"] if X is None else X
-        return graphize_vecfld(self.func, X, **kwargs)
-
-    def from_adata(self, adata, basis="", vf_key="VecFld"):
-        vf_dict, func = vecfld_from_adata(adata, basis=basis, vf_key=vf_key)
-        self.data["X"] = vf_dict["X"]
-        self.data["V"] = vf_dict["Y"]  # use the raw velocity
-        self.vf_dict = vf_dict
+        """
+        Args:
+            func: a function that takes an (n, 2) array of coordinates and returns an (n, 2) array of vectors
+            func_vx: a function that takes an (n, 2) array of coordinates and returns an (n,) array of x components of the vectors, Defaults to None.
+            func_vy: a function that takes an (n, 2) array of coordinates and returns an (n,) array of y components of the vectors, Defaults to None.
+            X_data: Defaults to None.
+        """
         self.func = func
 
-    def get_X(self, idx=None):
-        if idx is None:
-            return self.data["X"]
-        else:
-            return self.data["X"][idx]
+        def func_dim(x, func, dim):
+            y = func(x)
+            if y.ndim == 1:
+                y = y[dim]
+            else:
+                y = y[:, dim].flatten()
+            return y
 
-    def get_V(self, idx=None):
-        if idx is None:
-            return self.data["V"]
+        if func_vx is None:
+            self.fx = lambda x: func_dim(x, self.func, 0)
         else:
-            return self.data["V"][idx]
+            self.fx = func_vx
+        if func_vy is None:
+            self.fy = lambda x: func_dim(x, self.func, 1)
+        else:
+            self.fy = func_vy
+        self.Xss = FixedPoints()
+        self.X_data = X_data
+        self.NCx = None
+        self.NCy = None
 
-    def get_data(self):
-        return self.data["X"], self.data["V"]
+    def get_num_fixed_points(self) -> int:
+        """
+        Get the number of fixed points stored in the `Xss` attribute.
 
-    def find_fixed_points(self, n_x0=100, X0=None, domain=None, sampling_method="random", **kwargs):
+        Returns:
+            int: the number of fixed points
         """
-        Search for fixed points of the vector field function.
+        return len(self.Xss.get_X())
 
+    def get_fixed_points(self, get_types: Optional[bool] = True) -> Union[Tuple[np.ndarray, np.ndarray], np.ndarray]:
         """
-        if domain is not None:
-            domain = np.atleast_2d(domain)
+        Get the fixed points stored in the `Xss` attribute, along with their types (stable, saddle, or unstable) if `get_types` is `True`.
 
-        if self.data is None and X0 is None:
-            if domain is None:
-                raise Exception(
-                    "The initial points `X0` are not provided, "
-                    "no data is stored in the vector field, and no domain is provided for the sampling of initial points."
-                )
-            else:
-                main_info(f"Sampling {n_x0} initial points in the provided domain using the Latin Hypercube method.")
-                X0 = lhsclassic(n_x0, domain.shape[0], bounds=domain)
+        Args:
+            get_types: whether to include the types of the fixed points. Defaults to `True`.
 
-        elif X0 is None:
-            indices = sample(np.arange(len(self.data["X"])), n_x0, method=sampling_method)
-            X0 = self.data["X"][indices]
+        Returns:
+            tuple: a tuple containing:
+                - X (np.array): an (n, 2) array of coordinates of the fixed points
+                - ftype (np.array): an (n,) array of the types of the fixed points (-1 for stable, 0 for saddle, 1 for unstable). Only returned if `get_types` is `True`.
+        """
+        X = self.Xss.get_X()
+        if not get_types:
+            return X
+        else:
+            is_saddle, is_stable = self.Xss.is_saddle()
+            # -1 -- stable, 0 -- saddle, 1 -- unstable
+            ftype = np.ones(len(X))
+            for i in range(len(ftype)):
+                if is_saddle[i]:
+                    ftype[i] = 0
+                elif is_stable[i]:
+                    ftype[i] = -1
+            return X, ftype
 
-        if domain is None and self.data is not None:
-            domain = np.vstack((np.min(self.data["X"], axis=0), np.max(self.data["X"], axis=0))).T
+    def get_Xss_confidence(self, k: Optional[int] = 50) -> np.ndarray:
+        """Get the confidence of each fixed point stored in the `Xss` attribute.
 
-        X, J, _ = find_fixed_points(X0, self.func, domain=domain, **kwargs)
-        self.fixed_points = FixedPoints(X, J)
+        Args:
+            k: the number of nearest neighbors to consider for each fixed point. Defaults to 50.
 
-    def get_fixed_points(self, **kwargs):
+        Returns:
+            an (n,) array of confidences for the fixed points
         """
-        Get fixed points of the vector field function.
+        X = self.X_data
+        X = X.A if sp.issparse(X) else X
+        Xss = self.Xss.get_X()
+        Xref = np.median(X, 0)
+        Xss = np.vstack((Xss, Xref))
 
-        Returns
-        -------
-            Xss: :class:`~numpy.ndarray`
-                Coordinates of the fixed points.
-            ftype: :class:`~numpy.ndarray`
-                Types of the fixed points:
-                -1 -- stable,
-                 0 -- saddle,
-                 1 -- unstable
-        """
-        if self.fixed_points is None:
-            self.find_fixed_points(**kwargs)
+        if X.shape[0] > 200000 and X.shape[1] > 2:
+            from pynndescent import NNDescent
 
-        Xss = self.fixed_points.get_X()
-        ftype = self.fixed_points.get_fixed_point_types()
-        return Xss, ftype
-
-    def assign_fixed_points(self, domain=None, cores=1, **kwargs):
-        """assign each cell to the associated fixed points"""
-        if domain is None and self.data is not None:
-            domain = np.vstack((np.min(self.data["X"], axis=0), np.max(self.data["X"], axis=0))).T
-
-        if cores == 1:
-            X, J, _ = find_fixed_points(
-                self.data["X"],
-                self.func,
-                domain=domain,
-                return_all=True,
-                **kwargs,
+            nbrs = NNDescent(
+                X,
+                metric="euclidean",
+                n_neighbors=min(k, X.shape[0] - 1),
+                n_jobs=-1,
+                random_state=19491001,
             )
+            _, dist = nbrs.query(Xss, k=min(k, X.shape[0] - 1))
         else:
-            pool = ThreadPool(cores)
+            alg = "ball_tree" if X.shape[1] > 10 else "kd_tree"
+            nbrs = NearestNeighbors(n_neighbors=min(k, X.shape[0] - 1), algorithm=alg, n_jobs=-1).fit(X)
+            dist, _ = nbrs.kneighbors(Xss)
 
-            args_iter = zip(
-                [i[None, :] for i in self.data["X"]],
-                itertools.repeat(self.func),
-                itertools.repeat(domain),
-                itertools.repeat(True),
-            )
-            kwargs_iter = itertools.repeat(kwargs)
-            res = starmap_with_kwargs(pool, find_fixed_points, args_iter, kwargs_iter)
+        dist_m = dist.mean(1)
+        # confidence = 1 - dist_m / dist_m.max()
+        sigma = 0.1 * 0.5 * (np.max(X[:, 0]) - np.min(X[:, 0]) + np.max(X[:, 1]) - np.min(X[:, 1]))
+        confidence = gaussian_1d(dist_m, sigma=sigma)
+        confidence /= np.max(confidence)
+        return confidence[:-1]
 
-            pool.close()
-            pool.join()
+    def find_fixed_points_by_sampling(
+        self,
+        n: int,
+        x_range: Tuple[float, float],
+        y_range: Tuple[float, float],
+        lhs: Optional[bool] = True,
+        tol_redundant: float = 1e-4,
+    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+        """
+        Find fixed points by sampling the vector field within a specified range of coordinates.
 
-            (X, J, _) = zip(*res)
-            X = np.vstack([[i] * self.data["X"].shape[1] if i is None else i for i in X]).astype(float)
-            J = np.array(
-                [np.zeros((self.data["X"].shape[1], self.data["X"].shape[1])) * np.nan if i is None else i for i in J]
-            )
+        Args:
+            n: the number of samples to take
+            x_range: a tuple of two floats specifying the range of x coordinates to sample
+            y_range: a tuple of two floats specifying the range of y coordinates to sample
+            lhs: whether to use Latin Hypercube Sampling to generate the samples. Defaults to `True`.
+            tol_redundant: the tolerance for removing redundant fixed points. Defaults to 1e-4.
+        """
+        if lhs:
+            from ..tools.sampling import lhsclassic
 
-        self.fixed_points = FixedPoints(X, J)
-        fps_assignment = self.fixed_points.get_X()
-        fps_type_assignment = self.fixed_points.get_fixed_point_types()
-
-        valid_fps_assignment, valid_fps_type_assignment = (
-            fps_assignment[np.abs(fps_assignment).sum(1) > 0, :],
-            fps_type_assignment[np.abs(fps_assignment).sum(1) > 0],
+            X0 = lhsclassic(n, 2)
+        else:
+            X0 = np.random.rand(n, 2)
+        X0[:, 0] = X0[:, 0] * (x_range[1] - x_range[0]) + x_range[0]
+        X0[:, 1] = X0[:, 1] * (y_range[1] - y_range[0]) + y_range[0]
+        X, J, _ = find_fixed_points(
+            X0,
+            self.func,
+            domain=[x_range, y_range],
+            tol_redundant=tol_redundant,
         )
-        X, discard = remove_redundant_points(valid_fps_assignment, output_discard=True)
+        if X is None:
+            raise ValueError(f"No fixed points found. Try to increase the number of samples n.")
+        self.Xss.add_fixed_points(X, J, tol_redundant)
 
-        assignment_id = np.zeros(len(fps_assignment))
-        for i, cur_fps in enumerate(fps_assignment):
-            if np.isnan(cur_fps).any():
-                assignment_id[i] = np.nan
-            else:
-                assignment_id[i] = int(nearest_neighbors(cur_fps, X, 1))
+    def find_nearest_fixed_point(
+        self, x: np.ndarray, x_range: Tuple[float, float], y_range: Tuple[float, float], tol_redundant: float = 1e-4
+    ):
+        """Find the fixed point closest to a given initial guess within a given range.
 
-        return X, valid_fps_type_assignment[discard], assignment_id
+        Args:
+            x: an array specifying the initial guess
+            x_range: a tuple of two floats specifying the range of x coordinates
+            y_range: a tuple of two floats specifying the range of y coordinates
+                tol_redundant: the tolerance for removing redundant fixed points. Defaults to 1e-4.
+        """
+        X, J, _ = find_fixed_points(x, self.func, domain=[x_range, y_range], tol_redundant=tol_redundant)
+        if len(X) > 0:
+            self.Xss.add_fixed_points(X, J, tol_redundant)
 
-    def integrate(
+    def compute_nullclines(
         self,
-        init_states,
-        dims=None,
-        scale=1,
-        t_end=None,
-        step_size=None,
-        args=(),
-        integration_direction="forward",
-        interpolation_num=250,
-        average=True,
-        sampling="arc_length",
-        verbose=False,
-        disable=False,
+        x_range: Tuple[float, float],
+        y_range: Tuple[float, float],
+        find_new_fixed_points: Optional[bool] = False,
+        tol_redundant: Optional[float] = 1e-4,
     ):
+        """Compute nullclines. Nullclines are curves along which vector field is zero along a particular dimension.
 
-        from ..prediction.utils import integrate_vf_ivp
-        from ..tools.utils import getTend, getTseq
-
-        if np.isscalar(dims):
-            init_states = init_states[:, :dims]
-        elif dims is not None:
-            init_states = init_states[:, dims]
-
-        if self.func is None:
-            VecFld = self.vf_dict
-            self.func = lambda x: scale * vector_field_function(x=x, vf_dict=VecFld, dim=dims)
-        if t_end is None:
-            t_end = getTend(self.get_X(), self.get_V())
-
-        t_linspace = getTseq(init_states, t_end, step_size)
-        t, prediction = integrate_vf_ivp(
-            init_states,
-            t=t_linspace,
-            integration_direction=integration_direction,
-            f=self.func,
-            args=args,
-            interpolation_num=interpolation_num,
-            average=average,
-            sampling=sampling,
-            verbose=verbose,
-            disable=disable,
-        )
-
-        return t, prediction
-
-
-class DifferentiableVectorField(BaseVectorField):
-    def get_Jacobian(self, method=None):
-        # subclasses must implement this function.
-        pass
-
-    def compute_divergence(self, X=None, method="analytical", **kwargs):
-        X = self.data["X"] if X is None else X
-        f_jac = self.get_Jacobian(method=method)
-        return compute_divergence(f_jac, X, **kwargs)
-
-    def compute_curl(self, X=None, method="analytical", dim1=0, dim2=1, dim3=2, **kwargs):
-        X = self.data["X"] if X is None else X
-        if dim3 is None or X.shape[1] < 3:
-            X = X[:, [dim1, dim2]]
-        else:
-            X = X[:, [dim1, dim2, dim3]]
-        f_jac = self.get_Jacobian(method=method, **kwargs)
-        return compute_curl(f_jac, X, **kwargs)
-
-    def compute_acceleration(self, X=None, method="analytical", **kwargs):
-        X = self.data["X"] if X is None else X
-        f_jac = self.get_Jacobian(method=method)
-        return compute_acceleration(self.func, f_jac, X, **kwargs)
-
-    def compute_curvature(self, X=None, method="analytical", formula=2, **kwargs):
-        X = self.data["X"] if X is None else X
-        f_jac = self.get_Jacobian(method=method)
-        return compute_curvature(self.func, f_jac, X, formula=formula, **kwargs)
-
-    def compute_torsion(self, X=None, method="analytical", **kwargs):
-        X = self.data["X"] if X is None else X
-        f_jac = self.get_Jacobian(method=method)
-        return compute_torsion(self.func, f_jac, X, **kwargs)
-
-    def compute_sensitivity(self, X=None, method="analytical", **kwargs):
-        X = self.data["X"] if X is None else X
-        f_jac = self.get_Jacobian(method=method)
-        return compute_sensitivity(f_jac, X, **kwargs)
-
-
-class SvcVectorField(DifferentiableVectorField):
-    def __init__(self, X=None, V=None, Grid=None, *args, **kwargs):
-        """Initialize the VectorField class.
-
-        Parameters
-        ----------
-        X: :class:`~numpy.ndarray` (dimension: n_obs x n_features)
-                Original data.
-        V: :class:`~numpy.ndarray` (dimension: n_obs x n_features)
-                Velocities of cells in the same order and dimension of X.
-        Grid: :class:`~numpy.ndarray`
-                The function that returns diffusion matrix which can be dependent on the variables (for example, genes)
-        M: `int` (default: None)
-            The number of basis functions to approximate the vector field. By default it is calculated as
-            `min(len(X), int(1500 * np.log(len(X)) / (np.log(len(X)) + np.log(100))))`. So that any datasets with less
-            than  about 900 data points (cells) will use full data for vector field reconstruction while any dataset
-            larger than that will at most use 1500 data points.
-        a: `float` (default 5)
-            Parameter of the model of outliers. We assume the outliers obey uniform distribution, and the volume of
-            outlier's variation space is a.
-        beta: `float` (default: None)
-             Parameter of Gaussian Kernel, k(x, y) = exp(-beta*||x-y||^2).
-             If None, a rule-of-thumb bandwidth will be computed automatically.
-        ecr: `float` (default: 1e-5)
-            The minimum limitation of energy change rate in the iteration process.
-        gamma: `float` (default:  0.9)
-            Percentage of inliers in the samples. This is an inital value for EM iteration, and it is not important.
-            Default value is 0.9.
-        lambda_: `float` (default: 3)
-            Represents the trade-off between the goodness of data fit and regularization.
-        minP: `float` (default: 1e-5)
-            The posterior probability Matrix P may be singular for matrix inversion. We set the minimum value of P as
-            minP.
-        MaxIter: `int` (default: 500)
-            Maximum iteration times.
-        theta: `float` (default 0.75)
-            Define how could be an inlier. If the posterior probability of a sample is an inlier is larger than theta,
-            then it is regarded as an inlier.
-        div_cur_free_kernels: `bool` (default: False)
-            A logic flag to determine whether the divergence-free or curl-free kernels will be used for learning the
-            vector field.
-        sigma: `int`
-            Bandwidth parameter.
-        eta: `int`
-            Combination coefficient for the divergence-free or the curl-free kernels.
-        seed : int or 1-d array_like, optional (default: `0`)
-            Seed for RandomState. Must be convertible to 32 bit unsigned integers. Used in sampling control points.
-            Default is to be 0 for ensure consistency between different runs.
+        Args:
+            x_range: range of x
+            y_range: range of y
+            find_new_fixed_points: whether to find new fixed points along the nullclines and add to `self.Xss`. Defaults to False.
+            s_max: maximum integration length, Defaults to None.
+            ds: step size, Defaults to None.
         """
+        # compute arguments
+        s_max = 5 * ((x_range[1] - x_range[0]) + (y_range[1] - y_range[0]))
+        ds = s_max / 1e3
+        self.NCx, self.NCy = compute_nullclines_2d(
+            self.Xss.get_X(),
+            self.fx,
+            self.fy,
+            x_range,
+            y_range,
+            s_max=s_max,
+            ds=ds,
+        )
+        if find_new_fixed_points:
+            sample_interval = ds * 10
+            X, J = find_fixed_points_nullcline(self.func, self.NCx, self.NCy, sample_interval, tol_redundant)
+            outside = is_outside(X, [x_range, y_range])
+            self.Xss.add_fixed_points(X[~outside], J[~outside], tol_redundant)
+
+    # TODO Refactor dict_vf
+
+    def output_to_dict(self, dict_vf):
+        dict_vf["NCx"] = self.NCx
+        dict_vf["NCy"] = self.NCy
+        dict_vf["Xss"] = self.Xss.get_X()
+        dict_vf["confidence"] = self.get_Xss_confidence()
+        dict_vf["J"] = self.Xss.get_J()
+        return dict_vf
+
+
+def util_topology(
+    adata: AnnData,
+    basis: str,
+    dims: Tuple[int, int],
+    func: Callable,
+    VecFld: VecFldDict,
+    X: Optional[np.ndarray] = None,
+    n: Optional[int] = 25,
+    **kwargs,
+):
+    """A function that computes nullclines and fixed points defined by the function func.
 
-        super().__init__(X, V, Grid)
-        if X is not None and V is not None:
-            self.parameters = kwargs
-            self.parameters = update_n_merge_dict(
-                self.parameters,
-                {
-                    "M": kwargs.pop("M", None) or max(min([50, len(X)]), int(0.05 * len(X)) + 1),
-                    # min(len(X), int(1500 * np.log(len(X)) / (np.log(len(X)) + np.log(100)))),
-                    "a": kwargs.pop("a", 5),
-                    "beta": kwargs.pop("beta", None),
-                    "ecr": kwargs.pop("ecr", 1e-5),
-                    "gamma": kwargs.pop("gamma", 0.9),
-                    "lambda_": kwargs.pop("lambda_", 3),
-                    "minP": kwargs.pop("minP", 1e-5),
-                    "MaxIter": kwargs.pop("MaxIter", 500),
-                    "theta": kwargs.pop("theta", 0.75),
-                    "div_cur_free_kernels": kwargs.pop("div_cur_free_kernels", False),
-                    "velocity_based_sampling": kwargs.pop("velocity_based_sampling", True),
-                    "sigma": kwargs.pop("sigma", 0.8),
-                    "eta": kwargs.pop("eta", 0.5),
-                    "seed": kwargs.pop("seed", 0),
-                },
-            )
+    Args:
+        adata: `AnnData` object containing cell state information.
+        basis: A string specifying the reduced dimension embedding  to use for the computation.
+        dims: A tuple of two integers specifying the dimensions of X to consider.
+        func: A vector-valued function taking in coordinates and returning the vector field.
+        VecFld: `VecFldDict` TypedDict storing information about the vector field and SparseVFC-related parameters and computations.
+        X: an alternative to providing an `AnnData` object. Provide an np.ndarray from which `dims` are accessed, Defaults to None.
+        n: An optional integer specifying the number of points to use for computing fixed points. Defaults to 25.
+
+    Returns:
+        A tuple consisting of the following elements:
+            - X_basis: an array of shape (n, 2) where n is the number of points in X. This is the subset of X consisting of the first two dimensions specified by dims. If X is not provided, X_basis is taken from the obsm attribute of adata using the key "X_" + basis.
+            - xlim, ylim: a tuple of floats specifying the limits of the x and y axes, respectively. These are computed based on the minimum and maximum values of X_basis.
+            - confidence: an array of shape (n, ) containing the confidence scores of the fixed points.
+            - NCx, NCy: arrays of shape (n, ) containing the x and y coordinates of the nullclines (lines where the derivative of the system is zero), respectively.
+            - Xss: an array of shape (n, k) where k is the number of dimensions of the system, containing the fixed points.
+            - ftype: an array of shape (n, ) containing the types of fixed points (attractor, repeller, or saddle).
+            - an array of shape (n, ) containing the indices of the fixed points in the original data.
+    """
+    X_basis = adata.obsm["X_" + basis][:, dims] if X is None else X[:, dims]
 
-        self.norm_dict = {}
+    if X_basis.shape[1] == 2:
+        fp_ind = None
+        min_, max_ = X_basis.min(0), X_basis.max(0)
+
+        xlim = [
+            min_[0] - (max_[0] - min_[0]) * 0.1,
+            max_[0] + (max_[0] - min_[0]) * 0.1,
+        ]
+        ylim = [
+            min_[1] - (max_[1] - min_[1]) * 0.1,
+            max_[1] + (max_[1] - min_[1]) * 0.1,
+        ]
+
+        vecfld = VectorField2D(func, X_data=X_basis)
+        vecfld.find_fixed_points_by_sampling(n, xlim, ylim)
+        if vecfld.get_num_fixed_points() > 0:
+            vecfld.compute_nullclines(xlim, ylim, find_new_fixed_points=True)
+            NCx, NCy = vecfld.NCx, vecfld.NCy
 
-    def train(self, normalize=False, **kwargs):
-        """Learn an function of vector field from sparse single cell samples in the entire space robustly.
-        Reference: Regularized vector field learning with sparse approximation for mismatch removal, Ma, Jiayi, etc. al,
-        Pattern Recognition
-
-        Arguments
-        ---------
-            normalize: 'bool' (default: False)
-                Logic flag to determine whether to normalize the data to have zero means and unit covariance. This is
-                often required for raw dataset (for example, raw UMI counts and RNA velocity values in high dimension).
-                But it is normally not required for low dimensional embeddings by PCA or other non-linear dimension
-                reduction methods.
-            method: 'string'
-                Method that is used to reconstruct the vector field functionally. Currently only SparseVFC supported but
-                other improved approaches are under development.
-
-        Returns
-        -------
-            VecFld: `dict'
-                A dictionary which contains X, Y, beta, V, C, P, VFCIndex. Where V = f(X), P is the posterior
-                probability and VFCIndex is the indexes of inliers which found by VFC.
-        """
+        Xss, ftype = vecfld.get_fixed_points(get_types=True)
+        confidence = vecfld.get_Xss_confidence()
+    else:
+        fp_ind = None
+        xlim, ylim, confidence, NCx, NCy = None, None, None, None, None
+        vecfld = BaseVectorField(
+            X=VecFld["X"][VecFld["valid_ind"], :],
+            V=VecFld["Y"][VecFld["valid_ind"], :],
+            func=func,
+        )
 
-        if normalize:
-            X_norm, V_norm, T_norm, norm_dict = norm(self.data["X"], self.data["V"], self.data["Grid"])
-            (self.data["X"], self.data["V"], self.data["Grid"], self.norm_dict,) = (
-                X_norm,
-                V_norm,
-                T_norm,
-                norm_dict,
-            )
+        Xss, ftype = vecfld.get_fixed_points(n_x0=n, **kwargs)
+        if Xss.ndim > 1 and Xss.shape[1] > 2:
+            fp_ind = nearest_neighbors(Xss, vecfld.data["X"], 1).flatten()
+            Xss = vecfld.data["X"][fp_ind]
+
+    return X_basis, xlim, ylim, confidence, NCx, NCy, Xss, ftype, fp_ind
+
+
+def topography(
+    adata: AnnData,
+    basis: Optional[str] = "umap",
+    layer: Optional[str] = None,
+    X: Optional[np.ndarray] = None,
+    dims: Optional[list] = None,
+    n: Optional[int] = 25,
+    VecFld: Optional[VecFldDict] = None,
+    **kwargs,
+) -> AnnData:
+    """Map the topography of the single cell vector field in (first) two dimensions.
+
+    Args:
+        adata: an AnnData object.
+        basis: The reduced dimension embedding of cells to visualize.
+        layer: Which layer of the data will be used for vector field function reconstruction. This will be used in conjunction with X.
+        X: Original data. Not used
+        dims: The dimensions that will be used for vector field reconstruction.
+        n: Number of samples for calculating the fixed points.
+        VecFld: The reconstructed vector field function.
+        kwargs: Key word arguments passed to the find_fixed_point function of the vector field class for high dimension
+        fixed point identification.
+
+    Returns:
+        `AnnData` object that is updated with the `VecFld` or 'VecFld_' + basis dictionary in the `uns` attribute.
+        The `VecFld2D` key stores an instance of the VectorField2D class which presumably has fixed points, nullcline, separatrix, computed and stored.
+    """
 
-        verbose = kwargs.pop("verbose", 0)
-        lstsq_method = kwargs.pop("lstsq_method", "drouin")
-        with warnings.catch_warnings():
-            warnings.simplefilter("ignore")
+    if VecFld is None:
+        VecFld, func = vecfld_from_adata(adata, basis)
+    else:
+        if "velocity_loss_traj" in VecFld.keys():
 
-            VecFld = SparseVFC(
-                self.data["X"],
-                self.data["V"],
-                self.data["Grid"],
-                **self.parameters,
-                verbose=verbose,
-                lstsq_method=lstsq_method,
-            )
-        if normalize:
-            VecFld = denorm(VecFld, X_norm, V_norm, self.norm_dict)
+            def func(x):
+                return dynode_vector_field_function(x, VecFld)
 
-        self.parameters = update_dict(self.parameters, VecFld)
+        else:
 
-        self.vf_dict = VecFld
+            def func(x):
+                return vector_field_function(x, VecFld)
 
-        self.func = lambda x: vector_field_function(x, VecFld)
-        self.vf_dict["V"] = self.func(self.data["X"])
-        self.vf_dict["normalize"] = normalize
+    if dims is None:
+        dims = np.arange(adata.obsm["X_" + basis].shape[1])
 
-        return self.vf_dict
+    (
+        X_basis,
+        xlim,
+        ylim,
+        confidence,
+        NCx,
+        NCy,
+        Xss,
+        ftype,
+        fp_ind,
+    ) = util_topology(adata=adata, basis=basis, X=X, dims=dims, func=func, VecFld=VecFld, n=n, *kwargs)
 
-    def plot_energy(self, figsize=None, fig=None):
-        from ..plot.scVectorField import plot_energy
+    # commented for now, will go back to this later.
+    # sep = compute_separatrices(vecfld.Xss.get_X(), vecfld.Xss.get_J(), vecfld.func, xlim, ylim)
 
-        plot_energy(None, vecfld_dict=self.vf_dict, figsize=figsize, fig=fig)
+    if layer is None:
+        vf_key = "VecFld_" + basis
+    else:
+        vf_key = "VecFld" if layer == "X" else "VecFld_" + layer
 
-    def get_Jacobian(self, method="analytical", input_vector_convention="row", **kwargs):
-        """
-        Get the Jacobian of the vector field function.
-        If method is 'analytical':
-        The analytical Jacobian will be returned and it always
-        take row vectors as input no matter what input_vector_convention is.
-
-        If method is 'numerical':
-        If the input_vector_convention is 'row', it means that fjac takes row vectors
-        as input, otherwise the input should be an array of column vectors. Note that
-        the returned Jacobian would behave exactly the same if the input is an 1d array.
-
-        The column vector convention is slightly faster than the row vector convention.
-        So the matrix of row vector convention is converted into column vector convention
-        under the hood.
-
-        No matter the method and input vector convention, the returned Jacobian is of the
-        following format:
-                df_1/dx_1   df_1/dx_2   df_1/dx_3   ...
-                df_2/dx_1   df_2/dx_2   df_2/dx_3   ...
-                df_3/dx_1   df_3/dx_2   df_3/dx_3   ...
-                ...         ...         ...         ...
-        """
-        if method == "numerical":
-            return Jacobian_numerical(self.func, input_vector_convention, **kwargs)
-        elif method == "parallel":
-            return lambda x: Jacobian_rkhs_gaussian_parallel(x, self.vf_dict, **kwargs)
-        elif method == "analytical":
-            return lambda x: Jacobian_rkhs_gaussian(x, self.vf_dict, **kwargs)
-        else:
-            raise NotImplementedError(
-                f"The method {method} is not implemented. Currently only "
-                f"supports 'analytical', 'numerical', and 'parallel'."
-            )
+    if vf_key in adata.uns_keys():
+        adata.uns[vf_key].update(
+            {
+                "xlim": xlim,
+                "ylim": ylim,
+                "X_data": X_basis,
+                "Xss": Xss,
+                "ftype": ftype,
+                "confidence": confidence,
+                "nullcline": [NCx, NCy],
+                "separatrices": None,
+                "fp_ind": fp_ind,
+            }
+        )
+    else:
+        adata.uns[vf_key] = {
+            "xlim": xlim,
+            "ylim": ylim,
+            "X_data": X_basis,
+            "Xss": Xss,
+            "ftype": ftype,
+            "confidence": confidence,
+            "nullcline": [NCx, NCy],
+            "separatrices": None,
+            "fp_ind": fp_ind,
+        }
+
+    return adata
+
+
+def VectorField(
+    adata: anndata.AnnData,
+    basis: Optional[str] = None,
+    layer: Optional[str] = None,
+    dims: Optional[Union[int, list]] = None,
+    genes: Optional[list] = None,
+    normalize: Optional[bool] = False,
+    grid_velocity: bool = False,
+    grid_num: int = 50,
+    velocity_key: str = "velocity_S",
+    method: str = "SparseVFC",
+    min_vel_corr: float = 0.6,
+    restart_num: int = 5,
+    restart_seed: Optional[list] = [0, 100, 200, 300, 400],
+    model_buffer_path: Optional[str] = None,
+    return_vf_object: bool = False,
+    map_topography: bool = False,
+    pot_curl_div: bool = False,
+    cores: int = 1,
+    result_key: Optional[str] = None,
+    copy: bool = False,
+    **kwargs,
+) -> Union[anndata.AnnData, BaseVectorField]:
+    """Learn a function of high dimensional vector field from sparse single cell samples in the entire space robustly.
+
+    Args:
+        adata: AnnData object that contains embedding and velocity data
+        basis: The embedding data to use. The vector field function will be learned on the low  dimensional embedding and can be then projected
+            back to the high dimensional space.
+        layer: Which layer of the data will be used for vector field function reconstruction. The layer once provided, will override the `basis`
+            argument and then learn the vector field function in high dimensional space.
+        dims: The dimensions that will be used for reconstructing vector field functions. If it is an `int` all     dimension from the first
+            dimension to `dims` will be used; if it is a list, the dimensions in the list will be used.
+        genes: The gene names whose gene expression will be used for vector field reconstruction. By default (when genes is set to None), the genes
+            used for velocity embedding (var.use_for_transition) will be used for vector field reconstruction. Note that the genes to be used need to have velocity calculated.
+        normalize: Logic flag to determine whether to normalize the data to have zero means and unit covariance. This is often required for raw
+            dataset (for example, raw UMI counts and RNA velocity values in high dimension). But it is normally not required for low dimensional embeddings by PCA or other non-linear dimension reduction methods.
+        grid_velocity: Whether to generate grid velocity. Note that by default it is set to be False, but for datasets with embedding dimension
+            less than 4, the grid velocity will still be generated. Please note that number of total grids in the space increases exponentially as the number of dimensions increases. So it may quickly lead to lack of memory, for example, it cannot allocate the array with grid_num set to be 50 and dimension is 6 (50^6 total grids) on 32 G memory computer. Although grid velocity may not be generated, the vector field function can still be learned for thousands of dimensions and we can still predict the transcriptomic cell states over long time period.
+        grid_num: The number of grids in each dimension for generating the grid velocity.
+        velocity_key: The key from the adata layer that corresponds to the velocity matrix.
+        method: Method that is used to reconstruct the vector field functionally. Currently only SparseVFC supported but other improved approaches
+            are under development.
+        min_vel_corr: The minimal threshold for the cosine correlation between input velocities and learned velocities to consider as a successful
+            vector field reconstruction procedure. If the cosine correlation is less than this threshold and restart_num > 1, `restart_num` trials will be attempted with different seeds to reconstruct the vector field function. This can avoid some reconstructions to be trapped in some local optimal.
+        restart_num: The number of retrials for vector field reconstructions.
+        restart_seed: A list of seeds for each retrial. Must be the same length as `restart_num` or None.
+        buffer_path: The directory address keeping all the saved/to-be-saved torch variables and NN modules. When `method` is set to be `dynode`,
+            buffer_path will set to be
+        return_vf_object: Whether or not to include an instance of a vectorfield class in the the `VecFld` dictionary in the `uns`attribute.
+        map_topography: Whether to quantify the topography of vector field. Note that for higher than 2D vector     field, we can only identify
+            fixed points as high-dimensional nullcline and separatrices are mathematically difficult to be identified. Nullcline and separatrices will also be a surface or manifold in high-dimensional vector field.
+        pot_curl_div: Whether to calculate potential, curl or divergence for each cell. Potential can be calculated for any basis while curl and
+            divergence is by default only applied to 2D basis. However, divergence is applicable for any dimension while curl is generally only defined for 2/3 D systems.
+        cores: Number of cores to run the ddhodge function. If cores is set to be > 1, multiprocessing will be used to
+            parallel the ddhodge calculation.
+        result_key:
+            The key that will be used as prefix for the vector field key in .uns
+        copy: Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments and
+            returning `None`.
+        kwargs: Other additional parameters passed to the vectorfield class.
+
+    Returns:
+        If `copy` and `return_vf_object` arguments are set to False, `annData` object is updated with the `VecFld`dictionary in the `uns` attribute.
+        If `return_vf_object` is set to True, then a vector field class object is returned.
+        If `copy` is set to True, a deep copy of the original `adata` object is returned.
+    """
+    logger = LoggerManager.gen_logger("dynamo-topography")
+    logger.info("VectorField reconstruction begins...", indent_level=1)
+    logger.log_time()
+    adata = copy_adata(adata) if copy else adata
 
-    def get_Hessian(self, method="analytical", **kwargs):
-        """
-        Get the Hessian of the vector field function.
-        If method is 'analytical':
-        The analytical Hessian will be returned and it always
-        take row vectors as input no matter what input_vector_convention is.
-
-        No matter the method and input vector convention, the returned Hessian is of the
-        following format:
-                df^2/dx_1^2        df_1^2/(dx_1 dx_2)   df_1^2/(dx_1 dx_3)   ...
-                df^2/(dx_2 dx_1)   df^2/dx_2^2          df^2/(dx_2 dx_3)     ...
-                df^2/(dx_3 dx_1)   df^2/(dx_3 dx_2)     df^2/dx_3^2          ...
-                ...                ...                  ...                  ...
-        """
-        if method == "analytical":
-            return lambda x: Hessian_rkhs_gaussian(x, self.vf_dict, **kwargs)
-        elif method == "numerical":
-            if self.func is not None:
-                raise Exception("numerical Hessian for vector field is not defined.")
-            else:
-                raise Exception("The perturbed vector field function has not been set up.")
-        else:
-            raise NotImplementedError(f"The method {method} is not implemented. Currently only supports 'analytical'.")
+    if basis is not None:
+        logger.info(
+            "Retrieve X and V based on basis: %s. \n "
+            "       Vector field will be learned in the %s space." % (basis.upper(), basis.upper())
+        )
+        X = adata.obsm["X_" + basis].copy()
+        V = adata.obsm["velocity_" + basis].copy()
 
-    def get_Laplacian(self, method="analytical", **kwargs):
-        """
-        Get the Laplacian of the vector field. Laplacian is defined as the sum of the diagonal of the Hessian matrix.
-        Because Hessian is originally defined for scalar function and here we extend it to vector functions. We will
-        calculate the summation of the diagonal of each output (target) dimension.
-
-        A Laplacian filter is an edge detector used to compute the second derivatives of an image, measuring the rate
-        at which the first derivatives change (so it is the derivative of the Jacobian). This determines if a change in
-        adjacent pixel values is from an edge or continuous progression.
-        """
-        if method == "analytical":
-            return lambda x: Laplacian(H=x)
-        elif method == "numerical":
-            if self.func is not None:
-                raise Exception("Numerical Laplacian for vector field is not defined.")
-            else:
-                raise Exception("The perturbed vector field function has not been set up.")
+        if np.isscalar(dims):
+            X, V = X[:, :dims], V[:, :dims]
+        elif type(dims) is list:
+            X, V = X[:, dims], V[:, dims]
+    else:
+        logger.info(
+            "Retrieve X and V based on `genes`, layer: %s. \n "
+            "       Vector field will be learned in the gene expression space." % layer
+        )
+        valid_genes = (
+            list(set(genes).intersection(adata.var.index))
+            if genes is not None
+            else adata.var_names[adata.var.use_for_transition]
+        )
+        if layer == "X":
+            X = adata[:, valid_genes].X.copy()
+            X = np.expm1(X)
         else:
-            raise NotImplementedError(f"The method {method} is not implemented. Currently only supports 'analytical'.")
+            X = inverse_norm(adata, adata.layers[layer])
 
-    def evaluate(self, CorrectIndex, VFCIndex, siz):
-        """Evaluate the precision, recall, corrRate of the sparseVFC algorithm.
+        V = adata[:, valid_genes].layers[velocity_key].copy()
 
-        Arguments
-        ---------
-            CorrectIndex: 'List'
-                Ground truth indexes of the correct vector field samples.
-            VFCIndex: 'List'
-                Indexes of the correct vector field samples learned by VFC.
-            siz: 'int'
-                Number of initial matches.
-
-        Returns
-        -------
-        A tuple of precision, recall, corrRate:
-        Precision, recall, corrRate: Precision and recall of VFC, percentage of initial correct matches.
+        if sp.issparse(X):
+            X, V = X.A, V.A
 
-        See also:: :func:`sparseVFC`.
-        """
-
-        if len(VFCIndex) == 0:
-            VFCIndex = range(siz)
-
-        VFCCorrect = np.intersect1d(VFCIndex, CorrectIndex)
-        NumCorrectIndex = len(CorrectIndex)
-        NumVFCIndex = len(VFCIndex)
-        NumVFCCorrect = len(VFCCorrect)
-
-        corrRate = NumCorrectIndex / siz
-        precision = NumVFCCorrect / NumVFCIndex
-        recall = NumVFCCorrect / NumCorrectIndex
+        # keep only genes with finite velocity and expression values, useful when learning vector field in the original
+        # gene expression space.
+        finite_genes = np.logical_and(np.isfinite(X).all(axis=0), np.isfinite(V).all(axis=0))
+        X, V = X[:, finite_genes], V[:, finite_genes]
+        valid_genes = np.array(valid_genes)[np.where(finite_genes)[0]].tolist()
+        if sum(finite_genes) < len(finite_genes):
+            logger.warning(
+                f"There are {(len(finite_genes) - sum(finite_genes))} genes with infinite expression or velocity "
+                f"values. These genes will be excluded from vector field reconstruction. Please make sure the genes you "
+                f"selected has no non-infinite values"
+            )
 
-        print("correct correspondence rate in the original data: %d/%d = %f" % (NumCorrectIndex, siz, corrRate))
-        print("precision rate: %d/%d = %f" % (NumVFCCorrect, NumVFCIndex, precision))
-        print("recall rate: %d/%d = %f" % (NumVFCCorrect, NumCorrectIndex, recall))
+    Grid = None
+    if X.shape[1] < 4 or grid_velocity:
+        logger.info("Generating high dimensional grids and convert into a row matrix.")
+        # smart way for generating high dimensional grids and convert into a row matrix
+        min_vec, max_vec = (
+            X.min(0),
+            X.max(0),
+        )
+        min_vec = min_vec - 0.01 * np.abs(max_vec - min_vec)
+        max_vec = max_vec + 0.01 * np.abs(max_vec - min_vec)
 
-        return corrRate, precision, recall
+        Grid_list = np.meshgrid(*[np.linspace(i, j, grid_num) for i, j in zip(min_vec, max_vec)])
+        Grid = np.array([i.flatten() for i in Grid_list]).T
 
+    if X is None:
+        raise Exception(f"X is None. Make sure you passed the correct X or {basis} dimension reduction method.")
+    elif V is None:
+        raise Exception("V is None. Make sure you passed the correct V.")
+
+    logger.info("Learning vector field with method: %s." % (method.lower()))
+    if method.lower() == "sparsevfc":
+        vf_kwargs = {
+            "M": None,
+            "a": 5,
+            "beta": None,
+            "ecr": 1e-5,
+            "gamma": 0.9,
+            "lambda_": 3,
+            "minP": 1e-5,
+            "MaxIter": 30,
+            "theta": 0.75,
+            "div_cur_free_kernels": False,
+            "velocity_based_sampling": True,
+            "sigma": 0.8,
+            "eta": 0.5,
+            "seed": 0,
+        }
+    elif method.lower() == "dynode_old":
+        try:
+            from dynode.vectorfield import networkModels
+
+            # from dynode.vectorfield.losses_weighted import MAD, BinomialChannel, WassersteinDistance, CosineDistance
+            from dynode.vectorfield.losses_weighted import MSE
+            from dynode.vectorfield.samplers import VelocityDataSampler
+
+            from .scVectorField import dynode_vectorfield
+        except ImportError:
+            raise ImportError("You need to install the package `dynode`." "install dynode via `pip install dynode`")
+
+        velocity_data_sampler = VelocityDataSampler(adata={"X": X, "V": V}, normalize_velocity=normalize)
+        max_iter = 2 * 100000 * np.log(X.shape[0]) / (250 + np.log(X.shape[0]))
+
+        cwd, cwt = os.getcwd(), datetime.datetime.now()
+
+        if model_buffer_path is None:
+            model_buffer_path = cwd + "/" + basis + "_" + str(cwt.year) + "_" + str(cwt.month) + "_" + str(cwt.day)
+            main_warning("the buffer path saving the dynode model is in %s" % (model_buffer_path))
+
+        vf_kwargs = {
+            "model": networkModels,
+            "sirens": False,
+            "enforce_positivity": False,
+            "velocity_data_sampler": velocity_data_sampler,
+            "time_course_data_sampler": None,
+            "network_dim": X.shape[1],
+            "velocity_loss_function": MSE(),  # CosineDistance(), # #MSE(), MAD()
+            # BinomialChannel(p=0.1, alpha=1)
+            "time_course_loss_function": None,
+            "velocity_x_initialize": X,
+            "time_course_x0_initialize": None,
+            "smoothing_factor": None,
+            "stability_factor": None,
+            "load_model_from_buffer": False,
+            "buffer_path": model_buffer_path,
+            "hidden_features": 256,
+            "hidden_layers": 3,
+            "first_omega_0": 30.0,
+            "hidden_omega_0": 30.0,
+        }
+        train_kwargs = {
+            "max_iter": int(max_iter),
+            "velocity_batch_size": 50,
+            "time_course_batch_size": 100,
+            "autoencoder_batch_size": 50,
+            "velocity_lr": 1e-4,
+            "velocity_x_lr": 0,
+            "time_course_lr": 1e-4,
+            "time_course_x0_lr": 1e4,
+            "autoencoder_lr": 1e-4,
+            "velocity_sample_fraction": 1,
+            "time_course_sample_fraction": 1,
+            "iter_per_sample_update": None,
+        }
+    elif method.lower() == "dynode":
+        try:
+            from dynode.vectorfield import Dynode  # networkModels,
+
+            # from dynode.vectorfield.losses_weighted import MAD, BinomialChannel, WassersteinDistance, CosineDistance
+            # from dynode.vectorfield.losses_weighted import MSE
+            # from dynode.vectorfield.samplers import VelocityDataSampler
+            from .scVectorField import dynode_vectorfield
+        except ImportError:
+            raise ImportError("You need to install the package `dynode`." "install dynode via `pip install dynode`")
+
+        if not ("Dynode" in kwargs and type(kwargs["Dynode"]) == Dynode):
+            velocity_data_sampler = VelocityDataSampler(adata={"X": X, "V": V}, normalize_velocity=normalize)
+            max_iter = 2 * 100000 * np.log(X.shape[0]) / (250 + np.log(X.shape[0]))
+
+            cwd, cwt = os.getcwd(), datetime.datetime.now()
+
+            if model_buffer_path is None:
+                model_buffer_path = cwd + "/" + basis + "_" + str(cwt.year) + "_" + str(cwt.month) + "_" + str(cwt.day)
+                main_warning("the buffer path saving the dynode model is in %s" % (model_buffer_path))
 
-class KOVectorField(DifferentiableVectorField):
-    def __init__(
-        self, X=None, V=None, Grid=None, K=None, func_base=None, fjac_base=None, PCs=None, mean=None, *args, **kwargs
-    ):
-        super().__init__(X, V, Grid=Grid, *args, **kwargs)
-
-        if K.ndim == 2:
-            K = np.diag(K)
-        self.K = K
-        self.PCs = PCs
-        self.mean = mean
-        self.func_base = func_base
-        self.fjac_base = fjac_base
-
-        if self.K is not None and self.PCs is not None and self.mean is not None and self.func_base is not None:
-            self.setup_perturbed_func()
-
-    def setup_perturbed_func(self):
-        def vf_func_perturb(x):
-            x_gene = np.abs(x @ self.PCs.T + self.mean)
-            v_gene = vector_transformation(self.func_base(x), self.PCs)
-            v_gene = v_gene - self.K * x_gene
-            return v_gene @ self.PCs
+            vf_kwargs = {
+                "model": networkModels,
+                "sirens": False,
+                "enforce_positivity": False,
+                "velocity_data_sampler": velocity_data_sampler,
+                "time_course_data_sampler": None,
+                "network_dim": X.shape[1],
+                "velocity_loss_function": MSE(),  # CosineDistance(), # #MSE(), MAD()
+                # BinomialChannel(p=0.1, alpha=1)
+                "time_course_loss_function": None,
+                "velocity_x_initialize": X,
+                "time_course_x0_initialize": None,
+                "smoothing_factor": None,
+                "stability_factor": None,
+                "load_model_from_buffer": False,
+                "buffer_path": model_buffer_path,
+                "hidden_features": 256,
+                "hidden_layers": 3,
+                "first_omega_0": 30.0,
+                "hidden_omega_0": 30.0,
+            }
+            train_kwargs = {
+                "max_iter": int(max_iter),
+                "velocity_batch_size": 50,
+                "time_course_batch_size": 100,
+                "autoencoder_batch_size": 50,
+                "velocity_lr": 1e-4,
+                "velocity_x_lr": 0,
+                "time_course_lr": 1e-4,
+                "time_course_x0_lr": 1e4,
+                "autoencoder_lr": 1e-4,
+                "velocity_sample_fraction": 1,
+                "time_course_sample_fraction": 1,
+                "iter_per_sample_update": None,
+            }
+        else:
+            vf_kwargs, train_kwargs = {}, {}
+    else:
+        raise ValueError("current only support two methods, SparseVFC and dynode")
 
-        self.func = vf_func_perturb
+    vf_kwargs = update_dict(vf_kwargs, kwargs)
 
-    def get_Jacobian(self, method="analytical", **kwargs):
-        """
-        Get the Jacobian of the vector field function.
-        If method is 'analytical':
-        The analytical Jacobian will be returned and it always
-        take row vectors as input no matter what input_vector_convention is.
-
-        No matter the method and input vector convention, the returned Jacobian is of the
-        following format:
-                df_1/dx_1   df_1/dx_2   df_1/dx_3   ...
-                df_2/dx_1   df_2/dx_2   df_2/dx_3   ...
-                df_3/dx_1   df_3/dx_2   df_3/dx_3   ...
-                ...         ...         ...         ...
-        """
-        if method == "analytical":
-            exact = kwargs.pop("exact", False)
-            mu = kwargs.pop("mu", None)
-            if exact:
-                if mu is None:
-                    mu = self.mean
-                return lambda x: Jacobian_kovf(x, self.fjac_base, self.K, self.PCs, exact=True, mu=mu, **kwargs)
-            else:
-                return lambda x: Jacobian_kovf(x, self.fjac_base, self.K, self.PCs, **kwargs)
-        elif method == "numerical":
-            if self.func is not None:
-                return Jacobian_numerical(self.func, **kwargs)
-            else:
-                raise Exception("The perturbed vector field function has not been set up.")
-        else:
-            raise NotImplementedError(
-                f"The method {method} is not implemented. Currently only " f"supports 'analytical'."
+    if restart_num > 0:
+        if len(restart_seed) != restart_num:
+            main_warning(
+                f"the length of {restart_seed} is different from {restart_num}, " f"using `np.range(restart_num) * 100"
+            )
+            restart_seed = np.arange(restart_num) * 100
+        restart_counter, cur_vf_list, res_list = 0, [], []
+        while True:
+            if method.lower() == "sparsevfc":
+                kwargs.update({"seed": restart_seed[restart_counter]})
+                VecFld = SvcVectorField(X, V, Grid, **vf_kwargs)
+                cur_vf_dict = VecFld.train(normalize=normalize, **kwargs)
+            elif method.lower() == "dynode_old":
+                train_kwargs = update_dict(train_kwargs, kwargs)
+                VecFld = dynode_vectorfield(X, V, Grid, **vf_kwargs)
+                # {"VecFld": VecFld.train(**kwargs)}
+                cur_vf_dict = VecFld.train(**train_kwargs)
+            elif method.lower() == "dynode":
+                if not ("Dynode" in kwargs and type(kwargs["Dynode"]) == Dynode):
+                    train_kwargs = update_dict(train_kwargs, kwargs)
+                    VecFld = dynode_vectorfield(X, V, Grid, **vf_kwargs)
+                    # {"VecFld": VecFld.train(**kwargs)}
+                    cur_vf_dict = VecFld.train(**train_kwargs)
+                else:
+                    Dynode_obj = kwargs["Dynode"]
+                    VecFld = dynode_vectorfield.fromDynode(Dynode_obj)
+                    X, Y = Dynode_obj.Velocity["sampler"].X_raw, Dynode_obj.Velocity["sampler"].V_raw
+                    cur_vf_dict = {
+                        "X": X,
+                        "Y": Y,
+                        "V": Dynode_obj.predict_velocity(Dynode_obj.Velocity["sampler"].X_raw),
+                        "grid_V": Dynode_obj.predict_velocity(Dynode_obj.Velocity["sampler"].Grid),
+                        "valid_ind": Dynode_obj.Velocity["sampler"].valid_ind
+                        if hasattr(Dynode_obj.Velocity["sampler"], "valid_ind")
+                        else np.arange(X.shape[0]),
+                        "parameters": Dynode_obj.Velocity,
+                        "dynode_object": VecFld,
+                    }
+
+            # consider refactor with .simulation.evaluation.py
+            reference, prediction = (
+                cur_vf_dict["Y"][cur_vf_dict["valid_ind"]],
+                cur_vf_dict["V"][cur_vf_dict["valid_ind"]],
             )
+            true_normalized = reference / (np.linalg.norm(reference, axis=1).reshape(-1, 1) + 1e-20)
+            predict_normalized = prediction / (np.linalg.norm(prediction, axis=1).reshape(-1, 1) + 1e-20)
+            res = np.mean(true_normalized * predict_normalized) * prediction.shape[1]
+
+            cur_vf_list += [cur_vf_dict]
+            res_list += [res]
+            if res < min_vel_corr:
+                restart_counter += 1
+                main_info(
+                    f"current cosine correlation between input velocities and learned velocities is less than "
+                    f"{min_vel_corr}. Make a {restart_counter}-th vector field reconstruction trial.",
+                    indent_level=2,
+                )
+            else:
+                vf_dict = cur_vf_dict
+                break
 
+            if restart_counter > restart_num - 1:
+                main_warning(
+                    f"Cosine correlation between input velocities and learned velocities is less than"
+                    f" {min_vel_corr} after {restart_num} trials of vector field reconstruction."
+                )
+                vf_dict = cur_vf_list[np.argmax(np.array(res_list))]
 
-try:
-    from dynode.vectorfield import Dynode
+                break
+    else:
+        if method.lower() == "sparsevfc":
+            VecFld = SvcVectorField(X, V, Grid, **vf_kwargs)
+            vf_dict = VecFld.train(normalize=normalize, **kwargs)
+        elif method.lower() == "dynode":
+            train_kwargs = update_dict(train_kwargs, kwargs)
+            VecFld = dynode_vectorfield(X, V, Grid, **vf_kwargs)
+            # {"VecFld": VecFld.train(**kwargs)}
+            vf_dict = VecFld.train(**train_kwargs)
 
-    use_dynode = True
-except ImportError:
-    use_dynode = False
-
-if use_dynode:
-
-    class dynode_vectorfield(BaseVectorField, Dynode):  #
-        def __init__(self, X=None, V=None, Grid=None, dynode_object=None, *args, **kwargs):
-
-            self.norm_dict = {}
-
-            assert dynode_object is not None, "dynode_object argument is required."
-
-            valid_ind = None
-            if X is not None and V is not None:
-                pass
-            elif dynode_object.Velocity["sampler"] is not None:
-                X = dynode_object.Velocity["sampler"].X_raw
-                V = dynode_object.Velocity["sampler"].V_raw
-                Grid = (
-                    dynode_object.Velocity["sampler"].Grid
-                    if hasattr(dynode_object.Velocity["sampler"], "Grid")
-                    else None
-                )
-                # V = dynode_object.predict_velocity(dynode_object.Velocity["sampler"].X_raw)
-                valid_ind = dynode_object.Velocity["sampler"].valid_ind
-            else:
-                raise
+    if result_key is None:
+        vf_key = "VecFld" if basis is None else "VecFld_" + basis
+    else:
+        vf_key = result_key if basis is None else result_key + "_" + basis
 
-            self.parameters = update_n_merge_dict(kwargs, {"X": X, "V": V, "Grid": Grid})
+    vf_dict["method"] = method
+    if basis is not None:
+        key = "velocity_" + basis + "_" + method
+        X_copy_key = "X_" + basis + "_" + method
+
+        logger.info_insert_adata(key, adata_attr="obsm")
+        logger.info_insert_adata(X_copy_key, adata_attr="obsm")
+        adata.obsm[key] = vf_dict["V"]
+        adata.obsm[X_copy_key] = vf_dict["X"]
 
-            self.valid_ind = np.where(~np.isnan(V.sum(1)))[0] if valid_ind is None else valid_ind
+        vf_dict["dims"] = dims
 
-            vf_kwargs = {
-                "X": X,
-                "V": V,
-                "Grid": Grid,
-                "NNmodel": dynode_object.NNmodel,
-                "Velocity_sampler": dynode_object.Velocity["sampler"],
-                "TimeCourse_sampler": dynode_object.TimeCourse["sampler"],
-                "Velocity_ChannelModel": dynode_object.Velocity["channel_model"],
-                "TimeCourse_ChannelModel": dynode_object.TimeCourse["channel_model"],
-                "Velocity_x_initialize": dynode_object.Velocity["x_variable"],
-                "TimeCourse_x0_initialize": dynode_object.TimeCourse["x0_variable"],
-                "NNmodel_save_path": dynode_object.NNmodel_save_path,
-                "device": dynode_object.device,
-            }
+        logger.info_insert_adata(vf_key, adata_attr="uns")
+        adata.uns[vf_key] = vf_dict
+    else:
+        key = velocity_key + "_" + method
 
-            vf_kwargs = update_dict(vf_kwargs, self.parameters)
-            super().__init__(**vf_kwargs)
+        logger.info_insert_adata(key, adata_attr="layers")
+        adata.layers[key] = sp.csr_matrix((adata.shape))
+        adata.layers[key][:, [adata.var_names.get_loc(i) for i in valid_genes]] = vf_dict["V"]
 
-            self.func = self.predict_velocity
+        vf_dict["layer"] = layer
+        vf_dict["genes"] = genes
+        vf_dict["velocity_key"] = velocity_key
 
-            self.vf_dict = {
-                "X": self.data["X"],
-                "valid_ind": self.valid_ind,
-                "Y": self.data["V"],
-                "V": self.func(self.data["X"]),
-                "grid": self.data["Grid"],
-                "grid_V": self.func(self.data["Grid"]),
-                "iteration": int(dynode_object.max_iter),
-                "velocity_loss_traj": dynode_object.Velocity["loss_trajectory"],
-                "time_course_loss_traj": dynode_object.TimeCourse["loss_trajectory"],
-                "autoencoder_loss_traj": dynode_object.AutoEncoder["loss_trajectory"],
-                "parameters": self.parameters,
-            }
+        logger.info_insert_adata(vf_key, adata_attr="uns")
+        adata.uns[vf_key] = vf_dict
+
+    if map_topography:
+        tp_kwargs = {"n": 25}
+        tp_kwargs = update_dict(tp_kwargs, kwargs)
 
-        @classmethod
-        def fromDynode(cls, dynode_object: Dynode) -> "dynode_vectorfield":
-            return cls(X=None, V=None, Grid=None, dynode_object=dynode_object)
+        logger.info("Mapping topography...")
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
 
+            adata = topography(
+                adata,
+                basis=basis,
+                X=X,
+                layer=layer,
+                dims=None,
+                VecFld=vf_dict,
+                **tp_kwargs,
+            )
+    if pot_curl_div:
+        from .vector_calculus import curl, divergence
 
-def vector_field_function_knockout(
-    adata,
-    vecfld: Union[Callable, BaseVectorField],
-    ko_genes,
-    k_deg=None,
-    pca_genes="use_for_pca",
-    PCs="PCs",
-    mean="pca_mean",
-    return_vector_field_class=True,
-):
+        logger.info(f"Running ddhodge to estimate vector field based pseudotime in {basis} basis...")
+        from ..external.hodge import ddhodge
 
-    if type(pca_genes) is str:
-        pca_genes = adata.var[adata.var[pca_genes]].index
+        ddhodge(adata, basis=basis, cores=cores)
+        if X.shape[1] == 2:
+            logger.info("Computing curl...")
+            curl(adata, basis=basis)
+
+        logger.info("Computing divergence...")
+        divergence(adata, basis=basis)
+
+    control_point, inlier_prob, valid_ids = (
+        "control_point_" + basis if basis is not None else "control_point",
+        "inlier_prob_" + basis if basis is not None else "inlier_prob",
+        vf_dict["valid_ind"],
+    )
+    if method.lower() == "sparsevfc":
+        logger.info_insert_adata(control_point, adata_attr="obs")
+        logger.info_insert_adata(inlier_prob, adata_attr="obs")
+
+        adata.obs[control_point], adata.obs[inlier_prob] = False, np.nan
+        adata.obs.loc[adata.obs_names[vf_dict["ctrl_idx"]], control_point] = True
+        adata.obs.loc[adata.obs_names[valid_ids], inlier_prob] = vf_dict["P"].flatten()
+
+    # angles between observed velocity and that predicted by vector field across cells:
+    cell_angles = np.zeros(adata.n_obs, dtype=float)
+    for i, u, v in zip(valid_ids, V[valid_ids], vf_dict["V"]):
+        # fix the u, v norm == 0 in angle function
+        cell_angles[i] = angle(u.astype("float64"), v.astype("float64"))
 
-    g_mask = np.zeros(len(pca_genes), dtype=bool)
-    for i, g in enumerate(pca_genes):
-        if g in ko_genes:
-            g_mask[i] = True
-    if g_mask.sum() != len(ko_genes):
-        raise ValueError(f"the ko_genes {ko_genes} you provided don't all belong to {pca_genes}.")
-
-    k = np.zeros(len(pca_genes))
-    if k_deg is None:
-        k_deg = np.ones(len(ko_genes))
-    k[g_mask] = k_deg
-
-    if type(PCs) is str:
-        if PCs not in adata.uns.keys():
-            raise Exception(f"The key {PCs} is not in `.uns`.")
-        PCs = adata.uns[PCs]
-
-    if type(mean) is str:
-        if mean not in adata.uns.keys():
-            raise Exception(f"The key {mean} is not in `.uns`.")
-        mean = adata.uns[mean]
+    if basis is not None:
+        temp_key = "obs_vf_angle_" + basis
 
-    if not callable(vecfld):
-        vf_func = vecfld.func
+        logger.info_insert_adata(temp_key, adata_attr="obs")
+        adata.obs[temp_key] = cell_angles
     else:
-        vf_func = vecfld
+        temp_key = "obs_vf_angle"
+        logger.info_insert_adata(temp_key, adata_attr="obs")
+        adata.obs[temp_key] = cell_angles
+
+    logger.finish_progress("VectorField")
+    if return_vf_object:
+        return VecFld
+    elif copy:
+        return adata
+    return None
+
+
+def assign_fixedpoints(
+    adata: AnnData,
+    basis: str = "pca",
+    cores: int = 1,
+    copy: bool = False,
+) -> Optional[AnnData]:
+    """Assign each cell in our data to a fixed point.
+
+    Args:
+        adata: AnnData object that contains reconstructed vector field in the `basis` space.
+        basis: The vector field function for the `basis` that will be used to assign fixed points for each cell.
+        cores: Number of cores to run the fixed-point search for each cell.
+        copy: Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments and
+            returning `None`.
+
+    Returns:
+        adata: :class:`Union[None, anndata.AnnData]`
+            If `copy` is set to False, return None but the adata object will updated with a `fps_assignment` in .obs as
+            well as the `'fps_assignment_' + basis` in the .uns.
+            If `copy` is set to True, a deep copy of the original `adata` object is returned.
+    """
+    logger = LoggerManager.gen_logger("dynamo-assign_fixedpoints")
+    logger.info("assign_fixedpoints begins...", indent_level=1)
+    logger.log_time()
+    adata = copy_adata(adata) if copy else adata
 
-    """def vf_func_perturb(x):
-        x_gene = np.abs(x @ PCs.T + mean)
-        v_gene = vector_transformation(vf_func(x), PCs)
-        v_gene = v_gene - k * x_gene
-        return v_gene @ PCs"""
-
-    vf = KOVectorField(K=k, func_base=vf_func, fjac_base=vecfld.get_Jacobian(), PCs=PCs, mean=mean)
-    if not callable(vecfld):
-        vf.data["X"] = vecfld.data["X"]
-        vf.data["V"] = vf.func(vf.data["X"])
-    if return_vector_field_class:
-        # vf = ko_vectorfield(K=k, func_base=vf_func, fjac_base=vecfld.get_Jacobian(), Q=PCs, mean=mean)
-        # vf.func = vf_func_perturb
-        return vf
-    else:
-        return vf.func
+    VecFld, func = vecfld_from_adata(adata, basis=basis)
 
+    vecfld_class = BaseVectorField(
+        X=VecFld["X"],
+        V=VecFld["Y"],
+        func=func,
+    )
 
-class BifurcationTwoGenesVectorField(DifferentiableVectorField):
-    def __init__(self, param_dict, X=None, V=None, Grid=None, *args, **kwargs):
-        super().__init__(X, V, Grid, *args, **kwargs)
-        param_dict_ = param_dict.copy()
-        for k in param_dict_.keys():
-            if k not in ["a", "b", "S", "K", "m", "n", "gamma"]:
-                del param_dict_[k]
-                main_warning(f"The parameter {k} is not used for the vector field.")
-        self.vf_dict["params"] = param_dict_
-        self.func = lambda x: ode_bifur2genes(x, **param_dict_)
-
-    def get_Jacobian(self, method=None):
-        return lambda x: jacobian_bifur2genes(x, **self.vf_dict["params"])
-
-    def find_fixed_points(self, n_x0=10, **kwargs):
-        a = self.vf_dict["params"]["a"]
-        b = self.vf_dict["params"]["b"]
-        gamma = self.vf_dict["params"]["gamma"]
-        xss = (a + b) / gamma
-        margin = 10
-        domain = np.array([[0, xss[0] + margin], [0, xss[1] + margin]])
-        return super().find_fixed_points(n_x0, X0=None, domain=domain, **kwargs)
+    (
+        X,
+        valid_fps_type_assignment,
+        assignment_id,
+    ) = vecfld_class.assign_fixed_points(cores=cores)
+    assignment_id = [str(int(i)) if np.isfinite(i) else None for i in assignment_id]
+    adata.obs["fps_assignment"] = assignment_id
+    adata.uns["fps_assignment_" + basis] = {
+        "X": X,
+        "valid_fps_type_assignment": valid_fps_type_assignment,
+        "assignment_id": assignment_id,
+    }
 
-    # TODO: nullcline calculation
+    logger.finish_progress("assign_fixedpoints")
+    if copy:
+        return adata
+    return None
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/stochastic_process.py` & `dynamo-release-1.3.0/dynamo/vectorfield/stochastic_process.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,59 +1,49 @@
+from typing import List, Optional
+
 import numpy as np
+from anndata import AnnData
 from sklearn.neighbors import NearestNeighbors
 from tqdm import tqdm
 
 from ..tools.connectivity import _gen_neighbor_keys, check_and_recompute_neighbors
 from ..tools.utils import log1p_
-from .utils import vecfld_from_adata, vector_field_function
+from .utils import VecFldDict, vecfld_from_adata, vector_field_function
 
 
 def diffusionMatrix(
-    adata,
-    X_data=None,
-    V_data=None,
-    genes=None,
-    layer=None,
-    basis="umap",
-    dims=None,
-    n=30,
-    VecFld=None,
-    residual="vector_field",
-):
+    adata: AnnData,
+    X_data: Optional[np.ndarray] = None,
+    V_data: Optional[np.ndarray] = None,
+    genes: Optional[List] = None,
+    layer: Optional[str] = None,
+    basis: str = "umap",
+    dims: Optional[List] = None,
+    n: int = 30,
+    VecFld: Optional[VecFldDict] = None,
+    residual: str = "vector_field",
+) -> AnnData:
     """Calculate the diffusion matrix from the estimated velocity vector and the reconstructed vector field.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        X_data: `np.ndarray` (default: `None`)
-            The user supplied expression (embedding) data that will be used for calculating diffusion matrix directly.
-        V_data: `np.ndarray` (default: `None`)
-            The user supplied velocity data that will be used for calculating diffusion matrix directly.
-        genes: `list` or None (default: `None`)
-            The list of genes that will be used to subset the data. If `None`, all genes will be used.
-        layer: `str` or None (default: None)
-            Which layer of the data will be used for diffusion matrix calculation.
-        basis: `str` (default: `umap`)
-            Which basis of the data will be used for diffusion matrix calculation.
-        dims: `list` or None (default: `None`)
-            The list of dimensions that will be selected for diffusion matrix calculation. If `None`, all dimensions will be used.
-        n: `int` (default: `10`)
-            Number of nearest neighbors when the nearest neighbor graph is not included.
-        VecFld: `dictionary` or None (default: None)
-            The reconstructed vector field function.
-        residual: `str` or None (default: `vector_field`)
-            Method to calculate residual velocity vectors for diffusion matrix calculation. If `average`, all velocity
+    Args:
+        adata: an Annodata object.
+        X_data: The user supplied expression (embedding) data that will be used for calculating diffusion matrix directly.
+        V_data: The user supplied velocity data that will be used for calculating diffusion matrix directly.
+        genes: The list of genes that will be used to subset the data. If `None`, all genes will be used.
+        layer: Which layer of the data will be used for diffusion matrix calculation.
+        basis: Which basis of the data will be used for diffusion matrix calculation.
+        dims: The list of dimensions that will be selected for diffusion matrix calculation. If `None`, all dimensions will be used.
+        n: Number of nearest neighbors when the nearest neighbor graph is not included.
+        VecFld: The reconstructed vector field function.
+        residual: Method to calculate residual velocity vectors for diffusion matrix calculation. If `average`, all velocity
             of the nearest neighbor cells will be minused by its average velocity; if `vector_field`, all velocity will be
             minused by the predicted velocity from the reconstructed deterministic velocity vector field.
 
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            `AnnData` object that is updated with the `diffusion_matrix` key in the `uns` attribute which is a list of
+    Returns:
+        adata: `AnnData` object that is updated with the `diffusion_matrix` key in the `uns` attribute which is a list of
             the diffusion matrix for each cell. A column `diffusion` corresponds to the square root of the sum of all
             elements for each cell's diffusion matrix will also be added.
     """
 
     if X_data is None or V_data is not None:
         if genes is not None:
             genes = adata.var_name.intersection(genes).to_list()
@@ -196,26 +186,23 @@
         val[i] = np.sqrt(sum(sum(d)))
         dmatrix[i] = d
 
     adata.obs["diffusion"] = val
     adata.uns["diffusion_matrix"] = dmatrix
 
 
-def diffusionMatrix2D(V_mat):
+def diffusionMatrix2D(V_mat: np.ndarray) -> np.ndarray:
     """Function to calculate cell-specific diffusion matrix for based on velocity vectors of neighbors.
 
     This function works for two dimension. See :func:`diffusionMatrix` for generalization to arbitrary dimensions.
 
-    Parameters
-    ----------
-        V_mat: `np.ndarray`
-            velocity vectors of neighbors
+    Args:
+        V_mat: velocity vectors of neighbors
 
-    Returns
-    -------
+    Returns:
         Return the cell-specific diffusion matrix
 
     See also:: :func:`diffusionMatrix`
     """
 
     D = np.zeros((V_mat.shape[0], 2, 2))
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/topography.py` & `dynamo-release-1.3.0/dynamo/preprocessing/gene_selection.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1050 +1,884 @@
-# create by Yan Zhang, minor adjusted by Xiaojie Qiu
-import datetime
-import os
 import warnings
-from typing import Union
+from typing import Dict, List, Optional, Tuple, Union
+
+from numpy import ndarray
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
 
 import anndata
 import numpy as np
-import scipy.sparse as sp
-from scipy.integrate import odeint
-from scipy.linalg import eig
-from scipy.optimize import fsolve
-from sklearn.neighbors import NearestNeighbors
-
-from ..dynamo_logger import LoggerManager, main_info, main_warning
-from ..tools.utils import gaussian_1d, inverse_norm, nearest_neighbors, update_dict
-from ..utils import copy_adata
-from .FixedPoints import FixedPoints
-from .scVectorField import BaseVectorField, SvcVectorField
+import pandas as pd
+import scipy.sparse
+from anndata import AnnData
+from scipy.sparse import csr_matrix, issparse
+
+from ..configuration import DKM
+from ..dynamo_logger import (
+    LoggerManager,
+    main_critical,
+    main_debug,
+    main_info,
+    main_info_insert_adata_uns,
+    main_info_insert_adata_var,
+    main_warning,
+)
+from .pca import pca
 from .utils import (
-    angle,
-    dynode_vector_field_function,
-    find_fixed_points,
-    is_outside,
-    remove_redundant_points,
-    vecfld_from_adata,
-    vector_field_function,
+    compute_gene_exp_fraction,
+    get_gene_selection_filter,
+    get_nan_or_inf_data_bool_mask,
+    get_svr_filter,
+    merge_adata_attrs,
+    seurat_get_mean_var,
 )
-from .vector_calculus import curl, divergence
-
-
-def pac_onestep(x0, func, v0, ds=0.01):
-    x01 = x0 + v0 * ds
-
-    def F(x):
-        return np.array([func(x), (x - x0).dot(v0) - ds])
-
-    x1 = fsolve(F, x01)
-    return x1
 
 
-def continuation(x0, func, s_max, ds=0.01, v0=None, param_axis=0, param_direction=1):
-    ret = [x0]
-    if v0 is None:  # initialize tangent predictor
-        v = np.zeros_like(x0)
-        v[param_axis] = param_direction
-    else:
-        v = v0
-    s = 0
-    while s <= s_max:
-        x1 = ret[-1]
-        x = pac_onestep(x1, func, v, ds)
-        ret.append(x)
-        s += ds
-
-        # compute tangent predictor
-        v = x - x1
-        v /= np.linalg.norm(v)
-    return np.array(ret)
-
-
-def clip_curves(curves, domain, tol_discont=None):
-    ret = []
-    for cur in curves:
-        clip_away = np.zeros(len(cur), dtype=bool)
-        for i, p in enumerate(cur):
-            for j in range(len(domain)):
-                if p[j] < domain[j][0] or p[j] > domain[j][1]:
-                    clip_away[i] = True
-                    break
-            if tol_discont is not None and i > 0:
-                d = np.linalg.norm(p - cur[i - 1])
-                if d > tol_discont:
-                    clip_away[i] = True
-        # clip curve and assemble
-        i_start = 0
-        while i_start < len(cur) - 1:
-            if not clip_away[i_start]:
-                for i_end in range(i_start, len(cur)):
-                    if clip_away[i_end]:
-                        break
-                # a tiny bit of the end could be chopped off
-                ret.append(cur[i_start:i_end])
-                i_start = i_end
-            else:
-                i_start += 1
-    return ret
-
-
-def compute_nullclines_2d(X0, fdx, fdy, x_range, y_range, s_max=None, ds=None):
-    if s_max is None:
-        s_max = 5 * ((x_range[1] - x_range[0]) + (y_range[1] - y_range[0]))
-    if ds is None:
-        ds = s_max / 1e3
-
-    NCx = []
-    NCy = []
-    for x0 in X0:
-        # initialize tangent predictor
-        theta = np.random.rand() * 2 * np.pi
-        v0 = [np.cos(theta), np.sin(theta)]
-        v0 /= np.linalg.norm(v0)
-        # nullcline continuation
-        NCx.append(continuation(x0, fdx, s_max, ds, v0=v0))
-        NCx.append(continuation(x0, fdx, s_max, ds, v0=-v0))
-        NCy.append(continuation(x0, fdy, s_max, ds, v0=v0))
-        NCy.append(continuation(x0, fdy, s_max, ds, v0=-v0))
-    NCx = clip_curves(NCx, [x_range, y_range], ds * 10)
-    NCy = clip_curves(NCy, [x_range, y_range], ds * 10)
-    return NCx, NCy
-
-
-def compute_separatrices(Xss, Js, func, x_range, y_range, t=50, n_sample=500, eps=1e-6):
-    ret = []
-    for i, x in enumerate(Xss):
-        print(x)
-        J = Js[i]
-        w, v = eig(J)
-        I_stable = np.where(np.real(w) < 0)[0]
-        print(I_stable)
-        for j in I_stable:  # I_unstable
-            u = np.real(v[j])
-            u = u / np.linalg.norm(u)
-            print("u=%f, %f" % (u[0], u[1]))
-
-            # Parameters for building separatrix
-            T = np.linspace(0, t, n_sample)
-            # all_sep_a, all_sep_b = None, None
-            # Build upper right branch of separatrix
-            ab_upper = odeint(lambda x, _: -func(x), x + eps * u, T)
-            # Build lower left branch of separatrix
-            ab_lower = odeint(lambda x, _: -func(x), x - eps * u, T)
-
-            sep = np.vstack((ab_lower[::-1], ab_upper))
-            ret.append(sep)
-    ret = clip_curves(ret, [x_range, y_range])
-    return ret
-
-
-def set_test_points_on_curve(curve, interval):
-    P = [curve[0]]
-    dist = 0
-    for i in range(1, len(curve)):
-        dist += np.linalg.norm(curve[i] - curve[i - 1])
-        if dist >= interval:
-            P.append(curve[i])
-            dist = 0
-    return np.array(P)
-
-
-def find_intersection_2d(curve1, curve2, tol_redundant=1e-4):
-    P = []
-    for i in range(len(curve1) - 1):
-        for j in range(len(curve2) - 1):
-            p1 = curve1[i]
-            p2 = curve1[i + 1]
-            p3 = curve2[j]
-            p4 = curve2[j + 1]
-            denom = np.linalg.det([p1 - p2, p3 - p4])
-            if denom != 0:
-                t = np.linalg.det([p1 - p3, p3 - p4]) / denom
-                u = -np.linalg.det([p1 - p2, p1 - p3]) / denom
-                if t >= 0 and t <= 1 and u >= 0 and u <= 1:
-                    P.append(p1 + t * (p2 - p1))
-    if tol_redundant is not None:
-        remove_redundant_points(P, tol=tol_redundant)
-    return np.array(P)
-
-
-def find_fixed_points_nullcline(func, NCx, NCy, sample_interval=0.5, tol_redundant=1e-4):
-    test_Px = []
-    for i in range(len(NCx)):
-        test_Px.append(set_test_points_on_curve(NCx[i], sample_interval))
-
-    test_Py = []
-    for i in range(len(NCy)):
-        test_Py.append(set_test_points_on_curve(NCy[i], sample_interval))
-
-    int_P = []
-    for i in range(len(test_Px)):
-        for j in range(len(test_Py)):
-            p = find_intersection_2d(test_Px[i], test_Py[j], tol_redundant)
-            for k in range(len(p)):
-                int_P.append(p[k])
-    int_P = np.array(int_P)
-    P, J, _ = find_fixed_points(int_P, func, tol_redundant=tol_redundant)
-    return P, J
-
-
-def calc_fft(x):
-    out = np.fft.rfft(x)
-    n = len(x)
-    xFFT = abs(out) / n * 2
-    freq = np.arange(int(n / 2)) / n
-    return xFFT[: int(n / 2)], freq
-
-
-def dup_osc_idx(x, n_dom=3, tol=0.05):
-    l_int = int(np.floor(len(x) / n_dom))
-    ind_a, ind_b = np.arange((n_dom - 2) * l_int, (n_dom - 1) * l_int), np.arange((n_dom - 1) * l_int, n_dom * l_int)
-    y1 = x[ind_a]
-    y2 = x[ind_b]
-
-    def calc_fft_k(x):
-        ret = []
-        for k in range(x.shape[1]):
-            xFFT, _ = calc_fft(x[:, k])
-            ret.append(xFFT[1:])
-        return np.hstack(ret)
-
-    try:
-        xFFt1 = calc_fft_k(y1)
-        xFFt2 = calc_fft_k(y2)
-    except ValueError:
-        print("calc_fft_k run failed...")
-        return None, None
-
-    diff = np.linalg.norm(xFFt1 - xFFt2) / len(xFFt1)
-    if diff <= tol:
-        idx = (n_dom - 1) * l_int
-    else:
-        idx = None
-    return idx, diff
-
+def calc_Gini(adata: AnnData, layers: Union[Literal["all"], List[str]] = "all") -> AnnData:
+    """Calculate the Gini coefficient of a numpy array.
+    https://github.com/thomasmaxwellnorman/perturbseq_demo/blob/master/perturbseq/util.py
+
+    Args:
+        adata: an AnnData object
+        layers: the layer(s) to be normalized. Defaults to "all".
+
+    Returns:
+        An updated anndata object with gini score for the layers (include .X) in the corresponding var columns
+        (layer + '_gini').
+    """
 
-def dup_osc_idx_iter(x, max_iter=5, **kwargs):
-    stop = False
-    idx = len(x)
-    j = 0
-    D = []
-    while not stop:
-        i, d = dup_osc_idx(x[:idx], **kwargs)
-        D.append(d)
-        if i is None:
-            stop = True
-        else:
-            idx = i
-        j += 1
-        if j >= max_iter or idx == 0:
-            stop = True
-    D = np.array(D)
-    return idx, D
-
-
-# TODO: This should be inherited from the BaseVectorField/DifferentiatiableVectorField class,
-#       and BifurcationTwoGenes should be inherited from this class.
-class VectorField2D:
-    def __init__(self, func, func_vx=None, func_vy=None, X_data=None):
-        self.func = func
-
-        def func_dim(x, func, dim):
-            y = func(x)
-            if y.ndim == 1:
-                y = y[dim]
+    # From: https://github.com/oliviaguest/gini
+    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif
+    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm
+
+    layers = DKM.get_available_layer_keys(adata, layers)
+
+    def _compute_gini(CM):
+        # convert to dense array if sparse
+        if issparse(CM):
+            CM = CM.A
+
+        # shift all values to be non-negative
+        CM -= np.min(CM)
+
+        # add small constant to avoid zeros
+        CM = CM.astype(float) + 0.0000001  # values cannot be 0
+
+        # sort values along axis 0
+        CM = np.sort(CM, axis=0)
+
+        # compute index array
+        n = CM.shape[0]
+        index = 2 * (np.arange(1, n + 1)) - n - 1
+
+        # compute Gini coefficient for each feature
+        gini = (np.sum(index[:, np.newaxis] * CM, axis=0)) / (n * np.sum(CM, axis=0))
+
+        return gini
+
+    for layer in layers:
+        if layer == "raw":
+            CM = adata.raw.X
+        elif layer == "X":
+            CM = adata.X
+        elif layer == "protein":
+            if "protein" in adata.obsm_keys():
+                CM = adata.obsm[layer]
             else:
-                y = y[:, dim].flatten()
-            return y
-
-        if func_vx is None:
-            self.fx = lambda x: func_dim(x, self.func, 0)
-        else:
-            self.fx = func_vx
-        if func_vy is None:
-            self.fy = lambda x: func_dim(x, self.func, 1)
-        else:
-            self.fy = func_vy
-        self.Xss = FixedPoints()
-        self.X_data = X_data
-        self.NCx = None
-        self.NCy = None
-
-    def get_num_fixed_points(self):
-        return len(self.Xss.get_X())
-
-    def get_fixed_points(self, get_types=True):
-        X = self.Xss.get_X()
-        if not get_types:
-            return X
+                continue
         else:
-            is_saddle, is_stable = self.Xss.is_saddle()
-            # -1 -- stable, 0 -- saddle, 1 -- unstable
-            ftype = np.ones(len(X))
-            for i in range(len(ftype)):
-                if is_saddle[i]:
-                    ftype[i] = 0
-                elif is_stable[i]:
-                    ftype[i] = -1
-            return X, ftype
-
-    def get_Xss_confidence(self, k=50):
-        X = self.X_data
-        X = X.A if sp.issparse(X) else X
-        Xss = self.Xss.get_X()
-        Xref = np.median(X, 0)
-        Xss = np.vstack((Xss, Xref))
-
-        if X.shape[0] > 200000 and X.shape[1] > 2:
-            from pynndescent import NNDescent
-
-            nbrs = NNDescent(
-                X,
-                metric="euclidean",
-                n_neighbors=min(k, X.shape[0] - 1),
-                n_jobs=-1,
-                random_state=19491001,
-            )
-            _, dist = nbrs.query(Xss, k=min(k, X.shape[0] - 1))
-        else:
-            alg = "ball_tree" if X.shape[1] > 10 else "kd_tree"
-            nbrs = NearestNeighbors(n_neighbors=min(k, X.shape[0] - 1), algorithm=alg, n_jobs=-1).fit(X)
-            dist, _ = nbrs.kneighbors(Xss)
-
-        dist_m = dist.mean(1)
-        # confidence = 1 - dist_m / dist_m.max()
-        sigma = 0.1 * 0.5 * (np.max(X[:, 0]) - np.min(X[:, 0]) + np.max(X[:, 1]) - np.min(X[:, 1]))
-        confidence = gaussian_1d(dist_m, sigma=sigma)
-        confidence /= np.max(confidence)
-        return confidence[:-1]
-
-    def find_fixed_points_by_sampling(self, n, x_range, y_range, lhs=True, tol_redundant=1e-4):
-        if lhs:
-            from ..tools.sampling import lhsclassic
+            CM = adata.layers[layer]
 
-            X0 = lhsclassic(n, 2)
-        else:
-            X0 = np.random.rand(n, 2)
-        X0[:, 0] = X0[:, 0] * (x_range[1] - x_range[0]) + x_range[0]
-        X0[:, 1] = X0[:, 1] * (y_range[1] - y_range[0]) + y_range[0]
-        X, J, _ = find_fixed_points(
-            X0,
-            self.func,
-            domain=[x_range, y_range],
-            tol_redundant=tol_redundant,
-        )
-        if len(X) > 0:
-            self.Xss.add_fixed_points(X, J, tol_redundant)
+        var_gini = _compute_gini(CM)
+        adata.var[layer + "_gini"] = var_gini
 
-    def find_nearest_fixed_point(self, x, x_range, y_range, tol_redundant=1e-4):
-        X, J, _ = find_fixed_points(x, self.func, domain=[x_range, y_range], tol_redundant=tol_redundant)
-        if len(X) > 0:
-            self.Xss.add_fixed_points(X, J, tol_redundant)
-
-    def compute_nullclines(self, x_range, y_range, find_new_fixed_points=False, tol_redundant=1e-4):
-        # compute arguments
-        s_max = 5 * ((x_range[1] - x_range[0]) + (y_range[1] - y_range[0]))
-        ds = s_max / 1e3
-        self.NCx, self.NCy = compute_nullclines_2d(
-            self.Xss.get_X(),
-            self.fx,
-            self.fy,
-            x_range,
-            y_range,
-            s_max=s_max,
-            ds=ds,
-        )
-        if find_new_fixed_points:
-            sample_interval = ds * 10
-            X, J = find_fixed_points_nullcline(self.func, self.NCx, self.NCy, sample_interval, tol_redundant)
-            outside = is_outside(X, [x_range, y_range])
-            self.Xss.add_fixed_points(X[~outside], J[~outside], tol_redundant)
-
-    def output_to_dict(self, dict_vf):
-        dict_vf["NCx"] = self.NCx
-        dict_vf["NCy"] = self.NCy
-        dict_vf["Xss"] = self.Xss.get_X()
-        dict_vf["confidence"] = self.get_Xss_confidence()
-        dict_vf["J"] = self.Xss.get_J()
-        return dict_vf
-
-
-def util_topology(adata, basis, X, dims, func, VecFld, n=25, **kwargs):
-    X_basis = adata.obsm["X_" + basis][:, dims] if X is None else X[:, dims]
-
-    if X_basis.shape[1] == 2:
-        fp_ind = None
-        min_, max_ = X_basis.min(0), X_basis.max(0)
-
-        xlim = [
-            min_[0] - (max_[0] - min_[0]) * 0.1,
-            max_[0] + (max_[0] - min_[0]) * 0.1,
-        ]
-        ylim = [
-            min_[1] - (max_[1] - min_[1]) * 0.1,
-            max_[1] + (max_[1] - min_[1]) * 0.1,
-        ]
-
-        vecfld = VectorField2D(func, X_data=X_basis)
-        vecfld.find_fixed_points_by_sampling(n, xlim, ylim)
-        if vecfld.get_num_fixed_points() > 0:
-            vecfld.compute_nullclines(xlim, ylim, find_new_fixed_points=True)
-            NCx, NCy = vecfld.NCx, vecfld.NCy
+    return adata
 
-        Xss, ftype = vecfld.get_fixed_points(get_types=True)
-        confidence = vecfld.get_Xss_confidence()
-    else:
-        fp_ind = None
-        xlim, ylim, confidence, NCx, NCy = None, None, None, None, None
-        vecfld = BaseVectorField(
-            X=VecFld["X"][VecFld["valid_ind"], :],
-            V=VecFld["Y"][VecFld["valid_ind"], :],
-            func=func,
-        )
 
-        Xss, ftype = vecfld.get_fixed_points(n_x0=n, **kwargs)
-        if Xss.ndim > 1 and Xss.shape[1] > 2:
-            fp_ind = nearest_neighbors(Xss, vecfld.data["X"], 1).flatten()
-            Xss = vecfld.data["X"][fp_ind]
-
-    return X_basis, xlim, ylim, confidence, NCx, NCy, Xss, ftype, fp_ind
-
-
-def topography(
-    adata,
-    basis="umap",
-    layer=None,
-    X=None,
-    dims=None,
-    n=25,
-    VecFld=None,
-    **kwargs,
+def select_genes_monocle(
+    adata: AnnData,
+    layer: str = DKM.X_LAYER,
+    keep_filtered: bool = True,
+    n_top_genes: int = 2000,
+    sort_by: Literal["gini", "cv_dispersion", "fano_dispersion"] = "cv_dispersion",
+    exprs_frac_for_gene_exclusion: float = 1,
+    genes_to_exclude: Union[List[str], None] = None,
+    SVRs_kwargs: dict = {},
 ):
-    """Map the topography of the single cell vector field in (first) two dimensions.
+    """Select genes based on monocle recipe.
 
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            an Annodata object.
-        basis: `str` (default: `trimap`)
-            The reduced dimension embedding of cells to visualize.
-        layer: `str` or None (default: None)
-            Which layer of the data will be used for vector field function reconstruction. This will be used in
-            conjunction with X.
-        X: 'np.ndarray' (dimension: n_obs x n_features)
-                Original data. Not used
-        dims: `list` or `None` (default: `None`)
-            The dimensions that will be used for vector field reconstruction.
-        n: `int` (default: `10`)
-            Number of samples for calculating the fixed points.
-        VecFld: `dictionary` or None (default: None)
-            The reconstructed vector field function.
-        kwargs:
-            Key word arguments passed to the find_fixed_point function of the vector field class for high dimension
-            fixed point identification.
-
-    Returns
-    -------
-        adata: :class:`~anndata.AnnData`
-            `AnnData` object that is updated with the `VecFld` or 'VecFld_' + basis dictionary in the `uns` attribute.
-            The `VecFld2D` key stores an instance of the VectorField2D class which presumably has fixed points,
-            nullcline, separatrix, computed and stored.
-    """
+    This version is here for modularization of preprocessing, so that users may try combinations of different
+    preprocessing procedures in Preprocessor.
 
-    if VecFld is None:
-        VecFld, func = vecfld_from_adata(adata, basis)
-    else:
-        if "velocity_loss_traj" in VecFld.keys():
+    Args:
+        adata: an AnnData object.
+        layer: The data from a particular layer (include X) used for feature selection. Defaults to "X".
+        keep_filtered: Whether to keep genes that don't pass the filtering in the adata object. Defaults to True.
+        n_top_genes: the number of top genes based on scoring method (specified by sort_by) will be selected as feature
+            genes. Defaults to 2000.
+        sort_by: the sorting methods to be used to select genes. Should be one of the gini index or
+            dispersion of coefficient variation or fano. Defaults to cv_dispersion.
+        exprs_frac_for_gene_exclusion: threshold of fractions for high fraction genes. Defaults to 1.
+        genes_to_exclude: genes that are excluded from evaluation. Defaults to None.
+        SVRs_kwargs: kwargs for `SVRs`. Defaults to {}.
 
-            def func(x):
-                return dynode_vector_field_function(x, VecFld)
+    Raises:
+        NotImplementedError: the 'sort_by' algorithm is invalid/unsupported.
+    """
+
+    filter_bool = (
+        adata.var["pass_basic_filter"]
+        if "pass_basic_filter" in adata.var.columns
+        else np.ones(adata.shape[1], dtype=bool)
+    )
 
+    if adata.shape[1] <= n_top_genes:
+        filter_bool = np.ones(adata.shape[1], dtype=bool)
+    else:
+        if sort_by == "gini":
+            if layer + "_gini" is not adata.var.keys():
+                calc_Gini(adata)
+            filter_bool = get_gene_selection_filter(
+                adata.var[layer + "_gini"][filter_bool], n_top_genes=n_top_genes, basic_filter=filter_bool
+            )
+        elif sort_by == "cv_dispersion" or sort_by == "fano_dispersion":
+            if not any("velocyto_SVR" in key for key in adata.uns.keys()):
+                calc_dispersion_by_svr(
+                    adata,
+                    layers=layer,
+                    filter_bool=filter_bool,
+                    algorithm=sort_by,
+                    **SVRs_kwargs,
+                )
+            filter_bool = get_svr_filter(adata, layer=layer, n_top_genes=n_top_genes, return_adata=False)
         else:
+            raise NotImplementedError(f"The algorithm {sort_by} is invalid/unsupported")
 
-            def func(x):
-                return vector_field_function(x, VecFld)
+    invalid_ids = []
+    # filter genes by gene expression fraction as well
+    if "frac" not in adata.var.keys():
+        adata.var["frac"], invalid_ids = compute_gene_exp_fraction(X=adata.X, threshold=exprs_frac_for_gene_exclusion)
+
+    genes_to_exclude = (
+        list(adata.var_names[invalid_ids])
+        if genes_to_exclude is None
+        else genes_to_exclude + list(adata.var_names[invalid_ids])
+    )
+    if genes_to_exclude is not None and len(genes_to_exclude) > 0:
+        adata_exclude_genes = adata.var.index.intersection(genes_to_exclude)
+        adata.var.loc[adata_exclude_genes, "use_for_pca"] = False
+
+    if keep_filtered:
+        adata.var["use_for_pca"] = filter_bool
+    else:
+        adata._inplace_subset_var(filter_bool)
+        adata.var["use_for_pca"] = True
+
+    adata.uns["feature_selection"] = sort_by
+
+
+def calc_dispersion_by_svr(
+    adata_ori: AnnData,
+    filter_bool: Union[np.ndarray, None] = None,
+    layers: str = "X",
+    algorithm: Literal["cv_dispersion", "fano_dispersion"] = "cv_dispersion",
+    use_all_genes_cells: bool = False,
+    **SVRs_kwargs,
+) -> AnnData:
+    """Support Vector Regression to identify highly variable genes.
+
+    This function is modified from https://github.com/velocyto-team/velocyto.py/blob/master/velocyto/analysis.py
+
+    Args:
+        adata_ori: an AnnData object
+        filter_bool: A boolean array from the user to select genes for downstream analysis. Defaults to None.
+        layers: The layer(s) to be used for calculating dispersion score via support vector regression (SVR). Defaults
+            to "X".
+        algorithm: Method of calculating mean and coefficient of variation, either "cv_dispersion" or "fano_dispersion"
+        sort_inverse: whether to sort genes from less noisy to more noisy (to use for size estimation not for feature
+            selection). Defaults to False.
+        use_all_genes_cells: A logic flag to determine whether all cells and genes should be used for the size factor
+            calculation. Defaults to False.
+
+    Returns:
+        An updated annData object with `log_m`, `log_cv`, `score` added to .obs columns and `SVR` added to uns attribute
+        as a new key.
+    """
 
-    if dims is None:
-        dims = np.arange(adata.obsm["X_" + basis].shape[1])
+    layers = DKM.get_available_layer_keys(adata_ori, layers)
+    winsorize = SVRs_kwargs.get("winsorize", False)
+    winsor_perc = SVRs_kwargs.get("winsor_perc", (1, 99.5))
+    svr_gamma = SVRs_kwargs.pop("svr_gamma", None)
+    sort_inverse = SVRs_kwargs.pop("sort_inverse", False)
 
-    (
-        X_basis,
-        xlim,
-        ylim,
-        confidence,
-        NCx,
-        NCy,
-        Xss,
-        ftype,
-        fp_ind,
-    ) = util_topology(adata, basis, X, dims, func, VecFld, n=n, *kwargs)
+    if use_all_genes_cells:
+        # let us ignore the `inplace` parameter in pandas.Categorical.remove_unused_categories  warning.
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            adata = adata_ori[:, filter_bool].copy() if filter_bool is not None else adata_ori
+    else:
+        cell_inds = adata_ori.obs.use_for_pca if "use_for_pca" in adata_ori.obs.columns else adata_ori.obs.index
+        filter_list = ["use_for_pca", "pass_basic_filter"]
+        filter_checker = [i in adata_ori.var.columns for i in filter_list]
+        which_filter = np.where(filter_checker)[0]
 
-    # commented for now, will go back to this later.
-    # sep = compute_separatrices(vecfld.Xss.get_X(), vecfld.Xss.get_J(), vecfld.func, xlim, ylim)
+        gene_inds = adata_ori.var[filter_list[which_filter[0]]] if len(which_filter) > 0 else adata_ori.var.index
 
-    if layer is None:
-        vf_key = "VecFld_" + basis
-    else:
-        vf_key = "VecFld" if layer == "X" else "VecFld_" + layer
+        # let us ignore the `inplace` parameter in pandas.Categorical.remove_unused_categories  warning.
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            adata = adata_ori[cell_inds, gene_inds].copy()
 
-    if vf_key in adata.uns_keys():
-        adata.uns[vf_key].update(
-            {
-                "xlim": xlim,
-                "ylim": ylim,
-                "X_data": X_basis,
-                "Xss": Xss,
-                "ftype": ftype,
-                "confidence": confidence,
-                "nullcline": [NCx, NCy],
-                "separatrices": None,
-                "fp_ind": fp_ind,
-            }
+    for layer in layers:
+        valid_CM, detected_bool = get_vaild_CM(adata, layer, **SVRs_kwargs)
+        if valid_CM is None:
+            continue
+
+        mean, cv = get_mean_cv(adata, valid_CM, algorithm, winsorize, winsor_perc)
+        fitted_fun, svr_gamma = get_prediction_by_svr(mean, cv, svr_gamma)
+        score = cv - fitted_fun(mean)
+        if sort_inverse:
+            score = -score
+
+        # Now we can get "SVR" from get_prediction_by_svr
+        key = "velocyto_SVR" if layer == "raw" or layer == "X" else layer + "_velocyto_SVR"
+        adata_ori.uns[key] = {"mean": mean, "cv": cv, "svr_gamma": svr_gamma}
+
+        prefix = "" if layer == "X" else layer + "_"
+        (adata.var[prefix + "log_m"], adata.var[prefix + "log_cv"], adata.var[prefix + "score"],) = (
+            np.nan,
+            np.nan,
+            -np.inf,
+        )
+        (
+            adata.var.loc[detected_bool, prefix + "log_m"],
+            adata.var.loc[detected_bool, prefix + "log_cv"],
+            adata.var.loc[detected_bool, prefix + "score"],
+        ) = (
+            np.array(mean).flatten(),
+            np.array(cv).flatten(),
+            np.array(score).flatten(),
         )
-    else:
-        adata.uns[vf_key] = {
-            "xlim": xlim,
-            "ylim": ylim,
-            "X_data": X_basis,
-            "Xss": Xss,
-            "ftype": ftype,
-            "confidence": confidence,
-            "nullcline": [NCx, NCy],
-            "separatrices": None,
-            "fp_ind": fp_ind,
-        }
 
-    return adata
+    adata_ori = merge_adata_attrs(adata_ori, adata, attr="var")
 
+    return adata_ori
+
+
+def get_vaild_CM(
+    adata: AnnData,
+    layer: str = "X",
+    relative_expr: bool = True,
+    total_szfactor: str = "total_Size_Factor",
+    min_expr_cells: int = 0,
+    min_expr_avg: int = 0,
+    max_expr_avg: int = np.inf,
+    winsorize: bool = False,
+    winsor_perc: Tuple[float, float] = (1, 99.5),
+):
+    """Find a valid CM that is the data of the layer corresponding to the size factor.
 
-def VectorField(
-    adata: anndata.AnnData,
-    basis: Union[None, str] = None,
-    layer: Union[None, str] = None,
-    dims: Union[int, list, None] = None,
-    genes: Union[list, None] = None,
-    normalize: bool = False,
-    grid_velocity: bool = False,
-    grid_num: int = 50,
-    velocity_key: str = "velocity_S",
-    method: str = "SparseVFC",
-    min_vel_corr: float = 0.6,
-    restart_num: int = 5,
-    restart_seed: Union[None, list] = [0, 100, 200, 300, 400],
-    model_buffer_path: Union[str, None] = None,
-    return_vf_object: bool = False,
-    map_topography: bool = False,
-    pot_curl_div: bool = False,
-    cores: int = 1,
-    result_key: Union[str, None] = None,
-    copy: bool = False,
-    **kwargs,
-) -> Union[anndata.AnnData, BaseVectorField]:
-    """Learn a function of high dimensional vector field from sparse single cell samples in the entire space robustly.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains embedding and velocity data
-        basis:
-            The embedding data to use. The vector field function will be learned on the low dimensional embedding and
-            can be then projected back to the high dimensional space.
-        layer:
-            Which layer of the data will be used for vector field function reconstruction. The layer once provided, will
-            override the `basis` argument and then learn the vector field function in high dimensional space.
-        dims:
-            The dimensions that will be used for reconstructing vector field functions. If it is an `int` all dimension
-            from the first dimension to `dims` will be used; if it is a list, the dimensions in the list will be used.
-        genes:
-            The gene names whose gene expression will be used for vector field reconstruction. By default (when genes is
-            set to None), the genes used for velocity embedding (var.use_for_transition) will be used for vector field
-            reconstruction. Note that the genes to be used need to have velocity calculated.
-        normalize:
-            Logic flag to determine whether to normalize the data to have zero means and unit covariance. This is often
-            required for raw dataset (for example, raw UMI counts and RNA velocity values in high dimension). But it is
-            normally not required for low dimensional embeddings by PCA or other non-linear dimension reduction methods.
-        grid_velocity:
-            Whether to generate grid velocity. Note that by default it is set to be False, but for datasets with
-            embedding dimension less than 4, the grid velocity will still be generated. Please note that number of total
-            grids in the space increases exponentially as the number of dimensions increases. So it may quickly lead to
-            lack of memory, for example, it cannot allocate the array with grid_num set to be 50 and dimension is 6
-            (50^6 total grids) on 32 G memory computer. Although grid velocity may not be generated, the vector field
-            function can still be learned for thousands of dimensions and we can still predict the transcriptomic cell
-            states over long time period.
-        grid_num:
-            The number of grids in each dimension for generating the grid velocity.
-        velocity_key:
-            The key from the adata layer that corresponds to the velocity matrix.
-        method:
-            Method that is used to reconstruct the vector field functionally. Currently only SparseVFC supported but
-            other improved approaches are under development.
-        min_vel_corr:
-            The minimal threshold for the cosine correlation between input velocities and learned velocities to consider
-            as a successful vector field reconstruction procedure. If the cosine correlation is less than this
-            threshold and restart_num > 1, `restart_num` trials will be attempted with different seeds to reconstruct
-            the vector field function. This can avoid some reconstructions to be trapped in some local optimal.
-        restart_num:
-            The number of retrials for vector field reconstructions.
-        restart_seed:
-            A list of seeds for each retrial. Must be the same length as `restart_num` or None.
-        buffer_path:
-               The directory address keeping all the saved/to-be-saved torch variables and NN modules. When `method` is
-               set to be `dynode`, buffer_path will set to be
-        return_vf_object:
-            Whether or not to include an instance of a vectorfield class in the the `VecFld` dictionary in the `uns`
-            attribute.
-        map_topography:
-            Whether to quantify the topography of vector field. Note that for higher than 2D vector field, we can only
-            identify fixed points as high-dimensional nullcline and separatrices are mathematically difficult to be
-            identified. Nullcline and separatrices will also be a surface or manifold in high-dimensional vector field.
-        pot_curl_div:
-            Whether to calculate potential, curl or divergence for each cell. Potential can be calculated for any basis
-            while curl and divergence is by default only applied to 2D basis. However, divergence is applicable for any
-            dimension while curl is generally only defined for 2/3 D systems.
-        cores:
-            Number of cores to run the ddhodge function. If cores is set to be > 1, multiprocessing will be used to
-            parallel the ddhodge calculation.
-        result_key:
-            The key that will be used as prefix for the vector field key in .uns
-        copy:
-            Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments and
-            returning `None`.
-        kwargs:
-            Other additional parameters passed to the vectorfield class.
-
-    Returns
-    -------
-        adata: :class:`Union[anndata.AnnData, base_vectorfield]`
-            If `copy` and `return_vf_object` arguments are set to False, `annData` object is updated with the `VecFld`
-            dictionary in the `uns` attribute.
-            If `return_vf_object` is set to True, then a vector field class object is returned.
-            If `copy` is set to True, a deep copy of the original `adata` object is returned.
+    Args:
+        adata: an AnnData object.
+        layer: The data from a particular layer (include X) used for feature selection. Defaults to "X".
+        relative_expr: A logic flag to determine whether we need to divide gene expression values first by size factor
+            before run SVR. Defaults to True.
+        total_szfactor: The column name in the .obs attribute that corresponds to the size factor for the total mRNA.
+            Defaults to "total_Size_Factor".
+        min_expr_cells: minimum number of cells that express the gene for it to be considered in the fit. Defaults to 0.
+        min_expr_avg: The minimum average of genes across cells required for gene to be selected for SVR analyses.
+            Defaults to 0.
+        max_expr_avg: The maximum average of genes across cells required for gene to be selected for SVR analyses. Genes
+            with average gene expression larger than this value will be treated as house-keeping/outlier genes. Defaults
+            to np.inf.
+        winsorize: Weather to winsorize the data for the cv vs mean model. Defaults to False.
+        winsor_perc: the up and lower bound of the winsorization. Defaults to (1, 99.5).
+
+    Returns:
+        An updated annData object with `log_m`, `log_cv`, `score` added to .obs columns and `SVR` added to uns attribute
+        as a new key.
     """
-    logger = LoggerManager.gen_logger("dynamo-topography")
-    logger.info("VectorField reconstruction begins...", indent_level=1)
-    logger.log_time()
-    adata = copy_adata(adata) if copy else adata
-
-    if basis is not None:
-        logger.info(
-            "Retrieve X and V based on basis: %s. \n "
-            "       Vector field will be learned in the %s space." % (basis.upper(), basis.upper())
-        )
-        X = adata.obsm["X_" + basis].copy()
-        V = adata.obsm["velocity_" + basis].copy()
 
-        if np.isscalar(dims):
-            X, V = X[:, :dims], V[:, :dims]
-        elif type(dims) is list:
-            X, V = X[:, dims], V[:, dims]
-    else:
-        logger.info(
-            "Retrieve X and V based on `genes`, layer: %s. \n "
-            "       Vector field will be learned in the gene expression space." % layer
+    CM = None
+    if layer == "raw":
+        CM = adata.X.copy() if adata.raw is None else adata.raw
+        szfactors = (
+            adata.obs[layer + "_Size_Factor"].values[:, None]
+            if adata.raw.X is not None
+            else adata.obs["Size_Factor"].values[:, None]
         )
-        valid_genes = (
-            list(set(genes).intersection(adata.var.index))
-            if genes is not None
-            else adata.var_names[adata.var.use_for_transition]
+    elif layer == "X":
+        CM = adata.X.copy()
+        szfactors = adata.obs["Size_Factor"].values[:, None]
+    elif layer == "protein":
+        if "protein" in adata.obsm_keys():
+            CM = adata.obsm["protein"].copy()
+            szfactors = adata.obs[layer + "_Size_Factor"].values[:, None]
+    else:
+        CM = adata.layers[layer].copy()
+        szfactors = (
+            adata.obs[layer + "_Size_Factor"].values[:, None] if layer + "_Size_Factor" in adata.obs.columns else None
         )
-        if layer == "X":
-            X = adata[:, valid_genes].X.copy()
-            X = np.expm1(X)
-        else:
-            X = inverse_norm(adata, adata.layers[layer])
 
-        V = adata[:, valid_genes].layers[velocity_key].copy()
+    if total_szfactor is not None and total_szfactor in adata.obs.keys():
+        szfactors = adata.obs[total_szfactor].values[:, None] if total_szfactor in adata.obs.columns else None
 
-        if sp.issparse(X):
-            X, V = X.A, V.A
+    if szfactors is not None and relative_expr:
+        if issparse(CM):
+            from sklearn.utils import sparsefuncs
 
-    Grid = None
-    if X.shape[1] < 4 or grid_velocity:
-        logger.info("Generating high dimensional grids and convert into a row matrix.")
-        # smart way for generating high dimensional grids and convert into a row matrix
-        min_vec, max_vec = (
-            X.min(0),
-            X.max(0),
-        )
-        min_vec = min_vec - 0.01 * np.abs(max_vec - min_vec)
-        max_vec = max_vec + 0.01 * np.abs(max_vec - min_vec)
-
-        Grid_list = np.meshgrid(*[np.linspace(i, j, grid_num) for i, j in zip(min_vec, max_vec)])
-        Grid = np.array([i.flatten() for i in Grid_list]).T
-
-    if X is None:
-        raise Exception(f"X is None. Make sure you passed the correct X or {basis} dimension reduction method.")
-    elif V is None:
-        raise Exception("V is None. Make sure you passed the correct V.")
-
-    logger.info("Learning vector field with method: %s." % (method.lower()))
-    if method.lower() == "sparsevfc":
-        vf_kwargs = {
-            "M": None,
-            "a": 5,
-            "beta": None,
-            "ecr": 1e-5,
-            "gamma": 0.9,
-            "lambda_": 3,
-            "minP": 1e-5,
-            "MaxIter": 30,
-            "theta": 0.75,
-            "div_cur_free_kernels": False,
-            "velocity_based_sampling": True,
-            "sigma": 0.8,
-            "eta": 0.5,
-            "seed": 0,
-        }
-    elif method.lower() == "dynode_old":
-        try:
-            from dynode.vectorfield import networkModels
-
-            # from dynode.vectorfield.losses_weighted import MAD, BinomialChannel, WassersteinDistance, CosineDistance
-            from dynode.vectorfield.losses_weighted import MSE
-            from dynode.vectorfield.samplers import VelocityDataSampler
-
-            from .scVectorField import dynode_vectorfield
-        except ImportError:
-            raise ImportError("You need to install the package `dynode`." "install dynode via `pip install dynode`")
-
-        velocity_data_sampler = VelocityDataSampler(adata={"X": X, "V": V}, normalize_velocity=normalize)
-        max_iter = 2 * 100000 * np.log(X.shape[0]) / (250 + np.log(X.shape[0]))
-
-        cwd, cwt = os.getcwd(), datetime.datetime.now()
-
-        if model_buffer_path is None:
-            model_buffer_path = cwd + "/" + basis + "_" + str(cwt.year) + "_" + str(cwt.month) + "_" + str(cwt.day)
-            main_warning("the buffer path saving the dynode model is in %s" % (model_buffer_path))
-
-        vf_kwargs = {
-            "model": networkModels,
-            "sirens": False,
-            "enforce_positivity": False,
-            "velocity_data_sampler": velocity_data_sampler,
-            "time_course_data_sampler": None,
-            "network_dim": X.shape[1],
-            "velocity_loss_function": MSE(),  # CosineDistance(), # #MSE(), MAD()
-            # BinomialChannel(p=0.1, alpha=1)
-            "time_course_loss_function": None,
-            "velocity_x_initialize": X,
-            "time_course_x0_initialize": None,
-            "smoothing_factor": None,
-            "stability_factor": None,
-            "load_model_from_buffer": False,
-            "buffer_path": model_buffer_path,
-            "hidden_features": 256,
-            "hidden_layers": 3,
-            "first_omega_0": 30.0,
-            "hidden_omega_0": 30.0,
-        }
-        train_kwargs = {
-            "max_iter": int(max_iter),
-            "velocity_batch_size": 50,
-            "time_course_batch_size": 100,
-            "autoencoder_batch_size": 50,
-            "velocity_lr": 1e-4,
-            "velocity_x_lr": 0,
-            "time_course_lr": 1e-4,
-            "time_course_x0_lr": 1e4,
-            "autoencoder_lr": 1e-4,
-            "velocity_sample_fraction": 1,
-            "time_course_sample_fraction": 1,
-            "iter_per_sample_update": None,
-        }
-    elif method.lower() == "dynode":
-        try:
-            from dynode.vectorfield import Dynode  # networkModels,
-
-            # from dynode.vectorfield.losses_weighted import MAD, BinomialChannel, WassersteinDistance, CosineDistance
-            # from dynode.vectorfield.losses_weighted import MSE
-            # from dynode.vectorfield.samplers import VelocityDataSampler
-            from .scVectorField import dynode_vectorfield
-        except ImportError:
-            raise ImportError("You need to install the package `dynode`." "install dynode via `pip install dynode`")
-
-        if not ("Dynode" in kwargs and type(kwargs["Dynode"]) == Dynode):
-            velocity_data_sampler = VelocityDataSampler(adata={"X": X, "V": V}, normalize_velocity=normalize)
-            max_iter = 2 * 100000 * np.log(X.shape[0]) / (250 + np.log(X.shape[0]))
-
-            cwd, cwt = os.getcwd(), datetime.datetime.now()
-
-            if model_buffer_path is None:
-                model_buffer_path = cwd + "/" + basis + "_" + str(cwt.year) + "_" + str(cwt.month) + "_" + str(cwt.day)
-                main_warning("the buffer path saving the dynode model is in %s" % (model_buffer_path))
-
-            vf_kwargs = {
-                "model": networkModels,
-                "sirens": False,
-                "enforce_positivity": False,
-                "velocity_data_sampler": velocity_data_sampler,
-                "time_course_data_sampler": None,
-                "network_dim": X.shape[1],
-                "velocity_loss_function": MSE(),  # CosineDistance(), # #MSE(), MAD()
-                # BinomialChannel(p=0.1, alpha=1)
-                "time_course_loss_function": None,
-                "velocity_x_initialize": X,
-                "time_course_x0_initialize": None,
-                "smoothing_factor": None,
-                "stability_factor": None,
-                "load_model_from_buffer": False,
-                "buffer_path": model_buffer_path,
-                "hidden_features": 256,
-                "hidden_layers": 3,
-                "first_omega_0": 30.0,
-                "hidden_omega_0": 30.0,
-            }
-            train_kwargs = {
-                "max_iter": int(max_iter),
-                "velocity_batch_size": 50,
-                "time_course_batch_size": 100,
-                "autoencoder_batch_size": 50,
-                "velocity_lr": 1e-4,
-                "velocity_x_lr": 0,
-                "time_course_lr": 1e-4,
-                "time_course_x0_lr": 1e4,
-                "autoencoder_lr": 1e-4,
-                "velocity_sample_fraction": 1,
-                "time_course_sample_fraction": 1,
-                "iter_per_sample_update": None,
-            }
+            sparsefuncs.inplace_row_scale(CM, 1 / szfactors)
         else:
-            vf_kwargs, train_kwargs = {}, {}
-    else:
-        raise ValueError("current only support two methods, SparseVFC and dynode")
+            CM /= szfactors
 
-    vf_kwargs = update_dict(vf_kwargs, kwargs)
+    if winsorize:
+        if min_expr_cells <= ((100 - winsor_perc[1]) * CM.shape[0] * 0.01):
+            min_expr_cells = int(np.ceil((100 - winsor_perc[1]) * CM.shape[1] * 0.01)) + 2
+
+    detected_bool = np.array(
+        ((CM > 0).sum(0) >= min_expr_cells) & (CM.mean(0) <= max_expr_avg) & (CM.mean(0) >= min_expr_avg)
+    ).flatten()
+
+    return CM[:, detected_bool], detected_bool
+
+
+def get_mean_cv(
+    adata: AnnData,
+    valid_CM: Union[np.ndarray, scipy.sparse.csr_matrix, scipy.sparse.csc_matrix, scipy.sparse.coo_matrix],
+    algorithm: Literal["cv_dispersion", "fano_dispersion"] = "cv_dispersion",
+    winsorize: bool = False,
+    winsor_perc: Tuple[float, float] = (1, 99.5),
+) -> AnnData:
+    """Find the mean and coefficient of variation of gene expression.
+
+    Args:
+        adata: an AnnData object
+        algorithm: Method of calculating mean and coefficient of variation, either fano_dispersion or cv_dispersion.
+        valid_CM: Gene expression matrix to be used in a downstream analysis.
+        winsorize: Whether to winsorize the data for the cv vs mean model. Defaults to False.
+        winsor_perc: The up and lower bound of the winsorization. Defaults to (1, 99.5).
+
+    Returns:
+        mean: the array dataset that contains mean values of gene expression.
+        cv: the array dataset with coefficient of variation of gene expression.
+    """
 
-    if restart_num > 0:
-        if len(restart_seed) != restart_num:
-            main_warning(
-                f"the length of {restart_seed} is different from {restart_num}, " f"using `np.range(restart_num) * 100"
+    if algorithm == "fano_dispersion":
+        (gene_counts_stats, gene_fano_parameters) = get_highvar_genes_sparse(adata, valid_CM)
+        mean = np.array(gene_counts_stats["mean"]).flatten()[:, None]
+        cv = np.array(gene_counts_stats["fano"]).flatten()
+        return mean, cv
+    elif algorithm == "cv_dispersion":
+        if winsorize:
+            down, up = (
+                np.percentile(valid_CM.A, winsor_perc, 0)
+                if issparse(valid_CM)
+                else np.percentile(valid_CM, winsor_perc, 0)
             )
-            restart_seed = np.arange(restart_num) * 100
-        restart_counter, cur_vf_list, res_list = 0, [], []
-        while True:
-            if method.lower() == "sparsevfc":
-                kwargs.update({"seed": restart_seed[restart_counter]})
-                VecFld = SvcVectorField(X, V, Grid, **vf_kwargs)
-                cur_vf_dict = VecFld.train(normalize=normalize, **kwargs)
-            elif method.lower() == "dynode_old":
-                train_kwargs = update_dict(train_kwargs, kwargs)
-                VecFld = dynode_vectorfield(X, V, Grid, **vf_kwargs)
-                # {"VecFld": VecFld.train(**kwargs)}
-                cur_vf_dict = VecFld.train(**train_kwargs)
-            elif method.lower() == "dynode":
-                if not ("Dynode" in kwargs and type(kwargs["Dynode"]) == Dynode):
-                    train_kwargs = update_dict(train_kwargs, kwargs)
-                    VecFld = dynode_vectorfield(X, V, Grid, **vf_kwargs)
-                    # {"VecFld": VecFld.train(**kwargs)}
-                    cur_vf_dict = VecFld.train(**train_kwargs)
-                else:
-                    Dynode_obj = kwargs["Dynode"]
-                    VecFld = dynode_vectorfield.fromDynode(Dynode_obj)
-                    X, Y = Dynode_obj.Velocity["sampler"].X_raw, Dynode_obj.Velocity["sampler"].V_raw
-                    cur_vf_dict = {
-                        "X": X,
-                        "Y": Y,
-                        "V": Dynode_obj.predict_velocity(Dynode_obj.Velocity["sampler"].X_raw),
-                        "grid_V": Dynode_obj.predict_velocity(Dynode_obj.Velocity["sampler"].Grid),
-                        "valid_ind": Dynode_obj.Velocity["sampler"].valid_ind
-                        if hasattr(Dynode_obj.Velocity["sampler"], "valid_ind")
-                        else np.arange(X.shape[0]),
-                        "parameters": Dynode_obj.Velocity,
-                        "dynode_object": VecFld,
-                    }
-
-            # consider refactor with .simulation.evaluation.py
-            reference, prediction = (
-                cur_vf_dict["Y"][cur_vf_dict["valid_ind"]],
-                cur_vf_dict["V"][cur_vf_dict["valid_ind"]],
+            Sfw = (
+                np.clip(valid_CM.A, down[None, :], up[None, :])
+                if issparse(valid_CM)
+                else np.percentile(valid_CM, winsor_perc, 0)
             )
-            true_normalized = reference / (np.linalg.norm(reference, axis=1).reshape(-1, 1) + 1e-20)
-            predict_normalized = prediction / (np.linalg.norm(prediction, axis=1).reshape(-1, 1) + 1e-20)
-            res = np.mean(true_normalized * predict_normalized) * prediction.shape[1]
-
-            cur_vf_list += [cur_vf_dict]
-            res_list += [res]
-            if res < min_vel_corr:
-                restart_counter += 1
-                main_info(
-                    f"current cosine correlation between input velocities and learned velocities is less than "
-                    f"{min_vel_corr}. Make a {restart_counter}-th vector field reconstruction trial.",
-                    indent_level=2,
+            mu = Sfw.mean(0)
+            sigma = Sfw.std(0, ddof=1)
+        else:
+            mu = np.array(valid_CM.mean(0)).flatten()
+            sigma = (
+                np.array(
+                    np.sqrt(
+                        (valid_CM.multiply(valid_CM).mean(0).A1 - mu**2)
+                        # * (adata.n_obs)
+                        # / (adata.n_obs - 1)
+                    )
                 )
-            else:
-                vf_dict = cur_vf_dict
-                break
+                if issparse(valid_CM)
+                else valid_CM.std(0, ddof=1)
+            )
 
-            if restart_counter > restart_num - 1:
-                main_warning(
-                    f"Cosine correlation between input velocities and learned velocities is less than"
-                    f" {min_vel_corr} after {restart_num} trials of vector field reconstruction."
-                )
-                vf_dict = cur_vf_list[np.argmax(np.array(res_list))]
+        cv = sigma / mu
+        log_m = np.array(np.log2(mu)).flatten()
+        log_cv = np.array(np.log2(cv)).flatten()
+        log_m[mu == 0], log_cv[mu == 0] = 0, 0
+        return log_m[:, None], log_cv
+    else:
+        raise ValueError(f"The algorithm {algorithm} is not existed")
+
+
+def get_prediction_by_svr(ground: np.ndarray, target: np.ndarray, svr_gamma: Optional[float] = None):
+    """This function will return the base class for estimators that use libsvm as backing library.
+
+    Args:
+        ground: the training array dataset that contains mean values of gene expression.
+        target: the target array dataset with coefficient of variation of gene expression.
+        mean: the mean value to estimate a value of svr_gamma.
+        svr_gamma: the gamma hyperparameter of the SVR. Defaults to None.
 
-                break
-    else:
-        if method.lower() == "sparsevfc":
-            VecFld = SvcVectorField(X, V, Grid, **vf_kwargs)
-            vf_dict = VecFld.train(normalize=normalize, **kwargs)
-        elif method.lower() == "dynode":
-            train_kwargs = update_dict(train_kwargs, kwargs)
-            VecFld = dynode_vectorfield(X, V, Grid, **vf_kwargs)
-            # {"VecFld": VecFld.train(**kwargs)}
-            vf_dict = VecFld.train(**train_kwargs)
+    Returns:
+        A fitted SVM model according to the given training and target data.
+    """
+    from sklearn.svm import SVR
 
-    if result_key is None:
-        vf_key = "VecFld" if basis is None else "VecFld_" + basis
-    else:
-        vf_key = result_key if basis is None else result_key + "_" + basis
+    if svr_gamma is None:
+        svr_gamma = 150.0 / len(ground)
 
-    vf_dict["method"] = method
-    if basis is not None:
-        key = "velocity_" + basis + "_" + method
-        X_copy_key = "X_" + basis + "_" + method
-
-        logger.info_insert_adata(key, adata_attr="obsm")
-        logger.info_insert_adata(X_copy_key, adata_attr="obsm")
-        adata.obsm[key] = vf_dict["V"]
-        adata.obsm[X_copy_key] = vf_dict["X"]
+    # Fit the Support Vector Regression
+    clf = SVR(gamma=svr_gamma)
+    clf.fit(ground, target)
+    return clf.predict, svr_gamma
+
+
+# Highly variable gene selection function:
+def get_highvar_genes_sparse(
+    adata: AnnData,
+    expression: Union[
+        np.ndarray,
+        scipy.sparse.csr_matrix,
+        scipy.sparse.csc_matrix,
+        scipy.sparse.coo_matrix,
+    ],
+    expected_fano_threshold: Optional[float] = None,
+    numgenes: Optional[int] = None,
+    minimal_mean: float = 0.5,
+    save_key: Optional[str] = None,
+) -> Tuple[pd.DataFrame, Dict]:
+    """Find highly-variable genes in sparse single-cell data matrices.
+
+    Args:
+        adata: an AnnData object
+        expression: Gene expression matrix
+        expected_fano_threshold: Optionally can be used to set a manual dispersion threshold (for definition of
+            "highly-variable")
+        numgenes: Optionally can be used to find the n most variable genes
+        minimal_mean: Sets a threshold on the minimum mean expression to consider
+        save_key: the key to store the fano calculation results
+
+    Returns:
+        gene_counts_stats: Results dataframe containing pertinent information for each gene
+        gene_fano_parameters: Additional informative dictionary (w/ records of dispersion for each gene, threshold,
+        etc.)
+    """
+    gene_mean = np.array(expression.mean(axis=0)).astype(float).reshape(-1)
+    E2 = expression.copy()
+    E2.data **= 2
+    gene2_mean = np.array(E2.mean(axis=0)).reshape(-1)
+    gene_var = pd.Series(gene2_mean - (gene_mean**2))
+    del E2
+    gene_mean = pd.Series(gene_mean)
+    gene_fano = gene_var / gene_mean
+
+    # Find parameters for expected fano line -- this line can be non-linear...
+    top_genes = gene_mean.sort_values(ascending=False)[:20].index
+    A = (np.sqrt(gene_var) / gene_mean)[top_genes].min()
+
+    w_mean_low, w_mean_high = gene_mean.quantile([0.10, 0.90])
+    w_fano_low, w_fano_high = gene_fano.quantile([0.10, 0.90])
+    winsor_box = (
+        (gene_fano > w_fano_low) & (gene_fano < w_fano_high) & (gene_mean > w_mean_low) & (gene_mean < w_mean_high)
+    )
+    fano_median = gene_fano[winsor_box].median()
+    B = np.sqrt(fano_median)
 
-        vf_dict["dims"] = dims
+    gene_expected_fano = (A**2) * gene_mean + (B**2)
+    fano_ratio = gene_fano / gene_expected_fano
 
-        logger.info_insert_adata(vf_key, adata_attr="uns")
-        adata.uns[vf_key] = vf_dict
+    # Identify high var genes
+    if numgenes is not None:
+        highvargenes = fano_ratio.sort_values(ascending=False).index[:numgenes]
+        high_var_genes_ind = fano_ratio.index.isin(highvargenes)
+        T = None
     else:
-        key = velocity_key + "_" + method
+        if not expected_fano_threshold:
+            T = 1.0 + gene_fano[winsor_box].std()
+        else:
+            T = expected_fano_threshold
 
-        logger.info_insert_adata(key, adata_attr="layers")
-        adata.layers[key] = sp.csr_matrix((adata.shape))
-        adata.layers[key][:, [adata.var_names.get_loc(i) for i in valid_genes]] = vf_dict["V"]
+        high_var_genes_ind = (fano_ratio > T) & (gene_mean > minimal_mean)
 
-        vf_dict["layer"] = layer
-        vf_dict["genes"] = genes
-        vf_dict["velocity_key"] = velocity_key
+    gene_counts_stats = pd.DataFrame(
+        {
+            "mean": gene_mean,
+            "var": gene_var,
+            "fano": gene_fano,
+            "expected_fano": gene_expected_fano,
+            "high_var": high_var_genes_ind,
+            "fano_ratio": fano_ratio,
+        }
+    )
+    gene_fano_parameters = {
+        "A": A,
+        "B": B,
+        "T": T,
+        "minimal_mean": minimal_mean,
+    }
 
-        logger.info_insert_adata(vf_key, adata_attr="uns")
-        adata.uns[vf_key] = vf_dict
+    if save_key is not None:
+        LoggerManager.main_logger.info_insert_adata(save_key, "varm")
+        gene_counts_stats.set_index(adata.var.index, inplace=True)
+        adata.varm[save_key] = gene_counts_stats
+    return gene_counts_stats, gene_fano_parameters
+
+
+def select_genes_by_seurat_recipe(
+    adata: AnnData,
+    layer: str = DKM.X_LAYER,
+    nan_replace_val: Union[float, None] = None,
+    n_top_genes: int = 2000,
+    algorithm: Literal["seurat_dispersion", "fano_dispersion"] = "seurat_dispersion",
+    seurat_min_disp: Union[float, None] = None,
+    seurat_max_disp: Union[float, None] = None,
+    seurat_min_mean: Union[float, None] = None,
+    seurat_max_mean: Union[float, None] = None,
+    gene_names: Union[List[str], None] = None,
+    var_filter_key: str = "pass_basic_filter",
+    inplace: bool = False,
+) -> None:
+    """A general function for feature genes selection.
+
+    Preprocess adata and dispatch to different filtering methods, and eventually set keys in anndata to denote which
+    genes are wanted in downstream analysis.
+
+    Args:
+        adata: an AnnData object.
+        layer: the key of a sparse matrix in adata. Defaults to DKM.X_LAYER.
+        nan_replace_val: your choice of value to replace values in layer. Defaults to None.
+        n_top_genes: number of genes to select as highly variable genes. Defaults to 2000.
+        algorithm: a method for selecting genes; must be one of "seurat_dispersion" or "fano".
+        seurat_min_disp: seurat dispersion min cutoff. Defaults to None.
+        seurat_max_disp: seurat dispersion max cutoff. Defaults to None.
+        seurat_min_mean: seurat mean min cutoff. Defaults to None.
+        seurat_max_mean: seurat mean max cutoff. Defaults to None.
+        gene_names: name of genes to be selected. Defaults to None.
+        var_filter_key: filter gene names based on the key defined in adata.var before gene selection. Defaults to
+            "pass_basic_filter".
+        inplace: when inplace is True, subset adata according to selected genes. Defaults to False.
 
-    if map_topography:
-        tp_kwargs = {"n": 25}
-        tp_kwargs = update_dict(tp_kwargs, kwargs)
+    Raises:
+        NotImplementedError: the recipe is invalid/unsupported.
+    """
 
-        logger.info("Mapping topography...")
-        with warnings.catch_warnings():
-            warnings.simplefilter("ignore")
+    pass_filter_genes = adata.var_names
+    if gene_names:
+        main_info("select genes on gene names from arguments <gene_names>")
+        pass_filter_genes = gene_names
+    elif var_filter_key:
+        main_info("select genes on var key: %s" % (var_filter_key))
+        pass_filter_genes = adata.var_names[adata.var[var_filter_key]]
+
+    if len(pass_filter_genes) != len(set(pass_filter_genes)):
+        main_warning("gene names are not unique, please check your preprocessing procedure.")
+    subset_adata = adata[:, pass_filter_genes]
+    if n_top_genes is None:
+        main_info("n_top_genes is None, reserve all genes and add filter gene information")
+        n_top_genes = adata.n_vars
+    layer_mat = DKM.select_layer_data(subset_adata, layer)
+    if nan_replace_val:
+        main_info("replacing nan values with: %s" % (nan_replace_val))
+        _mask = get_nan_or_inf_data_bool_mask(layer_mat)
+        layer_mat[_mask] = nan_replace_val
+
+    if algorithm == "seurat_dispersion":
+        mean, variance, highly_variable_mask = select_genes_by_seurat_dispersion(
+            layer_mat,
+            min_disp=seurat_min_disp,
+            max_disp=seurat_max_disp,
+            min_mean=seurat_min_mean,
+            max_mean=seurat_max_mean,
+            n_top_genes=n_top_genes,
+        )
+        main_info_insert_adata_var(DKM.VAR_GENE_MEAN_KEY)
+        main_info_insert_adata_var(DKM.VAR_GENE_VAR_KEY)
+        main_info_insert_adata_var(DKM.VAR_GENE_HIGHLY_VARIABLE_KEY)
+        main_debug("type of variance:" + str(type(variance)))
+        main_debug("shape of variance:" + str(variance.shape))
+        adata.var[DKM.VAR_GENE_MEAN_KEY] = np.nan
+        adata.var[DKM.VAR_GENE_VAR_KEY] = np.nan
+        adata.var[DKM.VAR_GENE_HIGHLY_VARIABLE_KEY] = False
+        adata.var[DKM.VAR_USE_FOR_PCA] = False
+
+        adata.var[DKM.VAR_GENE_MEAN_KEY][pass_filter_genes] = mean.flatten()
+        adata.var[DKM.VAR_GENE_VAR_KEY][pass_filter_genes] = variance
+        adata.var[DKM.VAR_GENE_HIGHLY_VARIABLE_KEY][pass_filter_genes] = highly_variable_mask
+        adata.var[DKM.VAR_USE_FOR_PCA][pass_filter_genes] = highly_variable_mask
+
+    elif algorithm == "fano_dispersion":
+        select_genes_monocle(adata, layer=layer, sort_by=algorithm)
+        # adata = select_genes_by_svr(
+        #     adata,
+        #     layers=layer,
+        #     algorithm=algorithm,
+        # )
+        # filter_bool = get_svr_filter(adata, layer=layer, n_top_genes=n_top_genes, return_adata=False)
+    else:
+        raise ValueError(f"The algorithm {algorithm} is not existed")
+
+    main_info("number of selected highly variable genes: " + str(adata.var[DKM.VAR_USE_FOR_PCA].sum()))
+    if inplace:
+        main_info("inplace is True, subset adata according to selected genes.")
+        adata = adata[:, adata.var[DKM.VAR_USE_FOR_PCA]]
+
+
+def select_genes_by_seurat_dispersion(
+    sparse_layer_mat: csr_matrix,
+    n_bins: int = 20,
+    log_mean_and_dispersion: bool = True,
+    min_disp: float = None,
+    max_disp: float = None,
+    min_mean: float = None,
+    max_mean: float = None,
+    n_top_genes: Union[int, None] = None,
+) -> Tuple[ndarray, ndarray, Union[bool, ndarray]]:
+    """Apply seurat's gene selection recipe by cutoffs.
+
+    Args:
+        sparse_layer_mat: the sparse matrix used for gene selection.
+        n_bins: the number of bins for normalization. Defaults to 20.
+        log_mean_and_dispersion: whether log the gene expression values before calculating the dispersion values.
+            Defaults to True.
+        min_disp: seurat dispersion min cutoff. Defaults to None.
+        max_disp: seurat dispersion max cutoff. Defaults to None.
+        min_mean: seurat mean min cutoff. Defaults to None.
+        max_mean: seurat mean max cutoff. Defaults to None.
+        n_top_genes: number of top genes to be evaluated. If set to be None, genes are filtered by mean and dispersion
+            norm threshold. Defaults to None.
+
+    Returns:
+        A tuple (mean, variance, highly_variable_mask, highly_variable_scores), where mean is the mean of the provided
+        sparse matrix, variance is the variance of the provided sparse matrix, highly_variable_mask is a bool array
+        indicating whether an element (a gene) is highly variable in the matrix. highly_variable_scores is always none
+        since the scores are not applicable to Seurat recipe.
+    """
 
-            adata = topography(
-                adata,
-                basis=basis,
-                X=X,
-                layer=layer,
-                dims=None,
-                VecFld=vf_dict,
-                **tp_kwargs,
+    # default values from Seurat
+    if min_disp is None:
+        min_disp = 0.5
+    if max_disp is None:
+        max_disp = np.inf
+    if min_mean is None:
+        min_mean = 0.0125
+    if max_mean is None:
+        max_mean = 3
+
+    # mean, variance, dispersion = calc_mean_var_dispersion_sparse(sparse_layer_mat) # Dead
+    sc_mean, sc_var = seurat_get_mean_var(sparse_layer_mat)
+    mean, variance = sc_mean, sc_var
+    dispersion = variance / mean
+
+    if log_mean_and_dispersion:
+        mean = np.log1p(mean)
+        dispersion[np.equal(dispersion, 0)] = np.nan
+        dispersion = np.log(dispersion)
+
+    temp_df = pd.DataFrame()
+    temp_df["mean"], temp_df["dispersion"] = mean, dispersion
+
+    temp_df["mean_bin"] = pd.cut(temp_df["mean"], bins=n_bins)
+    disp_grouped = temp_df.groupby("mean_bin")["dispersion"]
+    disp_mean_bin = disp_grouped.mean()
+    disp_std_bin = disp_grouped.std(ddof=1)
+
+    # handle nan std
+    one_gene_per_bin = disp_std_bin.isnull()
+
+    disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin].values
+    disp_mean_bin[one_gene_per_bin] = 0
+
+    # normalized dispersion
+    mean = disp_mean_bin[temp_df["mean_bin"].values].values
+    std = disp_std_bin[temp_df["mean_bin"].values].values
+    variance = std**2
+    temp_df["dispersion_norm"] = ((temp_df["dispersion"] - mean) / std).fillna(0)
+    dispersion_norm = temp_df["dispersion_norm"].values
+
+    highly_variable_mask = None
+    if n_top_genes is not None:
+        main_info("choose %d top genes" % n_top_genes, indent_level=2)
+        threshold = temp_df["dispersion_norm"].nlargest(n_top_genes).values[-1]
+        highly_variable_mask = temp_df["dispersion_norm"].values >= threshold
+    else:
+        main_info("choose genes by mean and dispersion norm threshold", indent_level=2)
+        highly_variable_mask = np.logical_and.reduce(
+            (
+                mean > min_mean,
+                mean < max_mean,
+                dispersion_norm > min_disp,
+                dispersion_norm < max_disp,
             )
-    if pot_curl_div:
-        logger.info(f"Running ddhodge to estimate vector field based pseudotime in {basis} basis...")
-        from ..external.hodge import ddhodge
-
-        ddhodge(adata, basis=basis, cores=cores)
-        if X.shape[1] == 2:
-            logger.info("Computing curl...")
-            curl(adata, basis=basis)
-
-        logger.info("Computing divergence...")
-        divergence(adata, basis=basis)
-
-    control_point, inlier_prob, valid_ids = (
-        "control_point_" + basis if basis is not None else "control_point",
-        "inlier_prob_" + basis if basis is not None else "inlier_prob",
-        vf_dict["valid_ind"],
-    )
-    if method.lower() == "sparsevfc":
-        logger.info_insert_adata(control_point, adata_attr="obs")
-        logger.info_insert_adata(inlier_prob, adata_attr="obs")
-
-        adata.obs[control_point], adata.obs[inlier_prob] = False, np.nan
-        adata.obs.loc[adata.obs_names[vf_dict["ctrl_idx"]], control_point] = True
-        adata.obs.loc[adata.obs_names[valid_ids], inlier_prob] = vf_dict["P"].flatten()
-
-    # angles between observed velocity and that predicted by vector field across cells:
-    cell_angles = np.zeros(adata.n_obs, dtype=float)
-    for i, u, v in zip(valid_ids, V[valid_ids], vf_dict["V"]):
-        # fix the u, v norm == 0 in angle function
-        cell_angles[i] = angle(u.astype("float64"), v.astype("float64"))
+        )
 
-    if basis is not None:
-        temp_key = "obs_vf_angle_" + basis
+    return mean, variance, highly_variable_mask
 
-        logger.info_insert_adata(temp_key, adata_attr="obs")
-        adata.obs[temp_key] = cell_angles
-    else:
-        temp_key = "obs_vf_angle"
-        logger.info_insert_adata(temp_key, adata_attr="obs")
-        adata.obs[temp_key] = cell_angles
-
-    logger.finish_progress("VectorField")
-    if return_vf_object:
-        return VecFld
-    elif copy:
-        return adata
-    return None
-
-
-def assign_fixedpoints(
-    adata: anndata.AnnData,
-    basis: str = "pca",
-    cores: int = 1,
-    copy: bool = False,
-) -> Union[None, anndata.AnnData]:
-    """Assign each cell in our data to a fixed point.
-
-    Parameters
-    ----------
-        adata: :class:`~anndata.AnnData`
-            AnnData object that contains reconstructed vector field in the `basis` space.
-        basis:
-            The vector field function for the `basis` that will be used to assign fixed points for each cell.
-        cores:
-            Number of cores to run the fixed-point search for each cell.
-        copy:
-            Whether to return a new deep copy of `adata` instead of updating `adata` object passed in arguments and
-            returning `None`.
-
-    Returns
-    -------
-        adata: :class:`Union[None, anndata.AnnData]`
-            If `copy` is set to False, return None but the adata object will updated with a `fps_assignment` in .obs as
-            well as the `'fps_assignment_' + basis` in the .uns.
-            If `copy` is set to True, a deep copy of the original `adata` object is returned.
+
+def get_highly_variable_mask_by_dispersion_svr(
+    mean: np.ndarray,
+    var: np.ndarray,
+    n_top_genes: int,
+    svr_gamma: Optional[float] = None,
+    return_scores: bool = True,
+) -> Union[Tuple[np.ndarray, np.ndarray], np.ndarray]:
+    """Returns the mask with shape same as mean and var.
+
+    The mask indicates whether each index is highly variable or not. Each index should represent a gene.
+
+    Args:
+        mean: mean of the genes.
+        var: variance of the genes.
+        n_top_genes: the number of top genes to be inspected.
+        svr_gamma: coefficient for support vector regression. Defaults to None.
+        return_scores: whether return the dispersion scores. Defaults to True.
+
+    Returns:
+        A tuple (highly_variable_mask, scores) where highly_variable_mask is a bool array indicating whether an element
+        (a gene) is highly variable in the matrix and scores is an array recording variable score for each gene. scores
+        would only be returned when `return_scores` is True.
     """
-    logger = LoggerManager.gen_logger("dynamo-assign_fixedpoints")
-    logger.info("assign_fixedpoints begins...", indent_level=1)
-    logger.log_time()
-    adata = copy_adata(adata) if copy else adata
-
-    VecFld, func = vecfld_from_adata(adata, basis=basis)
-
-    vecfld_class = BaseVectorField(
-        X=VecFld["X"],
-        V=VecFld["Y"],
-        func=func,
-    )
 
-    (
-        X,
-        valid_fps_type_assignment,
-        assignment_id,
-    ) = vecfld_class.assign_fixed_points(cores=cores)
-    assignment_id = [str(int(i)) if np.isfinite(i) else None for i in assignment_id]
-    adata.obs["fps_assignment"] = assignment_id
-    adata.uns["fps_assignment_" + basis] = {
-        "X": X,
-        "valid_fps_type_assignment": valid_fps_type_assignment,
-        "assignment_id": assignment_id,
+    # normally, select svr_gamma based on #features
+    if svr_gamma is None:
+        svr_gamma = 150.0 / len(mean)
+    from sklearn.svm import SVR
+
+    mean_log = np.log2(mean)
+    cv_log = np.log2(np.sqrt(var) / mean)
+    classifier = SVR(gamma=svr_gamma)
+    # fit & prediction will complain about nan values if not take cared here
+    is_nan_indices = np.logical_or(np.isnan(mean_log), np.isnan(cv_log))
+    if np.sum(is_nan_indices) > 0:
+        main_warning(
+            (
+                "mean and cv_log contain NAN values. We exclude them in SVR training. Please use related gene filtering"
+                " methods to filter genes with zero means."
+            )
+        )
+
+    classifier.fit(mean_log[~is_nan_indices, np.newaxis], cv_log.reshape([-1, 1])[~is_nan_indices])
+    scores = np.repeat(np.nan, len(mean_log))
+    # TODO handle nan values during prediction here
+    scores[~is_nan_indices] = cv_log[~is_nan_indices] - classifier.predict(mean_log[~is_nan_indices, np.newaxis])
+    scores = scores.reshape([-1, 1])  # shape should be #genes x 1
+
+    # score threshold based on n top genes
+    n_top_genes = min(n_top_genes, len(mean))  # maybe not enough genes there
+    score_threshold = np.sort(-scores)[n_top_genes - 1]
+    highly_variable_mask = scores >= score_threshold
+    highly_variable_mask = np.array(highly_variable_mask).flatten()
+    if return_scores:
+        return highly_variable_mask, scores
+    return highly_variable_mask
+
+
+def highest_frac_genes(
+    adata: AnnData,
+    store_key: str = "highest_frac_genes",
+    n_top: int = 30,
+    gene_prefix_list: List[str] = None,
+    gene_prefix_only: bool = False,
+    layer: Union[str, None] = None,
+) -> anndata.AnnData:
+    """Compute top genes df and store results in `adata.uns`
+
+    Args:
+        adata: an AnnData object
+        store_key: key for storing expression percent results. Defaults to "highest_frac_genes".
+        n_top: number of top genes to show. Defaults to 30.
+        gene_prefix_list: a list of gene name prefixes used for gathering/calculating expression percents from genes
+            with these prefixes. Defaults to None.
+        gene_prefix_only: whether to calculate percentages for gene groups with the specified prefixes only. It only
+            takes effect if gene prefix list is provided. Defaults to False.
+        layer: layer on which the gene percents will be computed. Defaults to None.
+
+    Returns:
+        An updated adata with top genes df stored in `adata.uns`
+    """
+
+    gene_mat = adata.X
+    if layer is not None:
+        gene_mat = DKM.select_layer_data(adata, layer)
+    # compute gene percents at each cell row
+    cell_expression_sum = gene_mat.sum(axis=1).flatten()
+    # get rid of cells that have all zero counts
+    not_all_zero = cell_expression_sum != 0
+    filtered_adata = adata[not_all_zero, :]
+    cell_expression_sum = cell_expression_sum[not_all_zero]
+    main_debug("%d rows(cells or subsets) are not zero. zero total RNA cells are removed." % np.sum(not_all_zero))
+
+    valid_gene_set = set()
+    prefix_to_genes = {}
+    _adata = filtered_adata
+    if gene_prefix_list is not None:
+        prefix_to_genes = {prefix: [] for prefix in gene_prefix_list}
+        for name in _adata.var_names:
+            for prefix in gene_prefix_list:
+                length = len(prefix)
+                if name[:length] == prefix:
+                    valid_gene_set.add(name)
+                    prefix_to_genes[prefix].append(name)
+                    break
+        if len(valid_gene_set) == 0:
+            main_critical("NO VALID GENES FOUND WITH REQUIRED GENE PREFIX LIST, GIVING UP PLOTTING")
+            return None
+        if gene_prefix_only:
+            # gathering gene prefix set data
+            df = pd.DataFrame(index=_adata.obs.index)
+            for prefix in prefix_to_genes:
+                if len(prefix_to_genes[prefix]) == 0:
+                    main_info("There is no %s gene prefix in adata." % prefix)
+                    continue
+                df[prefix] = _adata[:, prefix_to_genes[prefix]].X.sum(axis=1)
+            # adata = adata[:, list(valid_gene_set)]
+
+            _adata = AnnData(X=df)
+            gene_mat = _adata.X
+
+    # compute gene's total percents in the dataset
+    gene_percents = np.array(gene_mat.sum(axis=0))
+    gene_percents = (gene_percents / gene_mat.shape[1]).flatten()
+    # obtain top genes
+    sorted_indices = np.argsort(-gene_percents)
+    selected_indices = sorted_indices[:n_top]
+    gene_names = _adata.var_names[selected_indices]
+
+    gene_X_percents = gene_mat / cell_expression_sum.reshape([-1, 1])
+
+    # assemble a dataframe
+    selected_gene_X_percents = np.array(gene_X_percents)[:, selected_indices]
+    selected_gene_X_percents = np.squeeze(selected_gene_X_percents)
+
+    top_genes_df = pd.DataFrame(
+        selected_gene_X_percents,
+        index=adata.obs_names,
+        columns=gene_names,
+    )
+    gene_percents_df = pd.DataFrame(
+        gene_percents, index=_adata.var_names, columns=["percent"]
+    )  # Series is not appropriate for h5ad format.
+
+    main_info_insert_adata_uns(store_key)
+    adata.uns[store_key] = {
+        "top_genes_df": top_genes_df,
+        "gene_mat": gene_mat,
+        "layer": layer,
+        "selected_indices": selected_indices,
+        "gene_prefix_list": gene_prefix_list,
+        "show_individual_prefix_gene": gene_prefix_only,
+        "gene_percents": gene_percents_df,
     }
 
-    logger.finish_progress("assign_fixedpoints")
-    if copy:
-        return adata
-    return None
+    return adata
+
+
+def pca_selected_genes_wrapper(
+    adata: AnnData, pca_input: Union[np.ndarray, None] = None, n_pca_components: int = 30, key: str = "X_pca"
+):
+    """A wrapper for pca function to reduce dimensions of the Adata with PCA.
+
+    Args:
+        adata: an AnnData object.
+        pca_input: an array for nearest neighbor search directly. Defaults to None.
+        n_pca_components: number of PCA components. Defaults to 30.
+        key: the key to store the calculation result. Defaults to "X_pca".
+    """
+
+    adata = pca(adata, pca_input, n_pca_components=n_pca_components, pca_key=key)
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/util_vector_calculus.py` & `dynamo-release-1.3.0/dynamo/vectorfield/util_vector_calculus.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/utils.py` & `dynamo-release-1.3.0/dynamo/vectorfield/utils.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,54 +1,102 @@
-import functools
-import inspect
 import itertools
 import multiprocessing as mp
+import sys
 from multiprocessing.dummy import Pool as ThreadPool
-from typing import Callable, Union
+from typing import Callable, Dict, List, Optional, Tuple, Union
 
 import numdifftools as nd
 import numpy as np
 import pandas as pd
+from anndata import AnnData
 from scipy.optimize import fsolve
 from scipy.sparse import issparse
 from scipy.spatial.distance import cdist, pdist
 from tqdm import tqdm
 
 from ..dynamo_logger import LoggerManager, main_info
-from ..tools.utils import (
-    form_triu_matrix,
-    index_condensed_matrix,
-    subset_dict_with_key_list,
-    timeit,
-)
+from ..tools.utils import form_triu_matrix, index_condensed_matrix, timeit
 from .FixedPoints import FixedPoints
 
+if sys.version_info >= (3, 8):
+    from typing import TypedDict
+else:
+    from typing_extensions import TypedDict
+
+
+class NormDict(TypedDict):
+    xm: np.ndarray
+    ym: np.ndarray
+    xscale: float
+    yscale: float
+    fix_velocity: bool
+
+
+class VecFldDict(TypedDict):
+    X: np.ndarray
+    valid_ind: float
+    X_ctrl: np.ndarray
+    ctrl_idx: float
+    Y: np.ndarray
+    beta: float
+    V: np.ndarray
+    C: np.ndarray
+    P: np.ndarray
+    VFCIndex: np.ndarray
+    sigma2: float
+    grid: np.ndarray
+    grid_V: np.ndarray
+    iteration: int
+    tecr_traj: np.ndarray
+    E_traj: np.ndarray
+    norm_dict: NormDict
 
-def is_outside_domain(x, domain):
+
+def is_outside_domain(x: np.ndarray, domain: Tuple[float, float]) -> np.ndarray:
     x = x[None, :] if x.ndim == 1 else x
     return np.any(np.logical_or(x < domain[0], x > domain[1]), axis=1)
 
 
-def grad(f, x):
+def grad(f: Callable, x: np.ndarray) -> nd.Gradient:
     """Gradient of scalar-valued function f evaluated at x"""
     return nd.Gradient(f)(x)
 
 
-def laplacian(f, x):
+def laplacian(f: Callable, x: np.ndarray) -> float:
     """Laplacian of scalar field f evaluated at x"""
     hes = nd.Hessdiag(f)(x)
     return sum(hes)
 
 
 # ---------------------------------------------------------------------------------------------------
 # vector field function
 @timeit
-def vector_field_function(x, vf_dict, dim=None, kernel="full", X_ctrl_ind=None, **kernel_kwargs):
+def vector_field_function(
+    x: np.ndarray,
+    vf_dict: VecFldDict,
+    dim: Optional[Union[int, np.ndarray]] = None,
+    kernel: str = "full",
+    X_ctrl_ind: Optional[List] = None,
+    **kernel_kwargs,
+) -> np.ndarray:
     """vector field function constructed by sparseVFC.
     Reference: Regularized vector field learning with sparse approximation for mismatch removal, Ma, Jiayi, etc. al, Pattern Recognition
+
+    Args:
+        x: Set of cell expression state samples
+        vf_dict: VecFldDict with stored parameters necessary for reconstruction
+        dim: Index or indices of dimensions of the K gram matrix to return. Defaults to None.
+        kernel: one of {"full", "df_kernel", "cf_kernel"}. Defaults to "full".
+        X_ctrl_ind: Indices of control points at which kernels will be centered. Defaults to None.
+
+    Raises:
+        ValueError: If the kernel value specified is not one of "full", "df_kernel", or "cf_kernel"
+
+    Returns:
+        np.ndarray storing the `dim` dimensions of m x m gram matrix K storing the kernel evaluated at each pair of control points
     """
     # x=np.array(x).reshape((1, -1))
     if "div_cur_free_kernels" in vf_dict.keys():
         has_div_cur_free_kernels = True
     else:
         has_div_cur_free_kernels = False
 
@@ -90,15 +138,17 @@
             K = K[:, :dim]
         elif dim is not None:
             K = K[:, dim]
 
     return K
 
 
-def dynode_vector_field_function(x, vf_dict, dim=None, **kwargs):
+def dynode_vector_field_function(
+    x: np.ndarray, vf_dict: VecFldDict, dim: Optional[Union[int, np.ndarray]] = None, **kwargs
+) -> np.ndarray:
     # try:
     #     import dynode
     #     from dynode.vectorfield import Dynode
     # except ImportError:
     #     raise ImportError("You need to install the package `dynode`." "install dynode via `pip install dynode`")
     # vf_dict["parameters"]["load_model_from_buffer"] = True
     # dynode_inspect = inspect.getfullargspec(Dynode)
@@ -123,32 +173,27 @@
     if to_flatten:
         res = res.flatten()
 
     return res
 
 
 @timeit
-def con_K(x, y, beta, method="cdist", return_d=False):
+def con_K(
+    x: np.ndarray, y: np.ndarray, beta: float = 0.1, method: str = "cdist", return_d: bool = False
+) -> Union[Tuple[np.ndarray, np.ndarray], np.ndarray]:
     """con_K constructs the kernel K, where K(i, j) = k(x, y) = exp(-beta * ||x - y||^2).
 
-    Arguments
-    ---------
-        x: :class:`~numpy.ndarray`
-            Original training data points.
-        y: :class:`~numpy.ndarray`
-            Control points used to build kernel basis functions.
-        beta: float (default: 0.1)
-            Paramerter of Gaussian Kernel, k(x, y) = exp(-beta*||x-y||^2),
-        return_d: bool
-            If True the intermediate 3D matrix x - y will be returned for analytical Jacobian.
-
-    Returns
-    -------
-    K: :class:`~numpy.ndarray`
-    the kernel to represent the vector field function.
+    Args:
+        x: Original training data points.
+        y: Control points used to build kernel basis functions.
+        beta: Paramerter of Gaussian Kernel, k(x, y) = exp(-beta*||x-y||^2),
+        return_d: If True the intermediate 3D matrix x - y will be returned for analytical Jacobian.
+
+    Returns:
+        Tuple(K: the kernel to represent the vector field function, D:
     """
     if method == "cdist" and not return_d:
         K = cdist(x, y, "sqeuclidean")
         if len(K) == 1:
             K = K.flatten()
     else:
         n = x.shape[0]
@@ -164,31 +209,27 @@
     if return_d:
         return K, D
     else:
         return K
 
 
 @timeit
-def con_K_div_cur_free(x, y, sigma=0.8, eta=0.5):
+def con_K_div_cur_free(
+    x: np.ndarray, y: np.ndarray, sigma: int = 0.8, eta: float = 0.5
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
     """Construct a convex combination of the divergence-free kernel T_df and curl-free kernel T_cf with a bandwidth sigma
     and a combination coefficient gamma.
 
-    Arguments
-    ---------
-        x: :class:`~numpy.ndarray`
-            Original training data points.
-        y: :class:`~numpy.ndarray`
-            Control points used to build kernel basis functions
-        sigma: int (default: `0.8`)
-            Bandwidth parameter.
-        eta: int (default: `0.5`)
-            Combination coefficient for the divergence-free or the curl-free kernels.
+    Args:
+        x: Original training data points.
+        y: Control points used to build kernel basis functions
+        sigma: Bandwidth parameter.
+        eta: Combination coefficient for the divergence-free or the curl-free kernels.
 
-    Returns
-    -------
+    Returns:
         A tuple of G (the combined kernel function), divergence-free kernel and curl-free kernel.
 
     See also: :func:`sparseVFC`.
     """
     m, d = x.shape
     n, d = y.shape
     sigma2 = sigma**2
@@ -221,112 +262,108 @@
     G_tmp4 = np.kron(np.ones((m, n)), np.eye(d)) - G_tmp2
     df_kernel, cf_kernel = (1 - eta) * G_tmp * (G_tmp2 + G_tmp3), eta * G_tmp * G_tmp4
     G = df_kernel + cf_kernel
 
     return G, df_kernel, cf_kernel
 
 
-def get_vf_dict(adata, basis="", vf_key="VecFld"):
+def get_vf_dict(adata: AnnData, basis: str = "", vf_key: str = "VecFld") -> VecFldDict:
+    """Get vector field dictionary from the `.uns` attribute of the AnnData object.
+
+    Args:
+        adata: `AnnData` object
+        basis: string indicating the embedding data to use for calculating velocities. Defaults to "".
+        vf_key: _description_. Defaults to "VecFld".
+
+    Raises:
+        ValueError: if vf_key or vfkey_basis is not included in the adata object.
+
+    Returns:
+        vector field dictionary
+    """
     if basis is not None:
         if len(basis) > 0:
             vf_key = "%s_%s" % (vf_key, basis)
 
     if vf_key not in adata.uns.keys():
         raise ValueError(
             f"Vector field function {vf_key} is not included in the adata object! "
             f"Try firstly running dyn.vf.VectorField(adata, basis='{basis}')"
         )
 
     vf_dict = adata.uns[vf_key]
     return vf_dict
 
 
-def vecfld_from_adata(adata, basis="", vf_key="VecFld"):
+def vecfld_from_adata(adata: AnnData, basis: str = "", vf_key: str = "VecFld") -> Tuple[VecFldDict, Callable]:
     vf_dict = get_vf_dict(adata, basis=basis, vf_key=vf_key)
 
     method = vf_dict["method"]
     if method.lower() == "sparsevfc":
         func = lambda x: vector_field_function(x, vf_dict)
     elif method.lower() == "dynode":
         func = lambda x: dynode_vector_field_function(x, vf_dict)
     else:
         raise ValueError(f"current only support two methods, SparseVFC and dynode")
 
     return vf_dict, func
 
 
-def vector_transformation(V, Q):
+def vector_transformation(V: np.ndarray, Q: np.ndarray) -> np.ndarray:
     """Transform vectors from PCA space to the original space using the formula:
                     :math:`\hat{v} = v Q^T`,
     where `Q, v, \hat{v}` are the PCA loading matrix, low dimensional vector and the
     transformed high dimensional vector.
 
-    Parameters
-    ----------
-        V: :class:`~numpy.ndarray`
-            The n x k array of vectors to be transformed, where n is the number of vectors,
+    Args:
+        V: The n x k array of vectors to be transformed, where n is the number of vectors,
             k the dimension.
-        Q: :class:`~numpy.ndarray`
-            PCA loading matrix with dimension d x k, where d is the dimension of the original space,
+        Q: PCA loading matrix with dimension d x k, where d is the dimension of the original space,
             and k the number of leading PCs.
 
-    Returns
-    -------
-        ret: :class:`~numpy.ndarray`
-            The array of transformed vectors.
-
+    Returns:
+        ret: The array of transformed vectors.
     """
     return V @ Q.T
 
 
-def vector_field_function_transformation(vf_func, Q, func_inv_x):
+def vector_field_function_transformation(vf_func: Callable, Q: np.ndarray, func_inv_x: Callable) -> Callable:
     """Transform vector field function from PCA space to the original space.
     The formula used for transformation:
                                             :math:`\hat{f} = f Q^T`,
     where `Q, f, \hat{f}` are the PCA loading matrix, low dimensional vector field function and the
     transformed high dimensional vector field function.
 
-    Parameters
-    ----------
-        vf_func: callable
-            The vector field function.
-        Q: :class:`~numpy.ndarray`
-            PCA loading matrix with dimension d x k, where d is the dimension of the original space,
+    Args:
+        vf_func: The vector field function.
+        Q: PCA loading matrix with dimension d x k, where d is the dimension of the original space,
             and k the number of leading PCs.
-        func_inv_x: callable
-            The function that transform x back into the PCA space.
+        func_inv_x: The function that transform x back into the PCA space.
 
-    Returns
-    -------
-        ret: callable
-            The transformed vector field function.
+    Returns:
+        The transformed vector field function.
 
     """
     return lambda x: vf_func(func_inv_x(x)) @ Q.T
 
 
 # ---------------------------------------------------------------------------------------------------
 # jacobian
-def Jacobian_rkhs_gaussian(x, vf_dict, vectorize=False):
+def Jacobian_rkhs_gaussian(x: np.ndarray, vf_dict: VecFldDict, vectorize: bool = False) -> np.ndarray:
     """analytical Jacobian for RKHS vector field functions with Gaussian kernel.
 
-    Arguments
-    ---------
-    x: :class:`~numpy.ndarray`
-        Coordinates where the Jacobian is evaluated.
-    vf_dict: dict
-        A dictionary containing RKHS vector field control points, Gaussian bandwidth,
+    Args:
+    x: Coordinates where the Jacobian is evaluated.
+    vf_dict: A dictionary containing RKHS vector field control points, Gaussian bandwidth,
         and RKHS coefficients.
         Essential keys: 'X_ctrl', 'beta', 'C'
 
-    Returns
-    -------
-    J: :class:`~numpy.ndarray`
+    Returns:
         Jacobian matrices stored as d-by-d-by-n numpy arrays evaluated at x.
-        d is the number of dimensions and n the number of coordinates in x.
+            d is the number of dimensions and n the number of coordinates in x.
     """
     if x.ndim == 1:
         K, D = con_K(x[None, :], vf_dict["X_ctrl"], vf_dict["beta"], return_d=True)
         J = (vf_dict["C"].T * K) @ D[0].T
     elif not vectorize:
         n, d = x.shape
         J = np.zeros((d, d, n))
@@ -338,15 +375,15 @@
         if K.ndim == 1:
             K = K[None, :]
         J = np.einsum("nm, mi, njm -> ijn", K, vf_dict["C"], D)
 
     return -2 * vf_dict["beta"] * J
 
 
-def Jacobian_rkhs_gaussian_parallel(x, vf_dict, cores=None):
+def Jacobian_rkhs_gaussian_parallel(x: np.ndarray, vf_dict: VecFldDict, cores: Optional[int] = None) -> np.ndarray:
     n = len(x)
     if cores is None:
         cores = mp.cpu_count()
     n_j_per_core = int(np.ceil(n / cores))
     xx = []
     for i in range(0, n, n_j_per_core):
         xx.append(x[i : i + n_j_per_core])
@@ -355,15 +392,15 @@
     with ThreadPool(cores) as p:
         ret = p.starmap(Jacobian_rkhs_gaussian, zip(xx, itertools.repeat(vf_dict)))
     ret = [np.transpose(r, axes=(2, 0, 1)) for r in ret]
     ret = np.transpose(np.vstack(ret), axes=(1, 2, 0))
     return ret
 
 
-def Jacobian_numerical(f: Callable, input_vector_convention: str = "row"):
+def Jacobian_numerical(f: Callable, input_vector_convention: str = "row") -> Union[Callable, nd.Jacobian]:
     """
     Get the numerical Jacobian of the vector field function.
     If the input_vector_convention is 'row', it means that fjac takes row vectors
     as input, otherwise the input should be an array of column vectors. Note that
     the returned Jacobian would behave exactly the same if the input is an 1d array.
 
     The column vector convention is slightly faster than the row vector convention.
@@ -386,63 +423,55 @@
 
         return f_aux
     else:
         return fjac
 
 
 @timeit
-def elementwise_jacobian_transformation(Js, qi, qj):
+def elementwise_jacobian_transformation(Js: np.ndarray, qi: np.ndarray, qj: np.ndarray) -> np.ndarray:
     """Inverse transform low dimensional k x k Jacobian matrix (:math:`\partial F_i / \partial x_j`) back to the
     d-dimensional gene expression space. The formula used to inverse transform Jacobian matrix calculated from
     low dimension (PCs) is:
                                             :math:`Jac = Q J Q^T`,
     where `Q, J, Jac` are the PCA loading matrix, low dimensional Jacobian matrix and the inverse transformed high
     dimensional Jacobian matrix. This function takes only one row from Q to form qi or qj.
 
-    Parameters
-    ----------
-        Js: :class:`~numpy.ndarray`
-            k x k x n matrices of n k-by-k Jacobians.
-        qi: :class:`~numpy.ndarray`
-            The i-th row of the PC loading matrix Q with dimension d x k, corresponding to the effector gene i.
-        qj: :class:`~numpy.ndarray`
-            The j-th row of the PC loading matrix Q with dimension d x k, corresponding to the regulator gene j.
-
-    Returns
-    -------
-        ret: :class:`~numpy.ndarray`
-            The calculated Jacobian elements (:math:`\partial F_i / \partial x_j`) for each cell.
+    Args:
+        Js: k x k x n matrices of n k-by-k Jacobians.
+        qi: The i-th row of the PC loading matrix Q with dimension d x k, corresponding to the effector gene i.
+        qj: The j-th row of the PC loading matrix Q with dimension d x k, corresponding to the regulator gene j.
+
+    Returns:
+        The calculated Jacobian elements (:math:`\partial F_i / \partial x_j`) for each cell.
     """
 
     Js = np.atleast_3d(Js)
     n = Js.shape[2]
     ret = np.zeros(n)
     for i in tqdm(range(n), "calculating Jacobian for each cell"):
         ret[i] = qi @ Js[:, :, i] @ qj
 
     return ret
 
 
-def Jacobian_kovf(x, fjac_base, K, Q, exact=False, mu=None):
+def Jacobian_kovf(
+    x: np.ndarray, fjac_base: Callable, K: np.ndarray, Q: np.ndarray, exact: bool = False, mu: Optional[float] = None
+) -> np.ndarray:
     """analytical Jacobian for RKHS vector field functions with Gaussian kernel.
 
-    Arguments
-    ---------
-    x: :class:`~numpy.ndarray`
-        Coordinates where the Jacobian is evaluated.
-    vf_dict: dict
-        A dictionary containing RKHS vector field control points, Gaussian bandwidth,
-        and RKHS coefficients.
-        Essential keys: 'X_ctrl', 'beta', 'C'
+    Args:
+        x: Coordinates where the Jacobian is evaluated.
+        vf_dict:
+            A dictionary containing RKHS vector field control points, Gaussian bandwidth,
+            and RKHS coefficients.
+            Essential keys: 'X_ctrl', 'beta', 'C'
 
-    Returns
-    -------
-    J: :class:`~numpy.ndarray`
+    Returns:
         Jacobian matrices stored as d-by-d-by-n numpy arrays evaluated at x.
-        d is the number of dimensions and n the number of coordinates in x.
+            d is the number of dimensions and n the number of coordinates in x.
     """
     if K.ndim == 1:
         K = np.diag(K)
 
     if exact:
         if mu is None:
             raise Exception("For exact calculations of the Jacobian, the mean of the PCA transformation is needed.")
@@ -461,43 +490,32 @@
         if x.ndim > 1:
             G = np.repeat(G[:, :, None], x.shape[0], axis=2)
 
     return fjac_base(x) - G
 
 
 @timeit
-def subset_jacobian_transformation(Js, Qi, Qj, cores=1):
+def subset_jacobian_transformation(Js: np.ndarray, Qi: np.ndarray, Qj: np.ndarray, cores: int = 1) -> np.ndarray:
     """Transform Jacobian matrix (:math:`\partial F_i / \partial x_j`) from PCA space to the original space.
     The formula used for transformation:
                                             :math:`\hat{J} = Q J Q^T`,
     where `Q, J, \hat{J}` are the PCA loading matrix, low dimensional Jacobian matrix and the inverse transformed high
     dimensional Jacobian matrix. This function takes multiple rows from Q to form Qi or Qj.
 
-    Parameters
-    ----------
-        fjac: callable
-            The function for calculating numerical Jacobian matrix.
-        X: :class:`~numpy.ndarray`
-            The samples coordinates with dimension n_obs x n_PCs, from which Jacobian will be calculated.
-        Qi: :class:`~numpy.ndarray`
-            PCA loading matrix with dimension n' x n_PCs of the effector genes, from which local dimension Jacobian matrix (k x k)
+    Args:
+        Js: Original (k x k) dimension Jacobian matrix
+        Qi: PCA loading matrix with dimension n' x n_PCs of the effector genes, from which local dimension Jacobian matrix (k x k)
             will be inverse transformed back to high dimension.
-        Qj: :class:`~numpy.ndarray`
-            PCs loading matrix with dimension n' x n_PCs of the regulator genes, from which local dimension Jacobian matrix (k x k)
+        Qj: PCs loading matrix with dimension n' x n_PCs of the regulator genes, from which local dimension Jacobian matrix (k x k)
             will be inverse transformed back to high dimension.
-        cores: int (default: 1):
-            Number of cores to calculate Jacobian. If cores is set to be > 1, multiprocessing will be used to
+        cores: Number of cores to calculate Jacobian. If cores is set to be > 1, multiprocessing will be used to
             parallel the Jacobian calculation.
-        return_J: bool (default: False)
-            Whether to return the raw tensor of Jacobian matrix of each cell before transformation.
 
-    Returns
-    -------
-        ret: :class:`~numpy.ndarray`
-            The calculated Jacobian matrix (n_gene x n_gene x n_obs) for each cell.
+    Returns:
+        The calculated Jacobian matrix (n_gene x n_gene x n_obs) for each cell.
     """
 
     Js = np.atleast_3d(Js)
     Qi = np.atleast_2d(Qi)
     Qj = np.atleast_2d(Qj)
     d1, d2, n = Qi.shape[0], Qj.shape[0], Js.shape[2]
 
@@ -519,31 +537,38 @@
             )
         ret = [np.transpose(r, axes=(2, 0, 1)) for r in ret]
         ret = np.transpose(np.vstack(ret), axes=(1, 2, 0))
 
     return ret
 
 
-def transform_jacobian(Js, Qi, Qj, pbar=False):
+def transform_jacobian(Js: np.ndarray, Qi: np.ndarray, Qj: np.ndarray, pbar=False) -> np.ndarray:
     d1, d2, n = Qi.shape[0], Qj.shape[0], Js.shape[2]
     ret = np.zeros((d1, d2, n), dtype=np.float32)
     if pbar:
         iterj = tqdm(range(n), desc="Transforming subset Jacobian")
     else:
         iterj = range(n)
     for i in iterj:
         J = Js[:, :, i]
         ret[:, :, i] = Qi @ J @ Qj.T
     return ret
 
 
-def average_jacobian_by_group(Js, group_labels):
+def average_jacobian_by_group(Js: np.ndarray, group_labels: List[str]) -> Dict[str, np.ndarray]:
     """
     Returns a dictionary of averaged jacobians with group names as the keys.
     No vectorized indexing was used due to its high memory cost.
+
+    Args:
+        Js: List of Jacobian matrices
+        group_labels: list of group labels
+
+    Returns:
+        dictionary with group labels as keys and average Jacobians as values
     """
     groups = np.unique(group_labels)
 
     J_mean = {}
     N = {}
     for i, g in enumerate(group_labels):
         if g in J_mean.keys():
@@ -557,31 +582,26 @@
     return J_mean
 
 
 # ---------------------------------------------------------------------------------------------------
 # Hessian
 
 
-def Hessian_rkhs_gaussian(x, vf_dict):
+def Hessian_rkhs_gaussian(x: np.ndarray, vf_dict: VecFldDict) -> np.ndarray:
     """analytical Hessian for RKHS vector field functions with Gaussian kernel.
 
-    Arguments
-    ---------
-    x: :class:`~numpy.ndarray`
-        Coordinates where the Hessian is evaluated. Note that x has to be 1D.
-    vf_dict: dict
-        A dictionary containing RKHS vector field control points, Gaussian bandwidth,
-        and RKHS coefficients.
-        Essential keys: 'X_ctrl', 'beta', 'C'
-
-    Returns
-    -------
-    H: :class:`~numpy.ndarray`
-        Hessian matrix stored as d-by-d-by-d numpy arrays evaluated at x.
-        d is the number of dimensions.
+    Args:
+        x: Coordinates where the Hessian is evaluated. Note that x has to be 1D.
+        vf_dict: A dictionary containing RKHS vector field control points, Gaussian bandwidth,
+            and RKHS coefficients.
+            Essential keys: 'X_ctrl', 'beta', 'C'
+
+    Returns:
+        H: Hessian matrix stored as d-by-d-by-d numpy arrays evaluated at x.
+            d is the number of dimensions.
     """
     x = np.atleast_2d(x)
 
     C = vf_dict["C"]
     beta = vf_dict["beta"]
     K, D = con_K(x, vf_dict["X_ctrl"], beta, return_d=True)
 
@@ -591,131 +611,149 @@
     D = np.eye(x.shape[1]) - 2 * beta * D @ np.transpose(D, axes=(0, 2, 1))
 
     H = -2 * beta * np.einsum("ij, jlm -> ilm", K, D)
 
     return H
 
 
-def hessian_transformation(H, qi, Qj, Qk):
+def hessian_transformation(H: np.ndarray, qi: np.ndarray, Qj: np.ndarray, Qk: np.ndarray) -> np.ndarray:
     """Inverse transform low dimensional k x k x k Hessian matrix (:math:`\partial^2 F_i / \partial x_j \partial x_k`)
     back to the d-dimensional gene expression space. The formula used to inverse transform Hessian matrix calculated
     from low dimension (PCs) is:
                                             :math:`h = \sum_i\sum_j\sum_k q_i q_j q_k H_ijk`,
     where `q, H, h` are the PCA loading matrix, low dimensional Hessian matrix and the inverse transformed element from
     the high dimensional Hessian matrix.
 
-    Parameters
-    ----------
-        H: :class:`~numpy.ndarray`
-            k x k x k matrix of the Hessian.
-        qi: :class:`~numpy.ndarray`
-            The i-th row of the PC loading matrix Q with dimension d x k, corresponding to the effector i.
-        Qj: :class:`~numpy.ndarray`
-            The submatrix of the PC loading matrix Q with dimension d x k, corresponding to regulators j.
-        Qk: :class:`~numpy.ndarray`
-            The submatrix of the PC loading matrix Q with dimension d x k, corresponding to co-regulators k.
-
-    Returns
-    -------
-        h: :class:`~numpy.ndarray`
-            The calculated Hessian matrix for the effector i w.r.t regulators j and co-regulators k.
+    Args:
+        H: k x k x k matrix of the Hessian.
+        qi: The i-th row of the PC loading matrix Q with dimension d x k, corresponding to the effector i.
+        Qj: The submatrix of the PC loading matrix Q with dimension d x k, corresponding to regulators j.
+        Qk: The submatrix of the PC loading matrix Q with dimension d x k, corresponding to co-regulators k.
+
+    Returns:
+        h: The calculated Hessian matrix for the effector i w.r.t regulators j and co-regulators k.
     """
 
     h = np.einsum("ijk, di -> djk", H, qi)
     Qj, Qk = np.atleast_2d(Qj), np.atleast_2d(Qk)
     h = Qj @ h @ Qk.T
 
     return h
 
 
-def elementwise_hessian_transformation(H, qi, qj, qk):
+def elementwise_hessian_transformation(H: np.ndarray, qi: np.ndarray, qj: np.ndarray, qk: np.ndarray) -> np.ndarray:
     """Inverse transform low dimensional k x k x k Hessian matrix (:math:`\partial^2 F_i / \partial x_j \partial x_k`) back to the
     d-dimensional gene expression space. The formula used to inverse transform Hessian matrix calculated from
     low dimension (PCs) is:
                                             :math:`Jac = Q J Q^T`,
     where `Q, J, Jac` are the PCA loading matrix, low dimensional Jacobian matrix and the inverse transformed high
     dimensional Jacobian matrix. This function takes only one row from Q to form qi or qj.
-    Parameters
-    ----------
-        H: :class:`~numpy.ndarray`
-            k x k x k matrix of the Hessian.
-        qi: :class:`~numpy.ndarray`
-            The i-th row of the PC loading matrix Q with dimension d x k, corresponding to the effector i.
-        qj: :class:`~numpy.ndarray`
-            The j-th row of the PC loading matrix Q with dimension d x k, corresponding to the regulator j.
-        qk: :class:`~numpy.ndarray`
-            The k-th row of the PC loading matrix Q with dimension d x k, corresponding to the co-regulator k.
-    Returns
-    -------
-        h: :class:`~numpy.ndarray`
-            The calculated Hessian elements for each cell.
+
+    Args:
+        H: k x k x k matrix of the Hessian.
+        qi: The i-th row of the PC loading matrix Q with dimension d x k, corresponding to the effector i.
+        qj: The j-th row of the PC loading matrix Q with dimension d x k, corresponding to the regulator j.
+        qk: The k-th row of the PC loading matrix Q with dimension d x k, corresponding to the co-regulator k.
+
+    Returns:
+        h: The calculated Hessian elements for each cell.
     """
 
     h = np.einsum("ijk, i -> jk", H, qi)
     h = qj @ h @ qk
 
     return h
 
 
 # ---------------------------------------------------------------------------------------------------
-def Laplacian(H):
+def Laplacian(H: np.ndarray) -> np.ndarray:
+    """
+    Computes the Laplacian of the Hessian matrix by summing the diagonal elements of the Hessian matrix (summing the unmixed second partial derivatives)
+                                            :math: `\Delta f = \sum_{i=1}^{n} \frac{\partial^2 f}{\partial x_i^2}`
+    Args:
+        H: Hessian matrix
+    """
+    # when H has four dimensions, H is calculated across all cells
     if H.ndim == 4:
         L = np.zeros([H.shape[2], H.shape[3]])
-        for sample_indx, i in enumerate(H):
+        for sample_indx in range(H.shape[3]):
             for out_indx in range(L.shape[0]):
-                L[out_indx, sample_indx] = np.diag(i[:, :, out_indx, sample_indx]).sum()
+                L[out_indx, sample_indx] = np.diag(H[:, :, out_indx, sample_indx]).sum()
     else:
+        # when H has three dimensions, H is calculated only on one single cell
         L = np.zeros([H.shape[2], 1])
         for out_indx in range(L.shape[0]):
             L[out_indx, 0] = np.diag(H[:, :, out_indx]).sum()
 
+    return L
+
 
 # ---------------------------------------------------------------------------------------------------
 # dynamical properties
-def _divergence(f, x):
+def _divergence(f: Callable, x: np.ndarray) -> float:
     """Divergence of the reconstructed vector field function f evaluated at x"""
     jac = nd.Jacobian(f)(x)
     return np.trace(jac)
 
 
 @timeit
-def compute_divergence(f_jac, X, Js=None, vectorize_size=1000):
+def compute_divergence(
+    f_jac: Callable, X: np.ndarray, Js: Optional[np.ndarray] = None, vectorize_size: int = 1000
+) -> np.ndarray:
     """Calculate divergence for many samples by taking the trace of a Jacobian matrix.
 
     vectorize_size is used to control the number of samples computed in each vectorized batch.
         If vectorize_size = 1, there's no vectorization whatsoever.
         If vectorize_size = None, all samples are vectorized.
+
+    Args:
+        f_jac: function for calculating Jacobian from cell states
+        X: cell states
+        Js: Jacobian matrices for each sample, if X is not provided
+        vectorize_size: number of Jacobian matrices to process at once in the vectorization
+
+    Returns:
+        divergence np.ndarray across Jacobians for many samples
     """
     n = len(X)
     if vectorize_size is None:
         vectorize_size = n
 
     div = np.zeros(n)
     for i in tqdm(range(0, n, vectorize_size), desc="Calculating divergence"):
         J = f_jac(X[i : i + vectorize_size]) if Js is None else Js[:, :, i : i + vectorize_size]
         div[i : i + vectorize_size] = np.trace(J)
     return div
 
 
-def acceleration_(v, J):
+def acceleration_(v: np.ndarray, J: np.ndarray) -> np.ndarray:
+    """Calculate acceleration by dotting the Jacobian and the velocity vector.
+
+    Args:
+        v: velocity vector
+        J: Jacobian matrix
+
+    Returns:
+        Acceleration vector, with one element for the acceleration of each component
+    """
     if v.ndim == 1:
         v = v[:, None]
     return J.dot(v)
 
 
-def curvature_method1(a: np.array, v: np.array):
+def curvature_method1(a: np.array, v: np.array) -> float:
     """https://link.springer.com/article/10.1007/s12650-018-0474-6"""
     if v.ndim == 1:
         v = v[:, None]
     kappa = np.linalg.norm(np.outer(v, a)) / np.linalg.norm(v) ** 3
 
     return kappa
 
 
-def curvature_method2(a: np.array, v: np.array):
+def curvature_method2(a: np.array, v: np.array) -> float:
     """https://dl.acm.org/doi/10.5555/319351.319441"""
     # if v.ndim == 1: v = v[:, None]
     kappa = (np.multiply(a, np.dot(v, v)) - np.multiply(v, np.dot(v, a))) / np.linalg.norm(v) ** 4
 
     return kappa
 
 
@@ -874,40 +912,76 @@
         curl[i] = f(None, None, method="analytical", VecFld=None, jac=J)
 
     return curl
 
 
 # ---------------------------------------------------------------------------------------------------
 # ranking related utilies
-def get_metric_gene_in_rank(mat: np.mat, genes: list, neg: bool = False):
+def get_metric_gene_in_rank(mat: np.mat, genes: list, neg: bool = False) -> Tuple[np.ndarray, np.ndarray]:
+    """Calculate ranking of genes based on mean value of matrix.
+
+    Args:
+        mat: A matrix with shape (n, m) where n is the number of samples and m is the number of genes.
+        genes: A list of m gene names.
+        neg: A boolean flag indicating whether the ranking should be in ascending (True) or descending (False) order. Defaults to False (descending order).
+
+    Returns:
+        metric_in_rank: A list of m gene scores, sorted in ascending or descending order depending on the value of neg.
+        genes_in_rank: A list of m gene names, sorted in the same order as metric_in_rank.
+    """
     metric_in_rank = mat.mean(0).A1 if issparse(mat) else mat.mean(0)
     rank = metric_in_rank.argsort() if neg else metric_in_rank.argsort()[::-1]
     metric_in_rank, genes_in_rank = metric_in_rank[rank], genes[rank]
 
     return metric_in_rank, genes_in_rank
 
 
 def get_metric_gene_in_rank_by_group(
     mat: np.mat, genes: list, groups: np.array, selected_group, neg: bool = False
-) -> tuple:
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """Calculate ranking of genes based on mean value of matrix, grouped by selected group.
+
+    Args:
+        mat: A matrix with shape (n, m) where n is the number of samples and m is the number of genes.
+        genes: A list of m gene names.
+        groups: A numpy array with shape (n,) indicating the group membership for each sample.
+        selected_group: The group for which the ranking should be calculated.
+        neg: A boolean flag indicating whether the ranking should be in ascending (True) or descending (False) order. Defaults to False (descending order).
+
+    Returns:
+        gene_wise_metrics: A list of m gene scores, sorted in ascending or descending order depending on the value of neg.
+        group_wise_metrics: A list of m gene scores, calculated as the mean value of mat for samples belonging to the selected group.
+        genes_in_rank: A list of m gene names, sorted in the same order as gene_wise_metrics.
+    """
     mask = groups == selected_group
     if type(mask) == pd.Series:
         mask = mask.values
 
     gene_wise_metrics, group_wise_metrics = (
         mat[mask, :].mean(0).A1 if issparse(mat) else mat[mask, :].mean(0),
         mat[mask, :].mean(0).A1 if issparse(mat) else mat[mask, :].mean(0),
     )
     rank = gene_wise_metrics.argsort() if neg else gene_wise_metrics.argsort()[::-1]
     gene_wise_metrics, genes_in_rank = gene_wise_metrics[rank], genes[rank]
 
     return gene_wise_metrics, group_wise_metrics, genes_in_rank
 
 
-def get_sorted_metric_genes_df(df: pd.DataFrame, genes: list, neg: bool = False) -> tuple:
+def get_sorted_metric_genes_df(df: pd.DataFrame, genes: list, neg: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
+    """Sort metric and gene lists based on values in dataframe.
+
+    Args:
+        df: A dataframe with shape (m, n) where m is the number of genes and n is the number of samples.
+        genes: A list of m gene names.
+        neg: A boolean flag indicating whether the ranking should be in ascending (True) or descending (False) order. Defaults to False (descending order).
+
+    Returns:
+        sorted_metric: A dataframe with shape (m, n) containing the sorted metric values.
+        sorted_genes: A dataframe with shape (m, n) containing the sorted gene names.
+    """
     sorted_metric = pd.DataFrame(
         {
             key: (sorted(values, reverse=False) if neg else sorted(values, reverse=True))
             for key, values in df.transpose().iterrows()
         }
     )
     sorted_genes = pd.DataFrame(
@@ -915,15 +989,42 @@
             key: (genes[values.argsort()] if neg else genes[values.argsort()[::-1]])
             for key, values in df.transpose().iterrows()
         }
     )
     return sorted_metric, sorted_genes
 
 
-def rank_vector_calculus_metrics(mat: np.mat, genes: list, group, groups: list, uniq_group: list) -> tuple:
+def rank_vector_calculus_metrics(mat: np.mat, genes: list, group, groups: list, uniq_group: list) -> Tuple:
+    """Calculate ranking of genes based on vector calculus metric.
+
+    Args:
+        mat: A matrix with shape (n, m) where n is the number of samples and m is the number of genes.
+        genes: A list of m gene names.
+        group: The group for which the ranking should be calculated. If group is None, the ranking is calculated for all samples.
+        groups: A list of n group labels, indicating the group membership for each sample.
+        uniq_group: A list of unique group labels.
+
+    Returns:
+        If group is None:
+            metric_in_rank: A list of m gene scores, sorted by the mean of the the absolute values of the elements in each column of mat.
+            genes_in_rank: A list of m gene names, sorted in the same order as metric_in_rank.
+            pos_metric_in_rank: A list of m gene scores, sorted by the mean of the positive elements in each column of mat.
+            pos_genes_in_rank: A list of m gene names, sorted in the same order as pos_metric_in_rank.
+            neg_metric_in_rank: A list of m gene scores, calculated as the mean of the negative elements in each column of mat.
+            neg_genes_in_rank: A list of m gene names, sorted in the same order as neg_metric_in_rank.
+        If group is not None:
+            gene_wise_metrics: A dictionary with keys equal to the unique group labels in uniq_group, and values equal to lists of m gene scores, calculated as the mean value of mat for samples belonging to each group.
+            group_wise_metrics: A dictionary with keys equal to the unique group labels in uniq_group, and values equal to lists of m gene scores, calculated as the mean value of mat for samples belonging to each group.
+            gene_wise_genes: A dictionary with keys equal to the unique group labels in uniq_group, and values equal to lists of m gene names, sorted in the same order as the corresponding values in gene_wise_metrics.
+            gene_wise_pos_metrics: A dictionary with keys equal to the unique group labels in uniq_group, and values equal to lists of m gene scores, calculated as the mean value of the positive elements in mat for samples belonging to each group.
+            group_wise_pos_metrics: A dictionary with keys equal to the unique group labels in uniq_group, and values equal to lists of m gene scores, calculated as the mean value of the positive elements in mat for samples belonging to each group.
+            gene_wise_pos_genes: A dictionary with keys equal to the unique group labels in uniq_group, and values equal to lists of m gene names, sorted in the same order as the corresponding values in gene_wise_pos_metrics.
+            gene_wise_neg_metrics: A dictionary with keys equal to the unique group labels in uniq_group, and values equal to lists of m gene scores, calculated as the mean value of the negative elements in mat for samples belonging to each group.
+            group_wise_neg_metrics: A dictionary with keys equal to the unique labels in uniq_group, and values equal to lists of m gene scores, calculated as the mean value of the negative elements in mat for samples belonging to each group.
+    """
     main_info("split mat to a positive matrix and a negative matrix.")
     if issparse(mat):
         mask = mat.data > 0
         pos_mat, neg_mat = mat.copy(), mat.copy()
         pos_mat.data[~mask], neg_mat.data[mask] = 0, 0
         pos_mat.eliminate_zeros()
         neg_mat.eliminate_zeros()
@@ -1110,18 +1211,32 @@
     else:
         return X
 
 
 def find_fixed_points(
     x0_list: Union[list, np.ndarray],
     func_vf: Callable,
-    domain=None,
+    domain: Optional[np.ndarray] = None,
     tol_redundant: float = 1e-4,
     return_all: bool = False,
-) -> tuple:
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """Given sampling points, a function, and a domain, finds points for which func_vf(x) = 0.
+
+    Args:
+        x0_list: Array-like structure with sampling points
+        func_vf: Function for which to find fixed points
+        domain: Finds fixed points within the given domain of shape (n_dim, 2)
+        tol_redundant: Margin outside of which points are considered distinct
+        return_all: If set to true, always return a tuple of three arrays as output
+
+    Returns:
+        A tuple with the solutions, Jacobian matrix, and function values at the solutions.
+
+    """
+
     def vf_aux(x):
         """auxillary function unifying dimensionality"""
         v = func_vf(x)
         if x.ndim == 1:
             v = v.flatten()
         return v
 
@@ -1201,27 +1316,21 @@
 def parse_int_df(
     df: pd.DataFrame,
     self_int: bool = False,
     genes: bool = None,
 ) -> pd.DataFrame:
     """parse the dataframe produced from vector field ranking for gene interactions or switch gene pairs
 
-    Parameters
-    ----------
-    df:
-        The dataframe that returned from performing the `int` or `switch` mode ranking via dyn.vf.rank_jacobian_genes.
-    self_int:
-        Whether to keep self-interactions pairs.
-    genes:
-        List of genes that are used to filter for gene interactions.
-
-    Returns
-    -------
-    res:
-        The parsed interaction dataframe.
+    Args:
+        df: The dataframe that returned from performing the `int` or `switch` mode ranking via dyn.vf.rank_jacobian_genes.
+        self_int: Whether to keep self-interactions pairs.
+        genes: List of genes that are used to filter for gene interactions.
+
+    Returns:
+        res: The parsed interaction dataframe.
     """
 
     df_shape, columns = df.shape, df.columns
     # first we have second column name ends with "_values", it means the data frame include ranking values.
     if columns[1].endswith("_values"):
         col_step = 2
     else:
@@ -1250,20 +1359,32 @@
 
     return pd.DataFrame(res)
 
 
 # ---------------------------------------------------------------------------------------------------
 # jacobian retrival related utilies
 def get_jacobian(
-    adata,
-    regulators,
-    effectors,
+    adata: AnnData,
+    regulators: np.ndarray,
+    effectors: np.ndarray,
     jkey: str = "jacobian",
     j_basis: str = "pca",
-):
+) -> pd.DataFrame:
+    """Return dataframe with Jacobian values (where regulators are in the denominator and effectors in the numerator)
+
+    Args:
+        adata: AnnData object
+        regulators: string labels capturing regulator genes of interest
+        effectors: string labels capturing effector genes of interest
+        jkey: Defaults to "jacobian".
+        j_basis: Defaults to "pca".
+
+    Returns:
+        dataframe with Jacobian values (where regulators are in the denominator and effectors in the numerator)
+    """
 
     regulators, effectors = (
         list(np.unique(regulators)) if regulators is not None else None,
         list(np.unique(effectors)) if effectors is not None else None,
     )
 
     Jacobian_ = jkey if j_basis is None else jkey + "_" + j_basis
@@ -1306,16 +1427,25 @@
             df.loc[:, key] = J
 
     return df
 
 
 # ---------------------------------------------------------------------------------------------------
 # jacobian subset related utilies
-def subset_jacobian(adata, cells, basis="pca"):
-    """Subset adata object while also subset the jacobian, cells must be a vector of cell indices."""
+def subset_jacobian(adata: AnnData, cells: np.ndarray, basis: str = "pca"):
+    """Subset adata object while also subset the jacobian
+
+    Args:
+        adata: AnnData object
+        cells: vector of cell indices
+        basis: string specifying jacobian layer to subset
+
+    Returns:
+        subset of adata
+    """
 
     adata_subset = adata[cells]
 
     jkey = "jacobian_" + basis
     adata_subset.uns[jkey].keys()
 
     # assume all cells are used to calculate Jacobian for now
```

### Comparing `dynamo-release-1.2.0/dynamo/vectorfield/vfGraph_deprecated.py` & `dynamo-release-1.3.0/dynamo/vectorfield/vfGraph_deprecated.py`

 * *Files identical despite different names*

### Comparing `dynamo-release-1.2.0/dynamo_release.egg-info/PKG-INFO` & `dynamo-release-1.3.0/dynamo_release.egg-info/PKG-INFO`

 * *Files 7% similar despite different names*

```diff
@@ -1,40 +1,42 @@
 Metadata-Version: 2.1
 Name: dynamo-release
-Version: 1.2.0
+Version: 1.3.0
 Summary: Mapping Vector Field of Single Cells
 Home-page: https://github.com/aristoteleo/dynamo-release
 Author: Xiaojie Qiu, Yan Zhang, Ke Ni
 Author-email: xqiu.sc@gmail.com
 License: BSD
 Download-URL: https://github.com/aristoteleo/dynamo-release
 Description: <p align="center">
           <img height="150" src="https://dynamo-release.readthedocs.io/en/latest/_static/logo_with_word.png" />
         </p>
         
         ##
         
-        [![package](https://github.com/aristoteleo/dynamo-release/workflows/Python%20package/badge.svg)](https://github.com/aristoteleo/dynamo-release/runs/950435412) 
-        [![upload](https://github.com/aristoteleo/dynamo-release/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/dynamo-release/) 
+        [![package](https://github.com/aristoteleo/dynamo-release/workflows/Python%20package/badge.svg)](https://github.com/aristoteleo/dynamo-release)
         [![documentation](https://readthedocs.org/projects/dynamo-release/badge/?version=latest)](https://dynamo-release.readthedocs.io/en/latest/)
+        [![upload](https://img.shields.io/pypi/v/dynamo-release?logo=PyPI)](https://pypi.org/project/dynamo-release/) 
+        [![download](https://static.pepy.tech/badge/dynamo-release)](https://pepy.tech/project/dynamo-release)
+        [![star](https://img.shields.io/github/stars/aristoteleo/dynamo-release?logo=GitHub&color=red)](https://github.com/aristoteleo/dynamo-release/stargazers)
         ![build](https://github.com/aristoteleo/dynamo-release/actions/workflows/python-package.yml/badge.svg)
         ![test](https://github.com/aristoteleo/dynamo-release/actions/workflows/python-plain-run-test.yml/badge.svg)
         
-        
         ## **Dynamo**: Mapping Transcriptomic Vector Fields of Single Cells
         
         Inclusive model of expression dynamics with metabolic labeling based scRNA-seq / multiomics, vector field reconstruction, potential landscape mapping, differential geometry analyses, and most probably paths / *in silico* perturbation predictions.
         
-        [Installation](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html#how-to-install) - [Ten minutes to dynamo](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html) - [Tutorials](https://dynamo-release.readthedocs.io/en/latest/notebooks/Differential_geometry.html) - [API](https://dynamo-release.readthedocs.io/en/latest/API.html) - [Citation](https://www.biorxiv.org/content/10.1101/696724v2) - [Theory](https://dynamo-release.readthedocs.io/en/latest/notebooks/Primer.html)
+        [Installation](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html#how-to-install) - [Ten minutes to dynamo](https://dynamo-release.readthedocs.io/en/latest/ten_minutes_to_dynamo.html) - [Tutorials](https://dynamo-release.readthedocs.io/en/latest/notebooks/Differential_geometry.html) - [API](https://dynamo-release.readthedocs.io/en/latest/API.html) - [Citation](https://www.sciencedirect.com/science/article/pii/S0092867421015774?via%3Dihub) - [Theory](https://dynamo-release.readthedocs.io/en/latest/notebooks/Primer.html)
         
         ![Dynamo](https://user-images.githubusercontent.com/7456281/152110270-7ee1b0ed-1205-495d-9d65-59c7984d2fa2.png)
         
         Single-cell (sc)RNA-seq, together with RNA velocity and metabolic labeling, reveals cellular states and transitions at unprecedented resolution. Fully exploiting these data, however, requires kinetic models capable of unveiling governing regulatory functions. Here, we introduce an analytical framework dynamo, which infers absolute RNA velocity, reconstructs continuous vector fields that predict cell fates, employs differential geometry to extract underlying regulations, and ultimately predicts optimal reprogramming paths and perturbation outcomes. We highlight dynamo’s power to overcome fundamental limitations of conventional splicing-based RNA velocity analyses to enable accurate velocity estimations on a metabolically labeled human hematopoiesis scRNA-seq dataset. Furthermore, differential geometry analyses reveal mechanisms driving early megakaryocyte appearance and elucidate asymmetrical regulation within the PU.1-GATA1 circuit. Leveraging the least-action-path method, dynamo accurately predicts drivers of numerous hematopoietic transitions. Finally, in silico perturbations predict cell-fate diversions induced by gene perturbations. Dynamo, thus, represents an important step in advancing quantitative and predictive theories of cell-state transitions.
         
         ## Highlights of dynamo
+        
         * Robust and accurate estimation of RNA velocities for regular scRNA-seq datasets:
             * Three methods for the velocity estimations (including the new negative binomial distribution based approach)
             * Improved kernels for transition matrix calculation and velocity projection 
             * Strategies to correct RNA velocity vectors (when your RNA velocity direction is problematic) 
         * Inclusive modeling of time-resolved metabolic labeling based scRNA-seq:
             * Overcome intrinsic limitation of the conventional splicing based RNA velocity analyses
             * Explicitly model RNA metabolic labeling, in conjunction with RNA bursting, transcription, splicing and degradation
@@ -44,27 +46,31 @@
             * Calculate RNA acceleration (reveals early drivers), curvature (reveals master regulators of fate decision points), divergence (stability of cell states) and RNA Jacobian (cell-state dependent regulatory networks) 
             * Various downstream differential geometry analyses to rank critical regulators/effectors,  and visualize regulatory networks at key fate decision points    
         * Non-trivial vector field predictions of cell fate transitions:
             * Least action path approach to predict the optimal paths and transcription factors of cell fate reprogrammings
             * In silico perturbation to predict the gene-wise perturbation effects and cell fate diversion after genetic perturbations
         
         ## News
-        * 2/1/2022: after 3.5+ years of perseverance, our dynamo paper is finally online in [Cell](https://www.sciencedirect.com/science/article/pii/S0092867421015774#tbl1) today!
-        * 2/15/2022: primers and tutorials on least action paths and in silico perturbation are released.
-        * 3/14/2022: Since today dynamo has its own logo! Here the arrow represents the RNA velocity vector field, while the helix the RNA molecule and the colored dots RNA metabolic labels (4sU labeling). See [readthedocs](https://dynamo-release.readthedocs.io/en/latest/index.html)
-        * 4/14/2022: dynamo 1.1.0 released
+        * 5/30/2023: dynamo 1.3.0 released!
+        * 3/1/2023: We welcome @Sichao25 to join the dynamo develop team!
+        * 1/28/2023: We welcome @Ukyeon to join the dynamo develop team! 
+        * 15/12/2022: *Thanks for @elfofmaxwell and @MukundhMurthy's contribution*. dynamo 1.2.0 released
         * 11/11/2022: the continuing development of dynamo and the Aristotle ecosystem will be supported by CZI. See [here](https://chanzuckerberg.com/eoss/proposals/predictive-modeling-of-single-cell-multiomics-over-time-and-space/)
-        * 15/12/2022: dynamo 1.2.0 released
+        * 4/14/2022: dynamo 1.1.0 released!
+        * 3/14/2022: Since today dynamo has its own logo! Here the arrow represents the RNA velocity vector field, while the helix the RNA molecule and the colored dots RNA metabolic labels (4sU labeling). See [readthedocs](https://dynamo-release.readthedocs.io/en/latest/index.html)
+        * 2/15/2022: primers and tutorials on least action paths and in silico perturbation are released.
+        * 2/1/2022: after 3.5+ years of perseverance, our dynamo paper is finally online in [Cell](https://www.sciencedirect.com/science/article/pii/S0092867421015774#tbl1) today!
         
         ## Discussion 
         Please use github issue tracker to report coding related [issues](https://github.com/aristoteleo/dynamo-release/issues) of dynamo. For community discussion of novel usage cases, analysis tips and biological interpretations of dynamo, please join our public slack workspace: [dynamo-discussion](https://join.slack.com/t/dynamo-discussionhq/shared_invite/zt-itnzjdxs-PV~C3Hr9uOArHZcmv622Kg) (Only a working email address is required from the slack side). 
         
         ## Contribution 
         If you want to contribute to the development of dynamo, please check out CONTRIBUTION instruction: [Contribution](https://github.com/aristoteleo/dynamo-release/blob/master/CONTRIBUTING.md)
         
 Keywords: VectorField,singlecell,velocity,scNT-seq,sci-fate,NASC-seq,scSLAMseq,potential
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
+Provides-Extra: docs
```

### Comparing `dynamo-release-1.2.0/dynamo_release.egg-info/SOURCES.txt` & `dynamo-release-1.3.0/dynamo_release.egg-info/SOURCES.txt`

 * *Files 9% similar despite different names*

```diff
@@ -28,18 +28,16 @@
 dynamo/estimation/tsc/estimation_kinetic.py
 dynamo/estimation/tsc/twostep.py
 dynamo/estimation/tsc/utils_kinetic.py
 dynamo/estimation/tsc/utils_moments.py
 dynamo/external/__init__.py
 dynamo/external/gseapy.py
 dynamo/external/hodge.py
-dynamo/external/pearson_residual_recipe.py
 dynamo/external/scifate.py
 dynamo/external/scribe.py
-dynamo/external/sctransform.py
 dynamo/external/utils.py
 dynamo/movie/__init__.py
 dynamo/movie/fate.py
 dynamo/movie/utils.py
 dynamo/plot/__init__.py
 dynamo/plot/cell_cycle.py
 dynamo/plot/clustering.py
@@ -54,14 +52,15 @@
 dynamo/plot/markers.py
 dynamo/plot/networks.py
 dynamo/plot/preprocess.py
 dynamo/plot/pseudotime.py
 dynamo/plot/scPotential.py
 dynamo/plot/scVectorField.py
 dynamo/plot/scatters.py
+dynamo/plot/sctransform.py
 dynamo/plot/space.py
 dynamo/plot/state_graph.py
 dynamo/plot/streamtube.py
 dynamo/plot/theme.py
 dynamo/plot/time_series.py
 dynamo/plot/topography.py
 dynamo/plot/utils.py
@@ -75,21 +74,28 @@
 dynamo/prediction/state_graph.py
 dynamo/prediction/trajectory.py
 dynamo/prediction/trajectory_analysis.py
 dynamo/prediction/tscRNA_seq.py
 dynamo/prediction/utils.py
 dynamo/preprocessing/CnmfPreprocessor.py
 dynamo/preprocessing/Preprocessor.py
+dynamo/preprocessing/QC.py
 dynamo/preprocessing/__init__.py
 dynamo/preprocessing/cell_cycle.py
+dynamo/preprocessing/deprecated.py
 dynamo/preprocessing/dynast.py
-dynamo/preprocessing/preprocess.py
-dynamo/preprocessing/preprocess_monocle_utils.py
-dynamo/preprocessing/preprocessor_utils.py
+dynamo/preprocessing/gene_selection.py
+dynamo/preprocessing/normalization.py
+dynamo/preprocessing/pca.py
+dynamo/preprocessing/transform.py
 dynamo/preprocessing/utils.py
+dynamo/preprocessing/external/__init__.py
+dynamo/preprocessing/external/integration.py
+dynamo/preprocessing/external/pearson_residual_recipe.py
+dynamo/preprocessing/external/sctransform.py
 dynamo/simulation/Gillespie.py
 dynamo/simulation/ODE.py
 dynamo/simulation/__init__.py
 dynamo/simulation/bif_os_inclusive_sim.py
 dynamo/simulation/evaluation.py
 dynamo/simulation/simulate_anndata.py
 dynamo/simulation/utils.py
@@ -128,14 +134,15 @@
 dynamo/vectorfield/FixedPoints.py
 dynamo/vectorfield/Wang.py
 dynamo/vectorfield/__init__.py
 dynamo/vectorfield/cell_vectors.py
 dynamo/vectorfield/clustering.py
 dynamo/vectorfield/least_action_path.py
 dynamo/vectorfield/networks.py
+dynamo/vectorfield/rank_vf.py
 dynamo/vectorfield/scPotential.py
 dynamo/vectorfield/scVectorField.py
 dynamo/vectorfield/stochastic_process.py
 dynamo/vectorfield/topography.py
 dynamo/vectorfield/util_vector_calculus.py
 dynamo/vectorfield/utils.py
 dynamo/vectorfield/vector_calculus.py
```

### Comparing `dynamo-release-1.2.0/setup.py` & `dynamo-release-1.3.0/setup.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,41 +1,31 @@
 from pathlib import Path
 
 from setuptools import find_packages, setup
-
-# import numpy as np
+import os
 # from version import __version__
 
 
 def read_requirements(path):
     with open(path, "r") as f:
         return [line.strip() for line in f if not line.isspace()]
 
 
 with open("README.md", "r") as fh:
     long_description = fh.read()
 
 if __name__ == "__main__":
     setup(
         name="dynamo-release",
-        version="v1.2.0",
+        version="v1.3.0",
         python_requires=">=3.7",
         install_requires=read_requirements("requirements.txt"),
-        # extras_require={
-        #     "spatial": ["pysal>2.0.0"],
-        #     "interactive_plots": ["plotly"],
-        #     "network": ["networkx", "nxviz", "hiveplotlib"],
-        #     "dimension_reduction": ["fitsne>=1.0.1", "dbmap>=1.1.2"],
-        #     "test": ["sympy>=1.4", "networkx"],
-        #     "bigdata_visualization": [
-        #         "datashader>=0.9.0",
-        #         "bokeh>=1.4.0",
-        #         "holoviews>=1.9.2",
-        #     ],
-        # },
+        extras_require={
+            "docs": read_requirements(os.path.join("docs", "requirements.txt")),
+        },
         packages=find_packages(exclude=("tests", "docs")),
         classifiers=[
             "Programming Language :: Python :: 3",
             "License :: OSI Approved :: BSD License",
             "Operating System :: OS Independent",
         ],
         #     include_dirs=[np.get_include()],
```

