# Comparing `tmp/scikit_recommender-0.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/scikit_recommender-0.0.3-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,70 +1,60 @@
-Zip file size: 2180940 bytes, number of entries: 68
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 scikit_recommender.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 scikit_recommender-0.0.2.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/utils/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/recommender/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/io/
--rw-r--r--  2.0 unx      181 b- defN 23-May-29 03:44 skrec/__init__.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/utils/py/
--rw-r--r--  2.0 unx     1217 b- defN 23-May-29 03:44 skrec/utils/tf1x.py
--rw-r--r--  2.0 unx      121 b- defN 23-May-29 03:44 skrec/utils/__init__.py
--rw-r--r--  2.0 unx     1217 b- defN 23-May-29 03:44 skrec/utils/registry.py
--rw-r--r--  2.0 unx       82 b- defN 23-May-29 03:44 skrec/utils/tf2x.py
--rw-r--r--  2.0 unx     4001 b- defN 23-May-29 03:44 skrec/utils/torch.py
--rw-r--r--  2.0 unx     1222 b- defN 23-May-29 03:44 skrec/utils/common.py
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/utils/py/cython/
--rw-r--r--  2.0 unx     2516 b- defN 23-May-29 03:44 skrec/utils/py/config.py
--rw-r--r--  2.0 unx     6566 b- defN 23-May-29 03:44 skrec/utils/py/batch_iterator.py
--rw-r--r--  2.0 unx     1410 b- defN 23-May-29 03:44 skrec/utils/py/random.py
--rw-r--r--  2.0 unx     8881 b- defN 23-May-29 03:44 skrec/utils/py/evaluator.py
--rw-r--r--  2.0 unx      518 b- defN 23-May-29 03:44 skrec/utils/py/__init__.py
--rw-r--r--  2.0 unx     4748 b- defN 23-May-29 03:44 skrec/utils/py/generic.py
--rw-r--r--  2.0 unx     1627 b- defN 23-May-29 03:44 skrec/utils/py/decorator.py
--rwxr-xr-x  2.0 unx   159064 b- defN 23-May-29 03:44 skrec/utils/py/cython/pyx_utils.cpython-39-x86_64-linux-gnu.so
--rwxr-xr-x  2.0 unx  2927960 b- defN 23-May-29 03:44 skrec/utils/py/cython/pyx_sort.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx      166 b- defN 23-May-29 03:44 skrec/utils/py/cython/__init__.py
--rwxr-xr-x  2.0 unx  1873584 b- defN 23-May-29 03:44 skrec/utils/py/cython/pyx_random.cpython-39-x86_64-linux-gnu.so
--rwxr-xr-x  2.0 unx  1952104 b- defN 23-May-29 03:44 skrec/utils/py/cython/pyx_eval_matrix.cpython-39-x86_64-linux-gnu.so
--rwxr-xr-x  2.0 unx   118544 b- defN 23-May-29 03:44 skrec/utils/py/cython/pyx_init.cpython-39-x86_64-linux-gnu.so
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/recommender/AOBPR/
-drwxr-xr-x  2.0 unx        0 b- stor 23-May-29 03:44 skrec/recommender/BERT4Rec/
--rw-r--r--  2.0 unx     8499 b- defN 23-May-29 03:44 skrec/recommender/MultVAE.py
--rw-r--r--  2.0 unx     1699 b- defN 23-May-29 03:44 skrec/recommender/base.py
--rw-r--r--  2.0 unx     9351 b- defN 23-May-29 03:44 skrec/recommender/Caser.py
--rw-r--r--  2.0 unx     9626 b- defN 23-May-29 03:44 skrec/recommender/CDAE.py
--rw-r--r--  2.0 unx     9425 b- defN 23-May-29 03:44 skrec/recommender/HGN.py
--rw-r--r--  2.0 unx    21235 b- defN 23-May-29 03:44 skrec/recommender/SASRec.py
--rw-r--r--  2.0 unx     9319 b- defN 23-May-29 03:44 skrec/recommender/LightGCN.py
--rw-r--r--  2.0 unx    12555 b- defN 23-May-29 03:44 skrec/recommender/GRU4Rec.py
--rw-r--r--  2.0 unx    15186 b- defN 23-May-29 03:44 skrec/recommender/SRGNN.py
--rw-r--r--  2.0 unx       82 b- defN 23-May-29 03:44 skrec/recommender/__init__.py
--rw-r--r--  2.0 unx    14315 b- defN 23-May-29 03:44 skrec/recommender/GRU4RecPlus.py
--rw-r--r--  2.0 unx     1588 b- defN 23-May-29 03:44 skrec/recommender/Pop.py
--rw-r--r--  2.0 unx     8609 b- defN 23-May-29 03:44 skrec/recommender/CML.py
--rw-r--r--  2.0 unx    15419 b- defN 23-May-29 03:44 skrec/recommender/SGAT.py
--rw-r--r--  2.0 unx     5650 b- defN 23-May-29 03:44 skrec/recommender/BPRMF.py
--rw-r--r--  2.0 unx     6934 b- defN 23-May-29 03:44 skrec/recommender/TransRec.py
--rw-r--r--  2.0 unx     6503 b- defN 23-May-29 03:44 skrec/recommender/FPMC.py
--rwxr-xr-x  2.0 unx   616344 b- defN 23-May-29 03:44 skrec/recommender/AOBPR/pyx_aobpr_func.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx       70 b- defN 23-May-29 03:44 skrec/recommender/AOBPR/__init__.py
--rw-r--r--  2.0 unx     4092 b- defN 23-May-29 03:44 skrec/recommender/AOBPR/AOBPR.py
--rw-r--r--  2.0 unx    40615 b- defN 23-May-29 03:44 skrec/recommender/BERT4Rec/modeling.py
--rw-r--r--  2.0 unx    11532 b- defN 23-May-29 03:44 skrec/recommender/BERT4Rec/bert4rec_utils.py
--rw-r--r--  2.0 unx     7729 b- defN 23-May-29 03:44 skrec/recommender/BERT4Rec/BERT4Rec.py
--rw-r--r--  2.0 unx    17959 b- defN 23-May-29 03:44 skrec/recommender/BERT4Rec/bert4rec_gen_data.py
--rw-r--r--  2.0 unx     2263 b- defN 23-May-29 03:44 skrec/recommender/BERT4Rec/vocab.py
--rw-r--r--  2.0 unx       85 b- defN 23-May-29 03:44 skrec/recommender/BERT4Rec/__init__.py
--rw-r--r--  2.0 unx     6505 b- defN 23-May-29 03:44 skrec/recommender/BERT4Rec/optimization.py
--rw-r--r--  2.0 unx    14302 b- defN 23-May-29 03:44 skrec/io/data_iterator.py
--rw-r--r--  2.0 unx     2539 b- defN 23-May-29 03:44 skrec/io/logger.py
--rw-r--r--  2.0 unx    13420 b- defN 23-May-29 03:44 skrec/io/dataset.py
--rw-r--r--  2.0 unx      526 b- defN 23-May-29 03:44 skrec/io/__init__.py
--rw-r--r--  2.0 unx    12843 b- defN 23-May-29 03:44 skrec/io/preprocessor.py
--rw-r--r--  2.0 unx     1437 b- defN 23-May-29 03:44 skrec/io/movielens.py
--rw-rw-r--  2.0 unx     5202 b- defN 23-May-29 03:44 scikit_recommender-0.0.2.dist-info/RECORD
--rw-r--r--  2.0 unx      148 b- defN 23-May-29 03:44 scikit_recommender-0.0.2.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 23-May-29 03:44 scikit_recommender-0.0.2.dist-info/top_level.txt
--rw-r--r--  2.0 unx        0 b- defN 23-May-29 03:44 scikit_recommender-0.0.2.dist-info/LICENSE
--rw-r--r--  2.0 unx     7169 b- defN 23-May-29 03:44 scikit_recommender-0.0.2.dist-info/METADATA
-68 files, 7986510 bytes uncompressed, 2171594 bytes compressed:  72.8%
+Zip file size: 295273 bytes, number of entries: 58
+-rw-rw-rw-  2.0 fat      192 b- defN 23-May-30 03:05 skrec/__init__.py
+-rw-rw-rw-  2.0 fat      543 b- defN 23-May-30 03:05 skrec/io/__init__.py
+-rw-rw-rw-  2.0 fat    14644 b- defN 23-May-30 03:05 skrec/io/data_iterator.py
+-rw-rw-rw-  2.0 fat    13778 b- defN 23-May-30 03:05 skrec/io/dataset.py
+-rw-rw-rw-  2.0 fat     2623 b- defN 23-May-30 03:05 skrec/io/logger.py
+-rw-rw-rw-  2.0 fat     1483 b- defN 23-May-30 03:05 skrec/io/movielens.py
+-rw-rw-rw-  2.0 fat    13177 b- defN 23-May-30 03:05 skrec/io/preprocessor.py
+-rw-rw-rw-  2.0 fat     5794 b- defN 23-May-30 03:05 skrec/recommender/BPRMF.py
+-rw-rw-rw-  2.0 fat     9858 b- defN 23-May-30 03:05 skrec/recommender/CDAE.py
+-rw-rw-rw-  2.0 fat     8800 b- defN 23-May-30 03:05 skrec/recommender/CML.py
+-rw-rw-rw-  2.0 fat     9587 b- defN 23-May-30 03:05 skrec/recommender/Caser.py
+-rw-rw-rw-  2.0 fat     6660 b- defN 23-May-30 03:05 skrec/recommender/FPMC.py
+-rw-rw-rw-  2.0 fat    12848 b- defN 23-May-30 03:05 skrec/recommender/GRU4Rec.py
+-rw-rw-rw-  2.0 fat    14646 b- defN 23-May-30 03:05 skrec/recommender/GRU4RecPlus.py
+-rw-rw-rw-  2.0 fat     9658 b- defN 23-May-30 03:05 skrec/recommender/HGN.py
+-rw-rw-rw-  2.0 fat     9540 b- defN 23-May-30 03:05 skrec/recommender/LightGCN.py
+-rw-rw-rw-  2.0 fat     8726 b- defN 23-May-30 03:05 skrec/recommender/MultVAE.py
+-rw-rw-rw-  2.0 fat     1635 b- defN 23-May-30 03:05 skrec/recommender/Pop.py
+-rw-rw-rw-  2.0 fat    21741 b- defN 23-May-30 03:05 skrec/recommender/SASRec.py
+-rw-rw-rw-  2.0 fat    15764 b- defN 23-May-30 03:05 skrec/recommender/SGAT.py
+-rw-rw-rw-  2.0 fat    15481 b- defN 23-May-30 03:05 skrec/recommender/SRGNN.py
+-rw-rw-rw-  2.0 fat     7098 b- defN 23-May-30 03:05 skrec/recommender/TransRec.py
+-rw-rw-rw-  2.0 fat       86 b- defN 23-May-30 03:05 skrec/recommender/__init__.py
+-rw-rw-rw-  2.0 fat     1699 b- defN 23-May-30 03:05 skrec/recommender/base.py
+-rw-rw-rw-  2.0 fat     4196 b- defN 23-May-30 03:05 skrec/recommender/AOBPR/AOBPR.py
+-rw-rw-rw-  2.0 fat       73 b- defN 23-May-30 03:05 skrec/recommender/AOBPR/__init__.py
+-rw-rw-rw-  2.0 fat    68608 b- defN 23-May-30 03:06 skrec/recommender/AOBPR/pyx_aobpr_func.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     7915 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/BERT4Rec.py
+-rw-rw-rw-  2.0 fat       88 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/__init__.py
+-rw-rw-rw-  2.0 fat    18467 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/bert4rec_gen_data.py
+-rw-rw-rw-  2.0 fat    11820 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/bert4rec_utils.py
+-rw-rw-rw-  2.0 fat    41607 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/modeling.py
+-rw-rw-rw-  2.0 fat     6676 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/optimization.py
+-rw-rw-rw-  2.0 fat     2336 b- defN 23-May-30 03:05 skrec/recommender/BERT4Rec/vocab.py
+-rw-rw-rw-  2.0 fat      127 b- defN 23-May-30 03:05 skrec/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1261 b- defN 23-May-30 03:05 skrec/utils/common.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 23-May-30 03:05 skrec/utils/registry.py
+-rw-rw-rw-  2.0 fat     1265 b- defN 23-May-30 03:05 skrec/utils/tf1x.py
+-rw-rw-rw-  2.0 fat       86 b- defN 23-May-30 03:05 skrec/utils/tf2x.py
+-rw-rw-rw-  2.0 fat     4122 b- defN 23-May-30 03:05 skrec/utils/torch.py
+-rw-rw-rw-  2.0 fat      538 b- defN 23-May-30 03:05 skrec/utils/py/__init__.py
+-rw-rw-rw-  2.0 fat     6779 b- defN 23-May-30 03:05 skrec/utils/py/batch_iterator.py
+-rw-rw-rw-  2.0 fat     2593 b- defN 23-May-30 03:05 skrec/utils/py/config.py
+-rw-rw-rw-  2.0 fat     1678 b- defN 23-May-30 03:05 skrec/utils/py/decorator.py
+-rw-rw-rw-  2.0 fat     9089 b- defN 23-May-30 03:05 skrec/utils/py/evaluator.py
+-rw-rw-rw-  2.0 fat     4876 b- defN 23-May-30 03:05 skrec/utils/py/generic.py
+-rw-rw-rw-  2.0 fat     1450 b- defN 23-May-30 03:05 skrec/utils/py/random.py
+-rw-rw-rw-  2.0 fat      173 b- defN 23-May-30 03:05 skrec/utils/py/cython/__init__.py
+-rw-rw-rw-  2.0 fat    84992 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_eval_matrix.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    26112 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_init.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat   109568 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_random.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat   115712 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    30720 b- defN 23-May-30 03:06 skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     7326 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     5065 b- defN 23-May-30 03:06 scikit_recommender-0.0.3.dist-info/RECORD
+58 files, 782740 bytes uncompressed, 287215 bytes compressed:  63.3%
```

## zipnote {}

```diff
@@ -1,205 +1,175 @@
-Filename: scikit_recommender.libs/
-Comment: 
-
-Filename: skrec/
-Comment: 
-
-Filename: scikit_recommender-0.0.2.dist-info/
-Comment: 
-
-Filename: skrec/utils/
-Comment: 
-
-Filename: skrec/recommender/
-Comment: 
-
-Filename: skrec/io/
-Comment: 
-
 Filename: skrec/__init__.py
 Comment: 
 
-Filename: skrec/utils/py/
-Comment: 
-
-Filename: skrec/utils/tf1x.py
-Comment: 
-
-Filename: skrec/utils/__init__.py
+Filename: skrec/io/__init__.py
 Comment: 
 
-Filename: skrec/utils/registry.py
+Filename: skrec/io/data_iterator.py
 Comment: 
 
-Filename: skrec/utils/tf2x.py
+Filename: skrec/io/dataset.py
 Comment: 
 
-Filename: skrec/utils/torch.py
+Filename: skrec/io/logger.py
 Comment: 
 
-Filename: skrec/utils/common.py
+Filename: skrec/io/movielens.py
 Comment: 
 
-Filename: skrec/utils/py/cython/
+Filename: skrec/io/preprocessor.py
 Comment: 
 
-Filename: skrec/utils/py/config.py
+Filename: skrec/recommender/BPRMF.py
 Comment: 
 
-Filename: skrec/utils/py/batch_iterator.py
+Filename: skrec/recommender/CDAE.py
 Comment: 
 
-Filename: skrec/utils/py/random.py
+Filename: skrec/recommender/CML.py
 Comment: 
 
-Filename: skrec/utils/py/evaluator.py
+Filename: skrec/recommender/Caser.py
 Comment: 
 
-Filename: skrec/utils/py/__init__.py
+Filename: skrec/recommender/FPMC.py
 Comment: 
 
-Filename: skrec/utils/py/generic.py
+Filename: skrec/recommender/GRU4Rec.py
 Comment: 
 
-Filename: skrec/utils/py/decorator.py
+Filename: skrec/recommender/GRU4RecPlus.py
 Comment: 
 
-Filename: skrec/utils/py/cython/pyx_utils.cpython-39-x86_64-linux-gnu.so
+Filename: skrec/recommender/HGN.py
 Comment: 
 
-Filename: skrec/utils/py/cython/pyx_sort.cpython-39-x86_64-linux-gnu.so
+Filename: skrec/recommender/LightGCN.py
 Comment: 
 
-Filename: skrec/utils/py/cython/__init__.py
+Filename: skrec/recommender/MultVAE.py
 Comment: 
 
-Filename: skrec/utils/py/cython/pyx_random.cpython-39-x86_64-linux-gnu.so
+Filename: skrec/recommender/Pop.py
 Comment: 
 
-Filename: skrec/utils/py/cython/pyx_eval_matrix.cpython-39-x86_64-linux-gnu.so
+Filename: skrec/recommender/SASRec.py
 Comment: 
 
-Filename: skrec/utils/py/cython/pyx_init.cpython-39-x86_64-linux-gnu.so
+Filename: skrec/recommender/SGAT.py
 Comment: 
 
-Filename: skrec/recommender/AOBPR/
+Filename: skrec/recommender/SRGNN.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/
+Filename: skrec/recommender/TransRec.py
 Comment: 
 
-Filename: skrec/recommender/MultVAE.py
+Filename: skrec/recommender/__init__.py
 Comment: 
 
 Filename: skrec/recommender/base.py
 Comment: 
 
-Filename: skrec/recommender/Caser.py
-Comment: 
-
-Filename: skrec/recommender/CDAE.py
-Comment: 
-
-Filename: skrec/recommender/HGN.py
+Filename: skrec/recommender/AOBPR/AOBPR.py
 Comment: 
 
-Filename: skrec/recommender/SASRec.py
+Filename: skrec/recommender/AOBPR/__init__.py
 Comment: 
 
-Filename: skrec/recommender/LightGCN.py
+Filename: skrec/recommender/AOBPR/pyx_aobpr_func.cp39-win_amd64.pyd
 Comment: 
 
-Filename: skrec/recommender/GRU4Rec.py
+Filename: skrec/recommender/BERT4Rec/BERT4Rec.py
 Comment: 
 
-Filename: skrec/recommender/SRGNN.py
+Filename: skrec/recommender/BERT4Rec/__init__.py
 Comment: 
 
-Filename: skrec/recommender/__init__.py
+Filename: skrec/recommender/BERT4Rec/bert4rec_gen_data.py
 Comment: 
 
-Filename: skrec/recommender/GRU4RecPlus.py
+Filename: skrec/recommender/BERT4Rec/bert4rec_utils.py
 Comment: 
 
-Filename: skrec/recommender/Pop.py
+Filename: skrec/recommender/BERT4Rec/modeling.py
 Comment: 
 
-Filename: skrec/recommender/CML.py
+Filename: skrec/recommender/BERT4Rec/optimization.py
 Comment: 
 
-Filename: skrec/recommender/SGAT.py
+Filename: skrec/recommender/BERT4Rec/vocab.py
 Comment: 
 
-Filename: skrec/recommender/BPRMF.py
+Filename: skrec/utils/__init__.py
 Comment: 
 
-Filename: skrec/recommender/TransRec.py
+Filename: skrec/utils/common.py
 Comment: 
 
-Filename: skrec/recommender/FPMC.py
+Filename: skrec/utils/registry.py
 Comment: 
 
-Filename: skrec/recommender/AOBPR/pyx_aobpr_func.cpython-39-x86_64-linux-gnu.so
+Filename: skrec/utils/tf1x.py
 Comment: 
 
-Filename: skrec/recommender/AOBPR/__init__.py
+Filename: skrec/utils/tf2x.py
 Comment: 
 
-Filename: skrec/recommender/AOBPR/AOBPR.py
+Filename: skrec/utils/torch.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/modeling.py
+Filename: skrec/utils/py/__init__.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/bert4rec_utils.py
+Filename: skrec/utils/py/batch_iterator.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/BERT4Rec.py
+Filename: skrec/utils/py/config.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/bert4rec_gen_data.py
+Filename: skrec/utils/py/decorator.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/vocab.py
+Filename: skrec/utils/py/evaluator.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/__init__.py
+Filename: skrec/utils/py/generic.py
 Comment: 
 
-Filename: skrec/recommender/BERT4Rec/optimization.py
+Filename: skrec/utils/py/random.py
 Comment: 
 
-Filename: skrec/io/data_iterator.py
+Filename: skrec/utils/py/cython/__init__.py
 Comment: 
 
-Filename: skrec/io/logger.py
+Filename: skrec/utils/py/cython/pyx_eval_matrix.cp39-win_amd64.pyd
 Comment: 
 
-Filename: skrec/io/dataset.py
+Filename: skrec/utils/py/cython/pyx_init.cp39-win_amd64.pyd
 Comment: 
 
-Filename: skrec/io/__init__.py
+Filename: skrec/utils/py/cython/pyx_random.cp39-win_amd64.pyd
 Comment: 
 
-Filename: skrec/io/preprocessor.py
+Filename: skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd
 Comment: 
 
-Filename: skrec/io/movielens.py
+Filename: skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd
 Comment: 
 
-Filename: scikit_recommender-0.0.2.dist-info/RECORD
+Filename: scikit_recommender-0.0.3.dist-info/LICENSE
 Comment: 
 
-Filename: scikit_recommender-0.0.2.dist-info/WHEEL
+Filename: scikit_recommender-0.0.3.dist-info/METADATA
 Comment: 
 
-Filename: scikit_recommender-0.0.2.dist-info/top_level.txt
+Filename: scikit_recommender-0.0.3.dist-info/WHEEL
 Comment: 
 
-Filename: scikit_recommender-0.0.2.dist-info/LICENSE
+Filename: scikit_recommender-0.0.3.dist-info/top_level.txt
 Comment: 
 
-Filename: scikit_recommender-0.0.2.dist-info/METADATA
+Filename: scikit_recommender-0.0.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=store
+Zip archive data, at least v2.0 to extract, compression method=deflate
```

## skrec/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-# __all__ = []
-
-from .io import *
-from .utils.py import *
-from .utils import *
-
-import colorama
-colorama.init()
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+# __all__ = []
+
+from .io import *
+from .utils.py import *
+from .utils import *
+
+import colorama
+colorama.init()
```

## skrec/utils/tf1x.py

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["inner_product", "bpr_loss", "l2_loss",
-           "euclidean_distance", "l2_distance",
-           "hinge_loss", "sp_mat_to_sp_tensor"]
-
-import numpy as np
-import scipy.sparse as sp
-import tensorflow as tf
-
-
-def inner_product(a, b, name="inner_product"):
-    with tf.name_scope(name=name):
-        return tf.reduce_sum(tf.multiply(a, b), axis=-1)
-
-
-def bpr_loss(y_pos, y_neg, name="bpr_loss"):
-    """ bpr loss
-    """
-    with tf.name_scope(name):
-        return -tf.log_sigmoid(y_pos - y_neg)
-
-
-def l2_loss(*params):
-    """
-        tf.nn.l2_loss = sum(t ** 2) / 2
-    """
-    return tf.add_n([tf.nn.l2_loss(w) for w in params])
-
-
-def euclidean_distance(a, b, dim: int=-1):
-    return tf.norm(a - b, ord='euclidean', axis=dim)
-
-
-def l2_distance(a, b, dim: int=-1):
-    return euclidean_distance(a, b, dim)
-
-
-def hinge_loss(yij, margin=1.0):
-    return tf.nn.relu(margin - yij)
-
-
-def sp_mat_to_sp_tensor(sp_mat):
-    if not isinstance(sp_mat, sp.coo_matrix):
-        sp_mat = sp_mat.tocoo().astype(np.float32)
-    indices = np.asarray([sp_mat.row, sp_mat.col]).transpose()
-    return tf.SparseTensor(indices, sp_mat.data, sp_mat.shape)
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["inner_product", "bpr_loss", "l2_loss",
+           "euclidean_distance", "l2_distance",
+           "hinge_loss", "sp_mat_to_sp_tensor"]
+
+import numpy as np
+import scipy.sparse as sp
+import tensorflow as tf
+
+
+def inner_product(a, b, name="inner_product"):
+    with tf.name_scope(name=name):
+        return tf.reduce_sum(tf.multiply(a, b), axis=-1)
+
+
+def bpr_loss(y_pos, y_neg, name="bpr_loss"):
+    """ bpr loss
+    """
+    with tf.name_scope(name):
+        return -tf.log_sigmoid(y_pos - y_neg)
+
+
+def l2_loss(*params):
+    """
+        tf.nn.l2_loss = sum(t ** 2) / 2
+    """
+    return tf.add_n([tf.nn.l2_loss(w) for w in params])
+
+
+def euclidean_distance(a, b, dim: int=-1):
+    return tf.norm(a - b, ord='euclidean', axis=dim)
+
+
+def l2_distance(a, b, dim: int=-1):
+    return euclidean_distance(a, b, dim)
+
+
+def hinge_loss(yij, margin=1.0):
+    return tf.nn.relu(margin - yij)
+
+
+def sp_mat_to_sp_tensor(sp_mat):
+    if not isinstance(sp_mat, sp.coo_matrix):
+        sp_mat = sp_mat.tocoo().astype(np.float32)
+    indices = np.asarray([sp_mat.row, sp_mat.col]).transpose()
+    return tf.SparseTensor(indices, sp_mat.data, sp_mat.shape)
```

## skrec/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-# __all__ = []
-
-from .registry import ModelRegistry
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+# __all__ = []
+
+from .registry import ModelRegistry
```

## skrec/utils/registry.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-from importlib.util import find_spec
-from importlib import import_module
-from collections import OrderedDict
-
-__all__ = ["ModelRegistry"]
-
-
-class ModelRegistry:
-    def __init__(self):
-        self.models = OrderedDict()
-
-    def register_model(self, model_name, model_class):
-        self.models[model_name] = model_class
-
-    def load_skrec_model(self, model_name: str) -> bool:
-        spec_path = f"skrec.recommender.{model_name}"
-        if find_spec(spec_path):
-            module = import_module(spec_path)
-        else:
-            print(f"Module '{spec_path}' is not found.")
-            # raise ModuleNotFoundError(f"Module '{spec_path}' is not found.")
-            return False
-
-        if hasattr(module, model_name):
-            model_class = getattr(module, model_name)
-        else:
-            print(f"Import {model_name} failed from {module.__file__}!")
-            # raise ImportError(f"Import {model_name} failed from {module.__file__}!")
-            return False
-
-        self.register_model(model_name, model_class)
-        return True
-
-    def get_model(self, model_name: str):
-        return self.models.get(model_name, None)
-
-    def list_models(self):
-        return list(self.models.keys())
+from importlib.util import find_spec
+from importlib import import_module
+from collections import OrderedDict
+
+__all__ = ["ModelRegistry"]
+
+
+class ModelRegistry:
+    def __init__(self):
+        self.models = OrderedDict()
+
+    def register_model(self, model_name, model_class):
+        self.models[model_name] = model_class
+
+    def load_skrec_model(self, model_name: str) -> bool:
+        spec_path = f"skrec.recommender.{model_name}"
+        if find_spec(spec_path):
+            module = import_module(spec_path)
+        else:
+            print(f"Module '{spec_path}' is not found.")
+            # raise ModuleNotFoundError(f"Module '{spec_path}' is not found.")
+            return False
+
+        if hasattr(module, model_name):
+            model_class = getattr(module, model_name)
+        else:
+            print(f"Import {model_name} failed from {module.__file__}!")
+            # raise ImportError(f"Import {model_name} failed from {module.__file__}!")
+            return False
+
+        self.register_model(model_name, model_class)
+        return True
+
+    def get_model(self, model_name: str):
+        return self.models.get(model_name, None)
+
+    def list_models(self):
+        return list(self.models.keys())
```

## skrec/utils/tf2x.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = []
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = []
```

## skrec/utils/torch.py

 * *Ordering differences only*

```diff
@@ -1,121 +1,121 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["inner_product", "euclidean_distance",
-           "l2_distance", "bpr_loss", "l2_loss",
-           "sigmoid_cross_entropy", "square_loss",
-           "sp_mat_to_sp_tensor", "dropout_sparse",
-           "get_initializer"]
-
-import numpy as np
-import scipy.sparse as sp
-import torch
-from torch import Tensor
-import torch.nn.functional as F
-from torch import nn
-from functools import partial
-from collections import OrderedDict
-
-
-def inner_product(a: Tensor, b: Tensor, dim: int=-1) -> Tensor:
-    return torch.sum(a*b, dim=dim)
-
-
-def euclidean_distance(a: Tensor, b: Tensor, dim: int=-1) -> Tensor:
-    return torch.norm(a-b, p=None, dim=dim)
-
-
-def l2_distance(a: Tensor, b: Tensor, dim: int=-1) -> Tensor:
-    return euclidean_distance(a, b, dim)
-
-
-def sp_mat_to_sp_tensor(sp_mat: sp.spmatrix) -> Tensor:
-    coo = sp_mat.tocoo().astype(np.float32)
-    indices = torch.from_numpy(np.asarray([coo.row, coo.col]))
-    return torch.sparse_coo_tensor(indices, coo.data, coo.shape).coalesce()
-
-
-def dropout_sparse(torch_sp_mat, keep_prob, training):
-    """Dropout for sparse tensors.
-    """
-    if keep_prob <= 0.0 or keep_prob > 1.0:
-        raise ValueError(f"'keep_prob' must be a float in the range (0, 1], got {keep_prob}")
-    if training and keep_prob < 1:
-        device = torch_sp_mat.device
-        values = torch_sp_mat.values()
-        noise_shape = values.shape
-
-        random_tensor = torch.Tensor(noise_shape).uniform_().to(device) + keep_prob
-        dropout_mask = random_tensor.floor().bool()
-
-        indices = torch_sp_mat.indices()
-        indices = indices[:, dropout_mask]
-        scale = 1.0 / keep_prob
-        values = values[dropout_mask]*scale
-        shape = torch_sp_mat.shape
-
-        torch_sp_mat = torch.sparse_coo_tensor(indices, values, shape).coalesce().to(device)
-
-    return torch_sp_mat
-
-
-def bpr_loss(y_pos: Tensor, y_neg: Tensor) -> Tensor:
-    return -F.logsigmoid(y_pos - y_neg)
-
-
-def l2_loss(*weights):
-    """L2 loss
-    Compute  the L2 norm of tensors without the `sqrt`:
-        output = sum([sum(w ** 2) / 2 for w in weights])
-    Args:
-        *weights: Variable length weight list.
-
-    """
-    return 0.5 * sum([torch.sum(torch.pow(w, 2)) for w in weights])
-
-
-def truncated_normal_(tensor, mean=0.0, std=1.0):
-    # https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/16
-    size = tensor.shape
-    tmp = tensor.new_empty(size + (4,)).normal_(mean=0, std=1)
-    valid = (tmp < 2) & (tmp > -2)
-    ind = valid.max(-1, keepdim=True)[1]
-    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))
-    tensor.data.mul_(std).add_(mean)
-    return tensor
-
-
-class InitArg(object):
-    MEAN = 0.0
-    STDDEV = 0.01
-    MIN_VAL = -0.05
-    MAX_VAL = 0.05
-
-
-_initializers = OrderedDict()
-_initializers["normal"] = partial(nn.init.normal_, mean=InitArg.MEAN, std=InitArg.STDDEV)
-_initializers["truncated_normal"] = partial(truncated_normal_, mean=InitArg.MEAN, std=InitArg.STDDEV)
-_initializers["uniform"] = partial(nn.init.uniform_, a=InitArg.MIN_VAL, b=InitArg.MAX_VAL)
-_initializers["he_normal"] = nn.init.kaiming_normal_
-_initializers["he_uniform"] = nn.init.kaiming_uniform_
-_initializers["xavier_normal"] = nn.init.xavier_normal_
-_initializers["xavier_uniform"] = nn.init.xavier_uniform_
-_initializers["zeros"] = nn.init.zeros_
-_initializers["ones"] = nn.init.ones_
-
-
-def get_initializer(init_method: str):
-    if init_method not in _initializers:
-        init_list = ', '.join(_initializers.keys())
-        raise ValueError(f"'init_method' is invalid, and must be one of '{init_list}'")
-    return _initializers[init_method]
-
-
-def sigmoid_cross_entropy(y_pre, y_true):
-    return F.binary_cross_entropy_with_logits(input=y_pre, target=y_true, reduction="none")
-
-
-def square_loss(y_pre, y_true):
-    if isinstance(y_true, (float, int)):
-        y_true = y_pre.new_full(y_pre.size(), y_true)
-    return F.mse_loss(input=y_pre, target=y_true, reduction="none")
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["inner_product", "euclidean_distance",
+           "l2_distance", "bpr_loss", "l2_loss",
+           "sigmoid_cross_entropy", "square_loss",
+           "sp_mat_to_sp_tensor", "dropout_sparse",
+           "get_initializer"]
+
+import numpy as np
+import scipy.sparse as sp
+import torch
+from torch import Tensor
+import torch.nn.functional as F
+from torch import nn
+from functools import partial
+from collections import OrderedDict
+
+
+def inner_product(a: Tensor, b: Tensor, dim: int=-1) -> Tensor:
+    return torch.sum(a*b, dim=dim)
+
+
+def euclidean_distance(a: Tensor, b: Tensor, dim: int=-1) -> Tensor:
+    return torch.norm(a-b, p=None, dim=dim)
+
+
+def l2_distance(a: Tensor, b: Tensor, dim: int=-1) -> Tensor:
+    return euclidean_distance(a, b, dim)
+
+
+def sp_mat_to_sp_tensor(sp_mat: sp.spmatrix) -> Tensor:
+    coo = sp_mat.tocoo().astype(np.float32)
+    indices = torch.from_numpy(np.asarray([coo.row, coo.col]))
+    return torch.sparse_coo_tensor(indices, coo.data, coo.shape).coalesce()
+
+
+def dropout_sparse(torch_sp_mat, keep_prob, training):
+    """Dropout for sparse tensors.
+    """
+    if keep_prob <= 0.0 or keep_prob > 1.0:
+        raise ValueError(f"'keep_prob' must be a float in the range (0, 1], got {keep_prob}")
+    if training and keep_prob < 1:
+        device = torch_sp_mat.device
+        values = torch_sp_mat.values()
+        noise_shape = values.shape
+
+        random_tensor = torch.Tensor(noise_shape).uniform_().to(device) + keep_prob
+        dropout_mask = random_tensor.floor().bool()
+
+        indices = torch_sp_mat.indices()
+        indices = indices[:, dropout_mask]
+        scale = 1.0 / keep_prob
+        values = values[dropout_mask]*scale
+        shape = torch_sp_mat.shape
+
+        torch_sp_mat = torch.sparse_coo_tensor(indices, values, shape).coalesce().to(device)
+
+    return torch_sp_mat
+
+
+def bpr_loss(y_pos: Tensor, y_neg: Tensor) -> Tensor:
+    return -F.logsigmoid(y_pos - y_neg)
+
+
+def l2_loss(*weights):
+    """L2 loss
+    Compute  the L2 norm of tensors without the `sqrt`:
+        output = sum([sum(w ** 2) / 2 for w in weights])
+    Args:
+        *weights: Variable length weight list.
+
+    """
+    return 0.5 * sum([torch.sum(torch.pow(w, 2)) for w in weights])
+
+
+def truncated_normal_(tensor, mean=0.0, std=1.0):
+    # https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/16
+    size = tensor.shape
+    tmp = tensor.new_empty(size + (4,)).normal_(mean=0, std=1)
+    valid = (tmp < 2) & (tmp > -2)
+    ind = valid.max(-1, keepdim=True)[1]
+    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))
+    tensor.data.mul_(std).add_(mean)
+    return tensor
+
+
+class InitArg(object):
+    MEAN = 0.0
+    STDDEV = 0.01
+    MIN_VAL = -0.05
+    MAX_VAL = 0.05
+
+
+_initializers = OrderedDict()
+_initializers["normal"] = partial(nn.init.normal_, mean=InitArg.MEAN, std=InitArg.STDDEV)
+_initializers["truncated_normal"] = partial(truncated_normal_, mean=InitArg.MEAN, std=InitArg.STDDEV)
+_initializers["uniform"] = partial(nn.init.uniform_, a=InitArg.MIN_VAL, b=InitArg.MAX_VAL)
+_initializers["he_normal"] = nn.init.kaiming_normal_
+_initializers["he_uniform"] = nn.init.kaiming_uniform_
+_initializers["xavier_normal"] = nn.init.xavier_normal_
+_initializers["xavier_uniform"] = nn.init.xavier_uniform_
+_initializers["zeros"] = nn.init.zeros_
+_initializers["ones"] = nn.init.ones_
+
+
+def get_initializer(init_method: str):
+    if init_method not in _initializers:
+        init_list = ', '.join(_initializers.keys())
+        raise ValueError(f"'init_method' is invalid, and must be one of '{init_list}'")
+    return _initializers[init_method]
+
+
+def sigmoid_cross_entropy(y_pre, y_true):
+    return F.binary_cross_entropy_with_logits(input=y_pre, target=y_true, reduction="none")
+
+
+def square_loss(y_pre, y_true):
+    if isinstance(y_true, (float, int)):
+        y_true = y_pre.new_full(y_pre.size(), y_true)
+    return F.mse_loss(input=y_pre, target=y_true, reduction="none")
```

## skrec/utils/common.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-import numpy as np
-import scipy.sparse as sp
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["normalize_adj_matrix"]
-
-
-def normalize_adj_matrix(sp_mat, norm_method="left"):
-    """Normalize adjacent matrix
-
-    Args:
-        sp_mat: A sparse adjacent matrix
-        norm_method (str): The normalization method, can be 'symmetric'
-            or 'left'.
-
-    Returns:
-        sp.spmatrix: The normalized adjacent matrix.
-
-    """
-
-    d_in = np.asarray(sp_mat.sum(axis=1))  # indegree
-    if norm_method == "left":
-        rec_d_in = np.power(d_in, -1).flatten()  # reciprocal
-        rec_d_in[np.isinf(rec_d_in)] = 0.  # replace inf
-        rec_d_in = sp.diags(rec_d_in)  # to diagonal matrix
-        norm_sp_mat = rec_d_in.dot(sp_mat)  # left matmul
-    elif norm_method == "symmetric":
-        rec_sqrt_d_in = np.power(d_in, -0.5).flatten()
-        rec_sqrt_d_in[np.isinf(rec_sqrt_d_in)] = 0.
-        rec_sqrt_d_in = sp.diags(rec_sqrt_d_in)
-
-        mid_sp_mat = rec_sqrt_d_in.dot(sp_mat)  # left matmul
-        norm_sp_mat = mid_sp_mat.dot(rec_sqrt_d_in)  # right matmul
-    else:
-        raise ValueError(f"'{norm_method}' is an invalid normalization method.")
-
-    return norm_sp_mat
+import numpy as np
+import scipy.sparse as sp
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["normalize_adj_matrix"]
+
+
+def normalize_adj_matrix(sp_mat, norm_method="left"):
+    """Normalize adjacent matrix
+
+    Args:
+        sp_mat: A sparse adjacent matrix
+        norm_method (str): The normalization method, can be 'symmetric'
+            or 'left'.
+
+    Returns:
+        sp.spmatrix: The normalized adjacent matrix.
+
+    """
+
+    d_in = np.asarray(sp_mat.sum(axis=1))  # indegree
+    if norm_method == "left":
+        rec_d_in = np.power(d_in, -1).flatten()  # reciprocal
+        rec_d_in[np.isinf(rec_d_in)] = 0.  # replace inf
+        rec_d_in = sp.diags(rec_d_in)  # to diagonal matrix
+        norm_sp_mat = rec_d_in.dot(sp_mat)  # left matmul
+    elif norm_method == "symmetric":
+        rec_sqrt_d_in = np.power(d_in, -0.5).flatten()
+        rec_sqrt_d_in[np.isinf(rec_sqrt_d_in)] = 0.
+        rec_sqrt_d_in = sp.diags(rec_sqrt_d_in)
+
+        mid_sp_mat = rec_sqrt_d_in.dot(sp_mat)  # left matmul
+        norm_sp_mat = mid_sp_mat.dot(rec_sqrt_d_in)  # right matmul
+    else:
+        raise ValueError(f"'{norm_method}' is an invalid normalization method.")
+
+    return norm_sp_mat
```

## skrec/utils/py/config.py

 * *Ordering differences only*

```diff
@@ -1,77 +1,77 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["Config", "merge_config_with_cmd_args"]
-
-import sys
-from typing import Dict
-from collections import OrderedDict
-from argparse import Namespace
-import copy
-
-
-class OrderedNamespace(Namespace):
-    def __init__(self):
-        self._ordered_key = []
-        super(OrderedNamespace, self).__init__()
-
-    def _get_kwargs(self):
-        # retrieve (key, value) pairs in the order they were initialized using _keys
-        return [(k, self.__dict__[k]) for k in self._ordered_key]
-
-    def __setattr__(self, key, value):
-        # store new attribute (key, value) pairs in builtin __dict__
-        self.__dict__[key] = value
-        # store the keys in self._keys in the order that they are initialized
-        # do not store '_keys' itself and don't enter any key more than once
-        if key not in ['_ordered_key'] + self._ordered_key:
-            self._ordered_key.append(key)
-
-    def items(self):
-        for key, value in self._get_kwargs():
-            yield (key, value)
-
-
-class Config(OrderedNamespace):
-    def __init__(self):
-        super(Config, self).__init__()
-
-    def _validate(self):
-        pass
-
-    def to_string(self, sep: str='\n'):
-        arg_strings = [f"{key}={value}" for key, value in self.items()]
-        return sep.join(arg_strings)
-
-
-def merge_config_with_cmd_args(config: Dict, inplace: bool = True) -> Dict:
-    args = sys.argv[1:]
-    if len(args) % 2 != 0:
-        raise SyntaxError("The numbers of arguments and its values are not equal.")
-
-    if inplace is False:
-        config = copy.deepcopy(config)
-
-    cmd_args = OrderedDict()
-    for arg_name, arg_value in zip(args[0::2], args[1::2]):
-        if not arg_name.startswith("--"):
-            raise SyntaxError("Command arg must start with '--', but '%s' is not!" % arg_name)
-        cmd_args[arg_name[2:]] = arg_value
-
-    # cover the arguments from ini files
-    for cmd_argn, cmd_argv in cmd_args.items():
-        # convert param from str to value, i.e. int, float or list etc.
-        try:
-            value = eval(cmd_argv)
-            if not isinstance(value, (str, int, float, list, tuple, bool, None.__class__)):
-                value = cmd_argv
-        except (NameError, SyntaxError):
-            if cmd_argv.lower() == "true":
-                value = True
-            elif cmd_argv.lower() == "false":
-                value = False
-            else:
-                value = cmd_argv
-        config[cmd_argn] = value
-
-    return config
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["Config", "merge_config_with_cmd_args"]
+
+import sys
+from typing import Dict
+from collections import OrderedDict
+from argparse import Namespace
+import copy
+
+
+class OrderedNamespace(Namespace):
+    def __init__(self):
+        self._ordered_key = []
+        super(OrderedNamespace, self).__init__()
+
+    def _get_kwargs(self):
+        # retrieve (key, value) pairs in the order they were initialized using _keys
+        return [(k, self.__dict__[k]) for k in self._ordered_key]
+
+    def __setattr__(self, key, value):
+        # store new attribute (key, value) pairs in builtin __dict__
+        self.__dict__[key] = value
+        # store the keys in self._keys in the order that they are initialized
+        # do not store '_keys' itself and don't enter any key more than once
+        if key not in ['_ordered_key'] + self._ordered_key:
+            self._ordered_key.append(key)
+
+    def items(self):
+        for key, value in self._get_kwargs():
+            yield (key, value)
+
+
+class Config(OrderedNamespace):
+    def __init__(self):
+        super(Config, self).__init__()
+
+    def _validate(self):
+        pass
+
+    def to_string(self, sep: str='\n'):
+        arg_strings = [f"{key}={value}" for key, value in self.items()]
+        return sep.join(arg_strings)
+
+
+def merge_config_with_cmd_args(config: Dict, inplace: bool = True) -> Dict:
+    args = sys.argv[1:]
+    if len(args) % 2 != 0:
+        raise SyntaxError("The numbers of arguments and its values are not equal.")
+
+    if inplace is False:
+        config = copy.deepcopy(config)
+
+    cmd_args = OrderedDict()
+    for arg_name, arg_value in zip(args[0::2], args[1::2]):
+        if not arg_name.startswith("--"):
+            raise SyntaxError("Command arg must start with '--', but '%s' is not!" % arg_name)
+        cmd_args[arg_name[2:]] = arg_value
+
+    # cover the arguments from ini files
+    for cmd_argn, cmd_argv in cmd_args.items():
+        # convert param from str to value, i.e. int, float or list etc.
+        try:
+            value = eval(cmd_argv)
+            if not isinstance(value, (str, int, float, list, tuple, bool, None.__class__)):
+                value = cmd_argv
+        except (NameError, SyntaxError):
+            if cmd_argv.lower() == "true":
+                value = True
+            elif cmd_argv.lower() == "false":
+                value = False
+            else:
+                value = cmd_argv
+        config[cmd_argn] = value
+
+    return config
```

## skrec/utils/py/batch_iterator.py

 * *Ordering differences only*

```diff
@@ -1,213 +1,213 @@
-# reference: the DataLoader of pytorch
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["BatchIterator"]
-
-import numpy as np
-
-
-class Sampler(object):
-    """Base class for all Samplers.
-
-    Every Sampler subclass has to provide an __iter__ method, providing a way
-    to iterate over indices of dataset elements, and a __len__ method that
-    returns the length of the returned iterators.
-    """
-
-    def __init__(self):
-        pass
-
-    def __iter__(self):
-        raise NotImplementedError
-
-    def __len__(self):
-        raise NotImplementedError
-
-
-class SequentialSampler(Sampler):
-    """Samples elements sequentially, always in the same order.
-    """
-
-    def __init__(self, data_source):
-        """Initializes a new `SequentialSampler` instance.
-
-        Args:
-            data_source (_Dataset): Dataset to sample from.
-        """
-        super(SequentialSampler, self).__init__()
-        self.data_source = data_source
-
-    def __iter__(self):
-        return iter(range(len(self.data_source)))
-
-    def __len__(self):
-        return len(self.data_source)
-
-
-class RandomSampler(Sampler):
-    """Samples elements randomly, without replacement.
-    """
-
-    def __init__(self, data_source):
-        """Initializes a new `SequentialSampler` instance.
-
-        Args:
-            data_source (_Dataset): Dataset to sample from.
-        """
-        super(RandomSampler, self).__init__()
-        self.data_source = data_source
-
-    def __iter__(self):
-        perm = np.random.permutation(len(self.data_source)).tolist()
-        return iter(perm)
-
-    def __len__(self):
-        return len(self.data_source)
-
-
-class BatchSampler(Sampler):
-    """Wraps another sampler to yield a mini-batch of indices.
-    """
-
-    def __init__(self, sampler, batch_size, drop_last):
-        """Initializes a new `BatchSampler` instance.
-
-        Args:
-            sampler (Sampler): Base sampler.
-            batch_size (int): Size of mini-batch.
-            drop_last (bool): If `True`, the sampler will drop the last batch
-                if its size would be less than `batch_size`.
-        """
-        super(BatchSampler, self).__init__()
-        if not isinstance(sampler, Sampler):
-            raise ValueError("sampler should be an instance of "
-                             "torch.utils.data.Sampler, but got sampler={}"
-                             .format(sampler))
-        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \
-                batch_size <= 0:
-            raise ValueError("batch_size should be a positive integeral value, "
-                             "but got batch_size={}".format(batch_size))
-        if not isinstance(drop_last, bool):
-            raise ValueError("drop_last should be a boolean value, but got "
-                             "drop_last={}".format(drop_last))
-        self.sampler = sampler
-        self.batch_size = batch_size
-        self.drop_last = drop_last
-
-    def __iter__(self):
-        batch = []
-        for idx in self.sampler:
-            batch.append(idx)
-            if len(batch) == self.batch_size:
-                yield batch
-                batch = []
-        if len(batch) > 0 and not self.drop_last:
-            yield batch
-
-    def __len__(self):
-        if self.drop_last:
-            return len(self.sampler) // self.batch_size
-        else:
-            return (len(self.sampler) + self.batch_size - 1) // self.batch_size
-
-
-class _Dataset(object):
-    """Pack the given data to one dataset.
-
-    Args:
-        data (list or tuple): a list of 'data'.
-    """
-
-    def __init__(self, data):
-        for d in data:
-            if len(d) != len(data[0]):
-                raise ValueError("The length of the given data are not equal!")
-            # assert len(d) == len(data[0])
-        self.data = data
-
-    def __len__(self):
-        return len(self.data[0])
-
-    def __getitem__(self, idx):
-        return [data[idx] for data in self.data]
-
-
-class _BatchIter(object):
-    """Iterates once over the dataset, as specified by the sampler.
-    """
-
-    def __init__(self, loader):
-        self.dataset = loader.dataset
-        self.batch_sampler = loader.batch_sampler
-        self.sample_iter = iter(self.batch_sampler)
-
-    def __len__(self):
-        return len(self.batch_sampler)
-
-    def __next__(self):
-        indices = next(self.sample_iter)  # may raise StopIteration
-        batch = [self.dataset[i] for i in indices]
-
-        transposed = [list(samples) for samples in zip(*batch)]
-        if len(transposed) == 1:
-            transposed = transposed[0]
-        return transposed
-
-    def __iter__(self):
-        return self
-
-
-class BatchIterator(object):
-    """`BatchIterator` provides iterators over the dataset.
-
-    This class combines some data sets and provides a batch iterator over them.
-    For example::
-
-        users = list(range(10))
-        items = list(range(10, 20))
-        labels = list(range(20, 30))
-
-        data_iter = BatchIterator(users, items, labels, batch_size=4, shuffle=False)
-        for bat_user, bat_item, bat_label in data_iter:
-            print(bat_user, bat_item, bat_label)
-
-        data_iter = BatchIterator(users, items, batch_size=4, shuffle=True, drop_last=True)
-        for bat_user, bat_item in data_iter:
-            print(bat_user, bat_item)
-
-    """
-
-    def __init__(self, *data, batch_size=1, shuffle=False, drop_last=False):
-        """
-        Args:
-            *data: Variable length data list.
-            batch_size (int): How many samples per batch to load. Defaults to `1`.
-            shuffle (bool): Set to `True` to have the data reshuffled at every
-                epoch. Defaults to `False`.
-            drop_last (bool): Set to `True` to drop the last incomplete batch,
-                if the dataset size is not divisible by the batch size.
-                If `False` and the size of dataset is not divisible by the
-                batch size, then the last batch will be smaller.
-                Defaults to `False`.
-
-        Raises:
-            ValueError: If the length of the given data are not equal.
-        """
-        dataset = _Dataset(list(data))
-        self.dataset = dataset
-        self.batch_size = batch_size
-        self.drop_last = drop_last
-
-        if shuffle:
-            sampler = RandomSampler(dataset)
-        else:
-            sampler = SequentialSampler(dataset)
-
-        self.batch_sampler = BatchSampler(sampler, batch_size, drop_last)
-
-    def __iter__(self):
-        return _BatchIter(self)
-
-    def __len__(self):
-        return len(self.batch_sampler)
+# reference: the DataLoader of pytorch
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["BatchIterator"]
+
+import numpy as np
+
+
+class Sampler(object):
+    """Base class for all Samplers.
+
+    Every Sampler subclass has to provide an __iter__ method, providing a way
+    to iterate over indices of dataset elements, and a __len__ method that
+    returns the length of the returned iterators.
+    """
+
+    def __init__(self):
+        pass
+
+    def __iter__(self):
+        raise NotImplementedError
+
+    def __len__(self):
+        raise NotImplementedError
+
+
+class SequentialSampler(Sampler):
+    """Samples elements sequentially, always in the same order.
+    """
+
+    def __init__(self, data_source):
+        """Initializes a new `SequentialSampler` instance.
+
+        Args:
+            data_source (_Dataset): Dataset to sample from.
+        """
+        super(SequentialSampler, self).__init__()
+        self.data_source = data_source
+
+    def __iter__(self):
+        return iter(range(len(self.data_source)))
+
+    def __len__(self):
+        return len(self.data_source)
+
+
+class RandomSampler(Sampler):
+    """Samples elements randomly, without replacement.
+    """
+
+    def __init__(self, data_source):
+        """Initializes a new `SequentialSampler` instance.
+
+        Args:
+            data_source (_Dataset): Dataset to sample from.
+        """
+        super(RandomSampler, self).__init__()
+        self.data_source = data_source
+
+    def __iter__(self):
+        perm = np.random.permutation(len(self.data_source)).tolist()
+        return iter(perm)
+
+    def __len__(self):
+        return len(self.data_source)
+
+
+class BatchSampler(Sampler):
+    """Wraps another sampler to yield a mini-batch of indices.
+    """
+
+    def __init__(self, sampler, batch_size, drop_last):
+        """Initializes a new `BatchSampler` instance.
+
+        Args:
+            sampler (Sampler): Base sampler.
+            batch_size (int): Size of mini-batch.
+            drop_last (bool): If `True`, the sampler will drop the last batch
+                if its size would be less than `batch_size`.
+        """
+        super(BatchSampler, self).__init__()
+        if not isinstance(sampler, Sampler):
+            raise ValueError("sampler should be an instance of "
+                             "torch.utils.data.Sampler, but got sampler={}"
+                             .format(sampler))
+        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \
+                batch_size <= 0:
+            raise ValueError("batch_size should be a positive integeral value, "
+                             "but got batch_size={}".format(batch_size))
+        if not isinstance(drop_last, bool):
+            raise ValueError("drop_last should be a boolean value, but got "
+                             "drop_last={}".format(drop_last))
+        self.sampler = sampler
+        self.batch_size = batch_size
+        self.drop_last = drop_last
+
+    def __iter__(self):
+        batch = []
+        for idx in self.sampler:
+            batch.append(idx)
+            if len(batch) == self.batch_size:
+                yield batch
+                batch = []
+        if len(batch) > 0 and not self.drop_last:
+            yield batch
+
+    def __len__(self):
+        if self.drop_last:
+            return len(self.sampler) // self.batch_size
+        else:
+            return (len(self.sampler) + self.batch_size - 1) // self.batch_size
+
+
+class _Dataset(object):
+    """Pack the given data to one dataset.
+
+    Args:
+        data (list or tuple): a list of 'data'.
+    """
+
+    def __init__(self, data):
+        for d in data:
+            if len(d) != len(data[0]):
+                raise ValueError("The length of the given data are not equal!")
+            # assert len(d) == len(data[0])
+        self.data = data
+
+    def __len__(self):
+        return len(self.data[0])
+
+    def __getitem__(self, idx):
+        return [data[idx] for data in self.data]
+
+
+class _BatchIter(object):
+    """Iterates once over the dataset, as specified by the sampler.
+    """
+
+    def __init__(self, loader):
+        self.dataset = loader.dataset
+        self.batch_sampler = loader.batch_sampler
+        self.sample_iter = iter(self.batch_sampler)
+
+    def __len__(self):
+        return len(self.batch_sampler)
+
+    def __next__(self):
+        indices = next(self.sample_iter)  # may raise StopIteration
+        batch = [self.dataset[i] for i in indices]
+
+        transposed = [list(samples) for samples in zip(*batch)]
+        if len(transposed) == 1:
+            transposed = transposed[0]
+        return transposed
+
+    def __iter__(self):
+        return self
+
+
+class BatchIterator(object):
+    """`BatchIterator` provides iterators over the dataset.
+
+    This class combines some data sets and provides a batch iterator over them.
+    For example::
+
+        users = list(range(10))
+        items = list(range(10, 20))
+        labels = list(range(20, 30))
+
+        data_iter = BatchIterator(users, items, labels, batch_size=4, shuffle=False)
+        for bat_user, bat_item, bat_label in data_iter:
+            print(bat_user, bat_item, bat_label)
+
+        data_iter = BatchIterator(users, items, batch_size=4, shuffle=True, drop_last=True)
+        for bat_user, bat_item in data_iter:
+            print(bat_user, bat_item)
+
+    """
+
+    def __init__(self, *data, batch_size=1, shuffle=False, drop_last=False):
+        """
+        Args:
+            *data: Variable length data list.
+            batch_size (int): How many samples per batch to load. Defaults to `1`.
+            shuffle (bool): Set to `True` to have the data reshuffled at every
+                epoch. Defaults to `False`.
+            drop_last (bool): Set to `True` to drop the last incomplete batch,
+                if the dataset size is not divisible by the batch size.
+                If `False` and the size of dataset is not divisible by the
+                batch size, then the last batch will be smaller.
+                Defaults to `False`.
+
+        Raises:
+            ValueError: If the length of the given data are not equal.
+        """
+        dataset = _Dataset(list(data))
+        self.dataset = dataset
+        self.batch_size = batch_size
+        self.drop_last = drop_last
+
+        if shuffle:
+            sampler = RandomSampler(dataset)
+        else:
+            sampler = SequentialSampler(dataset)
+
+        self.batch_sampler = BatchSampler(sampler, batch_size, drop_last)
+
+    def __iter__(self):
+        return _BatchIter(self)
+
+    def __len__(self):
+        return len(self.batch_sampler)
```

## skrec/utils/py/random.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["randint_choice", "batch_randint_choice"]
-
-from .cython import pyx_randint_choice, pyx_batch_randint_choice
-
-
-def randint_choice(high, size=1, replace=True, p=None, exclusion=None):
-    """Sample random integers from [0, high).
-
-    Args:
-        high (int): The largest integer (exclusive) to be drawn from the distribution.
-        size (int): The number of samples to be drawn.
-        replace (bool): Whether the sample is with or without replacement.
-        p: 1-D array-like, optional. The probabilities associated with each entry in [0, high).
-           If not given the sample assumes a uniform distribution.
-        exclusion: 1-D array-like. The integers in exclusion will be excluded.
-
-    Returns:
-        int or ndarray
-    """
-    return pyx_randint_choice(high, size, replace, p, exclusion)
-
-
-def batch_randint_choice(high, size, replace=True, p=None, exclusion=None, thread_num=1):
-    """Sample random integers from [0, high).
-
-    Args:
-        high (int):
-        size: 1-D array_like
-        replace (bool):
-        p: 2-D array_like
-        exclusion: a list of 1-D array_like
-        thread_num (int): the number of threads
-
-    Returns:
-        list: a list of 1-D array_like sample
-    """
-    return pyx_batch_randint_choice(high, size, replace=replace, p=p, exclusion=exclusion, thread_num=thread_num)
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["randint_choice", "batch_randint_choice"]
+
+from .cython import pyx_randint_choice, pyx_batch_randint_choice
+
+
+def randint_choice(high, size=1, replace=True, p=None, exclusion=None):
+    """Sample random integers from [0, high).
+
+    Args:
+        high (int): The largest integer (exclusive) to be drawn from the distribution.
+        size (int): The number of samples to be drawn.
+        replace (bool): Whether the sample is with or without replacement.
+        p: 1-D array-like, optional. The probabilities associated with each entry in [0, high).
+           If not given the sample assumes a uniform distribution.
+        exclusion: 1-D array-like. The integers in exclusion will be excluded.
+
+    Returns:
+        int or ndarray
+    """
+    return pyx_randint_choice(high, size, replace, p, exclusion)
+
+
+def batch_randint_choice(high, size, replace=True, p=None, exclusion=None, thread_num=1):
+    """Sample random integers from [0, high).
+
+    Args:
+        high (int):
+        size: 1-D array_like
+        replace (bool):
+        p: 2-D array_like
+        exclusion: a list of 1-D array_like
+        thread_num (int): the number of threads
+
+    Returns:
+        list: a list of 1-D array_like sample
+    """
+    return pyx_batch_randint_choice(high, size, replace=replace, p=p, exclusion=exclusion, thread_num=thread_num)
```

## skrec/utils/py/evaluator.py

 * *Ordering differences only*

```diff
@@ -1,208 +1,208 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["MetricReport", "RankingEvaluator", "MetricReport"]
-
-from typing import Sequence, Dict, Union, Optional, Tuple, List, Iterable
-from collections import OrderedDict
-import numpy as np
-import itertools
-from colorama import Fore, Style
-from .batch_iterator import BatchIterator
-from .cython import eval_score_matrix
-
-_text_colors = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN]
-
-
-class MetricReport(object):
-    def __init__(self, metrics: Sequence[str], values: Sequence[float]):
-        assert len(metrics) == len(values), f"The lengths of metrics and values " \
-                                            f"are not equal ({len(metrics)}!={len(values)})."
-        self._results = OrderedDict(zip(metrics, values))
-
-    def metrics(self):
-        return self._results.keys()
-
-    @property
-    def metrics_str(self) -> str:
-        _colors = itertools.cycle(_text_colors)
-        return '\t'.join([c+f"{m}".ljust(12)+Style.RESET_ALL
-                          for c, m in zip(_colors, self.metrics())])
-
-    def values(self):
-        return self._results.values()
-
-    @property
-    def values_str(self) -> str:
-        _colors = itertools.cycle(_text_colors)
-        return '\t'.join([c+f"{v:.8f}".ljust(12)+Style.RESET_ALL
-                          for c, v in zip(_colors, self.values())])
-
-    def items(self):
-        return self._results.items()
-
-    def __getitem__(self, item):
-        if item not in self._results:
-            raise KeyError(item)
-        return self._results[item]
-
-    def __str__(self):
-        return self._results.__str__()
-
-
-_metric2id = {"Precision": 1, "Recall": 2, "MAP": 3, "NDCG": 4, "MRR": 5}
-_id2metric = {value: key for key, value in _metric2id.items()}
-
-
-class RankingEvaluator(object):
-    """Evaluator for item ranking task.
-
-    Evaluation metrics of `Evaluator` are configurable and can
-    automatically fit both leave-one-out and fold-out data splitting
-    without specific indication:
-
-    * **First**, evaluation metrics of this class are configurable via the
-      argument `metric`. Now there are five configurable metrics: `Precision`,
-      `Recall`, `MAP`, `NDCG` and `MRR`.
-
-    * **Second**, this class and its evaluation metrics can automatically fit
-      both leave-one-out and fold-out data splitting without specific indication.
-
-      In **leave-one-out** evaluation:
-        1) `Recall` is equal to `HitRatio`;
-        2) The implementation of `NDCG` is compatible with fold-out;
-        3) `MAP` and `MRR` have same numeric values;
-        4) `Precision` is meaningless.
-    """
-
-    def __init__(self, user_train_dict: Optional[Dict[int, np.ndarray]],
-                 user_test_dict: Dict[int, np.ndarray],
-                 metric: Union[None, str, Tuple[str], List[str]]=None,
-                 top_k: Union[int, List[int], Tuple[int]]=50,
-                 batch_size: int=256, num_thread: int=8):
-        """Initializes a new `Evaluator` instance.
-
-        Args:
-            user_train_dict (dict, None): Each key is user ID and the corresponding
-                value is the list of **training items**.
-            user_test_dict (dict): Each key is user ID and the corresponding
-                value is the list of **test items**.
-            metric (None or list of str): If `metric == None`, metric will
-                be set to `["Precision", "Recall", "MAP", "NDCG", "MRR"]`.
-                Otherwise, `metric` must be one or a sublist of metrics
-                mentioned above. Defaults to `None`.
-            top_k (int or list of int): `top_k` controls the Top-K item ranking
-                performance. If `top_k` is an integer, K ranges from `1` to
-                `top_k`; If `top_k` is a list of integers, K are only assigned
-                these values. Defaults to `50`.
-            batch_size (int): An integer to control the test batch size.
-                Defaults to `1024`.
-            num_thread (int): An integer to control the test thread number.
-                Defaults to `8`.
-
-        Raises:
-             ValueError: If `metric` or one of its element is invalid.
-        """
-        super(RankingEvaluator, self).__init__()
-        if metric is None:
-            metric = ["Precision", "Recall", "MAP", "NDCG", "MRR"]
-        elif isinstance(metric, str):
-            metric = [metric]
-        elif isinstance(metric, (tuple, list)):
-            metric = list(metric)
-        else:
-            raise TypeError("The type of 'metric' (%s) is invalid!" % metric.__class__.__name__)
-
-        for m in metric:
-            assert m in _metric2id, f"'{metric}' is not in ('Precision', 'Recall', 'MAP', 'NDCG', 'MRR')."
-
-        self.user_pos_train = dict()
-        self.user_pos_test = dict()
-        self.set_train_data(user_train_dict)
-        self.set_test_data(user_test_dict)
-
-        self.metrics_num = len(metric)
-        self.metrics = [_metric2id[m] for m in metric]
-        self.num_thread = num_thread
-        self.batch_size = batch_size
-
-        if isinstance(top_k, int):
-            self.max_top = top_k
-            self.top_show = np.arange(top_k) + 1
-        else:
-            self.max_top = max(top_k)
-            self.top_show = np.sort(top_k)
-
-    def set_train_data(self, user_train_dict: Optional[Dict[int, np.ndarray]]=None):
-        self.user_pos_train = user_train_dict if user_train_dict is not None else dict()
-
-    def set_test_data(self, user_test_dict: Dict[int, np.ndarray]):
-        assert len(user_test_dict) > 0, "'user_test_dict' can be empty."
-        self.user_pos_test = user_test_dict
-
-    @property
-    def metrics_list(self) -> List[str]:
-        return [f"{_id2metric[mid]}@{str(k)}" for mid in self.metrics for k in self.top_show]
-
-    @property
-    def metrics_str(self) -> str:
-        """Get all metrics information.
-
-        Returns:
-            str: A string consist of all metrics information, such as
-                `"Precision@10    Precision@20    NDCG@10    NDCG@20"`.
-        """
-        _colors = itertools.cycle(_text_colors)
-        return '\t'.join([c + f"{m}".ljust(12) + Style.RESET_ALL
-                          for c, m in zip(_colors, self.metrics_list)])
-
-    def evaluate(self, model, test_users: Optional[Iterable[int]]=None) -> MetricReport:
-        """Evaluate `model`.
-
-        Args:
-            model: The model need to be evaluated. This model must have
-                a method `predict(self, users)`, where the argument
-                `users` is a list of users and the return is a 2-D array that
-                contains `users` rating/ranking scores on all items.
-            test_users: The users will be used to test.
-                Default is None and means test all users in user_pos_test.
-
-        Returns:
-            str: A single-line string consist of all results, such as
-                `"0.18663847    0.11239596    0.35824192    0.21479650"`.
-        """
-        # B: batch size
-        # N: the number of items
-        assert hasattr(model, "predict"), "the model must have attribute 'predict'."
-
-        test_users = test_users if test_users is not None else list(self.user_pos_test.keys())
-        assert isinstance(test_users, Iterable), "'test_user' must be iterable."
-        test_users = list(test_users)
-
-        test_users = BatchIterator(test_users, batch_size=self.batch_size, shuffle=False, drop_last=False)
-        bat_results = []
-        for batch_users in test_users:
-            test_items = [self.user_pos_test[u] for u in batch_users]
-            ranking_score = model.predict(batch_users)  # (B,N)
-            assert isinstance(ranking_score, np.ndarray), "'ranking_score' must be an np.ndarray"
-
-            # set the ranking scores of training items to -inf,
-            # then the training items will be sorted at the end of the ranking list.
-            for idx, user in enumerate(batch_users):
-                if user in self.user_pos_train and len(self.user_pos_train[user]) > 0:
-                    train_items = self.user_pos_train[user]
-                    ranking_score[idx][train_items] = -np.inf
-
-            result = eval_score_matrix(ranking_score, test_items, self.metrics,
-                                       top_k=self.max_top, thread_num=self.num_thread)  # (B,k*metric_num)
-            bat_results.append(result)
-
-        # concatenate the batch results to a matrix
-        all_results = np.concatenate(bat_results, axis=0)  # (num_users, metrics_num*max_top)
-        final_results = np.mean(all_results, axis=0)  # (1, metrics_num*max_top)
-
-        final_results = np.reshape(final_results, newshape=[self.metrics_num, self.max_top])  # (metrics_num, max_top)
-        final_results = final_results[:, self.top_show - 1]
-
-        final_results = np.reshape(final_results, newshape=[-1])
-        return MetricReport(self.metrics_list, final_results)
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["MetricReport", "RankingEvaluator", "MetricReport"]
+
+from typing import Sequence, Dict, Union, Optional, Tuple, List, Iterable
+from collections import OrderedDict
+import numpy as np
+import itertools
+from colorama import Fore, Style
+from .batch_iterator import BatchIterator
+from .cython import eval_score_matrix
+
+_text_colors = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN]
+
+
+class MetricReport(object):
+    def __init__(self, metrics: Sequence[str], values: Sequence[float]):
+        assert len(metrics) == len(values), f"The lengths of metrics and values " \
+                                            f"are not equal ({len(metrics)}!={len(values)})."
+        self._results = OrderedDict(zip(metrics, values))
+
+    def metrics(self):
+        return self._results.keys()
+
+    @property
+    def metrics_str(self) -> str:
+        _colors = itertools.cycle(_text_colors)
+        return '\t'.join([c+f"{m}".ljust(12)+Style.RESET_ALL
+                          for c, m in zip(_colors, self.metrics())])
+
+    def values(self):
+        return self._results.values()
+
+    @property
+    def values_str(self) -> str:
+        _colors = itertools.cycle(_text_colors)
+        return '\t'.join([c+f"{v:.8f}".ljust(12)+Style.RESET_ALL
+                          for c, v in zip(_colors, self.values())])
+
+    def items(self):
+        return self._results.items()
+
+    def __getitem__(self, item):
+        if item not in self._results:
+            raise KeyError(item)
+        return self._results[item]
+
+    def __str__(self):
+        return self._results.__str__()
+
+
+_metric2id = {"Precision": 1, "Recall": 2, "MAP": 3, "NDCG": 4, "MRR": 5}
+_id2metric = {value: key for key, value in _metric2id.items()}
+
+
+class RankingEvaluator(object):
+    """Evaluator for item ranking task.
+
+    Evaluation metrics of `Evaluator` are configurable and can
+    automatically fit both leave-one-out and fold-out data splitting
+    without specific indication:
+
+    * **First**, evaluation metrics of this class are configurable via the
+      argument `metric`. Now there are five configurable metrics: `Precision`,
+      `Recall`, `MAP`, `NDCG` and `MRR`.
+
+    * **Second**, this class and its evaluation metrics can automatically fit
+      both leave-one-out and fold-out data splitting without specific indication.
+
+      In **leave-one-out** evaluation:
+        1) `Recall` is equal to `HitRatio`;
+        2) The implementation of `NDCG` is compatible with fold-out;
+        3) `MAP` and `MRR` have same numeric values;
+        4) `Precision` is meaningless.
+    """
+
+    def __init__(self, user_train_dict: Optional[Dict[int, np.ndarray]],
+                 user_test_dict: Dict[int, np.ndarray],
+                 metric: Union[None, str, Tuple[str], List[str]]=None,
+                 top_k: Union[int, List[int], Tuple[int]]=50,
+                 batch_size: int=256, num_thread: int=8):
+        """Initializes a new `Evaluator` instance.
+
+        Args:
+            user_train_dict (dict, None): Each key is user ID and the corresponding
+                value is the list of **training items**.
+            user_test_dict (dict): Each key is user ID and the corresponding
+                value is the list of **test items**.
+            metric (None or list of str): If `metric == None`, metric will
+                be set to `["Precision", "Recall", "MAP", "NDCG", "MRR"]`.
+                Otherwise, `metric` must be one or a sublist of metrics
+                mentioned above. Defaults to `None`.
+            top_k (int or list of int): `top_k` controls the Top-K item ranking
+                performance. If `top_k` is an integer, K ranges from `1` to
+                `top_k`; If `top_k` is a list of integers, K are only assigned
+                these values. Defaults to `50`.
+            batch_size (int): An integer to control the test batch size.
+                Defaults to `1024`.
+            num_thread (int): An integer to control the test thread number.
+                Defaults to `8`.
+
+        Raises:
+             ValueError: If `metric` or one of its element is invalid.
+        """
+        super(RankingEvaluator, self).__init__()
+        if metric is None:
+            metric = ["Precision", "Recall", "MAP", "NDCG", "MRR"]
+        elif isinstance(metric, str):
+            metric = [metric]
+        elif isinstance(metric, (tuple, list)):
+            metric = list(metric)
+        else:
+            raise TypeError("The type of 'metric' (%s) is invalid!" % metric.__class__.__name__)
+
+        for m in metric:
+            assert m in _metric2id, f"'{metric}' is not in ('Precision', 'Recall', 'MAP', 'NDCG', 'MRR')."
+
+        self.user_pos_train = dict()
+        self.user_pos_test = dict()
+        self.set_train_data(user_train_dict)
+        self.set_test_data(user_test_dict)
+
+        self.metrics_num = len(metric)
+        self.metrics = [_metric2id[m] for m in metric]
+        self.num_thread = num_thread
+        self.batch_size = batch_size
+
+        if isinstance(top_k, int):
+            self.max_top = top_k
+            self.top_show = np.arange(top_k) + 1
+        else:
+            self.max_top = max(top_k)
+            self.top_show = np.sort(top_k)
+
+    def set_train_data(self, user_train_dict: Optional[Dict[int, np.ndarray]]=None):
+        self.user_pos_train = user_train_dict if user_train_dict is not None else dict()
+
+    def set_test_data(self, user_test_dict: Dict[int, np.ndarray]):
+        assert len(user_test_dict) > 0, "'user_test_dict' can be empty."
+        self.user_pos_test = user_test_dict
+
+    @property
+    def metrics_list(self) -> List[str]:
+        return [f"{_id2metric[mid]}@{str(k)}" for mid in self.metrics for k in self.top_show]
+
+    @property
+    def metrics_str(self) -> str:
+        """Get all metrics information.
+
+        Returns:
+            str: A string consist of all metrics information, such as
+                `"Precision@10    Precision@20    NDCG@10    NDCG@20"`.
+        """
+        _colors = itertools.cycle(_text_colors)
+        return '\t'.join([c + f"{m}".ljust(12) + Style.RESET_ALL
+                          for c, m in zip(_colors, self.metrics_list)])
+
+    def evaluate(self, model, test_users: Optional[Iterable[int]]=None) -> MetricReport:
+        """Evaluate `model`.
+
+        Args:
+            model: The model need to be evaluated. This model must have
+                a method `predict(self, users)`, where the argument
+                `users` is a list of users and the return is a 2-D array that
+                contains `users` rating/ranking scores on all items.
+            test_users: The users will be used to test.
+                Default is None and means test all users in user_pos_test.
+
+        Returns:
+            str: A single-line string consist of all results, such as
+                `"0.18663847    0.11239596    0.35824192    0.21479650"`.
+        """
+        # B: batch size
+        # N: the number of items
+        assert hasattr(model, "predict"), "the model must have attribute 'predict'."
+
+        test_users = test_users if test_users is not None else list(self.user_pos_test.keys())
+        assert isinstance(test_users, Iterable), "'test_user' must be iterable."
+        test_users = list(test_users)
+
+        test_users = BatchIterator(test_users, batch_size=self.batch_size, shuffle=False, drop_last=False)
+        bat_results = []
+        for batch_users in test_users:
+            test_items = [self.user_pos_test[u] for u in batch_users]
+            ranking_score = model.predict(batch_users)  # (B,N)
+            assert isinstance(ranking_score, np.ndarray), "'ranking_score' must be an np.ndarray"
+
+            # set the ranking scores of training items to -inf,
+            # then the training items will be sorted at the end of the ranking list.
+            for idx, user in enumerate(batch_users):
+                if user in self.user_pos_train and len(self.user_pos_train[user]) > 0:
+                    train_items = self.user_pos_train[user]
+                    ranking_score[idx][train_items] = -np.inf
+
+            result = eval_score_matrix(ranking_score, test_items, self.metrics,
+                                       top_k=self.max_top, thread_num=self.num_thread)  # (B,k*metric_num)
+            bat_results.append(result)
+
+        # concatenate the batch results to a matrix
+        all_results = np.concatenate(bat_results, axis=0)  # (num_users, metrics_num*max_top)
+        final_results = np.mean(all_results, axis=0)  # (1, metrics_num*max_top)
+
+        final_results = np.reshape(final_results, newshape=[self.metrics_num, self.max_top])  # (metrics_num, max_top)
+        final_results = final_results[:, self.top_show - 1]
+
+        final_results = np.reshape(final_results, newshape=[-1])
+        return MetricReport(self.metrics_list, final_results)
```

## skrec/utils/py/__init__.py

 * *Ordering differences only*

```diff
@@ -1,20 +1,20 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-from .generic import OrderedDefaultDict
-from .generic import pad_sequences
-from .generic import md5sum
-from .generic import slugify
-
-from .batch_iterator import BatchIterator
-
-from .decorator import timer
-from .decorator import typeassert
-
-from .random import randint_choice
-from .random import batch_randint_choice
-
-from .evaluator import RankingEvaluator
-from .evaluator import MetricReport
-
-from .config import Config, merge_config_with_cmd_args
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+from .generic import OrderedDefaultDict
+from .generic import pad_sequences
+from .generic import md5sum
+from .generic import slugify
+
+from .batch_iterator import BatchIterator
+
+from .decorator import timer
+from .decorator import typeassert
+
+from .random import randint_choice
+from .random import batch_randint_choice
+
+from .evaluator import RankingEvaluator
+from .evaluator import MetricReport
+
+from .config import Config, merge_config_with_cmd_args
```

## skrec/utils/py/generic.py

 * *Ordering differences only*

```diff
@@ -1,128 +1,128 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["OrderedDefaultDict", "pad_sequences", "md5sum", "slugify"]
-
-import os
-import sys
-import re
-import hashlib
-import unicodedata
-from collections import OrderedDict
-import numpy as np
-
-
-class OrderedDefaultDict(OrderedDict):
-    """ A defaultdict with OrderedDict as its base class.
-    Reference: https://stackoverflow.com/questions/4126348/4127426#4127426
-    """
-
-    def __init__(self, default_factory=None, *args, **kwargs):
-        if not (default_factory is None or callable(default_factory)):
-            raise TypeError('first argument must be callable or None')
-        super(OrderedDefaultDict, self).__init__(*args, **kwargs)
-        self.default_factory = default_factory  # called by __missing__()
-
-    def __missing__(self, key):
-        if self.default_factory is None:
-            raise KeyError(key,)
-        self[key] = value = self.default_factory()
-        return value
-
-    def __reduce__(self):  # Optional, for pickle support.
-        args = (self.default_factory,) if self.default_factory else tuple()
-        return self.__class__, args, None, None, iter(self.items())
-
-    def __repr__(self):  # Optional.
-        return '%s(%r, %r)' % (self.__class__.__name__, self.default_factory, self.items())
-
-
-def pad_sequences(sequences, value=0, max_len=None,
-                  padding='post', truncating='post', dtype=int):
-    """Pads sequences to the same length.
-
-    Args:
-        sequences (list): A list of lists, where each element is a sequence.
-        value (int, float): Padding value. Defaults to `0.`.
-        max_len (int or None): Maximum length of all sequences.
-        padding (str): `"pre"` or `"post"`: pad either before or after each
-            sequence. Defaults to `post`.
-        truncating (str): `"pre"` or `"post"`: remove values from sequences
-            larger than `max_len`, either at the beginning or at the end of
-            the sequences. Defaults to `post`.
-        dtype: Type of the output sequences. Defaults to `np.int32`.
-
-    Returns:
-        np.ndarray: Numpy array with shape `(len(sequences), max_len)`.
-
-    Raises:
-        ValueError: If `padding` or `truncating` is not understood.
-    """
-    lengths = []
-    for x in sequences:
-        try:
-            lengths.append(len(x))
-        except:
-            raise ValueError('`sequences` must be a list of iterables. '
-                             'Found non-iterable: ' + str(x))
-
-    if max_len is None:
-        max_len = np.max(lengths)
-
-    x = np.full([len(sequences), max_len], value, dtype=dtype)
-    for idx, s in enumerate(sequences):
-        if not len(s):
-            continue  # empty list/array was found
-        if truncating == 'pre':
-            trunc = s[-max_len:]
-        elif truncating == 'post':
-            trunc = s[:max_len]
-        else:
-            raise ValueError('Truncating type "%s" not understood' % truncating)
-
-        if padding == 'post':
-            x[idx, :len(trunc)] = trunc
-        elif padding == 'pre':
-            x[idx, -len(trunc):] = trunc
-        else:
-            raise ValueError('Padding type "%s" not understood' % padding)
-    return x
-
-
-def md5sum(*args):
-    """Compute and check MD5 message
-    Args:
-        *args: one or more file paths
-
-    Returns: a list of MD5 message
-    """
-    for filename in args:
-        if not os.path.isfile(filename):
-            raise FileNotFoundError(filename)
-    md5_list = []
-    for filename in args:
-        with open(filename, "rb") as fin:
-            readable_hash = hashlib.md5(fin.read()).hexdigest()
-            md5_list.append(readable_hash)
-    md5_list = md5_list[0] if len(args) == 1 else md5_list
-    return md5_list
-
-
-def slugify(filename, max_length: int=255) -> str:
-    """
-    url1: https://stackoverflow.com/questions/295135/turn-a-string-into-a-valid-filename
-    url2: https://gist.github.com/wassname/1393c4a57cfcbf03641dbc31886123b8
-    Taken from https://github.com/django/django/blob/master/django/utils/text.py
-    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated
-    dashes to single dashes. Remove characters that aren't alphanumerics,
-    underscores, or hyphens. Convert to lowercase. Also strip leading and
-    trailing whitespace, dashes, and underscores.
-    """
-    filename = str(filename)
-    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')
-    filename = re.sub(r'[^\w\s.+=-]', '', filename)
-    filename = re.sub(r'[\s]+', '_', filename).strip('-_')
-    if len(filename) > max_length:
-        print(f"Warning, filename truncated because it was over {max_length}. "
-              f"Filenames may no longer be unique")
-    return filename[:max_length]
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["OrderedDefaultDict", "pad_sequences", "md5sum", "slugify"]
+
+import os
+import sys
+import re
+import hashlib
+import unicodedata
+from collections import OrderedDict
+import numpy as np
+
+
+class OrderedDefaultDict(OrderedDict):
+    """ A defaultdict with OrderedDict as its base class.
+    Reference: https://stackoverflow.com/questions/4126348/4127426#4127426
+    """
+
+    def __init__(self, default_factory=None, *args, **kwargs):
+        if not (default_factory is None or callable(default_factory)):
+            raise TypeError('first argument must be callable or None')
+        super(OrderedDefaultDict, self).__init__(*args, **kwargs)
+        self.default_factory = default_factory  # called by __missing__()
+
+    def __missing__(self, key):
+        if self.default_factory is None:
+            raise KeyError(key,)
+        self[key] = value = self.default_factory()
+        return value
+
+    def __reduce__(self):  # Optional, for pickle support.
+        args = (self.default_factory,) if self.default_factory else tuple()
+        return self.__class__, args, None, None, iter(self.items())
+
+    def __repr__(self):  # Optional.
+        return '%s(%r, %r)' % (self.__class__.__name__, self.default_factory, self.items())
+
+
+def pad_sequences(sequences, value=0, max_len=None,
+                  padding='post', truncating='post', dtype=int):
+    """Pads sequences to the same length.
+
+    Args:
+        sequences (list): A list of lists, where each element is a sequence.
+        value (int, float): Padding value. Defaults to `0.`.
+        max_len (int or None): Maximum length of all sequences.
+        padding (str): `"pre"` or `"post"`: pad either before or after each
+            sequence. Defaults to `post`.
+        truncating (str): `"pre"` or `"post"`: remove values from sequences
+            larger than `max_len`, either at the beginning or at the end of
+            the sequences. Defaults to `post`.
+        dtype: Type of the output sequences. Defaults to `np.int32`.
+
+    Returns:
+        np.ndarray: Numpy array with shape `(len(sequences), max_len)`.
+
+    Raises:
+        ValueError: If `padding` or `truncating` is not understood.
+    """
+    lengths = []
+    for x in sequences:
+        try:
+            lengths.append(len(x))
+        except:
+            raise ValueError('`sequences` must be a list of iterables. '
+                             'Found non-iterable: ' + str(x))
+
+    if max_len is None:
+        max_len = np.max(lengths)
+
+    x = np.full([len(sequences), max_len], value, dtype=dtype)
+    for idx, s in enumerate(sequences):
+        if not len(s):
+            continue  # empty list/array was found
+        if truncating == 'pre':
+            trunc = s[-max_len:]
+        elif truncating == 'post':
+            trunc = s[:max_len]
+        else:
+            raise ValueError('Truncating type "%s" not understood' % truncating)
+
+        if padding == 'post':
+            x[idx, :len(trunc)] = trunc
+        elif padding == 'pre':
+            x[idx, -len(trunc):] = trunc
+        else:
+            raise ValueError('Padding type "%s" not understood' % padding)
+    return x
+
+
+def md5sum(*args):
+    """Compute and check MD5 message
+    Args:
+        *args: one or more file paths
+
+    Returns: a list of MD5 message
+    """
+    for filename in args:
+        if not os.path.isfile(filename):
+            raise FileNotFoundError(filename)
+    md5_list = []
+    for filename in args:
+        with open(filename, "rb") as fin:
+            readable_hash = hashlib.md5(fin.read()).hexdigest()
+            md5_list.append(readable_hash)
+    md5_list = md5_list[0] if len(args) == 1 else md5_list
+    return md5_list
+
+
+def slugify(filename, max_length: int=255) -> str:
+    """
+    url1: https://stackoverflow.com/questions/295135/turn-a-string-into-a-valid-filename
+    url2: https://gist.github.com/wassname/1393c4a57cfcbf03641dbc31886123b8
+    Taken from https://github.com/django/django/blob/master/django/utils/text.py
+    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated
+    dashes to single dashes. Remove characters that aren't alphanumerics,
+    underscores, or hyphens. Convert to lowercase. Also strip leading and
+    trailing whitespace, dashes, and underscores.
+    """
+    filename = str(filename)
+    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')
+    filename = re.sub(r'[^\w\s.+=-]', '', filename)
+    filename = re.sub(r'[\s]+', '_', filename).strip('-_')
+    if len(filename) > max_length:
+        print(f"Warning, filename truncated because it was over {max_length}. "
+              f"Filenames may no longer be unique")
+    return filename[:max_length]
```

## skrec/utils/py/decorator.py

 * *Ordering differences only*

```diff
@@ -1,51 +1,51 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["timer", "typeassert"]
-
-
-import time
-from functools import wraps
-from inspect import signature
-from collections import Iterable
-
-
-def typeassert(*type_args, **type_kwargs):
-    def decorate(func):
-        sig = signature(func)
-        bound_types = sig.bind_partial(*type_args, **type_kwargs).arguments
-
-        @wraps(func)
-        def wrapper(*args, **kwargs):
-            bound_values = sig.bind(*args, **kwargs)
-            for name, value in bound_values.arguments.items():
-                if name in bound_types:
-                    types = bound_types[name]
-                    if not isinstance(types, Iterable):
-                        types = [types]
-
-                    if value is None:
-                        if None in types:
-                            continue
-                        else:
-                            raise TypeError('Argument {} must be {}'.format(name, bound_types[name]))
-
-                    types = tuple([t for t in types if t is not None])
-                    if not isinstance(value, types):
-                        raise TypeError('Argument {} must be {}'.format(name, bound_types[name]))
-            return func(*args, **kwargs)
-        return wrapper
-    return decorate
-
-
-def timer(func):
-    """The timer decorator
-    """
-    @wraps(func)
-    def wrapper(*args, **kwargs):
-        start_time = time.time()
-        result = func(*args, **kwargs)
-        end_time = time.time()
-        print("%s function cost: %fs" % (func.__name__, end_time - start_time))
-        return result
-    return wrapper
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["timer", "typeassert"]
+
+
+import time
+from functools import wraps
+from inspect import signature
+from collections import Iterable
+
+
+def typeassert(*type_args, **type_kwargs):
+    def decorate(func):
+        sig = signature(func)
+        bound_types = sig.bind_partial(*type_args, **type_kwargs).arguments
+
+        @wraps(func)
+        def wrapper(*args, **kwargs):
+            bound_values = sig.bind(*args, **kwargs)
+            for name, value in bound_values.arguments.items():
+                if name in bound_types:
+                    types = bound_types[name]
+                    if not isinstance(types, Iterable):
+                        types = [types]
+
+                    if value is None:
+                        if None in types:
+                            continue
+                        else:
+                            raise TypeError('Argument {} must be {}'.format(name, bound_types[name]))
+
+                    types = tuple([t for t in types if t is not None])
+                    if not isinstance(value, types):
+                        raise TypeError('Argument {} must be {}'.format(name, bound_types[name]))
+            return func(*args, **kwargs)
+        return wrapper
+    return decorate
+
+
+def timer(func):
+    """The timer decorator
+    """
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        start_time = time.time()
+        result = func(*args, **kwargs)
+        end_time = time.time()
+        print("%s function cost: %fs" % (func.__name__, end_time - start_time))
+        return result
+    return wrapper
```

## skrec/utils/py/cython/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .pyx_init import *
-
-from .pyx_random import pyx_randint_choice
-from .pyx_random import pyx_batch_randint_choice
-
-
-from .pyx_eval_matrix import eval_score_matrix
+from .pyx_init import *
+
+from .pyx_random import pyx_randint_choice
+from .pyx_random import pyx_batch_randint_choice
+
+
+from .pyx_eval_matrix import eval_score_matrix
```

## skrec/recommender/MultVAE.py

 * *Ordering differences only*

```diff
@@ -1,227 +1,227 @@
-"""
-Paper: Variational Autoencoders for Collaborative Filtering
-Author: Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara
-Reference: https://github.com/dawenl/vae_cf
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["MultVAE"]
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from typing import List, Dict
-from .base import AbstractRecommender
-from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.py import BatchIterator
-from ..utils.torch import l2_loss, get_initializer
-
-
-class MultVAEConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 reg=0.0,
-                 p_dims=[64],
-                 q_dims=None,
-                 keep_prob=0.5,
-                 anneal_steps=200000,
-                 anneal_cap=0.2,
-                 batch_size=256,
-                 epochs=1000,
-                 early_stop=200,
-                 **kwargs):
-        super(MultVAEConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        # p_dims is decoder's dimensions and q_dims is encoder's dimensions
-        # if q_dims is None, it will be symmetrical with p_dims
-        self.p_dims: List[int] = p_dims
-        self.q_dims: List[int] = q_dims
-        self.keep_prob: float = keep_prob
-        self.anneal_steps: int = anneal_steps
-        self.anneal_cap: float = anneal_cap
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.p_dims, list)
-        assert self.q_dims is None or isinstance(self.q_dims, list)
-        assert isinstance(self.keep_prob, float) and self.keep_prob >= 0
-        assert isinstance(self.anneal_steps, int) and self.anneal_steps >= 0
-        assert isinstance(self.anneal_cap, float) and self.anneal_cap >= 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _MultVAE(nn.Module):
-    def __init__(self, q_dims, p_dims, keep_prob):
-        super(_MultVAE, self).__init__()
-
-        # user and item embeddings
-        self.layers_q = nn.ModuleList()
-        for i, (d_in, d_out) in enumerate(zip(q_dims[:-1], q_dims[1:])):
-            if i == len(q_dims[:-1]) - 1:
-                # we need two sets of parameters for mean and variance, respectively
-                d_out *= 2
-            self.layers_q.append(nn.Linear(d_in, d_out, bias=True))
-
-        self.layers_p = nn.ModuleList()
-        for i, (d_in, d_out) in enumerate(zip(p_dims[:-1], p_dims[1:])):
-            self.layers_p.append(nn.Linear(d_in, d_out, bias=True))
-
-        self.dropout = nn.Dropout(1-keep_prob)
-
-        # weight initialization
-
-        self.reg_params = [layer.weight for layer in self.layers_q] + \
-                          [layer.weight for layer in self.layers_p]
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init = get_initializer("normal")
-
-        for layer in self.layers_q:
-            init(layer.weight)
-            init(layer.bias)
-
-        for layer in self.layers_p:
-            init(layer.weight)
-            init(layer.bias)
-
-    def q_graph(self, input_x):
-        mu_q, std_q, kl_dist = None, None, None
-        h = F.normalize(input_x, p=2, dim=1)
-        h = self.dropout(h)
-
-        for i, layer in enumerate(self.layers_q):
-            h = layer(h)
-            if i != len(self.layers_q) - 1:
-                h = F.tanh(h)
-            else:
-                size = int(h.shape[1] / 2)
-                mu_q, logvar_q = torch.split(h, size, dim=1)
-                std_q = torch.exp(0.5 * logvar_q)
-                kl_dist = torch.sum(0.5*(-logvar_q + logvar_q.exp() + mu_q.pow(2) - 1), dim=1).mean()
-
-        return mu_q, std_q, kl_dist
-
-    def p_graph(self, z):
-        h = z
-
-        for i, layer in enumerate(self.layers_p):
-            h = layer(h)
-            if i != len(self.layers_p) - 1:
-                h = F.tanh(h)
-
-        return h
-
-    def forward(self, input_x):
-        # q-network
-        mu_q, std_q, kl_dist = self.q_graph(input_x)
-        epsilon = std_q.new_empty(std_q.shape)
-        epsilon.normal_()
-        sampled_z = mu_q + float(self.training)*epsilon*std_q
-
-        # p-network
-        logits = self.p_graph(sampled_z)
-
-        return logits, kl_dist
-
-    def predict(self, input_x):
-        ratings, _ = self.forward(input_x)
-
-        return ratings
-
-
-class MultVAE(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = MultVAEConfig(**cfg_dict)
-        super(MultVAE, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.train_csr_mat = self.dataset.train_data.to_csr_matrix()
-        self.train_csr_mat.data[:] = 1.0
-
-        p_dims = config.p_dims
-        self.p_dims = p_dims + [self.num_items]
-        if config.q_dims is None:
-            self.q_dims = self.p_dims[::-1]
-        else:
-            q_dims = config.q_dims
-            q_dims = [self.num_items] + q_dims
-            assert q_dims[0] == self.p_dims[-1], "Input and output dimension must equal each other for autoencoders."
-            assert q_dims[-1] == self.p_dims[0], "Latent dimension for p- and q-network mismatches."
-            self.q_dims = q_dims
-
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.multvae = _MultVAE(self.q_dims, self.p_dims, self.config.keep_prob).to(self.device)
-        self.optimizer = torch.optim.Adam(self.multvae.parameters(), lr=self.config.lr)
-
-    def fit(self):
-        train_users = [user for user in range(self.num_users) if self.train_csr_mat[user].nnz]
-        user_iter = BatchIterator(train_users, batch_size=self.config.batch_size, shuffle=True, drop_last=False)
-
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        update_count = 0.0
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            self.multvae.train()
-            for bat_users in user_iter:
-                bat_input = self.train_csr_mat[bat_users].toarray()
-                if self.config.anneal_steps > 0:
-                    anneal = min(self.config.anneal_cap, 1.*update_count/self.config.anneal_steps)
-                else:
-                    anneal = self.config.anneal_cap
-
-                bat_input = torch.from_numpy(bat_input).float().to(self.device)
-
-                logits, kl_dist = self.multvae(bat_input)
-                log_softmax_var = F.log_softmax(logits, dim=-1)
-                neg_ll = -torch.mul(log_softmax_var, bat_input).sum(dim=-1).mean()
-
-                # apply regularization to weights
-                reg_var = l2_loss(*self.multvae.reg_params)
-                reg_var *= self.config.reg
-
-                # l2 regularization multiply 0.5 to the l2 norm
-                # multiply 2 so that it is back in the same scale
-                neg_elbo = neg_ll + anneal*kl_dist + 2*reg_var
-
-                self.optimizer.zero_grad()
-                neg_elbo.backward()
-                self.optimizer.step()
-                update_count += 1
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.multvae.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        bat_input = self.train_csr_mat[users].toarray()
-        bat_input = torch.from_numpy(bat_input).float().to(self.device)
-        ratings = self.multvae.predict(bat_input).cpu().detach().numpy()
-        return ratings
+"""
+Paper: Variational Autoencoders for Collaborative Filtering
+Author: Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara
+Reference: https://github.com/dawenl/vae_cf
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["MultVAE"]
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from typing import List, Dict
+from .base import AbstractRecommender
+from ..utils.py import Config
+from ..io import Dataset
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import BatchIterator
+from ..utils.torch import l2_loss, get_initializer
+
+
+class MultVAEConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 reg=0.0,
+                 p_dims=[64],
+                 q_dims=None,
+                 keep_prob=0.5,
+                 anneal_steps=200000,
+                 anneal_cap=0.2,
+                 batch_size=256,
+                 epochs=1000,
+                 early_stop=200,
+                 **kwargs):
+        super(MultVAEConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        # p_dims is decoder's dimensions and q_dims is encoder's dimensions
+        # if q_dims is None, it will be symmetrical with p_dims
+        self.p_dims: List[int] = p_dims
+        self.q_dims: List[int] = q_dims
+        self.keep_prob: float = keep_prob
+        self.anneal_steps: int = anneal_steps
+        self.anneal_cap: float = anneal_cap
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.p_dims, list)
+        assert self.q_dims is None or isinstance(self.q_dims, list)
+        assert isinstance(self.keep_prob, float) and self.keep_prob >= 0
+        assert isinstance(self.anneal_steps, int) and self.anneal_steps >= 0
+        assert isinstance(self.anneal_cap, float) and self.anneal_cap >= 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _MultVAE(nn.Module):
+    def __init__(self, q_dims, p_dims, keep_prob):
+        super(_MultVAE, self).__init__()
+
+        # user and item embeddings
+        self.layers_q = nn.ModuleList()
+        for i, (d_in, d_out) in enumerate(zip(q_dims[:-1], q_dims[1:])):
+            if i == len(q_dims[:-1]) - 1:
+                # we need two sets of parameters for mean and variance, respectively
+                d_out *= 2
+            self.layers_q.append(nn.Linear(d_in, d_out, bias=True))
+
+        self.layers_p = nn.ModuleList()
+        for i, (d_in, d_out) in enumerate(zip(p_dims[:-1], p_dims[1:])):
+            self.layers_p.append(nn.Linear(d_in, d_out, bias=True))
+
+        self.dropout = nn.Dropout(1-keep_prob)
+
+        # weight initialization
+
+        self.reg_params = [layer.weight for layer in self.layers_q] + \
+                          [layer.weight for layer in self.layers_p]
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        init = get_initializer("normal")
+
+        for layer in self.layers_q:
+            init(layer.weight)
+            init(layer.bias)
+
+        for layer in self.layers_p:
+            init(layer.weight)
+            init(layer.bias)
+
+    def q_graph(self, input_x):
+        mu_q, std_q, kl_dist = None, None, None
+        h = F.normalize(input_x, p=2, dim=1)
+        h = self.dropout(h)
+
+        for i, layer in enumerate(self.layers_q):
+            h = layer(h)
+            if i != len(self.layers_q) - 1:
+                h = F.tanh(h)
+            else:
+                size = int(h.shape[1] / 2)
+                mu_q, logvar_q = torch.split(h, size, dim=1)
+                std_q = torch.exp(0.5 * logvar_q)
+                kl_dist = torch.sum(0.5*(-logvar_q + logvar_q.exp() + mu_q.pow(2) - 1), dim=1).mean()
+
+        return mu_q, std_q, kl_dist
+
+    def p_graph(self, z):
+        h = z
+
+        for i, layer in enumerate(self.layers_p):
+            h = layer(h)
+            if i != len(self.layers_p) - 1:
+                h = F.tanh(h)
+
+        return h
+
+    def forward(self, input_x):
+        # q-network
+        mu_q, std_q, kl_dist = self.q_graph(input_x)
+        epsilon = std_q.new_empty(std_q.shape)
+        epsilon.normal_()
+        sampled_z = mu_q + float(self.training)*epsilon*std_q
+
+        # p-network
+        logits = self.p_graph(sampled_z)
+
+        return logits, kl_dist
+
+    def predict(self, input_x):
+        ratings, _ = self.forward(input_x)
+
+        return ratings
+
+
+class MultVAE(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = MultVAEConfig(**cfg_dict)
+        super(MultVAE, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.train_csr_mat = self.dataset.train_data.to_csr_matrix()
+        self.train_csr_mat.data[:] = 1.0
+
+        p_dims = config.p_dims
+        self.p_dims = p_dims + [self.num_items]
+        if config.q_dims is None:
+            self.q_dims = self.p_dims[::-1]
+        else:
+            q_dims = config.q_dims
+            q_dims = [self.num_items] + q_dims
+            assert q_dims[0] == self.p_dims[-1], "Input and output dimension must equal each other for autoencoders."
+            assert q_dims[-1] == self.p_dims[0], "Latent dimension for p- and q-network mismatches."
+            self.q_dims = q_dims
+
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+        self.multvae = _MultVAE(self.q_dims, self.p_dims, self.config.keep_prob).to(self.device)
+        self.optimizer = torch.optim.Adam(self.multvae.parameters(), lr=self.config.lr)
+
+    def fit(self):
+        train_users = [user for user in range(self.num_users) if self.train_csr_mat[user].nnz]
+        user_iter = BatchIterator(train_users, batch_size=self.config.batch_size, shuffle=True, drop_last=False)
+
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        update_count = 0.0
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            self.multvae.train()
+            for bat_users in user_iter:
+                bat_input = self.train_csr_mat[bat_users].toarray()
+                if self.config.anneal_steps > 0:
+                    anneal = min(self.config.anneal_cap, 1.*update_count/self.config.anneal_steps)
+                else:
+                    anneal = self.config.anneal_cap
+
+                bat_input = torch.from_numpy(bat_input).float().to(self.device)
+
+                logits, kl_dist = self.multvae(bat_input)
+                log_softmax_var = F.log_softmax(logits, dim=-1)
+                neg_ll = -torch.mul(log_softmax_var, bat_input).sum(dim=-1).mean()
+
+                # apply regularization to weights
+                reg_var = l2_loss(*self.multvae.reg_params)
+                reg_var *= self.config.reg
+
+                # l2 regularization multiply 0.5 to the l2 norm
+                # multiply 2 so that it is back in the same scale
+                neg_elbo = neg_ll + anneal*kl_dist + 2*reg_var
+
+                self.optimizer.zero_grad()
+                neg_elbo.backward()
+                self.optimizer.step()
+                update_count += 1
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.multvae.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        bat_input = self.train_csr_mat[users].toarray()
+        bat_input = torch.from_numpy(bat_input).float().to(self.device)
+        ratings = self.multvae.predict(bat_input).cpu().detach().numpy()
+        return ratings
```

## skrec/recommender/Caser.py

 * *Ordering differences only*

```diff
@@ -1,236 +1,236 @@
-"""
-Paper: Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding
-Author: Jiaxi Tang, and Ke Wang
-Reference: https://github.com/graytowne/caser_pytorch
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["Caser"]
-
-import torch
-import torch.nn as nn
-import numpy as np
-import torch.nn.functional as F
-from typing import Dict
-from .base import AbstractRecommender
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.torch import get_initializer
-from ..utils.torch import sigmoid_cross_entropy
-from ..io import SequentialPairwiseIterator
-
-
-class CaserConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 l2_reg=1e-6,
-                 embed_size=64,
-                 seq_L=5,
-                 seq_T=3,
-                 nv=4,
-                 nh=16,
-                 dropout=0.5,
-                 batch_size=1024,
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(CaserConfig, self).__init__()
-        self.lr: float = lr
-        self.l2_reg: float = l2_reg
-        self.embed_size: int = embed_size
-        self.seq_L: int = seq_L
-        self.seq_T: int = seq_T
-        self.nv: int = nv
-        self.nh: int = nh
-        self.dropout: float = dropout
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.l2_reg, float) and self.l2_reg >= 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.seq_L, int) and self.seq_L > 0
-        assert isinstance(self.seq_T, int) and self.seq_T > 0
-        assert isinstance(self.nv, int) and self.nv > 0
-        assert isinstance(self.nh, int) and self.nh > 0
-        assert isinstance(self.dropout, float)
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _Caser(nn.Module):
-    def __init__(self, num_users, num_items, dims, config: CaserConfig, item_pad_idx=None):
-        super(_Caser, self).__init__()
-        self.args = config
-        self._pad_idx = item_pad_idx
-
-        # init args
-        L = config.seq_L
-        self.n_h = config.nh
-        self.n_v = config.nv
-        self.ac_conv = F.relu
-        self.ac_fc = F.relu
-
-        # user and item embeddings
-        self.user_embeddings = nn.Embedding(num_users, dims)
-        self.item_embeddings = nn.Embedding(num_items, dims, padding_idx=item_pad_idx)
-
-        # vertical conv layer
-        self.conv_v = nn.Conv2d(1, self.n_v, (L, 1))
-
-        # horizontal conv layer
-        lengths = [i + 1 for i in range(L)]
-        self.conv_h = nn.ModuleList([nn.Conv2d(1, self.n_h, (i, dims)) for i in lengths])
-
-        # fully-connected layer
-        self.fc1_dim_v = self.n_v * dims
-        self.fc1_dim_h = self.n_h * len(lengths)
-        fc1_dim_in = self.fc1_dim_v + self.fc1_dim_h
-        # W1, b1 can be encoded with nn.Linear
-        self.fc1 = nn.Linear(fc1_dim_in, dims)
-        # W2, b2 are encoded with nn.Embedding, as we don't need to compute scores for all items
-        self.W2 = nn.Embedding(num_items, dims+dims, padding_idx=item_pad_idx)
-        self.b2 = nn.Embedding(num_items, 1, padding_idx=item_pad_idx)
-        # dropout
-        self.dropout = nn.Dropout(config.dropout)
-
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        # weight initialization
-        init = get_initializer("normal")
-        zero_init = get_initializer("zeros")
-
-        init(self.user_embeddings.weight)
-        init(self.item_embeddings.weight)
-        init(self.W2.weight)
-        zero_init(self.b2.weight)
-        if self._pad_idx is not None:
-            zero_init(self.item_embeddings.weight[self._pad_idx])
-            zero_init(self.W2.weight[self._pad_idx])
-
-    def _forward_user(self, user_var, seq_var):
-        # Embedding Look-up
-        item_embs = self.item_embeddings(seq_var).unsqueeze(1)  # use unsqueeze() to get 4-D
-        user_emb = self.user_embeddings(user_var).squeeze(1)
-
-        # Convolutional Layers
-        out, out_h, out_v = None, None, None
-        # vertical conv layer
-        if self.n_v:
-            out_v = self.conv_v(item_embs)
-            out_v = out_v.view(-1, self.fc1_dim_v)  # prepare for fully connect
-
-        # horizontal conv layer
-        out_hs = list()
-        if self.n_h:
-            for conv in self.conv_h:
-                conv_out = self.ac_conv(conv(item_embs).squeeze(3))
-                pool_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
-                out_hs.append(pool_out)
-            out_h = torch.cat(out_hs, 1)  # prepare for fully connect
-
-        # Fully-connected Layers
-        out = torch.cat([out_v, out_h], 1)
-        # apply dropout
-        out = self.dropout(out)
-
-        # fully-connected layer
-        z = self.ac_fc(self.fc1(out))
-        x = torch.cat([z, user_emb], 1)
-        return x
-
-    def forward(self, user_var, seq_var, item_var):
-        x = self._forward_user(user_var, seq_var)
-
-        w2 = self.W2(item_var)
-        b2 = self.b2(item_var)
-
-        res = torch.baddbmm(b2, w2, x.unsqueeze(2)).squeeze()
-
-        return res
-
-    def predict(self, user_var, seq_var):
-        x = self._forward_user(user_var, seq_var)
-        res = torch.matmul(x, self.W2.weight.T) + self.b2.weight.squeeze()
-        return res
-
-
-class Caser(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = CaserConfig(**cfg_dict)
-        super(Caser, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.pad_idx = self.num_items
-        self.num_items += 1
-
-        self.user_truncated_seq = self.dataset.train_data.to_truncated_seq_dict(self.config.seq_L,
-                                                                                pad_value=self.pad_idx,
-                                                                                padding='pre', truncating='pre')
-
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.caser = _Caser(self.num_users, self.num_items, self.config.embed_size, config).to(self.device)
-        self.optimizer = torch.optim.Adam(self.caser.parameters(), weight_decay=self.config.l2_reg, lr=self.config.lr)
-
-    def fit(self):
-        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
-                                               num_previous=self.config.seq_L, num_next=self.config.seq_T,
-                                               pad=self.pad_idx, batch_size=self.config.batch_size,
-                                               shuffle=True, drop_last=False)
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-
-        for epoch in range(self.config.epochs):
-            self.caser.train()
-            for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
-                bat_users = torch.from_numpy(bat_users).long().to(self.device)
-                bat_item_seqs = torch.from_numpy(bat_item_seqs).long().to(self.device)
-                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
-                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
-                bat_items = torch.cat([bat_pos_items, bat_neg_items], dim=1)
-                bat_ratings = self.caser(bat_users.unsqueeze(dim=1), bat_item_seqs, bat_items)
-
-                yui, yuj = torch.split(bat_ratings, [self.config.seq_T, self.config.seq_T], dim=1)
-                ones = yui.new_ones(yui.size())
-                zeros = yuj.new_zeros(yuj.size())
-                loss = sigmoid_cross_entropy(yui, ones) + sigmoid_cross_entropy(yuj, zeros)
-                loss = loss.mean()
-
-                self.optimizer.zero_grad()
-                loss.backward()
-                self.optimizer.step()
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.caser.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        bat_seq = [self.user_truncated_seq[u] for u in users]
-        bat_seq = torch.from_numpy(np.asarray(bat_seq)).long().to(self.device)
-        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
-        all_ratings = self.caser.predict(users, bat_seq)
-        return all_ratings.cpu().detach().numpy()
+"""
+Paper: Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding
+Author: Jiaxi Tang, and Ke Wang
+Reference: https://github.com/graytowne/caser_pytorch
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["Caser"]
+
+import torch
+import torch.nn as nn
+import numpy as np
+import torch.nn.functional as F
+from typing import Dict
+from .base import AbstractRecommender
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.torch import get_initializer
+from ..utils.torch import sigmoid_cross_entropy
+from ..io import SequentialPairwiseIterator
+
+
+class CaserConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 l2_reg=1e-6,
+                 embed_size=64,
+                 seq_L=5,
+                 seq_T=3,
+                 nv=4,
+                 nh=16,
+                 dropout=0.5,
+                 batch_size=1024,
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(CaserConfig, self).__init__()
+        self.lr: float = lr
+        self.l2_reg: float = l2_reg
+        self.embed_size: int = embed_size
+        self.seq_L: int = seq_L
+        self.seq_T: int = seq_T
+        self.nv: int = nv
+        self.nh: int = nh
+        self.dropout: float = dropout
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.l2_reg, float) and self.l2_reg >= 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.seq_L, int) and self.seq_L > 0
+        assert isinstance(self.seq_T, int) and self.seq_T > 0
+        assert isinstance(self.nv, int) and self.nv > 0
+        assert isinstance(self.nh, int) and self.nh > 0
+        assert isinstance(self.dropout, float)
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _Caser(nn.Module):
+    def __init__(self, num_users, num_items, dims, config: CaserConfig, item_pad_idx=None):
+        super(_Caser, self).__init__()
+        self.args = config
+        self._pad_idx = item_pad_idx
+
+        # init args
+        L = config.seq_L
+        self.n_h = config.nh
+        self.n_v = config.nv
+        self.ac_conv = F.relu
+        self.ac_fc = F.relu
+
+        # user and item embeddings
+        self.user_embeddings = nn.Embedding(num_users, dims)
+        self.item_embeddings = nn.Embedding(num_items, dims, padding_idx=item_pad_idx)
+
+        # vertical conv layer
+        self.conv_v = nn.Conv2d(1, self.n_v, (L, 1))
+
+        # horizontal conv layer
+        lengths = [i + 1 for i in range(L)]
+        self.conv_h = nn.ModuleList([nn.Conv2d(1, self.n_h, (i, dims)) for i in lengths])
+
+        # fully-connected layer
+        self.fc1_dim_v = self.n_v * dims
+        self.fc1_dim_h = self.n_h * len(lengths)
+        fc1_dim_in = self.fc1_dim_v + self.fc1_dim_h
+        # W1, b1 can be encoded with nn.Linear
+        self.fc1 = nn.Linear(fc1_dim_in, dims)
+        # W2, b2 are encoded with nn.Embedding, as we don't need to compute scores for all items
+        self.W2 = nn.Embedding(num_items, dims+dims, padding_idx=item_pad_idx)
+        self.b2 = nn.Embedding(num_items, 1, padding_idx=item_pad_idx)
+        # dropout
+        self.dropout = nn.Dropout(config.dropout)
+
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        # weight initialization
+        init = get_initializer("normal")
+        zero_init = get_initializer("zeros")
+
+        init(self.user_embeddings.weight)
+        init(self.item_embeddings.weight)
+        init(self.W2.weight)
+        zero_init(self.b2.weight)
+        if self._pad_idx is not None:
+            zero_init(self.item_embeddings.weight[self._pad_idx])
+            zero_init(self.W2.weight[self._pad_idx])
+
+    def _forward_user(self, user_var, seq_var):
+        # Embedding Look-up
+        item_embs = self.item_embeddings(seq_var).unsqueeze(1)  # use unsqueeze() to get 4-D
+        user_emb = self.user_embeddings(user_var).squeeze(1)
+
+        # Convolutional Layers
+        out, out_h, out_v = None, None, None
+        # vertical conv layer
+        if self.n_v:
+            out_v = self.conv_v(item_embs)
+            out_v = out_v.view(-1, self.fc1_dim_v)  # prepare for fully connect
+
+        # horizontal conv layer
+        out_hs = list()
+        if self.n_h:
+            for conv in self.conv_h:
+                conv_out = self.ac_conv(conv(item_embs).squeeze(3))
+                pool_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
+                out_hs.append(pool_out)
+            out_h = torch.cat(out_hs, 1)  # prepare for fully connect
+
+        # Fully-connected Layers
+        out = torch.cat([out_v, out_h], 1)
+        # apply dropout
+        out = self.dropout(out)
+
+        # fully-connected layer
+        z = self.ac_fc(self.fc1(out))
+        x = torch.cat([z, user_emb], 1)
+        return x
+
+    def forward(self, user_var, seq_var, item_var):
+        x = self._forward_user(user_var, seq_var)
+
+        w2 = self.W2(item_var)
+        b2 = self.b2(item_var)
+
+        res = torch.baddbmm(b2, w2, x.unsqueeze(2)).squeeze()
+
+        return res
+
+    def predict(self, user_var, seq_var):
+        x = self._forward_user(user_var, seq_var)
+        res = torch.matmul(x, self.W2.weight.T) + self.b2.weight.squeeze()
+        return res
+
+
+class Caser(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = CaserConfig(**cfg_dict)
+        super(Caser, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.pad_idx = self.num_items
+        self.num_items += 1
+
+        self.user_truncated_seq = self.dataset.train_data.to_truncated_seq_dict(self.config.seq_L,
+                                                                                pad_value=self.pad_idx,
+                                                                                padding='pre', truncating='pre')
+
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+        self.caser = _Caser(self.num_users, self.num_items, self.config.embed_size, config).to(self.device)
+        self.optimizer = torch.optim.Adam(self.caser.parameters(), weight_decay=self.config.l2_reg, lr=self.config.lr)
+
+    def fit(self):
+        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
+                                               num_previous=self.config.seq_L, num_next=self.config.seq_T,
+                                               pad=self.pad_idx, batch_size=self.config.batch_size,
+                                               shuffle=True, drop_last=False)
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+
+        for epoch in range(self.config.epochs):
+            self.caser.train()
+            for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
+                bat_users = torch.from_numpy(bat_users).long().to(self.device)
+                bat_item_seqs = torch.from_numpy(bat_item_seqs).long().to(self.device)
+                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
+                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
+                bat_items = torch.cat([bat_pos_items, bat_neg_items], dim=1)
+                bat_ratings = self.caser(bat_users.unsqueeze(dim=1), bat_item_seqs, bat_items)
+
+                yui, yuj = torch.split(bat_ratings, [self.config.seq_T, self.config.seq_T], dim=1)
+                ones = yui.new_ones(yui.size())
+                zeros = yuj.new_zeros(yuj.size())
+                loss = sigmoid_cross_entropy(yui, ones) + sigmoid_cross_entropy(yuj, zeros)
+                loss = loss.mean()
+
+                self.optimizer.zero_grad()
+                loss.backward()
+                self.optimizer.step()
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.caser.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        bat_seq = [self.user_truncated_seq[u] for u in users]
+        bat_seq = torch.from_numpy(np.asarray(bat_seq)).long().to(self.device)
+        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
+        all_ratings = self.caser.predict(users, bat_seq)
+        return all_ratings.cpu().detach().numpy()
```

## skrec/recommender/CDAE.py

 * *Ordering differences only*

```diff
@@ -1,232 +1,232 @@
-"""
-Paper: Collaborative Denoising Auto-Encoder for Top-N Recommender Systems
-Author: Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["CDAE"]
-
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.sparse as torch_sp
-from typing import Dict
-from .base import AbstractRecommender
-from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.py import BatchIterator
-from ..utils.torch import l2_loss, get_initializer, inner_product
-from ..utils.torch import square_loss, sigmoid_cross_entropy
-from ..utils.torch import sp_mat_to_sp_tensor, dropout_sparse
-from ..utils.py import randint_choice
-
-
-class CDAEConfig(Config):
-    def __init__(self,
-                 lr=0.001,
-                 reg=0.001,
-                 hidden_dim=64,
-                 dropout=0.5,
-                 num_neg=5,
-                 hidden_act="sigmoid",
-                 loss_func="sigmoid_cross_entropy",
-                 batch_size=256,
-                 epochs=1000,
-                 early_stop=200,
-                 **kwargs):
-        super(CDAEConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.hidden_dim: int = hidden_dim
-        self.dropout: float = dropout
-        self.num_neg: int = num_neg
-        self.hidden_act: str = hidden_act  # hidden_act = identity, sigmoid
-        self.loss_func: str = loss_func  # loss_func = sigmoid_cross_entropy, square
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.hidden_dim, int) and self.hidden_dim > 0
-        assert isinstance(self.dropout, float) and self.dropout < 1.0
-        assert isinstance(self.num_neg, int) and self.num_neg >= 0
-        assert isinstance(self.hidden_act, str) and self.hidden_act in {"identity", "sigmoid"}
-        assert isinstance(self.loss_func, str) and self.loss_func in {"sigmoid_cross_entropy", "square"}
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _CDAE(nn.Module):
-    def __init__(self, num_users, num_items, embed_dim, dropout, hidden_act):
-        super(_CDAE, self).__init__()
-
-        # user and item embeddings
-        self.en_embeddings = nn.Embedding(num_items, embed_dim)
-        self.en_offset = nn.Parameter(torch.Tensor(embed_dim))
-        self.de_embeddings = nn.Embedding(num_items, embed_dim)
-        self.de_bias = nn.Embedding(num_items, 1)
-        self.user_embeddings = nn.Embedding(num_users, embed_dim)
-
-        self.dropout = dropout
-        self.hidden_act = hidden_act
-
-        # weight initialization
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init = get_initializer("normal")
-        zero_init = get_initializer("zeros")
-
-        init(self.en_embeddings.weight)
-        zero_init(self.en_offset)
-
-        init(self.de_embeddings.weight)
-        zero_init(self.de_bias.weight)
-
-        init(self.user_embeddings.weight)
-
-    def forward(self, user_ids, bat_idx, sp_item_mat, bat_items):
-        hidden = self._encoding(user_ids, sp_item_mat)  # (b,d)
-
-        # decoding
-        de_item_embs = self.de_embeddings(bat_items)  # (l,d)
-        de_bias = self.de_bias(bat_items).squeeze()
-        hidden = F.embedding(bat_idx, hidden)  # (l,d)
-
-        ratings = inner_product(hidden, de_item_embs) + de_bias
-
-        # reg loss
-        bat_items = torch.unique(bat_items, sorted=False)
-        reg_loss = l2_loss(self.en_embeddings(bat_items), self.en_offset,
-                           self.user_embeddings(user_ids),
-                           self.de_embeddings(bat_items), self.de_bias(bat_items))
-
-        return ratings, reg_loss
-
-    def _encoding(self, user_ids, sp_item_mat):
-
-        corruption = dropout_sparse(sp_item_mat, 1-self.dropout, self.training)
-
-        en_item_embs = self.en_embeddings.weight  # (n,d)
-        hidden = torch_sp.mm(corruption, en_item_embs)  # (b,n)x(n,d)->(b,d)
-
-        user_embs = self.user_embeddings(user_ids)  # (b,d)
-        hidden += user_embs  # add user vector
-        hidden += self.en_offset.view([1, -1])  # add bias
-        hidden = self.hidden_act(hidden)  # hidden activate, z_u
-        return hidden  # (b,d)
-
-    def predict(self, user_ids, sp_item_mat):
-        user_emb = self._encoding(user_ids, sp_item_mat)  # (b,d)
-        ratings = user_emb.matmul(self.de_embeddings.weight.T)  # (b,d)x(d,n)->(b,n)
-        ratings += self.de_bias.weight.view([1, -1])
-        return ratings
-
-
-class CDAE(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = CDAEConfig(**cfg_dict)
-        super(CDAE, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.train_csr_mat = self.dataset.train_data.to_csr_matrix()
-        self.train_csr_mat.data[:] = 1.0
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-
-        if self.config.hidden_act == "identity":
-            hidden_act = nn.Identity()
-        elif self.config.hidden_act == "sigmoid":
-            hidden_act = nn.Sigmoid()
-        else:
-            raise ValueError(f"hidden activate function '{self.config.hidden_act}' is invalid.")
-
-        if config.loss_func == "sigmoid_cross_entropy":
-            self.loss_func = sigmoid_cross_entropy
-        elif config.loss_func == "square_loss":
-            self.loss_func = square_loss
-        else:
-            raise ValueError(f"loss function '{self.config.loss_func}' is invalid.")
-
-        self.cdae = _CDAE(self.num_users, self.num_items, self.config.hidden_dim,
-                          self.config.dropout, hidden_act).to(self.device)
-        self.optimizer = torch.optim.Adam(self.cdae.parameters(), lr=self.config.lr)
-
-    def fit(self):
-        train_users = [user for user in range(self.num_users) if self.train_csr_mat[user].nnz]
-        user_iter = BatchIterator(train_users, batch_size=self.config.batch_size, shuffle=True, drop_last=False)
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            self.cdae.train()
-            for bat_users in user_iter:
-                bat_sp_mat = self.train_csr_mat[bat_users]
-                bat_items = []
-                bat_labels = []
-                bat_idx = []  # used to decoder
-                for idx, _ in enumerate(bat_users):
-                    pos_items = bat_sp_mat[idx].indices
-                    neg_items = randint_choice(self.num_items, size=bat_sp_mat[idx].nnz*self.config.num_neg,
-                                               replace=True, exclusion=pos_items)
-                    neg_items = np.unique(neg_items)
-                    bat_sp_mat[idx, neg_items] = 1
-
-                    bat_items.append(pos_items)
-                    bat_labels.append(np.ones_like(pos_items, dtype=np.float32))
-                    bat_items.append(neg_items)
-                    bat_labels.append(np.zeros_like(neg_items, dtype=np.float32))
-                    bat_idx.append(np.full(len(pos_items)+len(neg_items), idx, dtype=np.int32))
-
-                bat_items = np.concatenate(bat_items)
-                bat_labels = np.concatenate(bat_labels)
-                bat_idx = np.concatenate(bat_idx)
-                bat_users = np.asarray(bat_users)
-
-                bat_sp_mat = sp_mat_to_sp_tensor(bat_sp_mat).to(self.device)
-                bat_items = torch.from_numpy(bat_items).long().to(self.device)
-                bat_labels = torch.from_numpy(bat_labels).float().to(self.device)
-
-                bat_idx = torch.from_numpy(bat_idx).long().to(self.device)
-                bat_users = torch.from_numpy(bat_users).long().to(self.device)
-
-                hat_y, reg_loss = self.cdae(bat_users, bat_idx, bat_sp_mat, bat_items)
-
-                loss = self.loss_func(hat_y, bat_labels)
-                loss = loss.sum()
-
-                loss += self.config.reg * reg_loss
-                self.optimizer.zero_grad()
-                loss.backward()
-                self.optimizer.step()
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.cdae.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        user_ids = torch.from_numpy(np.asarray(users)).long().to(self.device)
-        sp_item_mat = sp_mat_to_sp_tensor(self.train_csr_mat[users]).to(self.device)
-        ratings = self.cdae.predict(user_ids, sp_item_mat).cpu().detach().numpy()
-        return ratings
+"""
+Paper: Collaborative Denoising Auto-Encoder for Top-N Recommender Systems
+Author: Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["CDAE"]
+
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.sparse as torch_sp
+from typing import Dict
+from .base import AbstractRecommender
+from ..utils.py import Config
+from ..io import Dataset
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import BatchIterator
+from ..utils.torch import l2_loss, get_initializer, inner_product
+from ..utils.torch import square_loss, sigmoid_cross_entropy
+from ..utils.torch import sp_mat_to_sp_tensor, dropout_sparse
+from ..utils.py import randint_choice
+
+
+class CDAEConfig(Config):
+    def __init__(self,
+                 lr=0.001,
+                 reg=0.001,
+                 hidden_dim=64,
+                 dropout=0.5,
+                 num_neg=5,
+                 hidden_act="sigmoid",
+                 loss_func="sigmoid_cross_entropy",
+                 batch_size=256,
+                 epochs=1000,
+                 early_stop=200,
+                 **kwargs):
+        super(CDAEConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.hidden_dim: int = hidden_dim
+        self.dropout: float = dropout
+        self.num_neg: int = num_neg
+        self.hidden_act: str = hidden_act  # hidden_act = identity, sigmoid
+        self.loss_func: str = loss_func  # loss_func = sigmoid_cross_entropy, square
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.hidden_dim, int) and self.hidden_dim > 0
+        assert isinstance(self.dropout, float) and self.dropout < 1.0
+        assert isinstance(self.num_neg, int) and self.num_neg >= 0
+        assert isinstance(self.hidden_act, str) and self.hidden_act in {"identity", "sigmoid"}
+        assert isinstance(self.loss_func, str) and self.loss_func in {"sigmoid_cross_entropy", "square"}
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _CDAE(nn.Module):
+    def __init__(self, num_users, num_items, embed_dim, dropout, hidden_act):
+        super(_CDAE, self).__init__()
+
+        # user and item embeddings
+        self.en_embeddings = nn.Embedding(num_items, embed_dim)
+        self.en_offset = nn.Parameter(torch.Tensor(embed_dim))
+        self.de_embeddings = nn.Embedding(num_items, embed_dim)
+        self.de_bias = nn.Embedding(num_items, 1)
+        self.user_embeddings = nn.Embedding(num_users, embed_dim)
+
+        self.dropout = dropout
+        self.hidden_act = hidden_act
+
+        # weight initialization
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        init = get_initializer("normal")
+        zero_init = get_initializer("zeros")
+
+        init(self.en_embeddings.weight)
+        zero_init(self.en_offset)
+
+        init(self.de_embeddings.weight)
+        zero_init(self.de_bias.weight)
+
+        init(self.user_embeddings.weight)
+
+    def forward(self, user_ids, bat_idx, sp_item_mat, bat_items):
+        hidden = self._encoding(user_ids, sp_item_mat)  # (b,d)
+
+        # decoding
+        de_item_embs = self.de_embeddings(bat_items)  # (l,d)
+        de_bias = self.de_bias(bat_items).squeeze()
+        hidden = F.embedding(bat_idx, hidden)  # (l,d)
+
+        ratings = inner_product(hidden, de_item_embs) + de_bias
+
+        # reg loss
+        bat_items = torch.unique(bat_items, sorted=False)
+        reg_loss = l2_loss(self.en_embeddings(bat_items), self.en_offset,
+                           self.user_embeddings(user_ids),
+                           self.de_embeddings(bat_items), self.de_bias(bat_items))
+
+        return ratings, reg_loss
+
+    def _encoding(self, user_ids, sp_item_mat):
+
+        corruption = dropout_sparse(sp_item_mat, 1-self.dropout, self.training)
+
+        en_item_embs = self.en_embeddings.weight  # (n,d)
+        hidden = torch_sp.mm(corruption, en_item_embs)  # (b,n)x(n,d)->(b,d)
+
+        user_embs = self.user_embeddings(user_ids)  # (b,d)
+        hidden += user_embs  # add user vector
+        hidden += self.en_offset.view([1, -1])  # add bias
+        hidden = self.hidden_act(hidden)  # hidden activate, z_u
+        return hidden  # (b,d)
+
+    def predict(self, user_ids, sp_item_mat):
+        user_emb = self._encoding(user_ids, sp_item_mat)  # (b,d)
+        ratings = user_emb.matmul(self.de_embeddings.weight.T)  # (b,d)x(d,n)->(b,n)
+        ratings += self.de_bias.weight.view([1, -1])
+        return ratings
+
+
+class CDAE(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = CDAEConfig(**cfg_dict)
+        super(CDAE, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.train_csr_mat = self.dataset.train_data.to_csr_matrix()
+        self.train_csr_mat.data[:] = 1.0
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+
+        if self.config.hidden_act == "identity":
+            hidden_act = nn.Identity()
+        elif self.config.hidden_act == "sigmoid":
+            hidden_act = nn.Sigmoid()
+        else:
+            raise ValueError(f"hidden activate function '{self.config.hidden_act}' is invalid.")
+
+        if config.loss_func == "sigmoid_cross_entropy":
+            self.loss_func = sigmoid_cross_entropy
+        elif config.loss_func == "square_loss":
+            self.loss_func = square_loss
+        else:
+            raise ValueError(f"loss function '{self.config.loss_func}' is invalid.")
+
+        self.cdae = _CDAE(self.num_users, self.num_items, self.config.hidden_dim,
+                          self.config.dropout, hidden_act).to(self.device)
+        self.optimizer = torch.optim.Adam(self.cdae.parameters(), lr=self.config.lr)
+
+    def fit(self):
+        train_users = [user for user in range(self.num_users) if self.train_csr_mat[user].nnz]
+        user_iter = BatchIterator(train_users, batch_size=self.config.batch_size, shuffle=True, drop_last=False)
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            self.cdae.train()
+            for bat_users in user_iter:
+                bat_sp_mat = self.train_csr_mat[bat_users]
+                bat_items = []
+                bat_labels = []
+                bat_idx = []  # used to decoder
+                for idx, _ in enumerate(bat_users):
+                    pos_items = bat_sp_mat[idx].indices
+                    neg_items = randint_choice(self.num_items, size=bat_sp_mat[idx].nnz*self.config.num_neg,
+                                               replace=True, exclusion=pos_items)
+                    neg_items = np.unique(neg_items)
+                    bat_sp_mat[idx, neg_items] = 1
+
+                    bat_items.append(pos_items)
+                    bat_labels.append(np.ones_like(pos_items, dtype=np.float32))
+                    bat_items.append(neg_items)
+                    bat_labels.append(np.zeros_like(neg_items, dtype=np.float32))
+                    bat_idx.append(np.full(len(pos_items)+len(neg_items), idx, dtype=np.int32))
+
+                bat_items = np.concatenate(bat_items)
+                bat_labels = np.concatenate(bat_labels)
+                bat_idx = np.concatenate(bat_idx)
+                bat_users = np.asarray(bat_users)
+
+                bat_sp_mat = sp_mat_to_sp_tensor(bat_sp_mat).to(self.device)
+                bat_items = torch.from_numpy(bat_items).long().to(self.device)
+                bat_labels = torch.from_numpy(bat_labels).float().to(self.device)
+
+                bat_idx = torch.from_numpy(bat_idx).long().to(self.device)
+                bat_users = torch.from_numpy(bat_users).long().to(self.device)
+
+                hat_y, reg_loss = self.cdae(bat_users, bat_idx, bat_sp_mat, bat_items)
+
+                loss = self.loss_func(hat_y, bat_labels)
+                loss = loss.sum()
+
+                loss += self.config.reg * reg_loss
+                self.optimizer.zero_grad()
+                loss.backward()
+                self.optimizer.step()
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.cdae.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        user_ids = torch.from_numpy(np.asarray(users)).long().to(self.device)
+        sp_item_mat = sp_mat_to_sp_tensor(self.train_csr_mat[users]).to(self.device)
+        ratings = self.cdae.predict(user_ids, sp_item_mat).cpu().detach().numpy()
+        return ratings
```

## skrec/recommender/HGN.py

 * *Ordering differences only*

```diff
@@ -1,233 +1,233 @@
-"""
-Paper: Hierarchical Gating Networks for Sequential Recommendation
-Author: Chen Ma, Peng Kang, and Xue Liu
-Reference: https://github.com/allenjack/HGN
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["HGN"]
-
-import numpy as np
-import torch
-import torch.nn as nn
-from torch.nn import Parameter
-from typing import Dict
-from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.torch import bpr_loss, get_initializer
-from ..io import SequentialPairwiseIterator
-
-
-class HGNConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 reg=1e-3,
-                 seq_L=5,
-                 seq_T=3,
-                 embed_size=64,
-                 batch_size=1024,
-                 epochs=1000,
-                 early_stop=100,
-                 **kwargs):
-        super(HGNConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.seq_L: int = seq_L
-        self.seq_T: int = seq_T
-        self.embed_size: int = embed_size
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.seq_L, int) and self.seq_L > 0
-        assert isinstance(self.seq_T, int) and self.seq_T > 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _HGN(nn.Module):
-    def __init__(self, num_users, num_items, dims, seq_L, item_pad_idx=None):
-        super(_HGN, self).__init__()
-        self._item_pad_idx = item_pad_idx
-        self.user_embeddings = nn.Embedding(num_users, dims)
-        self.item_embeddings = nn.Embedding(num_items, dims, padding_idx=item_pad_idx)
-
-        self.feature_gate_item = nn.Linear(dims, dims)
-        self.feature_gate_user = nn.Linear(dims, dims)
-
-        self.instance_gate_item = Parameter(torch.Tensor(dims, 1))
-        self.instance_gate_user = Parameter(torch.Tensor(dims, seq_L))
-
-        self.W2 = nn.Embedding(num_items, dims, padding_idx=item_pad_idx)
-        self.b2 = nn.Embedding(num_items, 1, padding_idx=item_pad_idx)
-
-        # weight initialization
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        # weight initialization
-        init = get_initializer("normal")
-        he_init = get_initializer("he_uniform")
-        xavier_init = get_initializer("xavier_uniform")
-        zero_init = get_initializer("zeros")
-
-        init(self.user_embeddings.weight)
-        init(self.item_embeddings.weight)
-
-        he_init(self.feature_gate_user.weight)
-        he_init(self.feature_gate_item.weight)
-        zero_init(self.feature_gate_user.bias)
-        zero_init(self.feature_gate_item.bias)
-
-        xavier_init(self.instance_gate_item)
-        xavier_init(self.instance_gate_user)
-
-        init(self.W2.weight)
-        zero_init(self.b2.weight)
-
-        if self._item_pad_idx is not None:
-            zero_init(self.item_embeddings.weight[self._item_pad_idx])
-            zero_init(self.W2.weight[self._item_pad_idx])
-
-    def _forward_user(self, user_emb, item_embs):
-        # feature gating
-        gate = torch.sigmoid(self.feature_gate_item(item_embs) +
-                             self.feature_gate_user(user_emb).unsqueeze(1))
-        gated_item = item_embs * gate
-
-        # instance gating
-        term1 = torch.matmul(gated_item, self.instance_gate_item.unsqueeze(0))  # (b,l,d)x(1,d,1)->(b,l,1)
-        term2 = user_emb.mm(self.instance_gate_user)  # (b,d)x(d,l)->(b,l)
-
-        instance_score = torch.sigmoid(term1.squeeze() + term2)  # (b,l)
-        union_out = gated_item * instance_score.unsqueeze(2)
-        union_out = torch.sum(union_out, dim=1)
-        union_out = union_out / torch.sum(instance_score, dim=1).unsqueeze(1)
-        return union_out
-
-    def forward(self, user_ids, item_seq_ids, target_item_ids):
-        item_embs = self.item_embeddings(item_seq_ids)
-        user_emb = self.user_embeddings(user_ids)
-        union_out = self._forward_user(user_emb, item_embs)
-
-        w2 = self.W2(target_item_ids)
-        b2 = self.b2(target_item_ids)
-
-        # MF
-        # res = w2.matmul(user_emb.unsqueeze(dim=2)).squeeze()+b2.squeeze()
-        torch.matmul(w2, user_emb.unsqueeze(dim=2))
-        res = torch.baddbmm(b2, w2, user_emb.unsqueeze(2)).squeeze()
-
-        # union-level
-        # res += union_out.unsqueeze(1).matmul(w2.permute(0, 2, 1)).squeeze()
-        res += torch.bmm(union_out.unsqueeze(1), w2.permute(0, 2, 1)).squeeze()
-
-        # item-item product
-        # res += item_embs.matmul(w2.permute(0, 2, 1)).squeeze().sum(dim=1)
-        rel_score = item_embs.bmm(w2.permute(0, 2, 1))
-        rel_score = torch.sum(rel_score, dim=1)
-        res += rel_score
-
-        return res
-
-    def clip_parameters_norm(self):
-        # to avoid NAN
-        self.feature_gate_user.weight.data.renorm_(p=2, dim=1, maxnorm=1)
-        self.feature_gate_item.weight.data.renorm_(p=2, dim=1, maxnorm=1)
-
-    def predict(self, user_ids, item_seq_ids):
-        item_embs = self.item_embeddings(item_seq_ids)
-        user_emb = self.user_embeddings(user_ids)
-        union_out = self._forward_user(user_emb, item_embs)
-        w2 = self.W2.weight
-        b2 = self.b2.weight.squeeze()
-
-        # MF
-        res = user_emb.mm(w2.T) + b2  # (b,d)x(d,n)->(b,n)
-
-        # union-level
-        res += union_out.mm(w2.T)  # (b,d)x(d,n)->(b,n)
-
-        # item-item product
-        res += torch.matmul(item_embs, w2.T.unsqueeze(dim=0)).sum(dim=1)  # (b,l,d)x(1,d,n)->(b,l,n)->(b,n)
-
-        return res
-
-
-class HGN(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = HGNConfig(**cfg_dict)
-        super(HGN, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.pad_idx = self.num_items
-        self.num_items += 1
-
-        self.user_truncated_seq = self.dataset.train_data.to_truncated_seq_dict(config.seq_L,
-                                                                                pad_value=self.pad_idx,
-                                                                                padding='pre', truncating='pre')
-
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.hgn = _HGN(self.num_users, self.num_items, config.embed_size, config.seq_L, self.pad_idx).to(self.device)
-        self.optimizer = torch.optim.Adam(self.hgn.parameters(), weight_decay=config.reg, lr=config.lr)
-
-    def fit(self):
-        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
-                                               num_previous=self.config.seq_L, num_next=self.config.seq_T,
-                                               pad=self.pad_idx, batch_size=self.config.batch_size,
-                                               shuffle=True, drop_last=False)
-
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            self.hgn.train()
-            for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
-                bat_users = torch.from_numpy(bat_users).long().to(self.device)
-                bat_item_seqs = torch.from_numpy(bat_item_seqs).long().to(self.device)
-                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
-                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
-                bat_items = torch.cat([bat_pos_items, bat_neg_items], dim=1)
-                bat_ratings = self.hgn(bat_users, bat_item_seqs, bat_items)
-
-                yui, yuj = torch.split(bat_ratings, [self.config.seq_T, self.config.seq_T], dim=1)
-                loss = bpr_loss(yui, yuj).sum()
-
-                self.optimizer.zero_grad()
-                loss.backward()
-                self.optimizer.step()
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.hgn.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        bat_seq = [self.user_truncated_seq[u] for u in users]
-        bat_seq = torch.from_numpy(np.asarray(bat_seq)).long().to(self.device)
-        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
-        all_ratings = self.hgn.predict(users, bat_seq)
-        return all_ratings.cpu().detach().numpy()
+"""
+Paper: Hierarchical Gating Networks for Sequential Recommendation
+Author: Chen Ma, Peng Kang, and Xue Liu
+Reference: https://github.com/allenjack/HGN
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["HGN"]
+
+import numpy as np
+import torch
+import torch.nn as nn
+from torch.nn import Parameter
+from typing import Dict
+from .base import AbstractRecommender
+from ..utils.py import RankingEvaluator, MetricReport
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.torch import bpr_loss, get_initializer
+from ..io import SequentialPairwiseIterator
+
+
+class HGNConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 reg=1e-3,
+                 seq_L=5,
+                 seq_T=3,
+                 embed_size=64,
+                 batch_size=1024,
+                 epochs=1000,
+                 early_stop=100,
+                 **kwargs):
+        super(HGNConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.seq_L: int = seq_L
+        self.seq_T: int = seq_T
+        self.embed_size: int = embed_size
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.seq_L, int) and self.seq_L > 0
+        assert isinstance(self.seq_T, int) and self.seq_T > 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _HGN(nn.Module):
+    def __init__(self, num_users, num_items, dims, seq_L, item_pad_idx=None):
+        super(_HGN, self).__init__()
+        self._item_pad_idx = item_pad_idx
+        self.user_embeddings = nn.Embedding(num_users, dims)
+        self.item_embeddings = nn.Embedding(num_items, dims, padding_idx=item_pad_idx)
+
+        self.feature_gate_item = nn.Linear(dims, dims)
+        self.feature_gate_user = nn.Linear(dims, dims)
+
+        self.instance_gate_item = Parameter(torch.Tensor(dims, 1))
+        self.instance_gate_user = Parameter(torch.Tensor(dims, seq_L))
+
+        self.W2 = nn.Embedding(num_items, dims, padding_idx=item_pad_idx)
+        self.b2 = nn.Embedding(num_items, 1, padding_idx=item_pad_idx)
+
+        # weight initialization
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        # weight initialization
+        init = get_initializer("normal")
+        he_init = get_initializer("he_uniform")
+        xavier_init = get_initializer("xavier_uniform")
+        zero_init = get_initializer("zeros")
+
+        init(self.user_embeddings.weight)
+        init(self.item_embeddings.weight)
+
+        he_init(self.feature_gate_user.weight)
+        he_init(self.feature_gate_item.weight)
+        zero_init(self.feature_gate_user.bias)
+        zero_init(self.feature_gate_item.bias)
+
+        xavier_init(self.instance_gate_item)
+        xavier_init(self.instance_gate_user)
+
+        init(self.W2.weight)
+        zero_init(self.b2.weight)
+
+        if self._item_pad_idx is not None:
+            zero_init(self.item_embeddings.weight[self._item_pad_idx])
+            zero_init(self.W2.weight[self._item_pad_idx])
+
+    def _forward_user(self, user_emb, item_embs):
+        # feature gating
+        gate = torch.sigmoid(self.feature_gate_item(item_embs) +
+                             self.feature_gate_user(user_emb).unsqueeze(1))
+        gated_item = item_embs * gate
+
+        # instance gating
+        term1 = torch.matmul(gated_item, self.instance_gate_item.unsqueeze(0))  # (b,l,d)x(1,d,1)->(b,l,1)
+        term2 = user_emb.mm(self.instance_gate_user)  # (b,d)x(d,l)->(b,l)
+
+        instance_score = torch.sigmoid(term1.squeeze() + term2)  # (b,l)
+        union_out = gated_item * instance_score.unsqueeze(2)
+        union_out = torch.sum(union_out, dim=1)
+        union_out = union_out / torch.sum(instance_score, dim=1).unsqueeze(1)
+        return union_out
+
+    def forward(self, user_ids, item_seq_ids, target_item_ids):
+        item_embs = self.item_embeddings(item_seq_ids)
+        user_emb = self.user_embeddings(user_ids)
+        union_out = self._forward_user(user_emb, item_embs)
+
+        w2 = self.W2(target_item_ids)
+        b2 = self.b2(target_item_ids)
+
+        # MF
+        # res = w2.matmul(user_emb.unsqueeze(dim=2)).squeeze()+b2.squeeze()
+        torch.matmul(w2, user_emb.unsqueeze(dim=2))
+        res = torch.baddbmm(b2, w2, user_emb.unsqueeze(2)).squeeze()
+
+        # union-level
+        # res += union_out.unsqueeze(1).matmul(w2.permute(0, 2, 1)).squeeze()
+        res += torch.bmm(union_out.unsqueeze(1), w2.permute(0, 2, 1)).squeeze()
+
+        # item-item product
+        # res += item_embs.matmul(w2.permute(0, 2, 1)).squeeze().sum(dim=1)
+        rel_score = item_embs.bmm(w2.permute(0, 2, 1))
+        rel_score = torch.sum(rel_score, dim=1)
+        res += rel_score
+
+        return res
+
+    def clip_parameters_norm(self):
+        # to avoid NAN
+        self.feature_gate_user.weight.data.renorm_(p=2, dim=1, maxnorm=1)
+        self.feature_gate_item.weight.data.renorm_(p=2, dim=1, maxnorm=1)
+
+    def predict(self, user_ids, item_seq_ids):
+        item_embs = self.item_embeddings(item_seq_ids)
+        user_emb = self.user_embeddings(user_ids)
+        union_out = self._forward_user(user_emb, item_embs)
+        w2 = self.W2.weight
+        b2 = self.b2.weight.squeeze()
+
+        # MF
+        res = user_emb.mm(w2.T) + b2  # (b,d)x(d,n)->(b,n)
+
+        # union-level
+        res += union_out.mm(w2.T)  # (b,d)x(d,n)->(b,n)
+
+        # item-item product
+        res += torch.matmul(item_embs, w2.T.unsqueeze(dim=0)).sum(dim=1)  # (b,l,d)x(1,d,n)->(b,l,n)->(b,n)
+
+        return res
+
+
+class HGN(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = HGNConfig(**cfg_dict)
+        super(HGN, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.pad_idx = self.num_items
+        self.num_items += 1
+
+        self.user_truncated_seq = self.dataset.train_data.to_truncated_seq_dict(config.seq_L,
+                                                                                pad_value=self.pad_idx,
+                                                                                padding='pre', truncating='pre')
+
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+        self.hgn = _HGN(self.num_users, self.num_items, config.embed_size, config.seq_L, self.pad_idx).to(self.device)
+        self.optimizer = torch.optim.Adam(self.hgn.parameters(), weight_decay=config.reg, lr=config.lr)
+
+    def fit(self):
+        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
+                                               num_previous=self.config.seq_L, num_next=self.config.seq_T,
+                                               pad=self.pad_idx, batch_size=self.config.batch_size,
+                                               shuffle=True, drop_last=False)
+
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            self.hgn.train()
+            for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
+                bat_users = torch.from_numpy(bat_users).long().to(self.device)
+                bat_item_seqs = torch.from_numpy(bat_item_seqs).long().to(self.device)
+                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
+                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
+                bat_items = torch.cat([bat_pos_items, bat_neg_items], dim=1)
+                bat_ratings = self.hgn(bat_users, bat_item_seqs, bat_items)
+
+                yui, yuj = torch.split(bat_ratings, [self.config.seq_T, self.config.seq_T], dim=1)
+                loss = bpr_loss(yui, yuj).sum()
+
+                self.optimizer.zero_grad()
+                loss.backward()
+                self.optimizer.step()
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.hgn.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        bat_seq = [self.user_truncated_seq[u] for u in users]
+        bat_seq = torch.from_numpy(np.asarray(bat_seq)).long().to(self.device)
+        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
+        all_ratings = self.hgn.predict(users, bat_seq)
+        return all_ratings.cpu().detach().numpy()
```

## skrec/recommender/SASRec.py

 * *Ordering differences only*

```diff
@@ -1,506 +1,506 @@
-"""
-Paper: Self-Attentive Sequential Recommendation
-Author: Wang-Cheng Kang, and Julian McAuley
-Reference: https://github.com/kang205/SASRec
-"""
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["SASRec"]
-
-import numpy as np
-from typing import Dict
-from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
-from ..utils.tf1x import inner_product
-from ..utils.py import pad_sequences, batch_randint_choice, BatchIterator
-from ..utils.py import Config
-import tensorflow as tf
-
-
-def normalize(inputs,
-              epsilon=1e-8,
-              scope="ln",
-              reuse=None):
-    '''Applies layer normalization.
-
-    Args:
-      inputs: A tensor with 2 or more dimensions, where the first dimension has
-        `batch_size`.
-      epsilon: A floating number. A very small number for preventing ZeroDivision Error.
-      scope: Optional scope for `variable_scope`.
-      reuse: Boolean, whether to reuse the weights of a previous layer
-        by the same name.
-
-    Returns:
-      A tensor with the same shape and data dtype as `inputs`.
-    '''
-    with tf.variable_scope(scope, reuse=reuse):
-        inputs_shape = inputs.get_shape()
-        params_shape = inputs_shape[-1:]
-
-        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)
-        beta = tf.Variable(tf.zeros(params_shape))
-        gamma = tf.Variable(tf.ones(params_shape))
-        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))
-        outputs = gamma * normalized + beta
-
-    return outputs
-
-
-def embedding(inputs,
-              vocab_size,
-              num_units,
-              zero_pad=True,
-              scale=True,
-              l2_reg=0.0,
-              scope="embedding",
-              with_t=False,
-              reuse=None):
-    '''Embeds a given tensor.
-
-    Args:
-      inputs: A `Tensor` with type `int32` or `int64` containing the ids
-         to be looked up in `lookup table`.
-      vocab_size: An int. Vocabulary size.
-      num_units: An int. Number of embedding hidden units.
-      zero_pad: A boolean. If True, all the values of the fist row (id 0)
-        should be constant zeros.
-      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.
-      scope: Optional scope for `variable_scope`.
-      reuse: Boolean, whether to reuse the weights of a previous layer
-        by the same name.
-
-    Returns:
-      A `Tensor` with one more rank than inputs's. The last dimensionality
-        should be `num_units`.
-
-    For example,
-
-    ```
-    import tensorflow as tf
-
-    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))
-    outputs = embedding(inputs, 6, 2, zero_pad=True)
-    with tf.Session() as sess:
-        sess.run(tf.global_variables_initializer())
-        print sess.run(outputs)
-    >>
-    [[[ 0.          0.        ]
-      [ 0.09754146  0.67385566]
-      [ 0.37864095 -0.35689294]]
-
-     [[-1.01329422 -1.09939694]
-      [ 0.7521342   0.38203377]
-      [-0.04973143 -0.06210355]]]
-    ```
-
-    ```
-    import tensorflow as tf
-
-    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))
-    outputs = embedding(inputs, 6, 2, zero_pad=False)
-    with tf.Session() as sess:
-        sess.run(tf.global_variables_initializer())
-        print sess.run(outputs)
-    >>
-    [[[-0.19172323 -0.39159766]
-      [-0.43212751 -0.66207761]
-      [ 1.03452027 -0.26704335]]
-
-     [[-0.11634696 -0.35983452]
-      [ 0.50208133  0.53509563]
-      [ 1.22204471 -0.96587461]]]
-    ```
-    '''
-    with tf.variable_scope(scope, reuse=reuse):
-        lookup_table = tf.get_variable('lookup_table',
-                                       dtype=tf.float32,
-                                       shape=[vocab_size, num_units],
-                                       # initializer=tf.contrib.layers.xavier_initializer(),
-                                       regularizer=tf.contrib.layers.l2_regularizer(l2_reg))
-        if zero_pad:
-            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),
-                                      lookup_table[1:, :]), 0)
-        outputs = tf.nn.embedding_lookup(lookup_table, inputs)
-
-        if scale:
-            outputs = outputs * (num_units ** 0.5)
-    if with_t:
-        return outputs, lookup_table
-    else:
-        return outputs
-
-
-def multihead_attention(queries,
-                        keys,
-                        num_units=None,
-                        num_heads=8,
-                        dropout_rate=0.0,
-                        is_training=True,
-                        causality=False,
-                        scope="multihead_attention",
-                        reuse=None,
-                        with_qk=False):
-    '''Applies multihead attention.
-
-    Args:
-      queries: A 3d tensor with shape of [N, T_q, C_q].
-      keys: A 3d tensor with shape of [N, T_k, C_k].
-      num_units: A scalar. Attention size.
-      dropout_rate: A floating point number.
-      is_training: Boolean. Controller of mechanism for dropout.
-      causality: Boolean. If true, units that reference the future are masked.
-      num_heads: An int. Number of heads.
-      scope: Optional scope for `variable_scope`.
-      reuse: Boolean, whether to reuse the weights of a previous layer
-        by the same name.
-
-    Returns
-      A 3d tensor with shape of (N, T_q, C)
-    '''
-    with tf.variable_scope(scope, reuse=reuse):
-        # Set the fall back option for num_units
-        if num_units is None:
-            num_units = queries.get_shape().as_list[-1]
-
-        # Linear projections
-        # Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)
-        # K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)
-        # V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)
-        Q = tf.layers.dense(queries, num_units, activation=None)  # (N, T_q, C)
-        K = tf.layers.dense(keys, num_units, activation=None)  # (N, T_k, C)
-        V = tf.layers.dense(keys, num_units, activation=None)  # (N, T_k, C)
-
-        # Split and concat
-        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)  # (h*N, T_q, C/h)
-        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)
-        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)
-
-        # Multiplication
-        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)
-
-        # Scale
-        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)
-
-        # Key Masking
-        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)
-        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)
-        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)
-
-        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)
-        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)
-
-        # Causality = Future blinding
-        if causality:
-            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)
-            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)
-            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)
-
-            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)
-            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (h*N, T_q, T_k)
-
-        # Activation
-        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)
-
-        # Query Masking
-        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)
-        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)
-        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)
-        outputs *= query_masks  # broadcasting. (N, T_q, C)
-
-        # Dropouts
-        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))
-
-        # Weighted sum
-        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)
-
-        # Restore shape
-        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)
-
-        # Residual connection
-        outputs += queries
-
-        # Normalize
-        # outputs = normalize(outputs) # (N, T_q, C)
-
-    if with_qk:
-        return Q, K
-    else:
-        return outputs
-
-
-def feedforward(inputs,
-                num_units=[2048, 512],
-                scope="multihead_attention",
-                dropout_rate=0.2,
-                is_training=True,
-                reuse=None):
-    '''Point-wise feed forward net.
-
-    Args:
-      inputs: A 3d tensor with shape of [N, T, C].
-      num_units: A list of two integers.
-      scope: Optional scope for `variable_scope`.
-      reuse: Boolean, whether to reuse the weights of a previous layer
-        by the same name.
-
-    Returns:
-      A 3d tensor with the same shape and dtype as inputs
-    '''
-    with tf.variable_scope(scope, reuse=reuse):
-        # Inner layer
-        params = {"inputs": inputs, "filters": num_units[0], "kernel_size": 1,
-                  "activation": tf.nn.relu, "use_bias": True}
-        outputs = tf.layers.conv1d(**params)
-        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))
-        # Readout layer
-        params = {"inputs": outputs, "filters": num_units[1], "kernel_size": 1,
-                  "activation": None, "use_bias": True}
-        outputs = tf.layers.conv1d(**params)
-        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))
-
-        # Residual connection
-        outputs += inputs
-
-        # Normalize
-        # outputs = normalize(outputs)
-
-    return outputs
-
-
-class SASRecConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 l2_emb=0.0,
-                 hidden_units=64,
-                 dropout_rate=0.5,
-                 max_len=50,
-                 num_blocks=2,
-                 num_heads=1,
-                 batch_size=128,
-                 epochs=1000,
-                 early_stop=100,
-                 **kwargs):
-        super(SASRecConfig, self).__init__()
-        self.lr: float = lr
-        self.l2_emb: float = l2_emb
-        self.hidden_units: int = hidden_units
-        self.dropout_rate: float = dropout_rate
-        self.max_len: int = max_len
-        self.num_blocks: int = num_blocks
-        self.num_heads: int = num_heads
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.l2_emb, float) and self.l2_emb >= 0
-        assert isinstance(self.hidden_units, int) and self.hidden_units > 0
-        assert isinstance(self.dropout_rate, float) and 1 > self.dropout_rate >= 0
-        assert isinstance(self.max_len, int) and self.max_len > 0
-        assert isinstance(self.num_blocks, int) and self.num_blocks > 0
-        assert isinstance(self.num_heads, int) and self.num_heads > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class SASRec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = SASRecConfig(**cfg_dict)
-        super(SASRec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
-        self.all_users = list(self.user_pos_train.keys())
-
-        self._build_model()
-        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
-        tf_config.gpu_options.allow_growth = True
-        self.sess = tf.Session(config=tf_config)
-        self.sess.run(tf.global_variables_initializer())
-        self.test_item_seqs = self._process_test()
-
-    def _process_test(self):
-        item_seqs = [self.user_pos_train[user][-self.config.max_len:] if user in self.user_pos_train else [self.items_num]
-                     for user in range(self.users_num)]
-
-        test_item_seqs = pad_sequences(item_seqs, value=self.items_num, max_len=self.config.max_len,
-                                       padding='pre', truncating='pre', dtype=np.int32)
-        return test_item_seqs
-
-    def _generate_train_data(self):
-        item_seq_list, item_pos_list = [], []
-        all_users = BatchIterator(self.all_users, batch_size=1024, shuffle=False, drop_last=False)
-        for bat_users in all_users:
-            bat_seq = [self.user_pos_train[u][:-1] for u in bat_users]
-            bat_pos = [self.user_pos_train[u][1:] for u in bat_users]
-
-            # padding
-            bat_seq = pad_sequences(bat_seq, value=self.items_num, max_len=self.config.max_len,
-                                    padding='pre', truncating='pre')
-            bat_pos = pad_sequences(bat_pos, value=self.items_num, max_len=self.config.max_len,
-                                    padding='pre', truncating='pre')
-
-            item_seq_list.extend(bat_seq)
-            item_pos_list.extend(bat_pos)
-        return item_seq_list, item_pos_list
-
-    def _sample_negative(self):
-        item_neg_list = []
-        all_users = BatchIterator(self.all_users, batch_size=1024, shuffle=False, drop_last=False)
-        for bat_users in all_users:
-            n_neg_items = [len(self.user_pos_train[u][1:]) for u in bat_users]
-            exclusion = [self.user_pos_train[u] for u in bat_users]
-            bat_neg = batch_randint_choice(self.items_num, n_neg_items, replace=True, exclusion=exclusion, thread_num=4)
-            # padding
-            bat_neg = pad_sequences(bat_neg, value=self.items_num, max_len=self.config.max_len,
-                                    padding='pre', truncating='pre')
-
-            item_neg_list.extend(bat_neg)
-        return item_neg_list
-
-    def _create_variable(self):
-        # self.user_ph = tf.placeholder(tf.int32, [None], name="user")
-        batch_size = None
-        self.item_seq_ph = tf.placeholder(tf.int32, [batch_size, self.config.max_len], name="item_seq")
-        self.item_pos_ph = tf.placeholder(tf.int32, [batch_size, self.config.max_len], name="item_pos")
-        self.item_neg_ph = tf.placeholder(tf.int32, [batch_size, self.config.max_len], name="item_neg")
-        self.is_training = tf.placeholder(tf.bool, name="training_flag")
-
-        l2_regularizer = tf.contrib.layers.l2_regularizer(self.config.l2_emb)
-        item_embeddings = tf.get_variable('item_embeddings', dtype=tf.float32,
-                                          shape=[self.items_num, self.config.hidden_units],
-                                          regularizer=l2_regularizer)
-
-        zero_pad = tf.zeros([1, self.config.hidden_units], name="padding")
-        item_embeddings = tf.concat([item_embeddings, zero_pad], axis=0)
-        self.item_embeddings = item_embeddings * (self.config.hidden_units ** 0.5)
-
-        self.position_embeddings = tf.get_variable('position_embeddings', dtype=tf.float32,
-                                                   shape=[self.config.max_len, self.config.hidden_units],
-                                                   regularizer=l2_regularizer)
-
-    def _build_model(self):
-        self._create_variable()
-        reuse = None
-        with tf.variable_scope("SASRec", reuse=reuse):
-            # sequence embedding, item embedding table
-            self.seq = tf.nn.embedding_lookup(self.item_embeddings, self.item_seq_ph)
-            item_emb_table = self.item_embeddings
-
-            # Positional Encoding
-            position = tf.tile(tf.expand_dims(tf.range(tf.shape(self.item_seq_ph)[1]), 0),
-                               [tf.shape(self.item_seq_ph)[0], 1])
-            t = tf.nn.embedding_lookup(self.position_embeddings, position)
-            pos_emb_table = self.position_embeddings
-
-            self.seq += t
-
-            # Dropout
-            self.seq = tf.layers.dropout(self.seq,
-                                         rate=self.config.dropout_rate,
-                                         training=tf.convert_to_tensor(self.is_training))
-
-            mask = tf.expand_dims(tf.to_float(tf.not_equal(self.item_seq_ph, self.items_num)), -1)
-            self.seq *= mask
-
-            # Build blocks
-
-            for i in range(self.config.num_blocks):
-                with tf.variable_scope("num_blocks_%d" % i):
-                    # Self-attention
-                    self.seq = multihead_attention(queries=normalize(self.seq),
-                                                   keys=self.seq,
-                                                   num_units=self.config.hidden_units,
-                                                   num_heads=self.config.num_heads,
-                                                   dropout_rate=self.config.dropout_rate,
-                                                   is_training=self.is_training,
-                                                   causality=True,
-                                                   scope="self_attention")
-
-                    # Feed forward
-                    self.seq = feedforward(normalize(self.seq),
-                                           num_units=[self.config.hidden_units, self.config.hidden_units],
-                                           dropout_rate=self.config.dropout_rate,
-                                           is_training=self.is_training)
-                    self.seq *= mask
-
-            self.seq = normalize(self.seq)  # (b, l, d)
-            last_emb = self.seq[:, -1, :]  # (b, d), the embedding of last item of each session
-
-        pos = tf.reshape(self.item_pos_ph, [tf.shape(self.item_seq_ph)[0] * self.config.max_len])  # (b*l,)
-        neg = tf.reshape(self.item_neg_ph, [tf.shape(self.item_seq_ph)[0] * self.config.max_len])  # (b*l,)
-        pos_emb = tf.nn.embedding_lookup(item_emb_table, pos)  # (b*l, d)
-        neg_emb = tf.nn.embedding_lookup(item_emb_table, neg)  # (b*l, d)
-        seq_emb = tf.reshape(self.seq, [tf.shape(self.item_seq_ph)[0] * self.config.max_len, self.config.hidden_units])  # (b*l, d)
-
-        # prediction layer
-        self.pos_logits = inner_product(pos_emb, seq_emb)  # (b*l,)
-        self.neg_logits = inner_product(neg_emb, seq_emb)  # (b*l,)
-
-        # ignore padding items (self.items_num)
-        is_target = tf.reshape(tf.to_float(tf.not_equal(pos, self.items_num)),
-                               [tf.shape(self.item_seq_ph)[0] * self.config.max_len])
-
-        pos_loss = -tf.log(tf.sigmoid(self.pos_logits) + 1e-24) * is_target
-        neg_loss = -tf.log(1 - tf.sigmoid(self.neg_logits) + 1e-24) * is_target
-        self.loss = tf.reduce_sum(pos_loss + neg_loss) / tf.reduce_sum(is_target)
-
-        try:
-            reg_losses = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
-            self.loss = self.loss + reg_losses
-        except:
-            pass
-
-        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.config.lr, beta2=0.98).minimize(self.loss)
-
-        # for predication/test
-        items_embeddings = item_emb_table[:-1]  # remove the padding item
-        self.all_logits = tf.matmul(last_emb, items_embeddings, transpose_b=True)
-
-    def fit(self):
-        item_seq_list, item_pos_list = self._generate_train_data()
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            item_neg_list = self._sample_negative()
-            data = BatchIterator(item_seq_list, item_pos_list, item_neg_list,
-                                 batch_size=self.config.batch_size, shuffle=True, drop_last=False)
-            for bat_item_seq, bat_item_pos, bat_item_neg in data:
-                feed = {self.item_seq_ph: bat_item_seq,
-                        self.item_pos_ph: bat_item_pos,
-                        self.item_neg_ph: bat_item_neg,
-                        self.is_training: True}
-
-                self.sess.run(self.train_opt, feed_dict=feed)
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        bat_seq = [self.test_item_seqs[u] for u in users]
-        feed = {self.item_seq_ph: bat_seq,
-                self.is_training: False}
-        bat_ratings = self.sess.run(self.all_logits, feed_dict=feed)
-        return bat_ratings
+"""
+Paper: Self-Attentive Sequential Recommendation
+Author: Wang-Cheng Kang, and Julian McAuley
+Reference: https://github.com/kang205/SASRec
+"""
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["SASRec"]
+
+import numpy as np
+from typing import Dict
+from .base import AbstractRecommender
+from ..utils.py import RankingEvaluator, MetricReport
+from ..io import Dataset
+from ..utils.tf1x import inner_product
+from ..utils.py import pad_sequences, batch_randint_choice, BatchIterator
+from ..utils.py import Config
+import tensorflow as tf
+
+
+def normalize(inputs,
+              epsilon=1e-8,
+              scope="ln",
+              reuse=None):
+    '''Applies layer normalization.
+
+    Args:
+      inputs: A tensor with 2 or more dimensions, where the first dimension has
+        `batch_size`.
+      epsilon: A floating number. A very small number for preventing ZeroDivision Error.
+      scope: Optional scope for `variable_scope`.
+      reuse: Boolean, whether to reuse the weights of a previous layer
+        by the same name.
+
+    Returns:
+      A tensor with the same shape and data dtype as `inputs`.
+    '''
+    with tf.variable_scope(scope, reuse=reuse):
+        inputs_shape = inputs.get_shape()
+        params_shape = inputs_shape[-1:]
+
+        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)
+        beta = tf.Variable(tf.zeros(params_shape))
+        gamma = tf.Variable(tf.ones(params_shape))
+        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))
+        outputs = gamma * normalized + beta
+
+    return outputs
+
+
+def embedding(inputs,
+              vocab_size,
+              num_units,
+              zero_pad=True,
+              scale=True,
+              l2_reg=0.0,
+              scope="embedding",
+              with_t=False,
+              reuse=None):
+    '''Embeds a given tensor.
+
+    Args:
+      inputs: A `Tensor` with type `int32` or `int64` containing the ids
+         to be looked up in `lookup table`.
+      vocab_size: An int. Vocabulary size.
+      num_units: An int. Number of embedding hidden units.
+      zero_pad: A boolean. If True, all the values of the fist row (id 0)
+        should be constant zeros.
+      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.
+      scope: Optional scope for `variable_scope`.
+      reuse: Boolean, whether to reuse the weights of a previous layer
+        by the same name.
+
+    Returns:
+      A `Tensor` with one more rank than inputs's. The last dimensionality
+        should be `num_units`.
+
+    For example,
+
+    ```
+    import tensorflow as tf
+
+    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))
+    outputs = embedding(inputs, 6, 2, zero_pad=True)
+    with tf.Session() as sess:
+        sess.run(tf.global_variables_initializer())
+        print sess.run(outputs)
+    >>
+    [[[ 0.          0.        ]
+      [ 0.09754146  0.67385566]
+      [ 0.37864095 -0.35689294]]
+
+     [[-1.01329422 -1.09939694]
+      [ 0.7521342   0.38203377]
+      [-0.04973143 -0.06210355]]]
+    ```
+
+    ```
+    import tensorflow as tf
+
+    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))
+    outputs = embedding(inputs, 6, 2, zero_pad=False)
+    with tf.Session() as sess:
+        sess.run(tf.global_variables_initializer())
+        print sess.run(outputs)
+    >>
+    [[[-0.19172323 -0.39159766]
+      [-0.43212751 -0.66207761]
+      [ 1.03452027 -0.26704335]]
+
+     [[-0.11634696 -0.35983452]
+      [ 0.50208133  0.53509563]
+      [ 1.22204471 -0.96587461]]]
+    ```
+    '''
+    with tf.variable_scope(scope, reuse=reuse):
+        lookup_table = tf.get_variable('lookup_table',
+                                       dtype=tf.float32,
+                                       shape=[vocab_size, num_units],
+                                       # initializer=tf.contrib.layers.xavier_initializer(),
+                                       regularizer=tf.contrib.layers.l2_regularizer(l2_reg))
+        if zero_pad:
+            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),
+                                      lookup_table[1:, :]), 0)
+        outputs = tf.nn.embedding_lookup(lookup_table, inputs)
+
+        if scale:
+            outputs = outputs * (num_units ** 0.5)
+    if with_t:
+        return outputs, lookup_table
+    else:
+        return outputs
+
+
+def multihead_attention(queries,
+                        keys,
+                        num_units=None,
+                        num_heads=8,
+                        dropout_rate=0.0,
+                        is_training=True,
+                        causality=False,
+                        scope="multihead_attention",
+                        reuse=None,
+                        with_qk=False):
+    '''Applies multihead attention.
+
+    Args:
+      queries: A 3d tensor with shape of [N, T_q, C_q].
+      keys: A 3d tensor with shape of [N, T_k, C_k].
+      num_units: A scalar. Attention size.
+      dropout_rate: A floating point number.
+      is_training: Boolean. Controller of mechanism for dropout.
+      causality: Boolean. If true, units that reference the future are masked.
+      num_heads: An int. Number of heads.
+      scope: Optional scope for `variable_scope`.
+      reuse: Boolean, whether to reuse the weights of a previous layer
+        by the same name.
+
+    Returns
+      A 3d tensor with shape of (N, T_q, C)
+    '''
+    with tf.variable_scope(scope, reuse=reuse):
+        # Set the fall back option for num_units
+        if num_units is None:
+            num_units = queries.get_shape().as_list[-1]
+
+        # Linear projections
+        # Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)
+        # K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)
+        # V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)
+        Q = tf.layers.dense(queries, num_units, activation=None)  # (N, T_q, C)
+        K = tf.layers.dense(keys, num_units, activation=None)  # (N, T_k, C)
+        V = tf.layers.dense(keys, num_units, activation=None)  # (N, T_k, C)
+
+        # Split and concat
+        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)  # (h*N, T_q, C/h)
+        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)
+        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)
+
+        # Multiplication
+        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)
+
+        # Scale
+        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)
+
+        # Key Masking
+        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)
+        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)
+        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)
+
+        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)
+        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)
+
+        # Causality = Future blinding
+        if causality:
+            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)
+            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)
+            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)
+
+            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)
+            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (h*N, T_q, T_k)
+
+        # Activation
+        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)
+
+        # Query Masking
+        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)
+        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)
+        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)
+        outputs *= query_masks  # broadcasting. (N, T_q, C)
+
+        # Dropouts
+        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))
+
+        # Weighted sum
+        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)
+
+        # Restore shape
+        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)
+
+        # Residual connection
+        outputs += queries
+
+        # Normalize
+        # outputs = normalize(outputs) # (N, T_q, C)
+
+    if with_qk:
+        return Q, K
+    else:
+        return outputs
+
+
+def feedforward(inputs,
+                num_units=[2048, 512],
+                scope="multihead_attention",
+                dropout_rate=0.2,
+                is_training=True,
+                reuse=None):
+    '''Point-wise feed forward net.
+
+    Args:
+      inputs: A 3d tensor with shape of [N, T, C].
+      num_units: A list of two integers.
+      scope: Optional scope for `variable_scope`.
+      reuse: Boolean, whether to reuse the weights of a previous layer
+        by the same name.
+
+    Returns:
+      A 3d tensor with the same shape and dtype as inputs
+    '''
+    with tf.variable_scope(scope, reuse=reuse):
+        # Inner layer
+        params = {"inputs": inputs, "filters": num_units[0], "kernel_size": 1,
+                  "activation": tf.nn.relu, "use_bias": True}
+        outputs = tf.layers.conv1d(**params)
+        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))
+        # Readout layer
+        params = {"inputs": outputs, "filters": num_units[1], "kernel_size": 1,
+                  "activation": None, "use_bias": True}
+        outputs = tf.layers.conv1d(**params)
+        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))
+
+        # Residual connection
+        outputs += inputs
+
+        # Normalize
+        # outputs = normalize(outputs)
+
+    return outputs
+
+
+class SASRecConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 l2_emb=0.0,
+                 hidden_units=64,
+                 dropout_rate=0.5,
+                 max_len=50,
+                 num_blocks=2,
+                 num_heads=1,
+                 batch_size=128,
+                 epochs=1000,
+                 early_stop=100,
+                 **kwargs):
+        super(SASRecConfig, self).__init__()
+        self.lr: float = lr
+        self.l2_emb: float = l2_emb
+        self.hidden_units: int = hidden_units
+        self.dropout_rate: float = dropout_rate
+        self.max_len: int = max_len
+        self.num_blocks: int = num_blocks
+        self.num_heads: int = num_heads
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.l2_emb, float) and self.l2_emb >= 0
+        assert isinstance(self.hidden_units, int) and self.hidden_units > 0
+        assert isinstance(self.dropout_rate, float) and 1 > self.dropout_rate >= 0
+        assert isinstance(self.max_len, int) and self.max_len > 0
+        assert isinstance(self.num_blocks, int) and self.num_blocks > 0
+        assert isinstance(self.num_heads, int) and self.num_heads > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class SASRec(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = SASRecConfig(**cfg_dict)
+        super(SASRec, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
+        self.all_users = list(self.user_pos_train.keys())
+
+        self._build_model()
+        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
+        tf_config.gpu_options.allow_growth = True
+        self.sess = tf.Session(config=tf_config)
+        self.sess.run(tf.global_variables_initializer())
+        self.test_item_seqs = self._process_test()
+
+    def _process_test(self):
+        item_seqs = [self.user_pos_train[user][-self.config.max_len:] if user in self.user_pos_train else [self.items_num]
+                     for user in range(self.users_num)]
+
+        test_item_seqs = pad_sequences(item_seqs, value=self.items_num, max_len=self.config.max_len,
+                                       padding='pre', truncating='pre', dtype=np.int32)
+        return test_item_seqs
+
+    def _generate_train_data(self):
+        item_seq_list, item_pos_list = [], []
+        all_users = BatchIterator(self.all_users, batch_size=1024, shuffle=False, drop_last=False)
+        for bat_users in all_users:
+            bat_seq = [self.user_pos_train[u][:-1] for u in bat_users]
+            bat_pos = [self.user_pos_train[u][1:] for u in bat_users]
+
+            # padding
+            bat_seq = pad_sequences(bat_seq, value=self.items_num, max_len=self.config.max_len,
+                                    padding='pre', truncating='pre')
+            bat_pos = pad_sequences(bat_pos, value=self.items_num, max_len=self.config.max_len,
+                                    padding='pre', truncating='pre')
+
+            item_seq_list.extend(bat_seq)
+            item_pos_list.extend(bat_pos)
+        return item_seq_list, item_pos_list
+
+    def _sample_negative(self):
+        item_neg_list = []
+        all_users = BatchIterator(self.all_users, batch_size=1024, shuffle=False, drop_last=False)
+        for bat_users in all_users:
+            n_neg_items = [len(self.user_pos_train[u][1:]) for u in bat_users]
+            exclusion = [self.user_pos_train[u] for u in bat_users]
+            bat_neg = batch_randint_choice(self.items_num, n_neg_items, replace=True, exclusion=exclusion, thread_num=4)
+            # padding
+            bat_neg = pad_sequences(bat_neg, value=self.items_num, max_len=self.config.max_len,
+                                    padding='pre', truncating='pre')
+
+            item_neg_list.extend(bat_neg)
+        return item_neg_list
+
+    def _create_variable(self):
+        # self.user_ph = tf.placeholder(tf.int32, [None], name="user")
+        batch_size = None
+        self.item_seq_ph = tf.placeholder(tf.int32, [batch_size, self.config.max_len], name="item_seq")
+        self.item_pos_ph = tf.placeholder(tf.int32, [batch_size, self.config.max_len], name="item_pos")
+        self.item_neg_ph = tf.placeholder(tf.int32, [batch_size, self.config.max_len], name="item_neg")
+        self.is_training = tf.placeholder(tf.bool, name="training_flag")
+
+        l2_regularizer = tf.contrib.layers.l2_regularizer(self.config.l2_emb)
+        item_embeddings = tf.get_variable('item_embeddings', dtype=tf.float32,
+                                          shape=[self.items_num, self.config.hidden_units],
+                                          regularizer=l2_regularizer)
+
+        zero_pad = tf.zeros([1, self.config.hidden_units], name="padding")
+        item_embeddings = tf.concat([item_embeddings, zero_pad], axis=0)
+        self.item_embeddings = item_embeddings * (self.config.hidden_units ** 0.5)
+
+        self.position_embeddings = tf.get_variable('position_embeddings', dtype=tf.float32,
+                                                   shape=[self.config.max_len, self.config.hidden_units],
+                                                   regularizer=l2_regularizer)
+
+    def _build_model(self):
+        self._create_variable()
+        reuse = None
+        with tf.variable_scope("SASRec", reuse=reuse):
+            # sequence embedding, item embedding table
+            self.seq = tf.nn.embedding_lookup(self.item_embeddings, self.item_seq_ph)
+            item_emb_table = self.item_embeddings
+
+            # Positional Encoding
+            position = tf.tile(tf.expand_dims(tf.range(tf.shape(self.item_seq_ph)[1]), 0),
+                               [tf.shape(self.item_seq_ph)[0], 1])
+            t = tf.nn.embedding_lookup(self.position_embeddings, position)
+            pos_emb_table = self.position_embeddings
+
+            self.seq += t
+
+            # Dropout
+            self.seq = tf.layers.dropout(self.seq,
+                                         rate=self.config.dropout_rate,
+                                         training=tf.convert_to_tensor(self.is_training))
+
+            mask = tf.expand_dims(tf.to_float(tf.not_equal(self.item_seq_ph, self.items_num)), -1)
+            self.seq *= mask
+
+            # Build blocks
+
+            for i in range(self.config.num_blocks):
+                with tf.variable_scope("num_blocks_%d" % i):
+                    # Self-attention
+                    self.seq = multihead_attention(queries=normalize(self.seq),
+                                                   keys=self.seq,
+                                                   num_units=self.config.hidden_units,
+                                                   num_heads=self.config.num_heads,
+                                                   dropout_rate=self.config.dropout_rate,
+                                                   is_training=self.is_training,
+                                                   causality=True,
+                                                   scope="self_attention")
+
+                    # Feed forward
+                    self.seq = feedforward(normalize(self.seq),
+                                           num_units=[self.config.hidden_units, self.config.hidden_units],
+                                           dropout_rate=self.config.dropout_rate,
+                                           is_training=self.is_training)
+                    self.seq *= mask
+
+            self.seq = normalize(self.seq)  # (b, l, d)
+            last_emb = self.seq[:, -1, :]  # (b, d), the embedding of last item of each session
+
+        pos = tf.reshape(self.item_pos_ph, [tf.shape(self.item_seq_ph)[0] * self.config.max_len])  # (b*l,)
+        neg = tf.reshape(self.item_neg_ph, [tf.shape(self.item_seq_ph)[0] * self.config.max_len])  # (b*l,)
+        pos_emb = tf.nn.embedding_lookup(item_emb_table, pos)  # (b*l, d)
+        neg_emb = tf.nn.embedding_lookup(item_emb_table, neg)  # (b*l, d)
+        seq_emb = tf.reshape(self.seq, [tf.shape(self.item_seq_ph)[0] * self.config.max_len, self.config.hidden_units])  # (b*l, d)
+
+        # prediction layer
+        self.pos_logits = inner_product(pos_emb, seq_emb)  # (b*l,)
+        self.neg_logits = inner_product(neg_emb, seq_emb)  # (b*l,)
+
+        # ignore padding items (self.items_num)
+        is_target = tf.reshape(tf.to_float(tf.not_equal(pos, self.items_num)),
+                               [tf.shape(self.item_seq_ph)[0] * self.config.max_len])
+
+        pos_loss = -tf.log(tf.sigmoid(self.pos_logits) + 1e-24) * is_target
+        neg_loss = -tf.log(1 - tf.sigmoid(self.neg_logits) + 1e-24) * is_target
+        self.loss = tf.reduce_sum(pos_loss + neg_loss) / tf.reduce_sum(is_target)
+
+        try:
+            reg_losses = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
+            self.loss = self.loss + reg_losses
+        except:
+            pass
+
+        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.config.lr, beta2=0.98).minimize(self.loss)
+
+        # for predication/test
+        items_embeddings = item_emb_table[:-1]  # remove the padding item
+        self.all_logits = tf.matmul(last_emb, items_embeddings, transpose_b=True)
+
+    def fit(self):
+        item_seq_list, item_pos_list = self._generate_train_data()
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            item_neg_list = self._sample_negative()
+            data = BatchIterator(item_seq_list, item_pos_list, item_neg_list,
+                                 batch_size=self.config.batch_size, shuffle=True, drop_last=False)
+            for bat_item_seq, bat_item_pos, bat_item_neg in data:
+                feed = {self.item_seq_ph: bat_item_seq,
+                        self.item_pos_ph: bat_item_pos,
+                        self.item_neg_ph: bat_item_neg,
+                        self.is_training: True}
+
+                self.sess.run(self.train_opt, feed_dict=feed)
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        bat_seq = [self.test_item_seqs[u] for u in users]
+        feed = {self.item_seq_ph: bat_seq,
+                self.is_training: False}
+        bat_ratings = self.sess.run(self.all_logits, feed_dict=feed)
+        return bat_ratings
```

## skrec/recommender/LightGCN.py

 * *Ordering differences only*

```diff
@@ -1,221 +1,221 @@
-"""
-Paper: LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation
-Author: Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang
-Reference: https://github.com/hexiangnan/LightGCN
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["LightGCN"]
-
-import os
-import torch
-import torch.sparse as torch_sp
-import torch.nn as nn
-import torch.nn.functional as F
-from typing import Dict
-from .base import AbstractRecommender
-from ..utils.torch import inner_product, bpr_loss, l2_loss, get_initializer
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import PairwiseIterator, Dataset
-import numpy as np
-import scipy.sparse as sp
-from ..utils.common import normalize_adj_matrix
-from ..utils.torch import sp_mat_to_sp_tensor
-from ..utils.py import Config
-
-
-class LightGCNConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 reg=1e-3,
-                 embed_size=64,
-                 n_layers=3,
-                 adj_type="pre",
-                 batch_size=1024,
-                 epochs=1000,
-                 early_stop=100,
-                 **kwargs):
-        super(LightGCNConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.embed_size: int = embed_size
-        self.n_layers: int = n_layers
-        self.adj_type: str = adj_type  # plain, norm, gcmc, pre
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.n_layers, int) and self.n_layers> 0
-        assert self.adj_type in {"plain", "norm", "gcmc", "pre"}
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _LightGCN(nn.Module):
-    def __init__(self, num_users, num_items, embed_dim, norm_adj, n_layers):
-        super(_LightGCN, self).__init__()
-        self.num_users = num_users
-        self.num_items = num_items
-        self.norm_adj = norm_adj
-        self.n_layers = n_layers
-        self.user_embeddings = nn.Embedding(num_users, embed_dim)
-        self.item_embeddings = nn.Embedding(num_items, embed_dim)
-        self._user_embeddings_final = None
-        self._item_embeddings_final = None
-
-        # weight initialization
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init = get_initializer("xavier_uniform")
-        init(self.user_embeddings.weight)
-        init(self.item_embeddings.weight)
-
-    def forward(self, users, items):
-        user_embeddings, item_embeddings = self._forward_gcn()
-        user_embs = F.embedding(users, user_embeddings)
-        item_embs = F.embedding(items, item_embeddings)
-        ratings = inner_product(user_embs, item_embs)
-        return ratings
-
-    def _forward_gcn(self):
-        ego_embeddings = torch.cat([self.user_embeddings.weight, self.item_embeddings.weight], dim=0)
-        all_embeddings = [ego_embeddings]
-
-        for k in range(self.n_layers):
-            ego_embeddings = torch_sp.mm(self.norm_adj, ego_embeddings)
-            all_embeddings += [ego_embeddings]
-
-        all_embeddings = torch.stack(all_embeddings, dim=1).mean(dim=1)
-        user_embeddings, item_embeddings = torch.split(all_embeddings, [self.num_users, self.num_items], dim=0)
-
-        return user_embeddings, item_embeddings
-
-    def predict(self, users):
-        if self._user_embeddings_final is None or self._item_embeddings_final is None:
-            raise ValueError("Please first switch to 'eval' mode.")
-        user_embs = F.embedding(users, self._user_embeddings_final)
-        ratings = torch.matmul(user_embs, self._item_embeddings_final.T)
-        return ratings
-
-    def eval(self):
-        super(_LightGCN, self).eval()
-        self._user_embeddings_final, self._item_embeddings_final = self._forward_gcn()
-
-
-class LightGCN(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = LightGCNConfig(**cfg_dict)
-        super(LightGCN, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-
-        adj_matrix = self._load_adj_mat(config.adj_type)
-        adj_matrix = sp_mat_to_sp_tensor(adj_matrix).to(self.device)
-
-        self.lightgcn = _LightGCN(self.num_users, self.num_items, config.embed_size,
-                                  adj_matrix, config.n_layers).to(self.device)
-        self.optimizer = torch.optim.Adam(self.lightgcn.parameters(), lr=config.lr)
-
-    def _load_adj_mat(self, adj_type):
-        output_dir = self.dataset.data_dir
-        output_dir = os.path.join(output_dir, f"_{self.__class__.__name__}_data")
-        if not os.path.exists(output_dir):
-            os.makedirs(output_dir)
-        adj_mat_file = os.path.join(output_dir, f"{adj_type}_adj.npz")
-        if os.path.exists(adj_mat_file):
-            adj_matrix = sp.load_npz(adj_mat_file)
-        else:
-            adj_matrix = self._create_adj_mat(adj_type)
-            sp.save_npz(adj_mat_file, adj_matrix)
-        return adj_matrix
-
-    def _create_adj_mat(self, adj_type):
-        users_items = self.dataset.train_data.to_user_item_pairs()
-        users_np, items_np = users_items[:, 0], users_items[:, 1]
-
-        ratings = np.ones_like(users_np, dtype=np.float32)
-        n_nodes = self.num_users + self.num_items
-        up_left_adj = sp.csr_matrix((ratings, (users_np, items_np+self.num_users)), shape=(n_nodes, n_nodes))
-        adj_mat = up_left_adj + up_left_adj.T
-
-        if adj_type == 'plain':
-            adj_matrix = adj_mat
-            print('use the plain adjacency matrix')
-        elif adj_type == 'norm':
-            adj_matrix = normalize_adj_matrix(adj_mat + sp.eye(adj_mat.shape[0]), norm_method="left")
-            print('use the normalized adjacency matrix')
-        elif adj_type == 'gcmc':
-            adj_matrix = normalize_adj_matrix(adj_mat, norm_method="left")
-            print('use the gcmc adjacency matrix')
-        elif adj_type == 'pre':
-            # pre adjcency matrix
-            adj_matrix = normalize_adj_matrix(adj_mat, norm_method="symmetric")
-            print('use the pre adjcency matrix')
-        else:
-            mean_adj = normalize_adj_matrix(adj_mat, norm_method="left")
-            adj_matrix = mean_adj + sp.eye(mean_adj.shape[0])
-            print('use the mean adjacency matrix')
-
-        return adj_matrix
-
-    def fit(self):
-        data_iter = PairwiseIterator(self.dataset.train_data,
-                                     batch_size=self.config.batch_size,
-                                     shuffle=True, drop_last=False)
-
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            self.lightgcn.train()
-            for bat_users, bat_pos_items, bat_neg_items in data_iter:
-                bat_users = torch.from_numpy(bat_users).long().to(self.device)
-                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
-                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
-
-                _bat_users = torch.cat([bat_users, bat_users], dim=0)
-                _bat_items = torch.cat([bat_pos_items, bat_neg_items], dim=0)
-
-                hat_y = self.lightgcn(_bat_users, _bat_items)
-                yui, yuj = torch.split(hat_y, [len(bat_pos_items), len(bat_neg_items)], dim=0)
-
-                loss = bpr_loss(yui, yuj).mean()
-                reg_loss = l2_loss(self.lightgcn.user_embeddings(bat_users),
-                                   self.lightgcn.item_embeddings(bat_pos_items),
-                                   self.lightgcn.item_embeddings(bat_neg_items)
-                                   )
-                loss += self.config.reg * reg_loss / self.config.batch_size
-                self.optimizer.zero_grad()
-                loss.backward()
-                self.optimizer.step()
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.lightgcn.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
-        return self.lightgcn.predict(users).cpu().detach().numpy()
+"""
+Paper: LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation
+Author: Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang
+Reference: https://github.com/hexiangnan/LightGCN
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["LightGCN"]
+
+import os
+import torch
+import torch.sparse as torch_sp
+import torch.nn as nn
+import torch.nn.functional as F
+from typing import Dict
+from .base import AbstractRecommender
+from ..utils.torch import inner_product, bpr_loss, l2_loss, get_initializer
+from ..utils.py import RankingEvaluator, MetricReport
+from ..io import PairwiseIterator, Dataset
+import numpy as np
+import scipy.sparse as sp
+from ..utils.common import normalize_adj_matrix
+from ..utils.torch import sp_mat_to_sp_tensor
+from ..utils.py import Config
+
+
+class LightGCNConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 reg=1e-3,
+                 embed_size=64,
+                 n_layers=3,
+                 adj_type="pre",
+                 batch_size=1024,
+                 epochs=1000,
+                 early_stop=100,
+                 **kwargs):
+        super(LightGCNConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.embed_size: int = embed_size
+        self.n_layers: int = n_layers
+        self.adj_type: str = adj_type  # plain, norm, gcmc, pre
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.n_layers, int) and self.n_layers> 0
+        assert self.adj_type in {"plain", "norm", "gcmc", "pre"}
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _LightGCN(nn.Module):
+    def __init__(self, num_users, num_items, embed_dim, norm_adj, n_layers):
+        super(_LightGCN, self).__init__()
+        self.num_users = num_users
+        self.num_items = num_items
+        self.norm_adj = norm_adj
+        self.n_layers = n_layers
+        self.user_embeddings = nn.Embedding(num_users, embed_dim)
+        self.item_embeddings = nn.Embedding(num_items, embed_dim)
+        self._user_embeddings_final = None
+        self._item_embeddings_final = None
+
+        # weight initialization
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        init = get_initializer("xavier_uniform")
+        init(self.user_embeddings.weight)
+        init(self.item_embeddings.weight)
+
+    def forward(self, users, items):
+        user_embeddings, item_embeddings = self._forward_gcn()
+        user_embs = F.embedding(users, user_embeddings)
+        item_embs = F.embedding(items, item_embeddings)
+        ratings = inner_product(user_embs, item_embs)
+        return ratings
+
+    def _forward_gcn(self):
+        ego_embeddings = torch.cat([self.user_embeddings.weight, self.item_embeddings.weight], dim=0)
+        all_embeddings = [ego_embeddings]
+
+        for k in range(self.n_layers):
+            ego_embeddings = torch_sp.mm(self.norm_adj, ego_embeddings)
+            all_embeddings += [ego_embeddings]
+
+        all_embeddings = torch.stack(all_embeddings, dim=1).mean(dim=1)
+        user_embeddings, item_embeddings = torch.split(all_embeddings, [self.num_users, self.num_items], dim=0)
+
+        return user_embeddings, item_embeddings
+
+    def predict(self, users):
+        if self._user_embeddings_final is None or self._item_embeddings_final is None:
+            raise ValueError("Please first switch to 'eval' mode.")
+        user_embs = F.embedding(users, self._user_embeddings_final)
+        ratings = torch.matmul(user_embs, self._item_embeddings_final.T)
+        return ratings
+
+    def eval(self):
+        super(_LightGCN, self).eval()
+        self._user_embeddings_final, self._item_embeddings_final = self._forward_gcn()
+
+
+class LightGCN(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = LightGCNConfig(**cfg_dict)
+        super(LightGCN, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+
+        adj_matrix = self._load_adj_mat(config.adj_type)
+        adj_matrix = sp_mat_to_sp_tensor(adj_matrix).to(self.device)
+
+        self.lightgcn = _LightGCN(self.num_users, self.num_items, config.embed_size,
+                                  adj_matrix, config.n_layers).to(self.device)
+        self.optimizer = torch.optim.Adam(self.lightgcn.parameters(), lr=config.lr)
+
+    def _load_adj_mat(self, adj_type):
+        output_dir = self.dataset.data_dir
+        output_dir = os.path.join(output_dir, f"_{self.__class__.__name__}_data")
+        if not os.path.exists(output_dir):
+            os.makedirs(output_dir)
+        adj_mat_file = os.path.join(output_dir, f"{adj_type}_adj.npz")
+        if os.path.exists(adj_mat_file):
+            adj_matrix = sp.load_npz(adj_mat_file)
+        else:
+            adj_matrix = self._create_adj_mat(adj_type)
+            sp.save_npz(adj_mat_file, adj_matrix)
+        return adj_matrix
+
+    def _create_adj_mat(self, adj_type):
+        users_items = self.dataset.train_data.to_user_item_pairs()
+        users_np, items_np = users_items[:, 0], users_items[:, 1]
+
+        ratings = np.ones_like(users_np, dtype=np.float32)
+        n_nodes = self.num_users + self.num_items
+        up_left_adj = sp.csr_matrix((ratings, (users_np, items_np+self.num_users)), shape=(n_nodes, n_nodes))
+        adj_mat = up_left_adj + up_left_adj.T
+
+        if adj_type == 'plain':
+            adj_matrix = adj_mat
+            print('use the plain adjacency matrix')
+        elif adj_type == 'norm':
+            adj_matrix = normalize_adj_matrix(adj_mat + sp.eye(adj_mat.shape[0]), norm_method="left")
+            print('use the normalized adjacency matrix')
+        elif adj_type == 'gcmc':
+            adj_matrix = normalize_adj_matrix(adj_mat, norm_method="left")
+            print('use the gcmc adjacency matrix')
+        elif adj_type == 'pre':
+            # pre adjcency matrix
+            adj_matrix = normalize_adj_matrix(adj_mat, norm_method="symmetric")
+            print('use the pre adjcency matrix')
+        else:
+            mean_adj = normalize_adj_matrix(adj_mat, norm_method="left")
+            adj_matrix = mean_adj + sp.eye(mean_adj.shape[0])
+            print('use the mean adjacency matrix')
+
+        return adj_matrix
+
+    def fit(self):
+        data_iter = PairwiseIterator(self.dataset.train_data,
+                                     batch_size=self.config.batch_size,
+                                     shuffle=True, drop_last=False)
+
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            self.lightgcn.train()
+            for bat_users, bat_pos_items, bat_neg_items in data_iter:
+                bat_users = torch.from_numpy(bat_users).long().to(self.device)
+                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
+                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
+
+                _bat_users = torch.cat([bat_users, bat_users], dim=0)
+                _bat_items = torch.cat([bat_pos_items, bat_neg_items], dim=0)
+
+                hat_y = self.lightgcn(_bat_users, _bat_items)
+                yui, yuj = torch.split(hat_y, [len(bat_pos_items), len(bat_neg_items)], dim=0)
+
+                loss = bpr_loss(yui, yuj).mean()
+                reg_loss = l2_loss(self.lightgcn.user_embeddings(bat_users),
+                                   self.lightgcn.item_embeddings(bat_pos_items),
+                                   self.lightgcn.item_embeddings(bat_neg_items)
+                                   )
+                loss += self.config.reg * reg_loss / self.config.batch_size
+                self.optimizer.zero_grad()
+                loss.backward()
+                self.optimizer.step()
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.lightgcn.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
+        return self.lightgcn.predict(users).cpu().detach().numpy()
```

## skrec/recommender/GRU4Rec.py

 * *Ordering differences only*

```diff
@@ -1,293 +1,293 @@
-"""
-Paper: Session-based Recommendations with Recurrent Neural Networks
-Author: Balzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk
-Reference: https://github.com/hidasib/GRU4Rec
-           https://github.com/Songweiping/GRU4Rec_TensorFlow
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["GRU4Rec"]
-
-import numpy as np
-import tensorflow as tf
-from typing import List, Dict
-from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.tf1x import bpr_loss, l2_loss
-
-
-class GRU4RecConfig(Config):
-    def __init__(self,
-                 lr=0.001,
-                 reg=0.0,
-                 layers=[64],
-                 batch_size=128,
-                 loss="top1",
-                 hidden_act="tanh",
-                 final_act="linear",
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(GRU4RecConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.layers: List[int] = layers
-        self.batch_size: int = batch_size
-        self.loss: str = loss  # loss = top1, bpr
-        self.hidden_act: str = hidden_act  # hidden_act = relu, tanh
-        self.final_act: str = final_act  # final_act = linear, relu, leaky_relu
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.layers, list)
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.loss, str) and self.loss in {"top1", "bpr"}
-        assert isinstance(self.hidden_act, str) and self.hidden_act in {"relu", "tanh"}
-        assert isinstance(self.final_act, str) and self.final_act in {"linear", "relu", "leaky_relu"}
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class GRU4Rec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = GRU4RecConfig(**cfg_dict)
-        super(GRU4Rec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        if config.hidden_act == "relu":
-            self.hidden_act = tf.nn.relu
-        elif config.hidden_act == "tanh":
-            self.hidden_act = tf.nn.tanh
-        else:
-            raise ValueError("There is not hidden_act named '%s'." % config.hidden_act)
-
-        # final_act = leaky-relu
-        if config.final_act == "relu":
-            self.final_act = tf.nn.relu
-        elif config.final_act == "linear":
-            self.final_act = tf.identity
-        elif config.final_act == "leaky_relu":
-            self.final_act = tf.nn.leaky_relu
-        else:
-            raise ValueError("There is not final_act named '%s'." % config.final_act)
-
-        if config.loss == "bpr":
-            self.loss_fun = self._bpr_loss
-        elif config.loss == "top1":
-            self.loss_fun = self._top1_loss
-        else:
-            raise ValueError("There is not loss named '%s'." % config.loss)
-
-        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
-
-        self.data_ui, self.offset_idx = self._init_data()
-
-        self._build_model()
-        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
-        tf_config.gpu_options.allow_growth = True
-        self.sess = tf.Session(config=tf_config)
-        self.sess.run(tf.global_variables_initializer())
-
-    def _init_data(self):
-        data_ui = self.dataset.train_data.to_user_item_pairs_by_time()
-        _, idx = np.unique(data_ui[:, 0], return_index=True)
-        offset_idx = np.zeros(len(idx)+1, dtype=np.int32)
-        offset_idx[:-1] = idx
-        offset_idx[-1] = len(data_ui)
-
-        return data_ui, offset_idx
-
-    def _create_variable(self):
-        self.X_ph = tf.placeholder(tf.int32, [self.config.batch_size], name='input')
-        self.Y_ph = tf.placeholder(tf.int32, [self.config.batch_size], name='output')
-        self.state_ph = [tf.placeholder(tf.float32, [self.config.batch_size, n_unit], name='layer_%d_state' % idx)
-                         for idx, n_unit in enumerate(self.config.layers)]
-
-        init = tf.random.truncated_normal([self.items_num, self.config.layers[0]], mean=0.0, stddev=0.01)
-        self.input_embeddings = tf.Variable(init, dtype=tf.float32, name="input_embeddings")
-
-        init = tf.random.truncated_normal([self.items_num, self.config.layers[-1]], mean=0.0, stddev=0.01)
-        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name="item_embeddings")
-        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name="item_biases")
-
-    def _bpr_loss(self, logits):
-        # logits: (b, size_y)
-        pos_logits = tf.matrix_diag_part(logits)  # (b,)
-        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
-        loss = tf.reduce_mean(bpr_loss(pos_logits, logits))
-        return loss
-
-    def _top1_loss(self, logits):
-        # logits: (b, size_y)
-        pos_logits = tf.matrix_diag_part(logits)  # (b,)
-        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
-        loss1 = tf.reduce_mean(tf.sigmoid(-pos_logits + logits), axis=-1)  # (b,)
-        loss2 = tf.reduce_mean(tf.sigmoid(tf.pow(logits, 2)), axis=-1) - \
-                tf.squeeze(tf.sigmoid(tf.pow(pos_logits, 2))/self.config.batch_size)  # (b,)
-        return tf.reduce_mean(loss1+loss2)
-
-    def _build_model(self):
-        self._create_variable()
-        # get embedding and bias
-        # b: batch size
-        # l1: the dim of the first layer
-        # ln: the dim of the last layer
-        # size_y: the length of Y_ph, i.e., n_sample+batch_size
-
-        cells = [tf.nn.rnn_cell.GRUCell(size, activation=self.hidden_act) for size in self.config.layers]
-        drop_cell = [tf.nn.rnn_cell.DropoutWrapper(cell) for cell in cells]
-        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(drop_cell)
-        inputs = tf.nn.embedding_lookup(self.input_embeddings, self.X_ph)  # (b, l1)
-        outputs, state = stacked_cell(inputs, state=self.state_ph)
-        self.u_emb = outputs  # outputs: (b, ln)
-        self.final_state = state  # [(b, l1), (b, l2), ..., (b, ln)]
-
-        # for training
-        items_embed = tf.nn.embedding_lookup(self.item_embeddings, self.Y_ph)  # (size_y, ln)
-        items_bias = tf.gather(self.item_biases, self.Y_ph)  # (size_y,)
-
-        logits = tf.matmul(outputs, items_embed, transpose_b=True) + items_bias  # (b, size_y)
-        logits = self.final_act(logits)
-
-        loss = self.loss_fun(logits)
-
-        # reg loss
-
-        reg_loss = l2_loss(inputs, items_embed, items_bias)
-        final_loss = loss + self.config.reg*reg_loss
-        self.update_opt = tf.train.AdamOptimizer(self.config.lr).minimize(final_loss)
-
-    def fit(self):
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-
-        data_ui, offset_idx = self.data_ui, self.offset_idx
-        data_items = data_ui[:, 1]
-
-        for epoch in range(self.config.epochs):
-            state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
-            user_idx = np.random.permutation(len(offset_idx) - 1)
-            iters = np.arange(self.config.batch_size, dtype=np.int32)
-            maxiter = iters.max()
-            start = offset_idx[user_idx[iters]]
-            end = offset_idx[user_idx[iters]+1]
-            finished = False
-            while not finished:
-                min_len = (end - start).min()
-                out_idx = data_items[start]
-                for i in range(min_len-1):
-                    in_idx = out_idx
-                    out_idx = data_items[start+i+1]
-                    out_items = out_idx
-
-                    feed = {self.X_ph: in_idx, self.Y_ph: out_items}
-                    for l in range(len(self.config.layers)):
-                        feed[self.state_ph[l]] = state[l]
-
-                    _, state = self.sess.run([self.update_opt, self.final_state], feed_dict=feed)
-
-                start = start+min_len-1
-                mask = np.arange(len(iters))[(end - start) <= 1]
-                for idx in mask:
-                    maxiter += 1
-                    if maxiter >= len(offset_idx)-1:
-                        finished = True
-                        break
-                    iters[idx] = maxiter
-                    start[idx] = offset_idx[user_idx[maxiter]]
-                    end[idx] = offset_idx[user_idx[maxiter]+1]
-                if len(mask):
-                    for i in range(len(self.config.layers)):
-                        state[i][mask] = 0
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def _get_user_embeddings(self):
-        users = np.array(list(self.user_pos_train.keys()), dtype=np.int32)
-        u_nnz = np.array([len(self.user_pos_train[u]) for u in users], dtype=np.int32)
-        users = users[np.argsort(-u_nnz)]
-        data_ui, offset_idx = self.data_ui, self.offset_idx
-        data_items = data_ui[:, 1]
-
-        state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
-        batch_iter = np.arange(self.config.batch_size, dtype=np.int32)
-        next_iter = batch_iter.max() + 1
-
-        start = offset_idx[users[batch_iter]]
-        end = offset_idx[users[batch_iter] + 1]  # the start index of next user
-
-        batch_mask = np.ones([self.config.batch_size], dtype=np.int32)
-        user_embeddings = np.zeros([self.users_num, self.config.layers[-1]], dtype=np.float32)  # saving user embedding
-        while np.sum(batch_mask) > 0:
-            min_len = (end - start).min()
-
-            for i in range(min_len):
-                cur_items = data_items[start + i]
-                feed = {self.X_ph: cur_items}
-                for l in range(len(self.config.layers)):
-                    feed[self.state_ph[l]] = state[l]
-
-                u_emb, state = self.sess.run([self.u_emb, self.final_state], feed_dict=feed)
-
-            start = start + min_len
-            mask = np.arange(self.config.batch_size)[(end - start) == 0]
-            for idx in mask:
-                u = users[batch_iter[idx]]
-                user_embeddings[u] = u_emb[idx]  # saving user embedding
-                if next_iter < self.users_num:
-                    batch_iter[idx] = next_iter
-                    start[idx] = offset_idx[users[next_iter]]
-                    end[idx] = offset_idx[users[next_iter] + 1]
-                    next_iter += 1
-                else:
-                    batch_mask[idx] = 0
-                    start[idx] = 0
-                    end[idx] = offset_idx[-1]
-
-            for i, _ in enumerate(self.config.layers):
-                state[i][mask] = 0
-
-        return user_embeddings
-
-    def evaluate(self):
-        self.cur_user_embeddings = self._get_user_embeddings()
-        self.cur_item_embeddings, self.cur_item_biases = self.sess.run([self.item_embeddings, self.item_biases])
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        user_embeddings = self.cur_user_embeddings[users]
-        all_ratings = np.matmul(user_embeddings, self.cur_item_embeddings.T) + self.cur_item_biases
-
-        # final_act = leaky-relu
-        if self.final_act == tf.nn.relu:
-            all_ratings = np.maximum(all_ratings, 0)
-        elif self.final_act == tf.identity:
-            all_ratings = all_ratings
-        elif self.final_act == tf.nn.leaky_relu:
-            all_ratings = np.maximum(all_ratings, all_ratings*0.2)
-        else:
-            pass
-
-        all_ratings = np.array(all_ratings, dtype=np.float32)
-        return all_ratings
+"""
+Paper: Session-based Recommendations with Recurrent Neural Networks
+Author: Balzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk
+Reference: https://github.com/hidasib/GRU4Rec
+           https://github.com/Songweiping/GRU4Rec_TensorFlow
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["GRU4Rec"]
+
+import numpy as np
+import tensorflow as tf
+from typing import List, Dict
+from .base import AbstractRecommender
+from ..utils.py import RankingEvaluator, MetricReport
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.tf1x import bpr_loss, l2_loss
+
+
+class GRU4RecConfig(Config):
+    def __init__(self,
+                 lr=0.001,
+                 reg=0.0,
+                 layers=[64],
+                 batch_size=128,
+                 loss="top1",
+                 hidden_act="tanh",
+                 final_act="linear",
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(GRU4RecConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.layers: List[int] = layers
+        self.batch_size: int = batch_size
+        self.loss: str = loss  # loss = top1, bpr
+        self.hidden_act: str = hidden_act  # hidden_act = relu, tanh
+        self.final_act: str = final_act  # final_act = linear, relu, leaky_relu
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.layers, list)
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.loss, str) and self.loss in {"top1", "bpr"}
+        assert isinstance(self.hidden_act, str) and self.hidden_act in {"relu", "tanh"}
+        assert isinstance(self.final_act, str) and self.final_act in {"linear", "relu", "leaky_relu"}
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class GRU4Rec(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = GRU4RecConfig(**cfg_dict)
+        super(GRU4Rec, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        if config.hidden_act == "relu":
+            self.hidden_act = tf.nn.relu
+        elif config.hidden_act == "tanh":
+            self.hidden_act = tf.nn.tanh
+        else:
+            raise ValueError("There is not hidden_act named '%s'." % config.hidden_act)
+
+        # final_act = leaky-relu
+        if config.final_act == "relu":
+            self.final_act = tf.nn.relu
+        elif config.final_act == "linear":
+            self.final_act = tf.identity
+        elif config.final_act == "leaky_relu":
+            self.final_act = tf.nn.leaky_relu
+        else:
+            raise ValueError("There is not final_act named '%s'." % config.final_act)
+
+        if config.loss == "bpr":
+            self.loss_fun = self._bpr_loss
+        elif config.loss == "top1":
+            self.loss_fun = self._top1_loss
+        else:
+            raise ValueError("There is not loss named '%s'." % config.loss)
+
+        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
+
+        self.data_ui, self.offset_idx = self._init_data()
+
+        self._build_model()
+        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
+        tf_config.gpu_options.allow_growth = True
+        self.sess = tf.Session(config=tf_config)
+        self.sess.run(tf.global_variables_initializer())
+
+    def _init_data(self):
+        data_ui = self.dataset.train_data.to_user_item_pairs_by_time()
+        _, idx = np.unique(data_ui[:, 0], return_index=True)
+        offset_idx = np.zeros(len(idx)+1, dtype=np.int32)
+        offset_idx[:-1] = idx
+        offset_idx[-1] = len(data_ui)
+
+        return data_ui, offset_idx
+
+    def _create_variable(self):
+        self.X_ph = tf.placeholder(tf.int32, [self.config.batch_size], name='input')
+        self.Y_ph = tf.placeholder(tf.int32, [self.config.batch_size], name='output')
+        self.state_ph = [tf.placeholder(tf.float32, [self.config.batch_size, n_unit], name='layer_%d_state' % idx)
+                         for idx, n_unit in enumerate(self.config.layers)]
+
+        init = tf.random.truncated_normal([self.items_num, self.config.layers[0]], mean=0.0, stddev=0.01)
+        self.input_embeddings = tf.Variable(init, dtype=tf.float32, name="input_embeddings")
+
+        init = tf.random.truncated_normal([self.items_num, self.config.layers[-1]], mean=0.0, stddev=0.01)
+        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name="item_embeddings")
+        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name="item_biases")
+
+    def _bpr_loss(self, logits):
+        # logits: (b, size_y)
+        pos_logits = tf.matrix_diag_part(logits)  # (b,)
+        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
+        loss = tf.reduce_mean(bpr_loss(pos_logits, logits))
+        return loss
+
+    def _top1_loss(self, logits):
+        # logits: (b, size_y)
+        pos_logits = tf.matrix_diag_part(logits)  # (b,)
+        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
+        loss1 = tf.reduce_mean(tf.sigmoid(-pos_logits + logits), axis=-1)  # (b,)
+        loss2 = tf.reduce_mean(tf.sigmoid(tf.pow(logits, 2)), axis=-1) - \
+                tf.squeeze(tf.sigmoid(tf.pow(pos_logits, 2))/self.config.batch_size)  # (b,)
+        return tf.reduce_mean(loss1+loss2)
+
+    def _build_model(self):
+        self._create_variable()
+        # get embedding and bias
+        # b: batch size
+        # l1: the dim of the first layer
+        # ln: the dim of the last layer
+        # size_y: the length of Y_ph, i.e., n_sample+batch_size
+
+        cells = [tf.nn.rnn_cell.GRUCell(size, activation=self.hidden_act) for size in self.config.layers]
+        drop_cell = [tf.nn.rnn_cell.DropoutWrapper(cell) for cell in cells]
+        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(drop_cell)
+        inputs = tf.nn.embedding_lookup(self.input_embeddings, self.X_ph)  # (b, l1)
+        outputs, state = stacked_cell(inputs, state=self.state_ph)
+        self.u_emb = outputs  # outputs: (b, ln)
+        self.final_state = state  # [(b, l1), (b, l2), ..., (b, ln)]
+
+        # for training
+        items_embed = tf.nn.embedding_lookup(self.item_embeddings, self.Y_ph)  # (size_y, ln)
+        items_bias = tf.gather(self.item_biases, self.Y_ph)  # (size_y,)
+
+        logits = tf.matmul(outputs, items_embed, transpose_b=True) + items_bias  # (b, size_y)
+        logits = self.final_act(logits)
+
+        loss = self.loss_fun(logits)
+
+        # reg loss
+
+        reg_loss = l2_loss(inputs, items_embed, items_bias)
+        final_loss = loss + self.config.reg*reg_loss
+        self.update_opt = tf.train.AdamOptimizer(self.config.lr).minimize(final_loss)
+
+    def fit(self):
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+
+        data_ui, offset_idx = self.data_ui, self.offset_idx
+        data_items = data_ui[:, 1]
+
+        for epoch in range(self.config.epochs):
+            state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
+            user_idx = np.random.permutation(len(offset_idx) - 1)
+            iters = np.arange(self.config.batch_size, dtype=np.int32)
+            maxiter = iters.max()
+            start = offset_idx[user_idx[iters]]
+            end = offset_idx[user_idx[iters]+1]
+            finished = False
+            while not finished:
+                min_len = (end - start).min()
+                out_idx = data_items[start]
+                for i in range(min_len-1):
+                    in_idx = out_idx
+                    out_idx = data_items[start+i+1]
+                    out_items = out_idx
+
+                    feed = {self.X_ph: in_idx, self.Y_ph: out_items}
+                    for l in range(len(self.config.layers)):
+                        feed[self.state_ph[l]] = state[l]
+
+                    _, state = self.sess.run([self.update_opt, self.final_state], feed_dict=feed)
+
+                start = start+min_len-1
+                mask = np.arange(len(iters))[(end - start) <= 1]
+                for idx in mask:
+                    maxiter += 1
+                    if maxiter >= len(offset_idx)-1:
+                        finished = True
+                        break
+                    iters[idx] = maxiter
+                    start[idx] = offset_idx[user_idx[maxiter]]
+                    end[idx] = offset_idx[user_idx[maxiter]+1]
+                if len(mask):
+                    for i in range(len(self.config.layers)):
+                        state[i][mask] = 0
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def _get_user_embeddings(self):
+        users = np.array(list(self.user_pos_train.keys()), dtype=np.int32)
+        u_nnz = np.array([len(self.user_pos_train[u]) for u in users], dtype=np.int32)
+        users = users[np.argsort(-u_nnz)]
+        data_ui, offset_idx = self.data_ui, self.offset_idx
+        data_items = data_ui[:, 1]
+
+        state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
+        batch_iter = np.arange(self.config.batch_size, dtype=np.int32)
+        next_iter = batch_iter.max() + 1
+
+        start = offset_idx[users[batch_iter]]
+        end = offset_idx[users[batch_iter] + 1]  # the start index of next user
+
+        batch_mask = np.ones([self.config.batch_size], dtype=np.int32)
+        user_embeddings = np.zeros([self.users_num, self.config.layers[-1]], dtype=np.float32)  # saving user embedding
+        while np.sum(batch_mask) > 0:
+            min_len = (end - start).min()
+
+            for i in range(min_len):
+                cur_items = data_items[start + i]
+                feed = {self.X_ph: cur_items}
+                for l in range(len(self.config.layers)):
+                    feed[self.state_ph[l]] = state[l]
+
+                u_emb, state = self.sess.run([self.u_emb, self.final_state], feed_dict=feed)
+
+            start = start + min_len
+            mask = np.arange(self.config.batch_size)[(end - start) == 0]
+            for idx in mask:
+                u = users[batch_iter[idx]]
+                user_embeddings[u] = u_emb[idx]  # saving user embedding
+                if next_iter < self.users_num:
+                    batch_iter[idx] = next_iter
+                    start[idx] = offset_idx[users[next_iter]]
+                    end[idx] = offset_idx[users[next_iter] + 1]
+                    next_iter += 1
+                else:
+                    batch_mask[idx] = 0
+                    start[idx] = 0
+                    end[idx] = offset_idx[-1]
+
+            for i, _ in enumerate(self.config.layers):
+                state[i][mask] = 0
+
+        return user_embeddings
+
+    def evaluate(self):
+        self.cur_user_embeddings = self._get_user_embeddings()
+        self.cur_item_embeddings, self.cur_item_biases = self.sess.run([self.item_embeddings, self.item_biases])
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        user_embeddings = self.cur_user_embeddings[users]
+        all_ratings = np.matmul(user_embeddings, self.cur_item_embeddings.T) + self.cur_item_biases
+
+        # final_act = leaky-relu
+        if self.final_act == tf.nn.relu:
+            all_ratings = np.maximum(all_ratings, 0)
+        elif self.final_act == tf.identity:
+            all_ratings = all_ratings
+        elif self.final_act == tf.nn.leaky_relu:
+            all_ratings = np.maximum(all_ratings, all_ratings*0.2)
+        else:
+            pass
+
+        all_ratings = np.array(all_ratings, dtype=np.float32)
+        return all_ratings
```

## skrec/recommender/SRGNN.py

 * *Ordering differences only*

```diff
@@ -1,295 +1,295 @@
-"""
-Paper: Session-Based Recommendation with Graph Neural Networks
-Author: Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan
-Reference: https://github.com/CRIPAC-DIG/SR-GNN
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["SRGNN"]
-
-import math
-import numpy as np
-import tensorflow as tf
-from typing import Dict
-from .base import AbstractRecommender
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.py import pad_sequences
-from ..utils.py import BatchIterator
-
-
-class SRGNNConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 l2_reg=1e-5,
-                 hidden_size=64,
-                 lr_dc=0.1,
-                 lr_dc_step=3,
-                 step=1,
-                 nonhybrid=False,
-                 max_seq_len=200,
-                 batch_size=256,
-                 epochs=500,
-                 early_stop=50,
-                 **kwargs):
-        super(SRGNNConfig, self).__init__()
-        self.lr: float = lr
-        self.l2_reg: float = l2_reg
-        self.hidden_size: int = hidden_size
-        self.lr_dc: float = lr_dc
-        self.lr_dc_step: int = lr_dc_step
-        self.step: int = step
-        self.nonhybrid: bool = nonhybrid
-        # max_seq_len is used to save gpu memory by limiting the max length of item sequence
-        self.max_seq_len: int = max_seq_len
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.l2_reg, float) and self.l2_reg >= 0
-        assert isinstance(self.hidden_size, int) and self.hidden_size > 0
-        assert isinstance(self.lr_dc, float) and self.lr_dc >= 0
-        assert isinstance(self.lr_dc_step, int) and self.lr_dc_step >= 0
-        assert isinstance(self.step, int) and self.step > 0
-        assert isinstance(self.nonhybrid, bool)
-        assert isinstance(self.max_seq_len, int) and self.max_seq_len > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class SRGNN(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = SRGNNConfig(**cfg_dict)
-        super(SRGNN, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_item = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
-
-        self.train_seq = []
-        self.train_tar = []
-        for user, seqs in self.user_pos_train.items():
-            for i in range(1, len(seqs)):
-                self.train_seq.append(seqs[-i-self.config.max_seq_len:-i])
-                self.train_tar.append(seqs[-i])
-
-        self._build_model()
-        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
-        tf_config.gpu_options.allow_growth = True
-        self.sess = tf.Session(config=tf_config)
-        self.sess.run(tf.global_variables_initializer())
-
-    def _create_variable(self):
-        self.mask_ph = tf.placeholder(dtype=tf.float32, shape=[self.config.batch_size, None])
-        self.alias_ph = tf.placeholder(dtype=tf.int32, shape=[self.config.batch_size, None])
-        self.item_ph = tf.placeholder(dtype=tf.int32, shape=[self.config.batch_size, None])
-        self.target_ph = tf.placeholder(dtype=tf.int32, shape=[self.config.batch_size])
-
-        self.adj_in_ph = tf.placeholder(dtype=tf.float32, shape=[self.config.batch_size, None, None])
-        self.adj_out_ph = tf.placeholder(dtype=tf.float32, shape=[self.config.batch_size, None, None])
-
-        stdv = 1.0 / math.sqrt(self.config.hidden_size)
-        w_init = tf.random_uniform_initializer(-stdv, stdv)
-        self.nasr_w1 = tf.get_variable('nasr_w1', [self.config.hidden_size, self.config.hidden_size],
-                                       dtype=tf.float32, initializer=w_init)
-        self.nasr_w2 = tf.get_variable('nasr_w2', [self.config.hidden_size, self.config.hidden_size],
-                                       dtype=tf.float32, initializer=w_init)
-        self.nasr_v = tf.get_variable('nasrv', [1, self.config.hidden_size], dtype=tf.float32, initializer=w_init)
-        self.nasr_b = tf.get_variable('nasr_b', [self.config.hidden_size], dtype=tf.float32,
-                                      initializer=tf.zeros_initializer())
-
-        embedding = tf.get_variable(shape=[self.num_item, self.config.hidden_size], name='embedding',
-                                    dtype=tf.float32, initializer=w_init)
-        zero_pad = tf.zeros([1, self.config.hidden_size], name="padding")
-        self.embedding = tf.concat([embedding, zero_pad], axis=0)
-
-        self.W_in = tf.get_variable('W_in', shape=[self.config.hidden_size, self.config.hidden_size],
-                                    dtype=tf.float32, initializer=w_init)
-        self.b_in = tf.get_variable('b_in', [self.config.hidden_size], dtype=tf.float32, initializer=w_init)
-        self.W_out = tf.get_variable('W_out', [self.config.hidden_size, self.config.hidden_size],
-                                     dtype=tf.float32, initializer=w_init)
-        self.b_out = tf.get_variable('b_out', [self.config.hidden_size], dtype=tf.float32, initializer=w_init)
-
-        self.B = tf.get_variable('B', [2 * self.config.hidden_size, self.config.hidden_size], initializer=w_init)
-
-    def ggnn(self):
-        fin_state = tf.nn.embedding_lookup(self.embedding, self.item_ph)  # (b,l,d)
-        cell = tf.nn.rnn_cell.GRUCell(self.config.hidden_size)
-        with tf.variable_scope('gru'):
-            for i in range(self.config.step):
-                fin_state = tf.reshape(fin_state, [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
-                fin_state_tmp = tf.reshape(fin_state, [-1, self.config.hidden_size])  # (b*l,d)
-
-                fin_state_in = tf.reshape(tf.matmul(fin_state_tmp, self.W_in) + self.b_in,
-                                          [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
-
-                # fin_state_tmp = tf.reshape(fin_state, [-1, self.hidden_size])  # (b*l,d)
-                fin_state_out = tf.reshape(tf.matmul(fin_state_tmp, self.W_out) + self.b_out,
-                                           [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
-
-                av_in = tf.matmul(self.adj_in_ph, fin_state_in)  # (b,l,d)
-                av_out = tf.matmul(self.adj_out_ph, fin_state_out)  # (b,l,d)
-                av = tf.concat([av_in, av_out], axis=-1)  # (b,l,2d)
-
-                av = tf.expand_dims(tf.reshape(av, [-1, 2 * self.config.hidden_size]), axis=1)  # (b*l,1,2d)
-                # fin_state_tmp = tf.reshape(fin_state, [-1, self.hidden_size])  # (b*l,d)
-
-                state_output, fin_state = tf.nn.dynamic_rnn(cell, av, initial_state=fin_state_tmp)
-        return tf.reshape(fin_state, [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
-
-    def _session_embedding(self, re_embedding):
-        # re_embedding  (b,l,d)
-        rm = tf.reduce_sum(self.mask_ph, 1)  # (b,), length of each session
-        last_idx = tf.stack([tf.range(self.config.batch_size), tf.to_int32(rm) - 1], axis=1)  # (b, 2) index of last item
-        last_id = tf.gather_nd(self.alias_ph, last_idx)  # (b,) alias id of last item
-        last_h = tf.gather_nd(re_embedding, tf.stack([tf.range(self.config.batch_size), last_id], axis=1))  # (b,d) embedding of last item
-
-        seq_h = [tf.nn.embedding_lookup(re_embedding[i], self.alias_ph[i]) for i in range(self.config.batch_size)]
-        seq_h = tf.stack(seq_h, axis=0)  # batch_size*T*d
-        last = tf.matmul(last_h, self.nasr_w1)
-        seq = tf.matmul(tf.reshape(seq_h, [-1, self.config.hidden_size]), self.nasr_w2)
-        last = tf.reshape(last, [self.config.batch_size, 1, -1])
-        m = tf.nn.sigmoid(last + tf.reshape(seq, [self.config.batch_size, -1, self.config.hidden_size]) + self.nasr_b)
-        coef = tf.matmul(tf.reshape(m, [-1, self.config.hidden_size]), self.nasr_v,
-                         transpose_b=True) * tf.reshape(self.mask_ph, [-1, 1])
-        if not self.config.nonhybrid:
-            ma = tf.concat([tf.reduce_sum(tf.reshape(coef, [self.config.batch_size, -1, 1]) * seq_h, 1),
-                            tf.reshape(last, [-1, self.config.hidden_size])], -1)
-            sess_embedding = tf.matmul(ma, self.B)
-        else:
-            sess_embedding = tf.reduce_sum(tf.reshape(coef, [self.config.batch_size, -1, 1]) * seq_h, 1)
-
-        return sess_embedding
-
-    def _build_model(self):
-        self._create_variable()
-        with tf.variable_scope('ggnn_model', reuse=None):
-            node_embedding = self.ggnn()
-            sess_embedding = self._session_embedding(node_embedding)
-
-        item_embedding = self.embedding[:-1]
-        self.all_logits = tf.matmul(sess_embedding, item_embedding, transpose_b=True)
-        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target_ph, logits=self.all_logits)
-        loss = tf.reduce_mean(loss)
-
-        var_list = tf.trainable_variables()
-        l2_loss = [tf.nn.l2_loss(v) for v in var_list if v.name not in ['bias', 'gamma', 'b', 'g', 'beta']]
-        loss_train = loss + self.config.l2_reg * tf.add_n(l2_loss)
-
-        global_step = tf.Variable(0)
-        decay = self.config.lr_dc_step * len(self.train_seq) / self.config.batch_size
-        learning_rate = tf.train.exponential_decay(self.config.lr, global_step=global_step, decay_steps=decay,
-                                                   decay_rate=self.config.lr_dc, staircase=True)
-        self.train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss_train, global_step=global_step)
-
-    def fit(self):
-        train_seq_len = [(idx, len(seq)) for idx, seq in enumerate(self.train_seq)]
-        train_seq_len = sorted(train_seq_len, key=lambda x: x[1], reverse=True)
-        train_seq_index, _ = list(zip(*train_seq_len))
-
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            for bat_index in self._shuffle_index(train_seq_index):
-                item_seqs = [self.train_seq[idx] for idx in bat_index]
-                bat_tars = [self.train_tar[idx] for idx in bat_index]
-                bat_adj_in, bat_adj_out, bat_alias, bat_items, bat_mask = self._build_session_graph(item_seqs)
-                feed = {self.target_ph: bat_tars,
-                        self.item_ph: bat_items,
-                        self.adj_in_ph: bat_adj_in,
-                        self.adj_out_ph: bat_adj_out,
-                        self.alias_ph: bat_alias,
-                        self.mask_ph: bat_mask}
-
-                self.sess.run(self.train_opt, feed_dict=feed)
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def _shuffle_index(self, seq_index):
-        """NOTE: two-step shuffle for saving memory"""
-        index_chunks = BatchIterator(seq_index, batch_size=self.config.batch_size*32,
-                                     shuffle=False, drop_last=False)  # chunking
-        index_chunks = list(index_chunks)
-        index_chunks_iter = BatchIterator(index_chunks, batch_size=1,
-                                          shuffle=True, drop_last=False)  # shuffle index chunk
-        for indexes in index_chunks_iter:  # outer-layer shuffle
-            indexes_iter = BatchIterator(indexes[0], batch_size=self.config.batch_size,
-                                         shuffle=True, drop_last=True)  # inner-layer shuffle
-            for bat_index in indexes_iter:
-                yield bat_index
-
-    def _build_session_graph(self, bat_items):
-        A_in, A_out, alias_inputs = [], [], []
-        all_mask = [[1] * len(items) for items in bat_items]
-        bat_items = pad_sequences(bat_items, value=self.num_item)
-
-        unique_nodes = [np.unique(items).tolist() for items in bat_items]
-        max_n_node = np.max([len(nodes) for nodes in unique_nodes])
-        for u_seq, u_node, mask in zip(bat_items, unique_nodes, all_mask):
-            adj_mat = np.zeros((max_n_node, max_n_node))
-            id_map = {node: idx for idx, node in enumerate(u_node)}
-            if len(u_seq) > 1:
-                alias_previous = [id_map[i] for i in u_seq[:len(mask) - 1]]
-                alias_next = [id_map[i] for i in u_seq[1:len(mask)]]
-                adj_mat[alias_previous, alias_next] = 1
-
-            u_sum_in = np.sum(adj_mat, axis=0)
-            u_sum_in[np.where(u_sum_in == 0)] = 1
-            u_A_in = np.divide(adj_mat, u_sum_in)
-
-            u_sum_out = np.sum(adj_mat, 1)
-            u_sum_out[np.where(u_sum_out == 0)] = 1
-            u_A_out = np.divide(adj_mat.transpose(), u_sum_out)
-
-            A_in.append(u_A_in)
-            A_out.append(u_A_out)
-            alias_inputs.append([id_map[i] for i in u_seq])
-
-        items = pad_sequences(unique_nodes, value=self.num_item)
-        all_mask = pad_sequences(all_mask, value=0)
-        return A_in, A_out, alias_inputs, items, all_mask
-
-    def evaluate(self):
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        bat_user = users
-        cur_batch_size = len(bat_user)
-        bat_items = [self.user_pos_train[user][-self.config.max_seq_len:] for user in bat_user]
-        bat_adj_in, bat_adj_out, bat_alias, bat_items, bat_mask = self._build_session_graph(bat_items)
-        if cur_batch_size < self.config.batch_size:  # padding
-            pad_size = self.config.batch_size - cur_batch_size
-            bat_adj_in = np.concatenate([bat_adj_in, [bat_adj_in[-1]] * pad_size], axis=0)
-            bat_adj_out = np.concatenate([bat_adj_out, [bat_adj_out[-1]] * pad_size], axis=0)
-            bat_alias = np.concatenate([bat_alias, [bat_alias[-1]] * pad_size], axis=0)
-            bat_items = np.concatenate([bat_items, [bat_items[-1]] * pad_size], axis=0)
-            bat_mask = np.concatenate([bat_mask, [bat_mask[-1]] * pad_size], axis=0)
-
-        feed = {self.item_ph: bat_items,
-                self.adj_in_ph: bat_adj_in,
-                self.adj_out_ph: bat_adj_out,
-                self.alias_ph: bat_alias,
-                self.mask_ph: bat_mask}
-        bat_ratings = self.sess.run(self.all_logits, feed_dict=feed)
-        ratings = bat_ratings[:cur_batch_size]
-        return ratings
+"""
+Paper: Session-Based Recommendation with Graph Neural Networks
+Author: Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan
+Reference: https://github.com/CRIPAC-DIG/SR-GNN
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["SRGNN"]
+
+import math
+import numpy as np
+import tensorflow as tf
+from typing import Dict
+from .base import AbstractRecommender
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import pad_sequences
+from ..utils.py import BatchIterator
+
+
+class SRGNNConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 l2_reg=1e-5,
+                 hidden_size=64,
+                 lr_dc=0.1,
+                 lr_dc_step=3,
+                 step=1,
+                 nonhybrid=False,
+                 max_seq_len=200,
+                 batch_size=256,
+                 epochs=500,
+                 early_stop=50,
+                 **kwargs):
+        super(SRGNNConfig, self).__init__()
+        self.lr: float = lr
+        self.l2_reg: float = l2_reg
+        self.hidden_size: int = hidden_size
+        self.lr_dc: float = lr_dc
+        self.lr_dc_step: int = lr_dc_step
+        self.step: int = step
+        self.nonhybrid: bool = nonhybrid
+        # max_seq_len is used to save gpu memory by limiting the max length of item sequence
+        self.max_seq_len: int = max_seq_len
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.l2_reg, float) and self.l2_reg >= 0
+        assert isinstance(self.hidden_size, int) and self.hidden_size > 0
+        assert isinstance(self.lr_dc, float) and self.lr_dc >= 0
+        assert isinstance(self.lr_dc_step, int) and self.lr_dc_step >= 0
+        assert isinstance(self.step, int) and self.step > 0
+        assert isinstance(self.nonhybrid, bool)
+        assert isinstance(self.max_seq_len, int) and self.max_seq_len > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class SRGNN(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = SRGNNConfig(**cfg_dict)
+        super(SRGNN, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_item = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
+
+        self.train_seq = []
+        self.train_tar = []
+        for user, seqs in self.user_pos_train.items():
+            for i in range(1, len(seqs)):
+                self.train_seq.append(seqs[-i-self.config.max_seq_len:-i])
+                self.train_tar.append(seqs[-i])
+
+        self._build_model()
+        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
+        tf_config.gpu_options.allow_growth = True
+        self.sess = tf.Session(config=tf_config)
+        self.sess.run(tf.global_variables_initializer())
+
+    def _create_variable(self):
+        self.mask_ph = tf.placeholder(dtype=tf.float32, shape=[self.config.batch_size, None])
+        self.alias_ph = tf.placeholder(dtype=tf.int32, shape=[self.config.batch_size, None])
+        self.item_ph = tf.placeholder(dtype=tf.int32, shape=[self.config.batch_size, None])
+        self.target_ph = tf.placeholder(dtype=tf.int32, shape=[self.config.batch_size])
+
+        self.adj_in_ph = tf.placeholder(dtype=tf.float32, shape=[self.config.batch_size, None, None])
+        self.adj_out_ph = tf.placeholder(dtype=tf.float32, shape=[self.config.batch_size, None, None])
+
+        stdv = 1.0 / math.sqrt(self.config.hidden_size)
+        w_init = tf.random_uniform_initializer(-stdv, stdv)
+        self.nasr_w1 = tf.get_variable('nasr_w1', [self.config.hidden_size, self.config.hidden_size],
+                                       dtype=tf.float32, initializer=w_init)
+        self.nasr_w2 = tf.get_variable('nasr_w2', [self.config.hidden_size, self.config.hidden_size],
+                                       dtype=tf.float32, initializer=w_init)
+        self.nasr_v = tf.get_variable('nasrv', [1, self.config.hidden_size], dtype=tf.float32, initializer=w_init)
+        self.nasr_b = tf.get_variable('nasr_b', [self.config.hidden_size], dtype=tf.float32,
+                                      initializer=tf.zeros_initializer())
+
+        embedding = tf.get_variable(shape=[self.num_item, self.config.hidden_size], name='embedding',
+                                    dtype=tf.float32, initializer=w_init)
+        zero_pad = tf.zeros([1, self.config.hidden_size], name="padding")
+        self.embedding = tf.concat([embedding, zero_pad], axis=0)
+
+        self.W_in = tf.get_variable('W_in', shape=[self.config.hidden_size, self.config.hidden_size],
+                                    dtype=tf.float32, initializer=w_init)
+        self.b_in = tf.get_variable('b_in', [self.config.hidden_size], dtype=tf.float32, initializer=w_init)
+        self.W_out = tf.get_variable('W_out', [self.config.hidden_size, self.config.hidden_size],
+                                     dtype=tf.float32, initializer=w_init)
+        self.b_out = tf.get_variable('b_out', [self.config.hidden_size], dtype=tf.float32, initializer=w_init)
+
+        self.B = tf.get_variable('B', [2 * self.config.hidden_size, self.config.hidden_size], initializer=w_init)
+
+    def ggnn(self):
+        fin_state = tf.nn.embedding_lookup(self.embedding, self.item_ph)  # (b,l,d)
+        cell = tf.nn.rnn_cell.GRUCell(self.config.hidden_size)
+        with tf.variable_scope('gru'):
+            for i in range(self.config.step):
+                fin_state = tf.reshape(fin_state, [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
+                fin_state_tmp = tf.reshape(fin_state, [-1, self.config.hidden_size])  # (b*l,d)
+
+                fin_state_in = tf.reshape(tf.matmul(fin_state_tmp, self.W_in) + self.b_in,
+                                          [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
+
+                # fin_state_tmp = tf.reshape(fin_state, [-1, self.hidden_size])  # (b*l,d)
+                fin_state_out = tf.reshape(tf.matmul(fin_state_tmp, self.W_out) + self.b_out,
+                                           [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
+
+                av_in = tf.matmul(self.adj_in_ph, fin_state_in)  # (b,l,d)
+                av_out = tf.matmul(self.adj_out_ph, fin_state_out)  # (b,l,d)
+                av = tf.concat([av_in, av_out], axis=-1)  # (b,l,2d)
+
+                av = tf.expand_dims(tf.reshape(av, [-1, 2 * self.config.hidden_size]), axis=1)  # (b*l,1,2d)
+                # fin_state_tmp = tf.reshape(fin_state, [-1, self.hidden_size])  # (b*l,d)
+
+                state_output, fin_state = tf.nn.dynamic_rnn(cell, av, initial_state=fin_state_tmp)
+        return tf.reshape(fin_state, [self.config.batch_size, -1, self.config.hidden_size])  # (b,l,d)
+
+    def _session_embedding(self, re_embedding):
+        # re_embedding  (b,l,d)
+        rm = tf.reduce_sum(self.mask_ph, 1)  # (b,), length of each session
+        last_idx = tf.stack([tf.range(self.config.batch_size), tf.to_int32(rm) - 1], axis=1)  # (b, 2) index of last item
+        last_id = tf.gather_nd(self.alias_ph, last_idx)  # (b,) alias id of last item
+        last_h = tf.gather_nd(re_embedding, tf.stack([tf.range(self.config.batch_size), last_id], axis=1))  # (b,d) embedding of last item
+
+        seq_h = [tf.nn.embedding_lookup(re_embedding[i], self.alias_ph[i]) for i in range(self.config.batch_size)]
+        seq_h = tf.stack(seq_h, axis=0)  # batch_size*T*d
+        last = tf.matmul(last_h, self.nasr_w1)
+        seq = tf.matmul(tf.reshape(seq_h, [-1, self.config.hidden_size]), self.nasr_w2)
+        last = tf.reshape(last, [self.config.batch_size, 1, -1])
+        m = tf.nn.sigmoid(last + tf.reshape(seq, [self.config.batch_size, -1, self.config.hidden_size]) + self.nasr_b)
+        coef = tf.matmul(tf.reshape(m, [-1, self.config.hidden_size]), self.nasr_v,
+                         transpose_b=True) * tf.reshape(self.mask_ph, [-1, 1])
+        if not self.config.nonhybrid:
+            ma = tf.concat([tf.reduce_sum(tf.reshape(coef, [self.config.batch_size, -1, 1]) * seq_h, 1),
+                            tf.reshape(last, [-1, self.config.hidden_size])], -1)
+            sess_embedding = tf.matmul(ma, self.B)
+        else:
+            sess_embedding = tf.reduce_sum(tf.reshape(coef, [self.config.batch_size, -1, 1]) * seq_h, 1)
+
+        return sess_embedding
+
+    def _build_model(self):
+        self._create_variable()
+        with tf.variable_scope('ggnn_model', reuse=None):
+            node_embedding = self.ggnn()
+            sess_embedding = self._session_embedding(node_embedding)
+
+        item_embedding = self.embedding[:-1]
+        self.all_logits = tf.matmul(sess_embedding, item_embedding, transpose_b=True)
+        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target_ph, logits=self.all_logits)
+        loss = tf.reduce_mean(loss)
+
+        var_list = tf.trainable_variables()
+        l2_loss = [tf.nn.l2_loss(v) for v in var_list if v.name not in ['bias', 'gamma', 'b', 'g', 'beta']]
+        loss_train = loss + self.config.l2_reg * tf.add_n(l2_loss)
+
+        global_step = tf.Variable(0)
+        decay = self.config.lr_dc_step * len(self.train_seq) / self.config.batch_size
+        learning_rate = tf.train.exponential_decay(self.config.lr, global_step=global_step, decay_steps=decay,
+                                                   decay_rate=self.config.lr_dc, staircase=True)
+        self.train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss_train, global_step=global_step)
+
+    def fit(self):
+        train_seq_len = [(idx, len(seq)) for idx, seq in enumerate(self.train_seq)]
+        train_seq_len = sorted(train_seq_len, key=lambda x: x[1], reverse=True)
+        train_seq_index, _ = list(zip(*train_seq_len))
+
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            for bat_index in self._shuffle_index(train_seq_index):
+                item_seqs = [self.train_seq[idx] for idx in bat_index]
+                bat_tars = [self.train_tar[idx] for idx in bat_index]
+                bat_adj_in, bat_adj_out, bat_alias, bat_items, bat_mask = self._build_session_graph(item_seqs)
+                feed = {self.target_ph: bat_tars,
+                        self.item_ph: bat_items,
+                        self.adj_in_ph: bat_adj_in,
+                        self.adj_out_ph: bat_adj_out,
+                        self.alias_ph: bat_alias,
+                        self.mask_ph: bat_mask}
+
+                self.sess.run(self.train_opt, feed_dict=feed)
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def _shuffle_index(self, seq_index):
+        """NOTE: two-step shuffle for saving memory"""
+        index_chunks = BatchIterator(seq_index, batch_size=self.config.batch_size*32,
+                                     shuffle=False, drop_last=False)  # chunking
+        index_chunks = list(index_chunks)
+        index_chunks_iter = BatchIterator(index_chunks, batch_size=1,
+                                          shuffle=True, drop_last=False)  # shuffle index chunk
+        for indexes in index_chunks_iter:  # outer-layer shuffle
+            indexes_iter = BatchIterator(indexes[0], batch_size=self.config.batch_size,
+                                         shuffle=True, drop_last=True)  # inner-layer shuffle
+            for bat_index in indexes_iter:
+                yield bat_index
+
+    def _build_session_graph(self, bat_items):
+        A_in, A_out, alias_inputs = [], [], []
+        all_mask = [[1] * len(items) for items in bat_items]
+        bat_items = pad_sequences(bat_items, value=self.num_item)
+
+        unique_nodes = [np.unique(items).tolist() for items in bat_items]
+        max_n_node = np.max([len(nodes) for nodes in unique_nodes])
+        for u_seq, u_node, mask in zip(bat_items, unique_nodes, all_mask):
+            adj_mat = np.zeros((max_n_node, max_n_node))
+            id_map = {node: idx for idx, node in enumerate(u_node)}
+            if len(u_seq) > 1:
+                alias_previous = [id_map[i] for i in u_seq[:len(mask) - 1]]
+                alias_next = [id_map[i] for i in u_seq[1:len(mask)]]
+                adj_mat[alias_previous, alias_next] = 1
+
+            u_sum_in = np.sum(adj_mat, axis=0)
+            u_sum_in[np.where(u_sum_in == 0)] = 1
+            u_A_in = np.divide(adj_mat, u_sum_in)
+
+            u_sum_out = np.sum(adj_mat, 1)
+            u_sum_out[np.where(u_sum_out == 0)] = 1
+            u_A_out = np.divide(adj_mat.transpose(), u_sum_out)
+
+            A_in.append(u_A_in)
+            A_out.append(u_A_out)
+            alias_inputs.append([id_map[i] for i in u_seq])
+
+        items = pad_sequences(unique_nodes, value=self.num_item)
+        all_mask = pad_sequences(all_mask, value=0)
+        return A_in, A_out, alias_inputs, items, all_mask
+
+    def evaluate(self):
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        bat_user = users
+        cur_batch_size = len(bat_user)
+        bat_items = [self.user_pos_train[user][-self.config.max_seq_len:] for user in bat_user]
+        bat_adj_in, bat_adj_out, bat_alias, bat_items, bat_mask = self._build_session_graph(bat_items)
+        if cur_batch_size < self.config.batch_size:  # padding
+            pad_size = self.config.batch_size - cur_batch_size
+            bat_adj_in = np.concatenate([bat_adj_in, [bat_adj_in[-1]] * pad_size], axis=0)
+            bat_adj_out = np.concatenate([bat_adj_out, [bat_adj_out[-1]] * pad_size], axis=0)
+            bat_alias = np.concatenate([bat_alias, [bat_alias[-1]] * pad_size], axis=0)
+            bat_items = np.concatenate([bat_items, [bat_items[-1]] * pad_size], axis=0)
+            bat_mask = np.concatenate([bat_mask, [bat_mask[-1]] * pad_size], axis=0)
+
+        feed = {self.item_ph: bat_items,
+                self.adj_in_ph: bat_adj_in,
+                self.adj_out_ph: bat_adj_out,
+                self.alias_ph: bat_alias,
+                self.mask_ph: bat_mask}
+        bat_ratings = self.sess.run(self.all_logits, feed_dict=feed)
+        ratings = bat_ratings[:cur_batch_size]
+        return ratings
```

## skrec/recommender/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = []
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = []
```

## skrec/recommender/GRU4RecPlus.py

 * *Ordering differences only*

```diff
@@ -1,331 +1,331 @@
-"""
-Paper: Recurrent Neural Networks with Top-k Gains for Session-based Recommendations
-Author: Balzs Hidasi, and Alexandros Karatzoglou
-Reference: https://github.com/hidasib/GRU4Rec
-           https://github.com/Songweiping/GRU4Rec_TensorFlow
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["GRU4RecPlus"]
-
-
-import numpy as np
-import tensorflow as tf
-from typing import List, Dict
-from .base import AbstractRecommender
-from ..utils.py import RankingEvaluator, MetricReport
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.tf1x import l2_loss
-
-
-class GRU4RecPlusConfig(Config):
-    def __init__(self,
-                 lr=0.001,
-                 reg=0.0,
-                 bpr_reg=1.0,
-                 layers=[64],
-                 batch_size=128,
-                 loss="bpr_max",
-                 hidden_act="tanh",
-                 final_act="linear",
-                 n_sample=2048,
-                 sample_alpha=0.75,
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(GRU4RecPlusConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.bpr_reg: float = bpr_reg
-        self.layers: List[int] = layers
-        self.batch_size: int = batch_size
-        self.loss: str = loss  # loss = top1_max, bpr_max
-        self.hidden_act: str = hidden_act  # hidden_act = relu, tanh
-        self.final_act: str = final_act  # final_act = linear, relu, leaky_relu
-        self.n_sample: int = n_sample
-        self.sample_alpha: float = sample_alpha  # 0 < sample_alpha <= 1
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.bpr_reg, float) and self.bpr_reg >= 0
-        assert isinstance(self.layers, list)
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.loss, str) and self.loss in {"top1_max", "bpr_max"}
-        assert isinstance(self.hidden_act, str) and self.hidden_act in {"relu", "tanh"}
-        assert isinstance(self.final_act, str) and self.final_act in {"linear", "relu", "leaky_relu"}
-        assert isinstance(self.n_sample, int) and self.n_sample >= 0
-        assert isinstance(self.sample_alpha, float) and 0 < self.sample_alpha <= 1
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class GRU4RecPlus(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = GRU4RecPlusConfig(**cfg_dict)
-        super(GRU4RecPlus, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        if config.hidden_act == "relu":
-            self.hidden_act = tf.nn.relu
-        elif config.hidden_act == "tanh":
-            self.hidden_act = tf.nn.tanh
-        else:
-            raise ValueError("There is not hidden_act named '%s'." % config.hidden_act)
-
-        # final_act = leaky-relu
-        if config.final_act == "relu":
-            self.final_act = tf.nn.relu
-        elif config.final_act == "linear":
-            self.final_act = tf.identity
-        elif config.final_act == "leaky_relu":
-            self.final_act = tf.nn.leaky_relu
-        else:
-            raise ValueError("There is not final_act named '%s'." % config.final_act)
-
-        if config.loss == "bpr_max":
-            self.loss_fun = self._bpr_max_loss
-        elif config.loss == "top1_max":
-            self.loss_fun = self._top1_max_loss
-        else:
-            raise ValueError("There is not loss named '%s'." % config.loss)
-
-        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
-        self.data_ui, self.offset_idx = self._init_data()
-
-        # for sampling negative items
-        _, pop = np.unique(self.data_ui[:, 1], return_counts=True)
-        pop = np.power(pop, self.config.sample_alpha)
-        pop_cumsum = np.cumsum(pop)
-        self.pop_cumsum = pop_cumsum / pop_cumsum[-1]
-
-        self._build_model()
-        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
-        tf_config.gpu_options.allow_growth = True
-        self.sess = tf.Session(config=tf_config)
-        self.sess.run(tf.global_variables_initializer())
-
-    def _init_data(self):
-        data_ui = self.dataset.train_data.to_user_item_pairs_by_time()
-
-        _, idx = np.unique(data_ui[:, 0], return_index=True)
-        offset_idx = np.zeros(len(idx)+1, dtype=np.int32)
-        offset_idx[:-1] = idx
-        offset_idx[-1] = len(data_ui)
-
-        return data_ui, offset_idx
-
-    def _create_variable(self):
-        self.X_ph = tf.placeholder(tf.int32, [self.config.batch_size], name='input')
-        self.Y_ph = tf.placeholder(tf.int32, [self.config.batch_size+self.config.n_sample], name='output')
-        self.state_ph = [tf.placeholder(tf.float32, [self.config.batch_size, n_unit], name='layer_%d_state' % idx)
-                         for idx, n_unit in enumerate(self.config.layers)]
-
-        init = tf.random.truncated_normal([self.items_num, self.config.layers[0]], mean=0.0, stddev=0.01)
-        self.input_embeddings = tf.Variable(init, dtype=tf.float32, name="input_embeddings")
-
-        init = tf.random.truncated_normal([self.items_num, self.config.layers[-1]], mean=0.0, stddev=0.01)
-        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name="item_embeddings")
-        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name="item_biases")
-
-    def _softmax_neg(self, logits):
-        # logits: (b, size_y)
-        hm = 1.0 - tf.eye(tf.shape(logits)[0], tf.shape(logits)[1])
-        logits = logits * hm
-        logits = logits - tf.reduce_max(logits, axis=1, keep_dims=True)
-        e_x = tf.exp(logits) * hm  # (b, size_y)
-        e_x = e_x / tf.reduce_sum(e_x, axis=1, keep_dims=True)
-        return e_x  # (b, size_y)
-
-    def _bpr_max_loss(self, logits):
-        # logits: (b, size_y)
-        softmax_scores = self._softmax_neg(logits)  # (b, size_y)
-        pos_logits = tf.matrix_diag_part(logits)  # (b,)
-        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
-        prob = tf.sigmoid((pos_logits - logits))  # (b, size_y)
-        prob = tf.reduce_sum(tf.multiply(prob, softmax_scores), axis=1)  # (b,)
-        loss = -tf.log(prob + 1e-24)
-        reg_loss = tf.reduce_sum(tf.multiply(tf.pow(logits, 2), softmax_scores), axis=1)  # (b,)
-
-        return tf.reduce_mean(loss + self.config.bpr_reg*reg_loss)
-
-    def _top1_max_loss(self, logits):
-        softmax_scores = self._softmax_neg(logits)  # (b, size_y)
-
-        pos_logits = tf.matrix_diag_part(logits)  # (b,)
-        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
-        prob = tf.sigmoid(-pos_logits + logits) + tf.sigmoid(tf.pow(logits, 2))
-        loss = tf.reduce_sum(tf.multiply(prob, softmax_scores), axis=1)
-
-        return tf.reduce_mean(loss)
-
-    def _build_model(self):
-        self._create_variable()
-        # get embedding and bias
-        # b: batch size
-        # l1: the dim of the first layer
-        # ln: the dim of the last layer
-        # size_y: the length of Y_ph, i.e., n_sample+batch_size
-
-        cells = [tf.nn.rnn_cell.GRUCell(size, activation=self.hidden_act) for size in self.config.layers]
-        drop_cell = [tf.nn.rnn_cell.DropoutWrapper(cell) for cell in cells]
-        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(drop_cell)
-        inputs = tf.nn.embedding_lookup(self.input_embeddings, self.X_ph)  # (b, l1)
-        outputs, state = stacked_cell(inputs, state=self.state_ph)
-        self.u_emb = outputs  # outputs: (b, ln)
-        self.final_state = state  # [(b, l1), (b, l2), ..., (b, ln)]
-
-        # for training
-        items_embed = tf.nn.embedding_lookup(self.item_embeddings, self.Y_ph)  # (size_y, ln)
-        items_bias = tf.gather(self.item_biases, self.Y_ph)  # (size_y,)
-
-        logits = tf.matmul(outputs, items_embed, transpose_b=True) + items_bias  # (b, size_y)
-        logits = self.final_act(logits)
-
-        loss = self.loss_fun(logits)
-
-        # reg loss
-        reg_loss = l2_loss(inputs, items_embed, items_bias)
-        final_loss = loss + self.config.reg*reg_loss
-        self.update_opt = tf.train.AdamOptimizer(self.config.lr).minimize(final_loss)
-
-    def _sample_neg_items(self, size):
-        samples = np.searchsorted(self.pop_cumsum, np.random.rand(size))
-        return samples
-
-    def fit(self):
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-
-        data_ui, offset_idx = self.data_ui, self.offset_idx
-        data_items = data_ui[:, 1]
-        for epoch in range(self.config.epochs):
-            state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
-            user_idx = np.random.permutation(len(offset_idx) - 1)
-            iters = np.arange(self.config.batch_size, dtype=np.int32)
-            maxiter = iters.max()
-            start = offset_idx[user_idx[iters]]
-            end = offset_idx[user_idx[iters]+1]
-            finished = False
-            while not finished:
-                min_len = (end - start).min()
-                out_idx = data_items[start]
-                for i in range(min_len-1):
-                    in_idx = out_idx
-                    out_idx = data_items[start+i+1]
-                    out_items = out_idx
-                    if self.config.n_sample:
-                        neg_items = self._sample_neg_items(self.config.n_sample)
-                        out_items = np.hstack([out_items, neg_items])
-
-                    feed = {self.X_ph: in_idx, self.Y_ph: out_items}
-                    for l in range(len(self.config.layers)):
-                        feed[self.state_ph[l]] = state[l]
-
-                    _, state = self.sess.run([self.update_opt, self.final_state], feed_dict=feed)
-
-                start = start+min_len-1
-                mask = np.arange(len(iters))[(end - start) <= 1]
-                for idx in mask:
-                    maxiter += 1
-                    if maxiter >= len(offset_idx)-1:
-                        finished = True
-                        break
-                    iters[idx] = maxiter
-                    start[idx] = offset_idx[user_idx[maxiter]]
-                    end[idx] = offset_idx[user_idx[maxiter]+1]
-                if len(mask):
-                    for i in range(len(self.config.layers)):
-                        state[i][mask] = 0
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def _get_user_embeddings(self):
-        users = np.array(list(self.user_pos_train.keys()), dtype=np.int32)
-        u_nnz = np.array([len(self.user_pos_train[u]) for u in users], dtype=np.int32)
-        users = users[np.argsort(-u_nnz)]
-        user_embeddings = np.zeros([self.users_num, self.config.layers[-1]], dtype=np.float32)  # saving user embedding
-
-        data_ui, offset_idx = self.data_ui, self.offset_idx
-        data_items = data_ui[:, 1]
-
-        state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
-        batch_iter = np.arange(self.config.batch_size, dtype=np.int32)
-        next_iter = batch_iter.max() + 1
-
-        start = offset_idx[users[batch_iter]]
-        end = offset_idx[users[batch_iter] + 1]  # the start index of next user
-
-        batch_mask = np.ones([self.config.batch_size], dtype=np.int32)
-        while np.sum(batch_mask) > 0:
-            min_len = (end - start).min()
-
-            for i in range(min_len):
-                cur_items = data_items[start + i]
-                feed = {self.X_ph: cur_items}
-                for l in range(len(self.config.layers)):
-                    feed[self.state_ph[l]] = state[l]
-
-                u_emb, state = self.sess.run([self.u_emb, self.final_state], feed_dict=feed)
-
-            start = start + min_len
-            mask = np.arange(self.config.batch_size)[(end - start) == 0]
-            for idx in mask:
-                u = users[batch_iter[idx]]
-                user_embeddings[u] = u_emb[idx]  # saving user embedding
-                if next_iter < self.users_num:
-                    batch_iter[idx] = next_iter
-                    start[idx] = offset_idx[users[next_iter]]
-                    end[idx] = offset_idx[users[next_iter] + 1]
-                    next_iter += 1
-                else:
-                    batch_mask[idx] = 0
-                    start[idx] = 0
-                    end[idx] = offset_idx[-1]
-
-            for i, _ in enumerate(self.config.layers):
-                state[i][mask] = 0
-
-        return user_embeddings
-
-    def evaluate(self):
-        self.cur_user_embeddings = self._get_user_embeddings()
-        self.cur_item_embeddings, self.cur_item_biases = self.sess.run([self.item_embeddings, self.item_biases])
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        user_embeddings = self.cur_user_embeddings[users]
-        all_ratings = np.matmul(user_embeddings, self.cur_item_embeddings.T) + self.cur_item_biases
-
-        # final_act = leaky-relu
-        if self.final_act == tf.nn.relu:
-            all_ratings = np.maximum(all_ratings, 0)
-        elif self.final_act == tf.identity:
-            all_ratings = all_ratings
-        elif self.final_act == tf.nn.leaky_relu:
-            all_ratings = np.maximum(all_ratings, all_ratings*0.2)
-        else:
-            pass
-
-        all_ratings = np.array(all_ratings, dtype=np.float32)
-        return all_ratings
+"""
+Paper: Recurrent Neural Networks with Top-k Gains for Session-based Recommendations
+Author: Balzs Hidasi, and Alexandros Karatzoglou
+Reference: https://github.com/hidasib/GRU4Rec
+           https://github.com/Songweiping/GRU4Rec_TensorFlow
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["GRU4RecPlus"]
+
+
+import numpy as np
+import tensorflow as tf
+from typing import List, Dict
+from .base import AbstractRecommender
+from ..utils.py import RankingEvaluator, MetricReport
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.tf1x import l2_loss
+
+
+class GRU4RecPlusConfig(Config):
+    def __init__(self,
+                 lr=0.001,
+                 reg=0.0,
+                 bpr_reg=1.0,
+                 layers=[64],
+                 batch_size=128,
+                 loss="bpr_max",
+                 hidden_act="tanh",
+                 final_act="linear",
+                 n_sample=2048,
+                 sample_alpha=0.75,
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(GRU4RecPlusConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.bpr_reg: float = bpr_reg
+        self.layers: List[int] = layers
+        self.batch_size: int = batch_size
+        self.loss: str = loss  # loss = top1_max, bpr_max
+        self.hidden_act: str = hidden_act  # hidden_act = relu, tanh
+        self.final_act: str = final_act  # final_act = linear, relu, leaky_relu
+        self.n_sample: int = n_sample
+        self.sample_alpha: float = sample_alpha  # 0 < sample_alpha <= 1
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.bpr_reg, float) and self.bpr_reg >= 0
+        assert isinstance(self.layers, list)
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.loss, str) and self.loss in {"top1_max", "bpr_max"}
+        assert isinstance(self.hidden_act, str) and self.hidden_act in {"relu", "tanh"}
+        assert isinstance(self.final_act, str) and self.final_act in {"linear", "relu", "leaky_relu"}
+        assert isinstance(self.n_sample, int) and self.n_sample >= 0
+        assert isinstance(self.sample_alpha, float) and 0 < self.sample_alpha <= 1
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class GRU4RecPlus(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = GRU4RecPlusConfig(**cfg_dict)
+        super(GRU4RecPlus, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        if config.hidden_act == "relu":
+            self.hidden_act = tf.nn.relu
+        elif config.hidden_act == "tanh":
+            self.hidden_act = tf.nn.tanh
+        else:
+            raise ValueError("There is not hidden_act named '%s'." % config.hidden_act)
+
+        # final_act = leaky-relu
+        if config.final_act == "relu":
+            self.final_act = tf.nn.relu
+        elif config.final_act == "linear":
+            self.final_act = tf.identity
+        elif config.final_act == "leaky_relu":
+            self.final_act = tf.nn.leaky_relu
+        else:
+            raise ValueError("There is not final_act named '%s'." % config.final_act)
+
+        if config.loss == "bpr_max":
+            self.loss_fun = self._bpr_max_loss
+        elif config.loss == "top1_max":
+            self.loss_fun = self._top1_max_loss
+        else:
+            raise ValueError("There is not loss named '%s'." % config.loss)
+
+        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
+        self.data_ui, self.offset_idx = self._init_data()
+
+        # for sampling negative items
+        _, pop = np.unique(self.data_ui[:, 1], return_counts=True)
+        pop = np.power(pop, self.config.sample_alpha)
+        pop_cumsum = np.cumsum(pop)
+        self.pop_cumsum = pop_cumsum / pop_cumsum[-1]
+
+        self._build_model()
+        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
+        tf_config.gpu_options.allow_growth = True
+        self.sess = tf.Session(config=tf_config)
+        self.sess.run(tf.global_variables_initializer())
+
+    def _init_data(self):
+        data_ui = self.dataset.train_data.to_user_item_pairs_by_time()
+
+        _, idx = np.unique(data_ui[:, 0], return_index=True)
+        offset_idx = np.zeros(len(idx)+1, dtype=np.int32)
+        offset_idx[:-1] = idx
+        offset_idx[-1] = len(data_ui)
+
+        return data_ui, offset_idx
+
+    def _create_variable(self):
+        self.X_ph = tf.placeholder(tf.int32, [self.config.batch_size], name='input')
+        self.Y_ph = tf.placeholder(tf.int32, [self.config.batch_size+self.config.n_sample], name='output')
+        self.state_ph = [tf.placeholder(tf.float32, [self.config.batch_size, n_unit], name='layer_%d_state' % idx)
+                         for idx, n_unit in enumerate(self.config.layers)]
+
+        init = tf.random.truncated_normal([self.items_num, self.config.layers[0]], mean=0.0, stddev=0.01)
+        self.input_embeddings = tf.Variable(init, dtype=tf.float32, name="input_embeddings")
+
+        init = tf.random.truncated_normal([self.items_num, self.config.layers[-1]], mean=0.0, stddev=0.01)
+        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name="item_embeddings")
+        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name="item_biases")
+
+    def _softmax_neg(self, logits):
+        # logits: (b, size_y)
+        hm = 1.0 - tf.eye(tf.shape(logits)[0], tf.shape(logits)[1])
+        logits = logits * hm
+        logits = logits - tf.reduce_max(logits, axis=1, keep_dims=True)
+        e_x = tf.exp(logits) * hm  # (b, size_y)
+        e_x = e_x / tf.reduce_sum(e_x, axis=1, keep_dims=True)
+        return e_x  # (b, size_y)
+
+    def _bpr_max_loss(self, logits):
+        # logits: (b, size_y)
+        softmax_scores = self._softmax_neg(logits)  # (b, size_y)
+        pos_logits = tf.matrix_diag_part(logits)  # (b,)
+        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
+        prob = tf.sigmoid((pos_logits - logits))  # (b, size_y)
+        prob = tf.reduce_sum(tf.multiply(prob, softmax_scores), axis=1)  # (b,)
+        loss = -tf.log(prob + 1e-24)
+        reg_loss = tf.reduce_sum(tf.multiply(tf.pow(logits, 2), softmax_scores), axis=1)  # (b,)
+
+        return tf.reduce_mean(loss + self.config.bpr_reg*reg_loss)
+
+    def _top1_max_loss(self, logits):
+        softmax_scores = self._softmax_neg(logits)  # (b, size_y)
+
+        pos_logits = tf.matrix_diag_part(logits)  # (b,)
+        pos_logits = tf.reshape(pos_logits, shape=[-1, 1])  # (b, 1)
+        prob = tf.sigmoid(-pos_logits + logits) + tf.sigmoid(tf.pow(logits, 2))
+        loss = tf.reduce_sum(tf.multiply(prob, softmax_scores), axis=1)
+
+        return tf.reduce_mean(loss)
+
+    def _build_model(self):
+        self._create_variable()
+        # get embedding and bias
+        # b: batch size
+        # l1: the dim of the first layer
+        # ln: the dim of the last layer
+        # size_y: the length of Y_ph, i.e., n_sample+batch_size
+
+        cells = [tf.nn.rnn_cell.GRUCell(size, activation=self.hidden_act) for size in self.config.layers]
+        drop_cell = [tf.nn.rnn_cell.DropoutWrapper(cell) for cell in cells]
+        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(drop_cell)
+        inputs = tf.nn.embedding_lookup(self.input_embeddings, self.X_ph)  # (b, l1)
+        outputs, state = stacked_cell(inputs, state=self.state_ph)
+        self.u_emb = outputs  # outputs: (b, ln)
+        self.final_state = state  # [(b, l1), (b, l2), ..., (b, ln)]
+
+        # for training
+        items_embed = tf.nn.embedding_lookup(self.item_embeddings, self.Y_ph)  # (size_y, ln)
+        items_bias = tf.gather(self.item_biases, self.Y_ph)  # (size_y,)
+
+        logits = tf.matmul(outputs, items_embed, transpose_b=True) + items_bias  # (b, size_y)
+        logits = self.final_act(logits)
+
+        loss = self.loss_fun(logits)
+
+        # reg loss
+        reg_loss = l2_loss(inputs, items_embed, items_bias)
+        final_loss = loss + self.config.reg*reg_loss
+        self.update_opt = tf.train.AdamOptimizer(self.config.lr).minimize(final_loss)
+
+    def _sample_neg_items(self, size):
+        samples = np.searchsorted(self.pop_cumsum, np.random.rand(size))
+        return samples
+
+    def fit(self):
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+
+        data_ui, offset_idx = self.data_ui, self.offset_idx
+        data_items = data_ui[:, 1]
+        for epoch in range(self.config.epochs):
+            state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
+            user_idx = np.random.permutation(len(offset_idx) - 1)
+            iters = np.arange(self.config.batch_size, dtype=np.int32)
+            maxiter = iters.max()
+            start = offset_idx[user_idx[iters]]
+            end = offset_idx[user_idx[iters]+1]
+            finished = False
+            while not finished:
+                min_len = (end - start).min()
+                out_idx = data_items[start]
+                for i in range(min_len-1):
+                    in_idx = out_idx
+                    out_idx = data_items[start+i+1]
+                    out_items = out_idx
+                    if self.config.n_sample:
+                        neg_items = self._sample_neg_items(self.config.n_sample)
+                        out_items = np.hstack([out_items, neg_items])
+
+                    feed = {self.X_ph: in_idx, self.Y_ph: out_items}
+                    for l in range(len(self.config.layers)):
+                        feed[self.state_ph[l]] = state[l]
+
+                    _, state = self.sess.run([self.update_opt, self.final_state], feed_dict=feed)
+
+                start = start+min_len-1
+                mask = np.arange(len(iters))[(end - start) <= 1]
+                for idx in mask:
+                    maxiter += 1
+                    if maxiter >= len(offset_idx)-1:
+                        finished = True
+                        break
+                    iters[idx] = maxiter
+                    start[idx] = offset_idx[user_idx[maxiter]]
+                    end[idx] = offset_idx[user_idx[maxiter]+1]
+                if len(mask):
+                    for i in range(len(self.config.layers)):
+                        state[i][mask] = 0
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def _get_user_embeddings(self):
+        users = np.array(list(self.user_pos_train.keys()), dtype=np.int32)
+        u_nnz = np.array([len(self.user_pos_train[u]) for u in users], dtype=np.int32)
+        users = users[np.argsort(-u_nnz)]
+        user_embeddings = np.zeros([self.users_num, self.config.layers[-1]], dtype=np.float32)  # saving user embedding
+
+        data_ui, offset_idx = self.data_ui, self.offset_idx
+        data_items = data_ui[:, 1]
+
+        state = [np.zeros([self.config.batch_size, n_unit], dtype=np.float32) for n_unit in self.config.layers]
+        batch_iter = np.arange(self.config.batch_size, dtype=np.int32)
+        next_iter = batch_iter.max() + 1
+
+        start = offset_idx[users[batch_iter]]
+        end = offset_idx[users[batch_iter] + 1]  # the start index of next user
+
+        batch_mask = np.ones([self.config.batch_size], dtype=np.int32)
+        while np.sum(batch_mask) > 0:
+            min_len = (end - start).min()
+
+            for i in range(min_len):
+                cur_items = data_items[start + i]
+                feed = {self.X_ph: cur_items}
+                for l in range(len(self.config.layers)):
+                    feed[self.state_ph[l]] = state[l]
+
+                u_emb, state = self.sess.run([self.u_emb, self.final_state], feed_dict=feed)
+
+            start = start + min_len
+            mask = np.arange(self.config.batch_size)[(end - start) == 0]
+            for idx in mask:
+                u = users[batch_iter[idx]]
+                user_embeddings[u] = u_emb[idx]  # saving user embedding
+                if next_iter < self.users_num:
+                    batch_iter[idx] = next_iter
+                    start[idx] = offset_idx[users[next_iter]]
+                    end[idx] = offset_idx[users[next_iter] + 1]
+                    next_iter += 1
+                else:
+                    batch_mask[idx] = 0
+                    start[idx] = 0
+                    end[idx] = offset_idx[-1]
+
+            for i, _ in enumerate(self.config.layers):
+                state[i][mask] = 0
+
+        return user_embeddings
+
+    def evaluate(self):
+        self.cur_user_embeddings = self._get_user_embeddings()
+        self.cur_item_embeddings, self.cur_item_biases = self.sess.run([self.item_embeddings, self.item_biases])
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        user_embeddings = self.cur_user_embeddings[users]
+        all_ratings = np.matmul(user_embeddings, self.cur_item_embeddings.T) + self.cur_item_biases
+
+        # final_act = leaky-relu
+        if self.final_act == tf.nn.relu:
+            all_ratings = np.maximum(all_ratings, 0)
+        elif self.final_act == tf.identity:
+            all_ratings = all_ratings
+        elif self.final_act == tf.nn.leaky_relu:
+            all_ratings = np.maximum(all_ratings, all_ratings*0.2)
+        else:
+            pass
+
+        all_ratings = np.array(all_ratings, dtype=np.float32)
+        return all_ratings
```

## skrec/recommender/Pop.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["Pop"]
-
-import numpy as np
-import pandas as pd
-from typing import Dict
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.py import RankingEvaluator
-from .base import AbstractRecommender
-
-
-class PopConfig(Config):
-    def __init__(self, **kwargs):
-        super(PopConfig, self).__init__()
-
-
-class Pop(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = PopConfig(**cfg_dict)
-        super(Pop, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
-        self.ranking_score = np.zeros([self.items_num], dtype=np.float32)
-
-    def fit(self):
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        items = self.dataset.train_data.to_user_item_pairs()[:, 1]
-        items_count = pd.value_counts(items, sort=False)
-        items = items_count.index.values
-        count = items_count.values
-        self.ranking_score[items] = count
-
-        result = self.evaluate()
-        self.logger.info(f"Pop results:".ljust(12) + f"\t{result.values_str}")
-
-    def evaluate(self):
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users, neg_items=None):
-        ratings = np.tile(self.ranking_score, len(users))
-        ratings = np.reshape(ratings, newshape=[len(users), self.items_num])
-        return ratings
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["Pop"]
+
+import numpy as np
+import pandas as pd
+from typing import Dict
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.py import RankingEvaluator
+from .base import AbstractRecommender
+
+
+class PopConfig(Config):
+    def __init__(self, **kwargs):
+        super(PopConfig, self).__init__()
+
+
+class Pop(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = PopConfig(**cfg_dict)
+        super(Pop, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
+        self.ranking_score = np.zeros([self.items_num], dtype=np.float32)
+
+    def fit(self):
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        items = self.dataset.train_data.to_user_item_pairs()[:, 1]
+        items_count = pd.value_counts(items, sort=False)
+        items = items_count.index.values
+        count = items_count.values
+        self.ranking_score[items] = count
+
+        result = self.evaluate()
+        self.logger.info(f"Pop results:".ljust(12) + f"\t{result.values_str}")
+
+    def evaluate(self):
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users, neg_items=None):
+        ratings = np.tile(self.ranking_score, len(users))
+        ratings = np.reshape(ratings, newshape=[len(users), self.items_num])
+        return ratings
```

## skrec/recommender/CML.py

 * *Ordering differences only*

```diff
@@ -1,191 +1,191 @@
-"""
-Paper: Collaborative Metric Learning
-Author: Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie and Deborah Estrin
-Reference: https://github.com/changun/CollMetric
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["CML"]
-
-import tensorflow as tf
-from tensorflow import keras
-from typing import Dict
-from .base import AbstractRecommender
-from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.py import BatchIterator
-from ..utils.tf1x import euclidean_distance, hinge_loss
-from ..io.data_iterator import _generate_positive_items, _sampling_negative_items
-
-
-class CMLConfig(Config):
-    def __init__(self,
-                 lr=0.05,
-                 reg=10.0,
-                 embed_size=64,
-                 margin=0.5,
-                 clip_norm=1.0,
-                 dns=10,
-                 batch_size=256,
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(CMLConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.embed_size: int = embed_size
-        self.margin: float = margin
-        self.clip_norm: float = clip_norm
-        self.dns: int = dns  # dns > 1
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, (float, int)) and self.reg >= 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.margin, float) and self.margin >= 0
-        assert isinstance(self.clip_norm, float) and self.clip_norm >= 0
-        assert isinstance(self.dns, int) and self.dns > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class CML(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = CMLConfig(**cfg_dict)
-        super(CML, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_train = self.dataset.train_data.to_user_dict()
-
-        self._build_model()
-        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
-        tf_config.gpu_options.allow_growth = True
-        self.sess = tf.Session(config=tf_config)
-        self.sess.run(tf.global_variables_initializer())
-
-    def _create_variable(self):
-        # B: batch size
-        # L: number of negative items
-        # D: embedding size
-        self.user_h = tf.placeholder(tf.int32, [None], name="user")  # (B,)
-        self.pos_item_h = tf.placeholder(tf.int32, [None], name="pos_item")  # (B,)
-        self.neg_item_h = tf.placeholder(tf.int32, [None, self.config.dns], name="neg_item")  # (B,L)
-        user_emb_init = tf.keras.initializers.normal(stddev=1 / (self.config.embed_size ** 0.5))
-        item_emb_init = tf.keras.initializers.normal(stddev=1 / (self.config.embed_size ** 0.5))
-
-        self.user_elayer = keras.layers.Embedding(self.num_users, self.config.embed_size,
-                                                  embeddings_initializer=user_emb_init)
-        self.item_elayer = keras.layers.Embedding(self.num_items, self.config.embed_size,
-                                                  embeddings_initializer=item_emb_init)
-
-    def cov_loss(self, matrix):
-        n_rows = tf.cast(tf.shape(matrix)[0], tf.float32)
-        matrix = matrix - tf.reduce_mean(matrix, axis=0)
-        cov = tf.matmul(matrix, matrix, transpose_a=True) / n_rows
-        cov = tf.matrix_set_diag(cov, tf.zeros(self.config.embed_size, dtype=tf.float32))
-
-        f2_norm = tf.sqrt(tf.reduce_sum(tf.square(cov)))
-        return f2_norm
-
-    def _build_model(self):
-        self._create_variable()
-        user_embedding = self.user_elayer(self.user_h)  # (B,D)
-        pos_item_embedding = self.item_elayer(self.pos_item_h)  # (B,D)
-        neg_item_embedding = self.item_elayer(self.neg_item_h)  # (B,L,D)
-
-        # positive item to user distance
-        # d_ui = tf.squared_difference(user_embedding, pos_item_embedding)
-        d_ui = euclidean_distance(user_embedding, pos_item_embedding)  # (B,)
-
-        # negative items to user distance
-        user_embedding_t = tf.expand_dims(user_embedding, axis=1)  # (B,1,D)
-        d_ujs = euclidean_distance(user_embedding_t, neg_item_embedding)  # (B,L)
-        # d_ujs = tf.squeeze(tf.squared_difference(user_embedding, neg_item_embedding))
-        d_uj = tf.reduce_min(d_ujs, axis=1)  # (B,)
-
-        loss = hinge_loss(d_uj-d_ui, margin=self.config.margin)
-
-        # calculate w_ui
-        impostors = tf.greater(tf.expand_dims(d_ui, axis=-1)-d_ujs+self.config.margin, 0)  # (B,L)
-        rank = tf.reduce_mean(tf.cast(impostors, tf.float32), axis=1) * self.num_items  # (B,)
-        w_ui = tf.log(rank + 1)
-
-        # embedding loss
-        loss = tf.reduce_sum(w_ui*loss)
-
-        # covariance regularization
-        j_idx = tf.expand_dims(tf.argmin(d_ujs, axis=1, output_type=tf.int32), -1)  # (B,1)
-        j_idx = tf.squeeze(tf.batch_gather(self.neg_item_h, j_idx))
-        neg_item_embedding = self.item_elayer(j_idx)  # (B,D)
-        item_embedding = tf.concat([pos_item_embedding, neg_item_embedding], axis=0)
-        f2_norm = self.cov_loss(user_embedding) + self.cov_loss(item_embedding)
-
-        # total loss and update operation
-        total_loss = loss + self.config.reg * f2_norm
-        self.update = tf.train.AdagradOptimizer(learning_rate=self.config.lr).minimize(total_loss)
-
-        with tf.control_dependencies([self.update]):
-            # training items id
-            train_item_idx = tf.concat([self.pos_item_h, tf.squeeze(j_idx)], axis=-1)
-            # updated embedding of users and items
-            user_emb = self.user_elayer(self.user_h)
-            item_emb = self.item_elayer(train_item_idx)
-
-            # constrain embedding
-            user_normed = tf.clip_by_norm(user_emb, self.config.clip_norm, axes=-1)
-            item_normed = tf.clip_by_norm(item_emb, self.config.clip_norm, axes=-1)
-
-            # assign
-            self.update = [tf.scatter_update(self.user_elayer.weights[0], self.user_h, user_normed),
-                           tf.scatter_update(self.item_elayer.weights[0], train_item_idx, item_normed)]
-
-        # for test
-        user_embedding = tf.reshape(self.user_elayer(self.user_h), [-1, 1, self.config.embed_size])
-        item_embedding = tf.reshape(self.item_elayer.weights[0], [1, self.num_items, self.config.embed_size])
-        self.pre_logits = -euclidean_distance(user_embedding, item_embedding)
-
-    def fit(self):
-        user_n_pos, all_users, pos_items = _generate_positive_items(self.user_pos_train)
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-
-        for epoch in range(self.config.epochs):
-            neg_items = _sampling_negative_items(user_n_pos, self.config.dns, self.num_items,
-                                                 self.user_pos_train).squeeze()
-            data_iter = BatchIterator(all_users, pos_items, neg_items,
-                                      batch_size=self.config.batch_size,
-                                      shuffle=True, drop_last=False)
-
-            for user, pos_item, neg_item in data_iter:
-                feed = {self.user_h: user, self.pos_item_h: pos_item, self.neg_item_h: neg_item}
-                self.sess.run(self.update, feed_dict=feed)
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        ratings = self.sess.run(self.pre_logits, feed_dict={self.user_h: users})
-        return ratings
+"""
+Paper: Collaborative Metric Learning
+Author: Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie and Deborah Estrin
+Reference: https://github.com/changun/CollMetric
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["CML"]
+
+import tensorflow as tf
+from tensorflow import keras
+from typing import Dict
+from .base import AbstractRecommender
+from ..utils.py import Config
+from ..io import Dataset
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import BatchIterator
+from ..utils.tf1x import euclidean_distance, hinge_loss
+from ..io.data_iterator import _generate_positive_items, _sampling_negative_items
+
+
+class CMLConfig(Config):
+    def __init__(self,
+                 lr=0.05,
+                 reg=10.0,
+                 embed_size=64,
+                 margin=0.5,
+                 clip_norm=1.0,
+                 dns=10,
+                 batch_size=256,
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(CMLConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.embed_size: int = embed_size
+        self.margin: float = margin
+        self.clip_norm: float = clip_norm
+        self.dns: int = dns  # dns > 1
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, (float, int)) and self.reg >= 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.margin, float) and self.margin >= 0
+        assert isinstance(self.clip_norm, float) and self.clip_norm >= 0
+        assert isinstance(self.dns, int) and self.dns > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class CML(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = CMLConfig(**cfg_dict)
+        super(CML, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_train = self.dataset.train_data.to_user_dict()
+
+        self._build_model()
+        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
+        tf_config.gpu_options.allow_growth = True
+        self.sess = tf.Session(config=tf_config)
+        self.sess.run(tf.global_variables_initializer())
+
+    def _create_variable(self):
+        # B: batch size
+        # L: number of negative items
+        # D: embedding size
+        self.user_h = tf.placeholder(tf.int32, [None], name="user")  # (B,)
+        self.pos_item_h = tf.placeholder(tf.int32, [None], name="pos_item")  # (B,)
+        self.neg_item_h = tf.placeholder(tf.int32, [None, self.config.dns], name="neg_item")  # (B,L)
+        user_emb_init = tf.keras.initializers.normal(stddev=1 / (self.config.embed_size ** 0.5))
+        item_emb_init = tf.keras.initializers.normal(stddev=1 / (self.config.embed_size ** 0.5))
+
+        self.user_elayer = keras.layers.Embedding(self.num_users, self.config.embed_size,
+                                                  embeddings_initializer=user_emb_init)
+        self.item_elayer = keras.layers.Embedding(self.num_items, self.config.embed_size,
+                                                  embeddings_initializer=item_emb_init)
+
+    def cov_loss(self, matrix):
+        n_rows = tf.cast(tf.shape(matrix)[0], tf.float32)
+        matrix = matrix - tf.reduce_mean(matrix, axis=0)
+        cov = tf.matmul(matrix, matrix, transpose_a=True) / n_rows
+        cov = tf.matrix_set_diag(cov, tf.zeros(self.config.embed_size, dtype=tf.float32))
+
+        f2_norm = tf.sqrt(tf.reduce_sum(tf.square(cov)))
+        return f2_norm
+
+    def _build_model(self):
+        self._create_variable()
+        user_embedding = self.user_elayer(self.user_h)  # (B,D)
+        pos_item_embedding = self.item_elayer(self.pos_item_h)  # (B,D)
+        neg_item_embedding = self.item_elayer(self.neg_item_h)  # (B,L,D)
+
+        # positive item to user distance
+        # d_ui = tf.squared_difference(user_embedding, pos_item_embedding)
+        d_ui = euclidean_distance(user_embedding, pos_item_embedding)  # (B,)
+
+        # negative items to user distance
+        user_embedding_t = tf.expand_dims(user_embedding, axis=1)  # (B,1,D)
+        d_ujs = euclidean_distance(user_embedding_t, neg_item_embedding)  # (B,L)
+        # d_ujs = tf.squeeze(tf.squared_difference(user_embedding, neg_item_embedding))
+        d_uj = tf.reduce_min(d_ujs, axis=1)  # (B,)
+
+        loss = hinge_loss(d_uj-d_ui, margin=self.config.margin)
+
+        # calculate w_ui
+        impostors = tf.greater(tf.expand_dims(d_ui, axis=-1)-d_ujs+self.config.margin, 0)  # (B,L)
+        rank = tf.reduce_mean(tf.cast(impostors, tf.float32), axis=1) * self.num_items  # (B,)
+        w_ui = tf.log(rank + 1)
+
+        # embedding loss
+        loss = tf.reduce_sum(w_ui*loss)
+
+        # covariance regularization
+        j_idx = tf.expand_dims(tf.argmin(d_ujs, axis=1, output_type=tf.int32), -1)  # (B,1)
+        j_idx = tf.squeeze(tf.batch_gather(self.neg_item_h, j_idx))
+        neg_item_embedding = self.item_elayer(j_idx)  # (B,D)
+        item_embedding = tf.concat([pos_item_embedding, neg_item_embedding], axis=0)
+        f2_norm = self.cov_loss(user_embedding) + self.cov_loss(item_embedding)
+
+        # total loss and update operation
+        total_loss = loss + self.config.reg * f2_norm
+        self.update = tf.train.AdagradOptimizer(learning_rate=self.config.lr).minimize(total_loss)
+
+        with tf.control_dependencies([self.update]):
+            # training items id
+            train_item_idx = tf.concat([self.pos_item_h, tf.squeeze(j_idx)], axis=-1)
+            # updated embedding of users and items
+            user_emb = self.user_elayer(self.user_h)
+            item_emb = self.item_elayer(train_item_idx)
+
+            # constrain embedding
+            user_normed = tf.clip_by_norm(user_emb, self.config.clip_norm, axes=-1)
+            item_normed = tf.clip_by_norm(item_emb, self.config.clip_norm, axes=-1)
+
+            # assign
+            self.update = [tf.scatter_update(self.user_elayer.weights[0], self.user_h, user_normed),
+                           tf.scatter_update(self.item_elayer.weights[0], train_item_idx, item_normed)]
+
+        # for test
+        user_embedding = tf.reshape(self.user_elayer(self.user_h), [-1, 1, self.config.embed_size])
+        item_embedding = tf.reshape(self.item_elayer.weights[0], [1, self.num_items, self.config.embed_size])
+        self.pre_logits = -euclidean_distance(user_embedding, item_embedding)
+
+    def fit(self):
+        user_n_pos, all_users, pos_items = _generate_positive_items(self.user_pos_train)
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+
+        for epoch in range(self.config.epochs):
+            neg_items = _sampling_negative_items(user_n_pos, self.config.dns, self.num_items,
+                                                 self.user_pos_train).squeeze()
+            data_iter = BatchIterator(all_users, pos_items, neg_items,
+                                      batch_size=self.config.batch_size,
+                                      shuffle=True, drop_last=False)
+
+            for user, pos_item, neg_item in data_iter:
+                feed = {self.user_h: user, self.pos_item_h: pos_item, self.neg_item_h: neg_item}
+                self.sess.run(self.update, feed_dict=feed)
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        ratings = self.sess.run(self.pre_logits, feed_dict={self.user_h: users})
+        return ratings
```

## skrec/recommender/SGAT.py

 * *Ordering differences only*

```diff
@@ -1,345 +1,345 @@
-"""
-Paper: Sequential Graph Collaborative Filtering
-Author: Zhongchuan Sun, Bin Wu, Youwei Wang, and Yangdong Ye
-Reference: https://github.com/ZhongchuanSun/SGAT
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["SGAT"]
-
-import os
-import numpy as np
-import tensorflow as tf
-import scipy.sparse as sp
-from typing import Dict
-from .base import AbstractRecommender
-from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.py import pad_sequences
-from ..utils.tf1x import bpr_loss, l2_loss, l2_distance
-from collections import defaultdict
-from ..io import SequentialPairwiseIterator
-from ..utils.common import normalize_adj_matrix
-
-
-class SGATConfig(Config):
-    def __init__(self,
-                 lr=0.001,
-                 reg=1e-4,
-                 n_layers=5,
-                 n_seqs=5,
-                 n_next=3,
-                 embed_size=64,
-                 batch_size=1024,
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(SGATConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.n_layers: int = n_layers
-        self.n_seqs: int = n_seqs
-        self.n_next: int = n_next
-        self.embed_size: int = embed_size
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.n_layers, int) and self.n_layers >= 0
-        assert isinstance(self.n_seqs, int) and self.n_seqs > 0
-        assert isinstance(self.n_next, int) and self.n_next > 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-def mexp(x, tau=1.0):
-    # normalize att_logit to avoid negative value
-    x_max = tf.reduce_max(x)
-    x_min = tf.reduce_min(x)
-    norm_x = (x-x_min) / (x_max-x_min)
-
-    # calculate attention for each pair of items
-    # used for calculating softmax
-    exp_x = tf.exp(norm_x/tau)
-    return exp_x
-
-
-class SGAT(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = SGATConfig(**cfg_dict)
-        super(SGAT, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
-        self._process_test()
-
-        self._build_model()
-        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
-        tf_config.gpu_options.allow_growth = True
-        self.sess = tf.Session(config=tf_config)
-        self.sess.run(tf.global_variables_initializer())
-
-    def _process_test(self):
-        item_seqs = [self.user_pos_train[user][-self.config.n_seqs:] if user in self.user_pos_train else [self.items_num]
-                     for user in range(self.users_num)]
-        self.test_item_seqs = pad_sequences(item_seqs, value=self.items_num, max_len=self.config.n_seqs,
-                                            padding='pre', truncating='pre', dtype=np.int32)
-
-    def _create_placeholder(self):
-        self.user_ph = tf.placeholder(tf.int32, [None], name="user")
-        self.head_ph = tf.placeholder(tf.int32, [None, self.config.n_seqs], name="head_item")  # the previous item
-        self.pos_tail_ph = tf.placeholder(tf.int32, [None, self.config.n_next], name="pos_tail_item")  # the next item
-        self.neg_tail_ph = tf.placeholder(tf.int32, [None, self.config.n_next], name="neg_tail_item")  # the negative item
-
-    def _construct_graph(self):
-        th_rs_dict = defaultdict(list)
-        for user, pos_items in self.user_pos_train.items():
-            for h, t in zip(pos_items[:-1], pos_items[1:]):
-                th_rs_dict[(t, h)].append(user)
-
-        th_rs_list = sorted(th_rs_dict.items(), key=lambda x: x[0])
-
-        user_list, head_list, tail_list = [], [], []
-        for (t, h), r in th_rs_list:
-            user_list.extend(r)
-            head_list.extend([h] * len(r))
-            tail_list.extend([t] * len(r))
-
-        # attention mechanism
-
-        # the auxiliary constant to calculate softmax
-        row_idx, nnz = np.unique(tail_list, return_counts=True)
-        count = {r: n for r, n in zip(row_idx, nnz)}
-        nnz = [count[i] if i in count else 0 for i in range(self.items_num)]
-        nnz = np.concatenate([[0], nnz])
-        rows_idx = np.cumsum(nnz)
-
-        # the auxiliary constant to calculate the weight between two node
-        edge_num = np.array([len(r) for (t, h), r in th_rs_list], dtype=np.int32)
-        edge_num = np.concatenate([[0], edge_num])
-        edge_idx = np.cumsum(edge_num)
-
-        sp_idx = [[t, h] for (t, h), r in th_rs_list]
-
-        adj_mean_norm = self._get_mean_norm(edge_num[1:], sp_idx)
-
-        return head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_mean_norm
-
-    def _load_graph(self):
-        dir_name = os.path.dirname(self.dataset.data_dir)
-        dir_name = os.path.join(dir_name, "_sgat_data")
-        elem_file = os.path.join(dir_name, "graph_elem.npy")
-        index_file = os.path.join(dir_name, "index.npy")
-        adj_file = os.path.join(dir_name, "adj_mean_norm.npz")
-
-        if os.path.exists(elem_file) and \
-                os.path.exists(index_file) and \
-                os.path.exists(adj_file):
-            head_list, tail_list, user_list = np.load(elem_file, allow_pickle=True)
-            rows_idx, edge_idx, sp_idx = np.load(index_file, allow_pickle=True)
-            adj_mean_norm = sp.load_npz(adj_file)
-        else:
-            head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_mean_norm = self._construct_graph()
-            if not os.path.exists(dir_name):
-                os.makedirs(dir_name)
-            np.save(elem_file, [head_list, tail_list, user_list])
-            np.save(index_file, [rows_idx, edge_idx, sp_idx])
-            sp.save_npz(adj_file, adj_mean_norm)
-
-        return head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_mean_norm
-
-    def _init_constant(self):
-        head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_norm = self._load_graph()
-
-        # attention mechanism
-        self.att_head_idx = tf.constant(head_list, dtype=tf.int32, shape=None, name="att_head_idx")
-        self.att_tail_idx = tf.constant(tail_list, dtype=tf.int32, shape=None, name="att_tail_idx")
-        self.att_user_idx = tf.constant(user_list, dtype=tf.int32, shape=None, name="att_user_idx")
-
-        # the auxiliary constant to calculate softmax
-        self.rows_end_idx = tf.constant(rows_idx[1:], dtype=tf.int32, shape=None, name="rows_end_idx")
-        self.row_begin_idx = tf.constant(rows_idx[:-1], dtype=tf.int32, shape=None, name="row_begin_idx")
-
-        # the auxiliary constant to calculate the weight between two node
-        self.edge_end_idx = tf.constant(edge_idx[1:], dtype=tf.int32, shape=None, name="edge_end_idx")
-        self.edge_begin_idx = tf.constant(edge_idx[:-1], dtype=tf.int32, shape=None, name="edge_begin_idx")
-
-        # the index of sparse matrix
-        self.sp_tensor_idx = tf.constant(sp_idx, dtype=tf.int64)
-
-    def _get_mean_norm(self, edge_num, sp_idx):
-        adj_num = np.array(edge_num, dtype=np.float32)
-        rows, cols = list(zip(*sp_idx))
-        adj_mat = sp.csr_matrix((adj_num, (rows, cols)), shape=(self.items_num, self.items_num))
-
-        return normalize_adj_matrix(adj_mat, "left").astype(np.float32)
-
-    def _init_variable(self):
-        # embedding parameters
-        init = tf.random.truncated_normal([self.users_num, self.config.embed_size], mean=0.0, stddev=0.01)
-        self.user_embeddings = tf.Variable(init, dtype=tf.float32, name="user_embeddings")
-
-        init = tf.random.truncated_normal([self.items_num, self.config.embed_size], mean=0.0, stddev=0.01)
-        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name="item_embeddings")
-
-        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name="item_biases")
-
-    def _get_attention(self, item_embeddings):
-        h_embed = tf.nn.embedding_lookup(item_embeddings, self.att_head_idx)
-        r_embed = tf.nn.embedding_lookup(self.user_embeddings, self.att_user_idx)
-        t_embed = tf.nn.embedding_lookup(item_embeddings, self.att_tail_idx)
-
-        att_logit = l2_distance(h_embed+r_embed, t_embed)
-        exp_logit = mexp(-att_logit)
-
-        exp_logit = tf.concat([[0], exp_logit], axis=0)
-        sum_exp_logit = tf.cumsum(exp_logit)
-
-        pre_sum = tf.gather(sum_exp_logit, self.edge_begin_idx)
-        next_sum = tf.gather(sum_exp_logit, self.edge_end_idx)
-        sum_exp_logit_per_edge = next_sum - pre_sum
-
-        # convert to spares tensor
-        exp_logit = tf.SparseTensor(indices=self.sp_tensor_idx, values=sum_exp_logit_per_edge,
-                                    dense_shape=[self.items_num, self.items_num])
-
-        # normalize attention score to a probability vector
-        next_sum = tf.gather(sum_exp_logit, self.rows_end_idx)
-        pre_sum = tf.gather(sum_exp_logit, self.row_begin_idx)
-        sum_exp_logit_per_row = next_sum - pre_sum + 1e-6
-        sum_exp_logit_per_row = tf.reshape(sum_exp_logit_per_row, shape=[self.items_num, 1])
-
-        att_score = exp_logit / sum_exp_logit_per_row
-        return att_score
-
-    def _aggregate(self, item_embedding, neighbor_embedding):
-        item_embeddings = item_embedding + neighbor_embedding
-        return item_embeddings
-
-    def _mean_history(self, item_embeddings):
-        self.logger.info("mean_history")
-        # fuse to get short-term embeddings
-        pad_value = tf.zeros([1, self.config.embed_size], dtype=tf.float32)
-        item_embeddings = tf.concat([item_embeddings, pad_value], axis=0)
-
-        item_seq_embs = tf.nn.embedding_lookup(item_embeddings, self.head_ph)  # (b,l,d)
-        mask = tf.cast(tf.not_equal(self.head_ph, self.items_num), dtype=tf.float32)  # (b,l)
-        his_emb = tf.reduce_sum(item_seq_embs, axis=1) / tf.reduce_sum(mask, axis=1, keepdims=True)  # (b,d)/(b,1)
-        return his_emb
-
-    def _fusion_layer(self, embed_g, embed_s=None):
-        head_emb = embed_g
-        if embed_s is not None:
-            head_emb += embed_s
-        return head_emb
-
-    def _forward_head_emb(self, user_emb, item_embeddings):
-        # embed item sequence
-        his_embs = self._mean_history(item_embeddings)
-
-        # fusion representation
-        head_emb_g = tf.nn.embedding_lookup(item_embeddings, self.head_ph[:, -1])  # b*d
-        head_emb = self._fusion_layer(head_emb_g, his_embs)
-
-        return head_emb
-
-    def _build_model(self):
-        self._create_placeholder()
-        self._init_constant()
-        self._init_variable()
-        item_embeddings = self.item_embeddings
-        # Graph Convolution
-        for k in range(self.config.n_layers):
-            att_scores = self._get_attention(item_embeddings)
-            neighbor_embeddings = tf.sparse_tensor_dense_matmul(att_scores, item_embeddings)
-            item_embeddings = self._aggregate(item_embeddings, neighbor_embeddings)
-
-        # Translation-based Recommendation
-        user_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)  # b*d
-        head_emb = self._forward_head_emb(user_emb, item_embeddings)
-
-        pos_tail_emb = tf.nn.embedding_lookup(item_embeddings, self.pos_tail_ph)  # b*t*d
-        neg_tail_emb = tf.nn.embedding_lookup(item_embeddings, self.neg_tail_ph)  # b*t*d
-
-        pos_tail_bias = tf.gather(self.item_biases, self.pos_tail_ph)  # b*t
-        neg_tail_bias = tf.gather(self.item_biases, self.neg_tail_ph)  # b*t
-
-        pre_emb = head_emb + user_emb  # b*d
-        pre_emb = tf.expand_dims(pre_emb, axis=1)  # b*1*d
-        pos_rating = -l2_distance(pre_emb, pos_tail_emb) + pos_tail_bias  # b*t
-        neg_rating = -l2_distance(pre_emb, neg_tail_emb) + neg_tail_bias  # b*t
-
-        pairwise_loss = tf.reduce_sum(bpr_loss(pos_rating, neg_rating))
-
-        # reg loss
-        emb_reg = l2_loss(user_emb, head_emb, pos_tail_emb, neg_tail_emb, pos_tail_bias, neg_tail_bias)
-
-        # objective loss and optimizer
-        obj_loss = pairwise_loss + self.config.reg*emb_reg
-        self.update_opt = tf.train.AdamOptimizer(self.config.lr).minimize(obj_loss)
-
-        # for prediction
-        self.item_embeddings_final = tf.Variable(tf.zeros([self.items_num, self.config.embed_size]),
-                                                 dtype=tf.float32, name="item_embeddings_final", trainable=False)
-        self.assign_opt = tf.assign(self.item_embeddings_final, item_embeddings)
-
-        user_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)  # b*d
-        head_emb = self._forward_head_emb(user_emb, self.item_embeddings_final)
-        pre_emb = head_emb + user_emb  # b*d
-
-        pre_emb = tf.expand_dims(pre_emb, axis=1)  # b*1*d
-        j_emb = tf.expand_dims(self.item_embeddings_final, axis=0)  # 1*n*d
-
-        self.prediction = -l2_distance(pre_emb, j_emb) + self.item_biases  # b*n
-
-    def fit(self):
-        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
-                                               num_previous=self.config.n_seqs, num_next=self.config.n_next,
-                                               pad=self.items_num, batch_size=self.config.batch_size,
-                                               shuffle=True, drop_last=False)
-
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            for bat_users, bat_head, bat_pos_tail, bat_neg_tail in data_iter:
-                feed = {self.user_ph: bat_users,
-                        self.head_ph: bat_head.reshape([-1, self.config.n_seqs]),
-                        self.pos_tail_ph: bat_pos_tail.reshape([-1, self.config.n_next]),
-                        self.neg_tail_ph: bat_neg_tail.reshape([-1, self.config.n_next])
-                        }
-                self.sess.run(self.update_opt, feed_dict=feed)
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.sess.run(self.assign_opt)
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        last_items = [self.test_item_seqs[u] for u in users]
-        feed = {self.user_ph: users, self.head_ph: last_items}
-        bat_ratings = self.sess.run(self.prediction, feed_dict=feed)
-        return bat_ratings
+"""
+Paper: Sequential Graph Collaborative Filtering
+Author: Zhongchuan Sun, Bin Wu, Youwei Wang, and Yangdong Ye
+Reference: https://github.com/ZhongchuanSun/SGAT
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["SGAT"]
+
+import os
+import numpy as np
+import tensorflow as tf
+import scipy.sparse as sp
+from typing import Dict
+from .base import AbstractRecommender
+from ..utils.py import Config
+from ..io import Dataset
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.py import pad_sequences
+from ..utils.tf1x import bpr_loss, l2_loss, l2_distance
+from collections import defaultdict
+from ..io import SequentialPairwiseIterator
+from ..utils.common import normalize_adj_matrix
+
+
+class SGATConfig(Config):
+    def __init__(self,
+                 lr=0.001,
+                 reg=1e-4,
+                 n_layers=5,
+                 n_seqs=5,
+                 n_next=3,
+                 embed_size=64,
+                 batch_size=1024,
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(SGATConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.n_layers: int = n_layers
+        self.n_seqs: int = n_seqs
+        self.n_next: int = n_next
+        self.embed_size: int = embed_size
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.n_layers, int) and self.n_layers >= 0
+        assert isinstance(self.n_seqs, int) and self.n_seqs > 0
+        assert isinstance(self.n_next, int) and self.n_next > 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+def mexp(x, tau=1.0):
+    # normalize att_logit to avoid negative value
+    x_max = tf.reduce_max(x)
+    x_min = tf.reduce_min(x)
+    norm_x = (x-x_min) / (x_max-x_min)
+
+    # calculate attention for each pair of items
+    # used for calculating softmax
+    exp_x = tf.exp(norm_x/tau)
+    return exp_x
+
+
+class SGAT(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = SGATConfig(**cfg_dict)
+        super(SGAT, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.users_num, self.items_num = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_train = self.dataset.train_data.to_user_dict_by_time()
+        self._process_test()
+
+        self._build_model()
+        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
+        tf_config.gpu_options.allow_growth = True
+        self.sess = tf.Session(config=tf_config)
+        self.sess.run(tf.global_variables_initializer())
+
+    def _process_test(self):
+        item_seqs = [self.user_pos_train[user][-self.config.n_seqs:] if user in self.user_pos_train else [self.items_num]
+                     for user in range(self.users_num)]
+        self.test_item_seqs = pad_sequences(item_seqs, value=self.items_num, max_len=self.config.n_seqs,
+                                            padding='pre', truncating='pre', dtype=np.int32)
+
+    def _create_placeholder(self):
+        self.user_ph = tf.placeholder(tf.int32, [None], name="user")
+        self.head_ph = tf.placeholder(tf.int32, [None, self.config.n_seqs], name="head_item")  # the previous item
+        self.pos_tail_ph = tf.placeholder(tf.int32, [None, self.config.n_next], name="pos_tail_item")  # the next item
+        self.neg_tail_ph = tf.placeholder(tf.int32, [None, self.config.n_next], name="neg_tail_item")  # the negative item
+
+    def _construct_graph(self):
+        th_rs_dict = defaultdict(list)
+        for user, pos_items in self.user_pos_train.items():
+            for h, t in zip(pos_items[:-1], pos_items[1:]):
+                th_rs_dict[(t, h)].append(user)
+
+        th_rs_list = sorted(th_rs_dict.items(), key=lambda x: x[0])
+
+        user_list, head_list, tail_list = [], [], []
+        for (t, h), r in th_rs_list:
+            user_list.extend(r)
+            head_list.extend([h] * len(r))
+            tail_list.extend([t] * len(r))
+
+        # attention mechanism
+
+        # the auxiliary constant to calculate softmax
+        row_idx, nnz = np.unique(tail_list, return_counts=True)
+        count = {r: n for r, n in zip(row_idx, nnz)}
+        nnz = [count[i] if i in count else 0 for i in range(self.items_num)]
+        nnz = np.concatenate([[0], nnz])
+        rows_idx = np.cumsum(nnz)
+
+        # the auxiliary constant to calculate the weight between two node
+        edge_num = np.array([len(r) for (t, h), r in th_rs_list], dtype=np.int32)
+        edge_num = np.concatenate([[0], edge_num])
+        edge_idx = np.cumsum(edge_num)
+
+        sp_idx = [[t, h] for (t, h), r in th_rs_list]
+
+        adj_mean_norm = self._get_mean_norm(edge_num[1:], sp_idx)
+
+        return head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_mean_norm
+
+    def _load_graph(self):
+        dir_name = os.path.dirname(self.dataset.data_dir)
+        dir_name = os.path.join(dir_name, "_sgat_data")
+        elem_file = os.path.join(dir_name, "graph_elem.npy")
+        index_file = os.path.join(dir_name, "index.npy")
+        adj_file = os.path.join(dir_name, "adj_mean_norm.npz")
+
+        if os.path.exists(elem_file) and \
+                os.path.exists(index_file) and \
+                os.path.exists(adj_file):
+            head_list, tail_list, user_list = np.load(elem_file, allow_pickle=True)
+            rows_idx, edge_idx, sp_idx = np.load(index_file, allow_pickle=True)
+            adj_mean_norm = sp.load_npz(adj_file)
+        else:
+            head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_mean_norm = self._construct_graph()
+            if not os.path.exists(dir_name):
+                os.makedirs(dir_name)
+            np.save(elem_file, [head_list, tail_list, user_list])
+            np.save(index_file, [rows_idx, edge_idx, sp_idx])
+            sp.save_npz(adj_file, adj_mean_norm)
+
+        return head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_mean_norm
+
+    def _init_constant(self):
+        head_list, tail_list, user_list, rows_idx, edge_idx, sp_idx, adj_norm = self._load_graph()
+
+        # attention mechanism
+        self.att_head_idx = tf.constant(head_list, dtype=tf.int32, shape=None, name="att_head_idx")
+        self.att_tail_idx = tf.constant(tail_list, dtype=tf.int32, shape=None, name="att_tail_idx")
+        self.att_user_idx = tf.constant(user_list, dtype=tf.int32, shape=None, name="att_user_idx")
+
+        # the auxiliary constant to calculate softmax
+        self.rows_end_idx = tf.constant(rows_idx[1:], dtype=tf.int32, shape=None, name="rows_end_idx")
+        self.row_begin_idx = tf.constant(rows_idx[:-1], dtype=tf.int32, shape=None, name="row_begin_idx")
+
+        # the auxiliary constant to calculate the weight between two node
+        self.edge_end_idx = tf.constant(edge_idx[1:], dtype=tf.int32, shape=None, name="edge_end_idx")
+        self.edge_begin_idx = tf.constant(edge_idx[:-1], dtype=tf.int32, shape=None, name="edge_begin_idx")
+
+        # the index of sparse matrix
+        self.sp_tensor_idx = tf.constant(sp_idx, dtype=tf.int64)
+
+    def _get_mean_norm(self, edge_num, sp_idx):
+        adj_num = np.array(edge_num, dtype=np.float32)
+        rows, cols = list(zip(*sp_idx))
+        adj_mat = sp.csr_matrix((adj_num, (rows, cols)), shape=(self.items_num, self.items_num))
+
+        return normalize_adj_matrix(adj_mat, "left").astype(np.float32)
+
+    def _init_variable(self):
+        # embedding parameters
+        init = tf.random.truncated_normal([self.users_num, self.config.embed_size], mean=0.0, stddev=0.01)
+        self.user_embeddings = tf.Variable(init, dtype=tf.float32, name="user_embeddings")
+
+        init = tf.random.truncated_normal([self.items_num, self.config.embed_size], mean=0.0, stddev=0.01)
+        self.item_embeddings = tf.Variable(init, dtype=tf.float32, name="item_embeddings")
+
+        self.item_biases = tf.Variable(tf.zeros([self.items_num]), dtype=tf.float32, name="item_biases")
+
+    def _get_attention(self, item_embeddings):
+        h_embed = tf.nn.embedding_lookup(item_embeddings, self.att_head_idx)
+        r_embed = tf.nn.embedding_lookup(self.user_embeddings, self.att_user_idx)
+        t_embed = tf.nn.embedding_lookup(item_embeddings, self.att_tail_idx)
+
+        att_logit = l2_distance(h_embed+r_embed, t_embed)
+        exp_logit = mexp(-att_logit)
+
+        exp_logit = tf.concat([[0], exp_logit], axis=0)
+        sum_exp_logit = tf.cumsum(exp_logit)
+
+        pre_sum = tf.gather(sum_exp_logit, self.edge_begin_idx)
+        next_sum = tf.gather(sum_exp_logit, self.edge_end_idx)
+        sum_exp_logit_per_edge = next_sum - pre_sum
+
+        # convert to spares tensor
+        exp_logit = tf.SparseTensor(indices=self.sp_tensor_idx, values=sum_exp_logit_per_edge,
+                                    dense_shape=[self.items_num, self.items_num])
+
+        # normalize attention score to a probability vector
+        next_sum = tf.gather(sum_exp_logit, self.rows_end_idx)
+        pre_sum = tf.gather(sum_exp_logit, self.row_begin_idx)
+        sum_exp_logit_per_row = next_sum - pre_sum + 1e-6
+        sum_exp_logit_per_row = tf.reshape(sum_exp_logit_per_row, shape=[self.items_num, 1])
+
+        att_score = exp_logit / sum_exp_logit_per_row
+        return att_score
+
+    def _aggregate(self, item_embedding, neighbor_embedding):
+        item_embeddings = item_embedding + neighbor_embedding
+        return item_embeddings
+
+    def _mean_history(self, item_embeddings):
+        self.logger.info("mean_history")
+        # fuse to get short-term embeddings
+        pad_value = tf.zeros([1, self.config.embed_size], dtype=tf.float32)
+        item_embeddings = tf.concat([item_embeddings, pad_value], axis=0)
+
+        item_seq_embs = tf.nn.embedding_lookup(item_embeddings, self.head_ph)  # (b,l,d)
+        mask = tf.cast(tf.not_equal(self.head_ph, self.items_num), dtype=tf.float32)  # (b,l)
+        his_emb = tf.reduce_sum(item_seq_embs, axis=1) / tf.reduce_sum(mask, axis=1, keepdims=True)  # (b,d)/(b,1)
+        return his_emb
+
+    def _fusion_layer(self, embed_g, embed_s=None):
+        head_emb = embed_g
+        if embed_s is not None:
+            head_emb += embed_s
+        return head_emb
+
+    def _forward_head_emb(self, user_emb, item_embeddings):
+        # embed item sequence
+        his_embs = self._mean_history(item_embeddings)
+
+        # fusion representation
+        head_emb_g = tf.nn.embedding_lookup(item_embeddings, self.head_ph[:, -1])  # b*d
+        head_emb = self._fusion_layer(head_emb_g, his_embs)
+
+        return head_emb
+
+    def _build_model(self):
+        self._create_placeholder()
+        self._init_constant()
+        self._init_variable()
+        item_embeddings = self.item_embeddings
+        # Graph Convolution
+        for k in range(self.config.n_layers):
+            att_scores = self._get_attention(item_embeddings)
+            neighbor_embeddings = tf.sparse_tensor_dense_matmul(att_scores, item_embeddings)
+            item_embeddings = self._aggregate(item_embeddings, neighbor_embeddings)
+
+        # Translation-based Recommendation
+        user_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)  # b*d
+        head_emb = self._forward_head_emb(user_emb, item_embeddings)
+
+        pos_tail_emb = tf.nn.embedding_lookup(item_embeddings, self.pos_tail_ph)  # b*t*d
+        neg_tail_emb = tf.nn.embedding_lookup(item_embeddings, self.neg_tail_ph)  # b*t*d
+
+        pos_tail_bias = tf.gather(self.item_biases, self.pos_tail_ph)  # b*t
+        neg_tail_bias = tf.gather(self.item_biases, self.neg_tail_ph)  # b*t
+
+        pre_emb = head_emb + user_emb  # b*d
+        pre_emb = tf.expand_dims(pre_emb, axis=1)  # b*1*d
+        pos_rating = -l2_distance(pre_emb, pos_tail_emb) + pos_tail_bias  # b*t
+        neg_rating = -l2_distance(pre_emb, neg_tail_emb) + neg_tail_bias  # b*t
+
+        pairwise_loss = tf.reduce_sum(bpr_loss(pos_rating, neg_rating))
+
+        # reg loss
+        emb_reg = l2_loss(user_emb, head_emb, pos_tail_emb, neg_tail_emb, pos_tail_bias, neg_tail_bias)
+
+        # objective loss and optimizer
+        obj_loss = pairwise_loss + self.config.reg*emb_reg
+        self.update_opt = tf.train.AdamOptimizer(self.config.lr).minimize(obj_loss)
+
+        # for prediction
+        self.item_embeddings_final = tf.Variable(tf.zeros([self.items_num, self.config.embed_size]),
+                                                 dtype=tf.float32, name="item_embeddings_final", trainable=False)
+        self.assign_opt = tf.assign(self.item_embeddings_final, item_embeddings)
+
+        user_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)  # b*d
+        head_emb = self._forward_head_emb(user_emb, self.item_embeddings_final)
+        pre_emb = head_emb + user_emb  # b*d
+
+        pre_emb = tf.expand_dims(pre_emb, axis=1)  # b*1*d
+        j_emb = tf.expand_dims(self.item_embeddings_final, axis=0)  # 1*n*d
+
+        self.prediction = -l2_distance(pre_emb, j_emb) + self.item_biases  # b*n
+
+    def fit(self):
+        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
+                                               num_previous=self.config.n_seqs, num_next=self.config.n_next,
+                                               pad=self.items_num, batch_size=self.config.batch_size,
+                                               shuffle=True, drop_last=False)
+
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            for bat_users, bat_head, bat_pos_tail, bat_neg_tail in data_iter:
+                feed = {self.user_ph: bat_users,
+                        self.head_ph: bat_head.reshape([-1, self.config.n_seqs]),
+                        self.pos_tail_ph: bat_pos_tail.reshape([-1, self.config.n_next]),
+                        self.neg_tail_ph: bat_neg_tail.reshape([-1, self.config.n_next])
+                        }
+                self.sess.run(self.update_opt, feed_dict=feed)
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.sess.run(self.assign_opt)
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        last_items = [self.test_item_seqs[u] for u in users]
+        feed = {self.user_ph: users, self.head_ph: last_items}
+        bat_ratings = self.sess.run(self.prediction, feed_dict=feed)
+        return bat_ratings
```

## skrec/recommender/BPRMF.py

 * *Ordering differences only*

```diff
@@ -1,144 +1,144 @@
-"""
-Paper: BPR: Bayesian Personalized Ranking from Implicit Feedback
-Author: Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["BPRMF"]
-
-import torch
-import torch.nn as nn
-import numpy as np
-from typing import Dict
-from .base import AbstractRecommender
-from ..utils.py import Config
-from ..io import Dataset
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.torch import inner_product, bpr_loss, l2_loss, get_initializer
-from ..io import PairwiseIterator
-
-
-class BPRMFConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 reg=1e-3,
-                 n_dim=64,
-                 batch_size=1024,
-                 epochs=1000,
-                 early_stop=200,
-                 **kwargs):
-        super(BPRMFConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.n_dim: int = n_dim
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.n_dim, int) and self.n_dim > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _MF(nn.Module):
-    def __init__(self, num_users, num_items, embed_dim):
-        super(_MF, self).__init__()
-
-        # user and item embeddings
-        self.user_embeddings = nn.Embedding(num_users, embed_dim)
-        self.item_embeddings = nn.Embedding(num_items, embed_dim)
-
-        self.item_biases = nn.Embedding(num_items, 1)
-
-        # weight initialization
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init = get_initializer("normal")
-        zero_init = get_initializer("zeros")
-
-        init(self.user_embeddings.weight)
-        init(self.item_embeddings.weight)
-        zero_init(self.item_biases.weight)
-
-    def forward(self, user_ids, item_ids):
-        user_embeds = self.user_embeddings(user_ids)
-        item_embeds = self.item_embeddings(item_ids)
-        item_bias = self.item_biases(item_ids).squeeze()
-        ratings = inner_product(user_embeds, item_embeds) + item_bias
-        return ratings
-
-    def predict(self, user_ids):
-        user_embeds = self.user_embeddings(user_ids)
-        ratings = torch.matmul(user_embeds, self.item_embeddings.weight.T)
-        ratings += self.item_biases.weight.squeeze()
-        return ratings
-
-
-class BPRMF(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = BPRMFConfig(**cfg_dict)
-        super(BPRMF, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-
-        self.mf = _MF(self.num_users, self.num_items, config.n_dim).to(self.device)
-        self.optimizer = torch.optim.Adam(self.mf.parameters(), lr=config.lr)
-
-    def fit(self):
-        data_iter = PairwiseIterator(self.dataset.train_data,
-                                     batch_size=self.config.batch_size,
-                                     shuffle=True, drop_last=False)
-
-        self.logger.info("metrics:".ljust(12)+f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            self.mf.train()
-            for bat_users, bat_pos_items, bat_neg_items in data_iter:
-                bat_users = torch.from_numpy(bat_users).long().to(self.device)
-                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
-                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
-                yui = self.mf(bat_users, bat_pos_items)
-                yuj = self.mf(bat_users, bat_neg_items)
-
-                loss = bpr_loss(yui, yuj).sum()
-                reg_loss = l2_loss(self.mf.user_embeddings(bat_users),
-                                   self.mf.item_embeddings(bat_pos_items),
-                                   self.mf.item_embeddings(bat_neg_items),
-                                   self.mf.item_biases(bat_pos_items),
-                                   self.mf.item_biases(bat_neg_items)
-                                   )
-                loss += self.config.reg * reg_loss
-                self.optimizer.zero_grad()
-                loss.backward()
-                self.optimizer.step()
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12)+f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12)+f"\t{best_result.values_str}")
-
-    def evaluate(self) -> MetricReport:
-        self.mf.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users) -> np.ndarray:
-        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
-        return self.mf.predict(users).cpu().detach().numpy()
+"""
+Paper: BPR: Bayesian Personalized Ranking from Implicit Feedback
+Author: Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["BPRMF"]
+
+import torch
+import torch.nn as nn
+import numpy as np
+from typing import Dict
+from .base import AbstractRecommender
+from ..utils.py import Config
+from ..io import Dataset
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.torch import inner_product, bpr_loss, l2_loss, get_initializer
+from ..io import PairwiseIterator
+
+
+class BPRMFConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 reg=1e-3,
+                 n_dim=64,
+                 batch_size=1024,
+                 epochs=1000,
+                 early_stop=200,
+                 **kwargs):
+        super(BPRMFConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.n_dim: int = n_dim
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.n_dim, int) and self.n_dim > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _MF(nn.Module):
+    def __init__(self, num_users, num_items, embed_dim):
+        super(_MF, self).__init__()
+
+        # user and item embeddings
+        self.user_embeddings = nn.Embedding(num_users, embed_dim)
+        self.item_embeddings = nn.Embedding(num_items, embed_dim)
+
+        self.item_biases = nn.Embedding(num_items, 1)
+
+        # weight initialization
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        init = get_initializer("normal")
+        zero_init = get_initializer("zeros")
+
+        init(self.user_embeddings.weight)
+        init(self.item_embeddings.weight)
+        zero_init(self.item_biases.weight)
+
+    def forward(self, user_ids, item_ids):
+        user_embeds = self.user_embeddings(user_ids)
+        item_embeds = self.item_embeddings(item_ids)
+        item_bias = self.item_biases(item_ids).squeeze()
+        ratings = inner_product(user_embeds, item_embeds) + item_bias
+        return ratings
+
+    def predict(self, user_ids):
+        user_embeds = self.user_embeddings(user_ids)
+        ratings = torch.matmul(user_embeds, self.item_embeddings.weight.T)
+        ratings += self.item_biases.weight.squeeze()
+        return ratings
+
+
+class BPRMF(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = BPRMFConfig(**cfg_dict)
+        super(BPRMF, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+
+        self.mf = _MF(self.num_users, self.num_items, config.n_dim).to(self.device)
+        self.optimizer = torch.optim.Adam(self.mf.parameters(), lr=config.lr)
+
+    def fit(self):
+        data_iter = PairwiseIterator(self.dataset.train_data,
+                                     batch_size=self.config.batch_size,
+                                     shuffle=True, drop_last=False)
+
+        self.logger.info("metrics:".ljust(12)+f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            self.mf.train()
+            for bat_users, bat_pos_items, bat_neg_items in data_iter:
+                bat_users = torch.from_numpy(bat_users).long().to(self.device)
+                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
+                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
+                yui = self.mf(bat_users, bat_pos_items)
+                yuj = self.mf(bat_users, bat_neg_items)
+
+                loss = bpr_loss(yui, yuj).sum()
+                reg_loss = l2_loss(self.mf.user_embeddings(bat_users),
+                                   self.mf.item_embeddings(bat_pos_items),
+                                   self.mf.item_embeddings(bat_neg_items),
+                                   self.mf.item_biases(bat_pos_items),
+                                   self.mf.item_biases(bat_neg_items)
+                                   )
+                loss += self.config.reg * reg_loss
+                self.optimizer.zero_grad()
+                loss.backward()
+                self.optimizer.step()
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12)+f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12)+f"\t{best_result.values_str}")
+
+    def evaluate(self) -> MetricReport:
+        self.mf.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users) -> np.ndarray:
+        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
+        return self.mf.predict(users).cpu().detach().numpy()
```

## skrec/recommender/TransRec.py

 * *Ordering differences only*

```diff
@@ -1,164 +1,164 @@
-"""
-Paper: Translation-based Recommendation
-Author: Ruining He, Wang-Cheng Kang, and Julian McAuley
-Reference: https://drive.google.com/file/d/0B9Ck8jw-TZUEVmdROWZKTy1fcEE/view?usp=sharing
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["TransRec"]
-
-import numpy as np
-import torch
-import torch.nn as nn
-from torch.nn.parameter import Parameter
-from typing import Dict
-from .base import AbstractRecommender
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.torch import bpr_loss, l2_loss, l2_distance
-from ..utils.torch import get_initializer
-from ..io import SequentialPairwiseIterator
-
-
-class TransRecConfig(Config):
-    def __init__(self,
-                 lr=1e-3,
-                 reg=0.0,
-                 embed_size=64,
-                 batch_size=1024,
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(TransRecConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.embed_size: int = embed_size
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _TransRec(nn.Module):
-    def __init__(self, num_users, num_items, embed_dim):
-        super(_TransRec, self).__init__()
-
-        # user and item embeddings
-        self.user_embeddings = nn.Embedding(num_users, embed_dim)
-        self.item_embeddings = nn.Embedding(num_items, embed_dim)
-        self.global_transition = Parameter(torch.Tensor(1, embed_dim))
-
-        self.item_biases = nn.Embedding(num_items, 1)
-
-        # weight initialization
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init = get_initializer("normal")
-        zero_init = get_initializer("zeros")
-
-        zero_init(self.user_embeddings.weight)
-        init(self.global_transition)
-        init(self.item_embeddings.weight)
-        zero_init(self.item_biases.weight)
-
-    def forward(self, user_ids, last_items, pre_items):
-        user_embs = self.user_embeddings(user_ids)
-        last_item_embs = self.item_embeddings(last_items)
-        pre_item_embs = self.item_embeddings(pre_items)
-        pre_item_bias = self.item_biases(pre_items)
-
-        transed_emb = user_embs + self.global_transition + last_item_embs
-        hat_y = -l2_distance(transed_emb, pre_item_embs) + torch.squeeze(pre_item_bias)
-
-        return hat_y
-
-    def predict(self, user_ids, last_items):
-        user_embs = self.user_embeddings(user_ids)
-        last_item_embs = self.item_embeddings(last_items)
-        transed_emb = user_embs + self.global_transition + last_item_embs
-        ratings = -l2_distance(transed_emb.unsqueeze(dim=1), self.item_embeddings.weight)
-
-        ratings += torch.squeeze(self.item_biases.weight)
-        return ratings
-
-
-class TransRec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = TransRecConfig(**cfg_dict)
-        super(TransRec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_dict = self.dataset.train_data.to_user_dict_by_time()
-
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.transrec = _TransRec(self.num_users, self.num_items, config.embed_size).to(self.device)
-        self.optimizer = torch.optim.Adam(self.transrec.parameters(), lr=config.lr)
-
-    def fit(self):
-        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
-                                               num_previous=1, num_next=1,
-                                               batch_size=self.config.batch_size,
-                                               shuffle=True, drop_last=False)
-
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            self.transrec.train()
-            for bat_users, bat_last_items, bat_pos_items, bat_neg_items in data_iter:
-                bat_users = torch.from_numpy(bat_users).long().to(self.device)
-                bat_last_items = torch.from_numpy(bat_last_items).long().to(self.device)
-                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
-                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
-                yui = self.transrec(bat_users, bat_last_items, bat_pos_items)
-                yuj = self.transrec(bat_users, bat_last_items, bat_neg_items)
-
-                loss = bpr_loss(yui, yuj).sum()
-                reg_loss = l2_loss(self.transrec.user_embeddings(bat_users),
-                                   self.transrec.global_transition,
-                                   self.transrec.item_embeddings(bat_last_items),
-                                   self.transrec.item_embeddings(bat_pos_items),
-                                   self.transrec.item_embeddings(bat_neg_items),
-                                   self.transrec.item_biases(bat_pos_items),
-                                   self.transrec.item_biases(bat_neg_items)
-                                   )
-                loss += self.config.reg * reg_loss
-                self.optimizer.zero_grad()
-                loss.backward()
-                self.optimizer.step()
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.transrec.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        last_items = [self.user_pos_dict[u][-1] for u in users]
-        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
-        last_items = torch.from_numpy(np.asarray(last_items)).long().to(self.device)
-        return self.transrec.predict(users, last_items).cpu().detach().numpy()
+"""
+Paper: Translation-based Recommendation
+Author: Ruining He, Wang-Cheng Kang, and Julian McAuley
+Reference: https://drive.google.com/file/d/0B9Ck8jw-TZUEVmdROWZKTy1fcEE/view?usp=sharing
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["TransRec"]
+
+import numpy as np
+import torch
+import torch.nn as nn
+from torch.nn.parameter import Parameter
+from typing import Dict
+from .base import AbstractRecommender
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.torch import bpr_loss, l2_loss, l2_distance
+from ..utils.torch import get_initializer
+from ..io import SequentialPairwiseIterator
+
+
+class TransRecConfig(Config):
+    def __init__(self,
+                 lr=1e-3,
+                 reg=0.0,
+                 embed_size=64,
+                 batch_size=1024,
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(TransRecConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.embed_size: int = embed_size
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _TransRec(nn.Module):
+    def __init__(self, num_users, num_items, embed_dim):
+        super(_TransRec, self).__init__()
+
+        # user and item embeddings
+        self.user_embeddings = nn.Embedding(num_users, embed_dim)
+        self.item_embeddings = nn.Embedding(num_items, embed_dim)
+        self.global_transition = Parameter(torch.Tensor(1, embed_dim))
+
+        self.item_biases = nn.Embedding(num_items, 1)
+
+        # weight initialization
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        init = get_initializer("normal")
+        zero_init = get_initializer("zeros")
+
+        zero_init(self.user_embeddings.weight)
+        init(self.global_transition)
+        init(self.item_embeddings.weight)
+        zero_init(self.item_biases.weight)
+
+    def forward(self, user_ids, last_items, pre_items):
+        user_embs = self.user_embeddings(user_ids)
+        last_item_embs = self.item_embeddings(last_items)
+        pre_item_embs = self.item_embeddings(pre_items)
+        pre_item_bias = self.item_biases(pre_items)
+
+        transed_emb = user_embs + self.global_transition + last_item_embs
+        hat_y = -l2_distance(transed_emb, pre_item_embs) + torch.squeeze(pre_item_bias)
+
+        return hat_y
+
+    def predict(self, user_ids, last_items):
+        user_embs = self.user_embeddings(user_ids)
+        last_item_embs = self.item_embeddings(last_items)
+        transed_emb = user_embs + self.global_transition + last_item_embs
+        ratings = -l2_distance(transed_emb.unsqueeze(dim=1), self.item_embeddings.weight)
+
+        ratings += torch.squeeze(self.item_biases.weight)
+        return ratings
+
+
+class TransRec(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = TransRecConfig(**cfg_dict)
+        super(TransRec, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_dict = self.dataset.train_data.to_user_dict_by_time()
+
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+        self.transrec = _TransRec(self.num_users, self.num_items, config.embed_size).to(self.device)
+        self.optimizer = torch.optim.Adam(self.transrec.parameters(), lr=config.lr)
+
+    def fit(self):
+        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
+                                               num_previous=1, num_next=1,
+                                               batch_size=self.config.batch_size,
+                                               shuffle=True, drop_last=False)
+
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            self.transrec.train()
+            for bat_users, bat_last_items, bat_pos_items, bat_neg_items in data_iter:
+                bat_users = torch.from_numpy(bat_users).long().to(self.device)
+                bat_last_items = torch.from_numpy(bat_last_items).long().to(self.device)
+                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
+                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
+                yui = self.transrec(bat_users, bat_last_items, bat_pos_items)
+                yuj = self.transrec(bat_users, bat_last_items, bat_neg_items)
+
+                loss = bpr_loss(yui, yuj).sum()
+                reg_loss = l2_loss(self.transrec.user_embeddings(bat_users),
+                                   self.transrec.global_transition,
+                                   self.transrec.item_embeddings(bat_last_items),
+                                   self.transrec.item_embeddings(bat_pos_items),
+                                   self.transrec.item_embeddings(bat_neg_items),
+                                   self.transrec.item_biases(bat_pos_items),
+                                   self.transrec.item_biases(bat_neg_items)
+                                   )
+                loss += self.config.reg * reg_loss
+                self.optimizer.zero_grad()
+                loss.backward()
+                self.optimizer.step()
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.transrec.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        last_items = [self.user_pos_dict[u][-1] for u in users]
+        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
+        last_items = torch.from_numpy(np.asarray(last_items)).long().to(self.device)
+        return self.transrec.predict(users, last_items).cpu().detach().numpy()
```

## skrec/recommender/FPMC.py

 * *Ordering differences only*

```diff
@@ -1,157 +1,157 @@
-"""
-Paper: Factorizing Personalized Markov Chains for Next-Basket Recommendation
-Author: Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["FPMC"]
-
-
-import torch
-import torch.nn as nn
-import numpy as np
-from typing import Dict
-from .base import AbstractRecommender
-from ..io import Dataset
-from ..utils.py import Config
-from ..utils.py import RankingEvaluator, MetricReport
-from ..utils.torch import get_initializer
-from ..utils.torch import bpr_loss, l2_loss, inner_product
-from ..io import SequentialPairwiseIterator
-
-
-class FPMCConfig(Config):
-    def __init__(self,
-                 lr=0.001,
-                 reg=0.001,
-                 embed_size=64,
-                 batch_size=1024,
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(FPMCConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.embed_size: int = embed_size
-        self.batch_size: int = batch_size
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.batch_size, int) and self.batch_size > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class _FPMC(nn.Module):
-    def __init__(self, num_users, num_items, embed_dim):
-        super(_FPMC, self).__init__()
-
-        # user and item embeddings
-        self.UI_embeddings = nn.Embedding(num_users, embed_dim)
-        self.IU_embeddings = nn.Embedding(num_items, embed_dim)
-        self.IL_embeddings = nn.Embedding(num_items, embed_dim)
-        self.LI_embeddings = nn.Embedding(num_items, embed_dim)
-
-        # weight initialization
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        init = get_initializer("normal")
-        init(self.UI_embeddings.weight)
-        init(self.IU_embeddings.weight)
-        init(self.IL_embeddings.weight)
-        init(self.LI_embeddings.weight)
-
-    def forward(self, user_ids, last_items, pre_items):
-        ui_emb = self.UI_embeddings(user_ids)  # b*d
-        pre_iu_emb = self.IU_embeddings(pre_items)  # b*d
-        pre_il_emb = self.IL_embeddings(pre_items)  # b*d
-        last_emb = self.LI_embeddings(last_items)  # b*d
-
-        hat_y = inner_product(ui_emb, pre_iu_emb) + inner_product(last_emb, pre_il_emb)
-
-        return hat_y
-
-    def predict(self, user_ids, last_items):
-        ui_emb = self.UI_embeddings(user_ids)  # b*d
-        last_emb = self.LI_embeddings(last_items)  # b*d
-        ratings = torch.matmul(ui_emb, self.IU_embeddings.weight.T) + \
-                  torch.matmul(last_emb, self.IL_embeddings.weight.T)
-
-        return ratings
-
-
-class FPMC(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = FPMCConfig(**cfg_dict)
-        super(FPMC, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-        self.user_pos_dict = self.dataset.train_data.to_user_dict_by_time()
-
-        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
-        self.fpmc = _FPMC(self.num_users, self.num_items, config.embed_size).to(self.device)
-        self.optimizer = torch.optim.Adam(self.fpmc.parameters(), lr=config.lr)
-
-    def fit(self):
-        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
-                                               num_previous=1, num_next=1,
-                                               batch_size=self.config.batch_size,
-                                               shuffle=True, drop_last=False)
-
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        for epoch in range(self.config.epochs):
-            self.fpmc.train()
-            for bat_users, bat_last_items, bat_pos_items, bat_neg_items in data_iter:
-                bat_users = torch.from_numpy(bat_users).long().to(self.device)
-                bat_last_items = torch.from_numpy(bat_last_items).long().to(self.device)
-                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
-                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
-                yui = self.fpmc(bat_users, bat_last_items, bat_pos_items)
-                yuj = self.fpmc(bat_users, bat_last_items, bat_neg_items)
-
-                loss = bpr_loss(yui, yuj).sum()
-                reg_loss = l2_loss(self.fpmc.UI_embeddings(bat_users),
-                                   self.fpmc.LI_embeddings(bat_last_items),
-                                   self.fpmc.IU_embeddings(bat_pos_items),
-                                   self.fpmc.IU_embeddings(bat_neg_items),
-                                   self.fpmc.IL_embeddings(bat_pos_items),
-                                   self.fpmc.IL_embeddings(bat_neg_items)
-                                   )
-                loss += self.config.reg * reg_loss
-                self.optimizer.zero_grad()
-                loss.backward()
-                self.optimizer.step()
-
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        self.fpmc.eval()
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        last_items = [self.user_pos_dict[u][-1] for u in users]
-        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
-        last_items = torch.from_numpy(np.asarray(last_items)).long().to(self.device)
-        return self.fpmc.predict(users, last_items).cpu().detach().numpy()
+"""
+Paper: Factorizing Personalized Markov Chains for Next-Basket Recommendation
+Author: Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["FPMC"]
+
+
+import torch
+import torch.nn as nn
+import numpy as np
+from typing import Dict
+from .base import AbstractRecommender
+from ..io import Dataset
+from ..utils.py import Config
+from ..utils.py import RankingEvaluator, MetricReport
+from ..utils.torch import get_initializer
+from ..utils.torch import bpr_loss, l2_loss, inner_product
+from ..io import SequentialPairwiseIterator
+
+
+class FPMCConfig(Config):
+    def __init__(self,
+                 lr=0.001,
+                 reg=0.001,
+                 embed_size=64,
+                 batch_size=1024,
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(FPMCConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.embed_size: int = embed_size
+        self.batch_size: int = batch_size
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.batch_size, int) and self.batch_size > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class _FPMC(nn.Module):
+    def __init__(self, num_users, num_items, embed_dim):
+        super(_FPMC, self).__init__()
+
+        # user and item embeddings
+        self.UI_embeddings = nn.Embedding(num_users, embed_dim)
+        self.IU_embeddings = nn.Embedding(num_items, embed_dim)
+        self.IL_embeddings = nn.Embedding(num_items, embed_dim)
+        self.LI_embeddings = nn.Embedding(num_items, embed_dim)
+
+        # weight initialization
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        init = get_initializer("normal")
+        init(self.UI_embeddings.weight)
+        init(self.IU_embeddings.weight)
+        init(self.IL_embeddings.weight)
+        init(self.LI_embeddings.weight)
+
+    def forward(self, user_ids, last_items, pre_items):
+        ui_emb = self.UI_embeddings(user_ids)  # b*d
+        pre_iu_emb = self.IU_embeddings(pre_items)  # b*d
+        pre_il_emb = self.IL_embeddings(pre_items)  # b*d
+        last_emb = self.LI_embeddings(last_items)  # b*d
+
+        hat_y = inner_product(ui_emb, pre_iu_emb) + inner_product(last_emb, pre_il_emb)
+
+        return hat_y
+
+    def predict(self, user_ids, last_items):
+        ui_emb = self.UI_embeddings(user_ids)  # b*d
+        last_emb = self.LI_embeddings(last_items)  # b*d
+        ratings = torch.matmul(ui_emb, self.IU_embeddings.weight.T) + \
+                  torch.matmul(last_emb, self.IL_embeddings.weight.T)
+
+        return ratings
+
+
+class FPMC(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = FPMCConfig(**cfg_dict)
+        super(FPMC, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+        self.user_pos_dict = self.dataset.train_data.to_user_dict_by_time()
+
+        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+        self.fpmc = _FPMC(self.num_users, self.num_items, config.embed_size).to(self.device)
+        self.optimizer = torch.optim.Adam(self.fpmc.parameters(), lr=config.lr)
+
+    def fit(self):
+        data_iter = SequentialPairwiseIterator(self.dataset.train_data,
+                                               num_previous=1, num_next=1,
+                                               batch_size=self.config.batch_size,
+                                               shuffle=True, drop_last=False)
+
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        for epoch in range(self.config.epochs):
+            self.fpmc.train()
+            for bat_users, bat_last_items, bat_pos_items, bat_neg_items in data_iter:
+                bat_users = torch.from_numpy(bat_users).long().to(self.device)
+                bat_last_items = torch.from_numpy(bat_last_items).long().to(self.device)
+                bat_pos_items = torch.from_numpy(bat_pos_items).long().to(self.device)
+                bat_neg_items = torch.from_numpy(bat_neg_items).long().to(self.device)
+                yui = self.fpmc(bat_users, bat_last_items, bat_pos_items)
+                yuj = self.fpmc(bat_users, bat_last_items, bat_neg_items)
+
+                loss = bpr_loss(yui, yuj).sum()
+                reg_loss = l2_loss(self.fpmc.UI_embeddings(bat_users),
+                                   self.fpmc.LI_embeddings(bat_last_items),
+                                   self.fpmc.IU_embeddings(bat_pos_items),
+                                   self.fpmc.IU_embeddings(bat_neg_items),
+                                   self.fpmc.IL_embeddings(bat_pos_items),
+                                   self.fpmc.IL_embeddings(bat_neg_items)
+                                   )
+                loss += self.config.reg * reg_loss
+                self.optimizer.zero_grad()
+                loss.backward()
+                self.optimizer.step()
+
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        self.fpmc.eval()
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        last_items = [self.user_pos_dict[u][-1] for u in users]
+        users = torch.from_numpy(np.asarray(users)).long().to(self.device)
+        last_items = torch.from_numpy(np.asarray(last_items)).long().to(self.device)
+        return self.fpmc.predict(users, last_items).cpu().detach().numpy()
```

## skrec/recommender/AOBPR/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-from .AOBPR import AOBPR, AOBPRConfig
-
-__all__ = [AOBPR, AOBPRConfig]
+from .AOBPR import AOBPR, AOBPRConfig
+
+__all__ = [AOBPR, AOBPRConfig]
```

## skrec/recommender/AOBPR/AOBPR.py

 * *Ordering differences only*

```diff
@@ -1,104 +1,104 @@
-"""
-Paper: Improving Pairwise Learning for Item Recommendation from Implicit Feedback
-Author: Steffen Rendle, and Christoph Freudenthaler
-"""
-
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["AOBPR"]
-
-import numpy as np
-from typing import Dict
-from ..base import AbstractRecommender
-from ...io import PairwiseIterator
-from ...utils.py import randint_choice
-from ...utils.py import Config
-from ...utils.py import RankingEvaluator, MetricReport
-from ...io import Dataset
-from .pyx_aobpr_func import aobpr_update
-
-
-class AOBPRConfig(Config):
-    def __init__(self,
-                 lr=1e-2,
-                 reg=5e-2,
-                 embed_size=64,
-                 alpha=6682,
-                 epochs=500,
-                 early_stop=100,
-                 **kwargs):
-        super(AOBPRConfig, self).__init__()
-        self.lr: float = lr
-        self.reg: float = reg
-        self.embed_size: int = embed_size
-        self.alpha: int = alpha
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self._validate()
-
-    def _validate(self):
-        assert isinstance(self.lr, float) and self.lr > 0
-        assert isinstance(self.reg, float) and self.reg >= 0
-        assert isinstance(self.embed_size, int) and self.embed_size > 0
-        assert isinstance(self.alpha, int) and self.alpha > 0
-        assert isinstance(self.epochs, int) and self.epochs >= 0
-        assert isinstance(self.early_stop, int)
-
-
-class AOBPR(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = AOBPRConfig(**cfg_dict)
-        super(AOBPR, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
-
-        low, high = 0.0, 1.0
-        self.user_embeds = np.random.uniform(low=low, high=high, size=[self.num_users, self.config.embed_size]).astype(np.float32)
-        self.item_embeds = np.random.uniform(low=low, high=high, size=[self.num_items, self.config.embed_size]).astype(np.float32)
-
-        rank = np.arange(1, self.num_items+1)
-        rank_prob = np.exp(-rank/self.config.alpha)
-        self.rank_prob = rank_prob/np.sum(rank_prob)
-
-    def fit(self):
-        data_iter = PairwiseIterator(self.dataset.train_data,
-                                     batch_size=len(self.dataset.train_data),
-                                     shuffle=False, drop_last=False)
-
-        user1d, pos_item1d, _ = list(data_iter)[0]
-        len_data = len(user1d)
-        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
-        stop_counter = 0
-        best_result: MetricReport = None
-        shuffle_idx = np.arange(len_data)
-        for epoch in range(self.config.epochs):
-            rank_idx = randint_choice(self.num_items, size=len_data,
-                                      replace=True, p=self.rank_prob)
-            np.random.shuffle(shuffle_idx)
-            aobpr_update(user1d[shuffle_idx], pos_item1d[shuffle_idx],
-                         rank_idx, self.config.lr, self.config.reg,
-                         self.user_embeds, self.item_embeds)
-
-            # self.logger.info("epoch %d:\t%s" % (epoch, self.evaluate_model()))
-            cur_result = self.evaluate()
-            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
-            stop_counter += 1
-            if stop_counter > self.config.early_stop:
-                self.logger.info("early stop")
-                break
-            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
-                best_result = cur_result
-                stop_counter = 0
-
-        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
-
-    def evaluate(self):
-        return self.evaluator.evaluate(self)
-
-    def predict(self, users):
-        user_embedding = self.user_embeds[users]
-        all_ratings = np.matmul(user_embedding, self.item_embeds.T)
-        return all_ratings
+"""
+Paper: Improving Pairwise Learning for Item Recommendation from Implicit Feedback
+Author: Steffen Rendle, and Christoph Freudenthaler
+"""
+
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["AOBPR"]
+
+import numpy as np
+from typing import Dict
+from ..base import AbstractRecommender
+from ...io import PairwiseIterator
+from ...utils.py import randint_choice
+from ...utils.py import Config
+from ...utils.py import RankingEvaluator, MetricReport
+from ...io import Dataset
+from .pyx_aobpr_func import aobpr_update
+
+
+class AOBPRConfig(Config):
+    def __init__(self,
+                 lr=1e-2,
+                 reg=5e-2,
+                 embed_size=64,
+                 alpha=6682,
+                 epochs=500,
+                 early_stop=100,
+                 **kwargs):
+        super(AOBPRConfig, self).__init__()
+        self.lr: float = lr
+        self.reg: float = reg
+        self.embed_size: int = embed_size
+        self.alpha: int = alpha
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self._validate()
+
+    def _validate(self):
+        assert isinstance(self.lr, float) and self.lr > 0
+        assert isinstance(self.reg, float) and self.reg >= 0
+        assert isinstance(self.embed_size, int) and self.embed_size > 0
+        assert isinstance(self.alpha, int) and self.alpha > 0
+        assert isinstance(self.epochs, int) and self.epochs >= 0
+        assert isinstance(self.early_stop, int)
+
+
+class AOBPR(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = AOBPRConfig(**cfg_dict)
+        super(AOBPR, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+        self.num_users, self.num_items = self.dataset.num_users, self.dataset.num_items
+
+        low, high = 0.0, 1.0
+        self.user_embeds = np.random.uniform(low=low, high=high, size=[self.num_users, self.config.embed_size]).astype(np.float32)
+        self.item_embeds = np.random.uniform(low=low, high=high, size=[self.num_items, self.config.embed_size]).astype(np.float32)
+
+        rank = np.arange(1, self.num_items+1)
+        rank_prob = np.exp(-rank/self.config.alpha)
+        self.rank_prob = rank_prob/np.sum(rank_prob)
+
+    def fit(self):
+        data_iter = PairwiseIterator(self.dataset.train_data,
+                                     batch_size=len(self.dataset.train_data),
+                                     shuffle=False, drop_last=False)
+
+        user1d, pos_item1d, _ = list(data_iter)[0]
+        len_data = len(user1d)
+        self.logger.info("metrics:".ljust(12) + f"\t{self.evaluator.metrics_str}")
+        stop_counter = 0
+        best_result: MetricReport = None
+        shuffle_idx = np.arange(len_data)
+        for epoch in range(self.config.epochs):
+            rank_idx = randint_choice(self.num_items, size=len_data,
+                                      replace=True, p=self.rank_prob)
+            np.random.shuffle(shuffle_idx)
+            aobpr_update(user1d[shuffle_idx], pos_item1d[shuffle_idx],
+                         rank_idx, self.config.lr, self.config.reg,
+                         self.user_embeds, self.item_embeds)
+
+            # self.logger.info("epoch %d:\t%s" % (epoch, self.evaluate_model()))
+            cur_result = self.evaluate()
+            self.logger.info(f"epoch {epoch}:".ljust(12) + f"\t{cur_result.values_str}")
+            stop_counter += 1
+            if stop_counter > self.config.early_stop:
+                self.logger.info("early stop")
+                break
+            if best_result is None or cur_result["NDCG@10"] >= best_result["NDCG@10"]:
+                best_result = cur_result
+                stop_counter = 0
+
+        self.logger.info("best:".ljust(12) + f"\t{best_result.values_str}")
+
+    def evaluate(self):
+        return self.evaluator.evaluate(self)
+
+    def predict(self, users):
+        user_embedding = self.user_embeds[users]
+        all_ratings = np.matmul(user_embedding, self.item_embeds.T)
+        return all_ratings
```

## skrec/recommender/BERT4Rec/modeling.py

 * *Ordering differences only*

```diff
@@ -1,992 +1,992 @@
-# coding=utf-8
-
-"""The main BERT model and related functions."""
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import collections
-import copy
-import json
-import math
-import re
-import six
-import tensorflow as tf
-
-
-class BertConfig(object):
-    """Configuration for `BertModel`."""
-
-    def __init__(self,
-                 vocab_size,
-                 hidden_size=768,
-                 num_hidden_layers=12,
-                 num_attention_heads=12,
-                 intermediate_size=3072,
-                 hidden_act="gelu",
-                 hidden_dropout_prob=0.1,
-                 attention_probs_dropout_prob=0.1,
-                 max_position_embeddings=512,
-                 type_vocab_size=16,
-                 initializer_range=0.02):
-        """Constructs BertConfig.
-
-        Args:
-        vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.
-        hidden_size: Size of the encoder layers and the pooler layer.
-        num_hidden_layers: Number of hidden layers in the Transformer encoder.
-        num_attention_heads: Number of attention heads for each attention layer in
-            the Transformer encoder.
-        intermediate_size: The size of the "intermediate" (i.e., feed-forward)
-            layer in the Transformer encoder.
-        hidden_act: The non-linear activation function (function or string) in the
-            encoder and pooler.
-        hidden_dropout_prob: The dropout probability for all fully connected
-            layers in the embeddings, encoder, and pooler.
-        attention_probs_dropout_prob: The dropout ratio for the attention
-            probabilities.
-        max_position_embeddings: The maximum sequence length that this model might
-            ever be used with. Typically set this to something large just in case
-            (e.g., 512 or 1024 or 2048).
-        type_vocab_size: The vocabulary size of the `token_type_ids` passed into
-            `BertModel`.
-        initializer_range: The stdev of the truncated_normal_initializer for
-            initializing all weight matrices.
-        """
-        self.vocab_size = vocab_size
-        self.hidden_size = hidden_size
-        self.num_hidden_layers = num_hidden_layers
-        self.num_attention_heads = num_attention_heads
-        self.hidden_act = hidden_act
-        self.intermediate_size = intermediate_size
-        self.hidden_dropout_prob = hidden_dropout_prob
-        self.attention_probs_dropout_prob = attention_probs_dropout_prob
-        self.max_position_embeddings = max_position_embeddings
-        self.type_vocab_size = type_vocab_size
-        self.initializer_range = initializer_range
-
-    @classmethod
-    def from_dict(cls, json_object):
-        """Constructs a `BertConfig` from a Python dictionary of parameters."""
-        config = BertConfig(vocab_size=None)
-        for (key, value) in six.iteritems(json_object):
-            config.__dict__[key] = value
-        return config
-
-    @classmethod
-    def from_json_file(cls, json_file):
-        """Constructs a `BertConfig` from a json file of parameters."""
-        with tf.gfile.GFile(json_file, "r") as reader:
-            text = reader.read()
-        return cls.from_dict(json.loads(text))
-
-    def to_dict(self):
-        """Serializes this instance to a Python dictionary."""
-        output = copy.deepcopy(self.__dict__)
-        return output
-
-    def to_json_string(self):
-        """Serializes this instance to a JSON string."""
-        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + "\n"
-
-
-class BertModel(object):
-    """BERT model ("Bidirectional Embedding Representations from a Transformer").
-
-    Example usage:
-
-    ```python
-    # Already been converted into WordPiece token ids
-    input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])
-    input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
-    token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])
-
-    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,
-        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)
-
-    model = modeling.BertModel(config=config, is_training=True,
-        input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)
-
-    label_embeddings = tf.get_variable(...)
-    logits = tf.matmul(pooled_output, label_embeddings)
-    ...
-    ```
-    """
-    def __init__(self,
-                 config,
-                 is_training,
-                 input_ids,
-                 input_mask=None,
-                 token_type_ids=None,
-                 use_one_hot_embeddings=True,
-                 scope=None):
-        """Constructor for BertModel.
-
-        Args:
-        config: `BertConfig` instance.
-        is_training: bool. rue for training model, false for eval model. Controls
-            whether dropout will be applied.
-        input_ids: int32 Tensor of shape [batch_size, seq_length].
-        input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].
-        token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
-        use_one_hot_embeddings: (optional) bool. Whether to use one-hot word
-            embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,
-            it is must faster if this is True, on the CPU or GPU, it is faster if
-            this is False.
-        scope: (optional) variable scope. Defaults to "bert".
-
-        Raises:
-        ValueError: The config is invalid or one of the input tensor shapes
-            is invalid.
-        """
-        config = copy.deepcopy(config)
-        if not is_training:
-            config.hidden_dropout_prob = 0.0
-            config.attention_probs_dropout_prob = 0.0
-
-        input_shape = get_shape_list(input_ids, expected_rank=2)
-        batch_size = input_shape[0]
-        seq_length = input_shape[1]
-
-        if input_mask is None:
-            input_mask = tf.ones(
-                shape=[batch_size, seq_length], dtype=tf.int32)
-
-        if token_type_ids is None:
-            token_type_ids = tf.zeros(
-                shape=[batch_size, seq_length], dtype=tf.int32)
-
-        with tf.variable_scope(scope, default_name="bert"):
-            with tf.variable_scope("embeddings"):
-                # Perform embedding lookup on the word ids.
-                (self.embedding_output,
-                 self.embedding_table) = embedding_lookup(
-                     input_ids=input_ids,
-                     vocab_size=config.vocab_size,
-                     embedding_size=config.hidden_size,
-                     initializer_range=config.initializer_range,
-                     word_embedding_name="word_embeddings",
-                     use_one_hot_embeddings=use_one_hot_embeddings)
-
-                # Add positional embeddings and token type embeddings, then layer
-                # normalize and perform dropout.
-                self.embedding_output = embedding_postprocessor(
-                    input_tensor=self.embedding_output,
-                    use_token_type=True,
-                    token_type_ids=token_type_ids,
-                    token_type_vocab_size=config.type_vocab_size,
-                    token_type_embedding_name="token_type_embeddings",
-                    use_position_embeddings=True,
-                    position_embedding_name="position_embeddings",
-                    initializer_range=config.initializer_range,
-                    max_position_embeddings=config.max_position_embeddings,
-                    dropout_prob=config.hidden_dropout_prob)
-
-            with tf.variable_scope("encoder"):
-                # This converts a 2D mask of shape [batch_size, seq_length] to a 3D
-                # mask of shape [batch_size, seq_length, seq_length] which is used
-                # for the attention scores.
-                attention_mask = create_attention_mask_from_input_mask(
-                    input_ids, input_mask)
-
-                # Run the stacked transformer.
-                # `sequence_output` shape = [batch_size, seq_length, hidden_size].
-                self.all_encoder_layers = transformer_model(
-                    input_tensor=self.embedding_output,
-                    attention_mask=attention_mask,
-                    hidden_size=config.hidden_size,
-                    num_hidden_layers=config.num_hidden_layers,
-                    num_attention_heads=config.num_attention_heads,
-                    intermediate_size=config.intermediate_size,
-                    intermediate_act_fn=get_activation(config.hidden_act),
-                    hidden_dropout_prob=config.hidden_dropout_prob,
-                    attention_probs_dropout_prob=config.
-                    attention_probs_dropout_prob,
-                    initializer_range=config.initializer_range,
-                    do_return_all_layers=True)
-
-            self.sequence_output = self.all_encoder_layers[-1]
-            # The "pooler" converts the encoded sequence tensor of shape
-            # [batch_size, seq_length, hidden_size] to a tensor of shape
-            # [batch_size, hidden_size]. This is necessary for segment-level
-            # (or segment-pair-level) classification tasks where we need a fixed
-            # dimensional representation of the segment.
-            # with tf.variable_scope("pooler"):
-            #     # We "pool" the model by simply taking the hidden state corresponding
-            #     # to the first token. We assume that this has been pre-trained
-            #     first_token_tensor = tf.squeeze(
-            #         self.sequence_output[:, 0:1, :], axis=1)
-            #     self.pooled_output = tf.layers.dense(
-            #         first_token_tensor,
-            #         config.hidden_size,
-            #         activation=tf.tanh,
-            #         kernel_initializer=create_initializer(
-            #             config.initializer_range))
-
-    def get_pooled_output(self):
-        return self.pooled_output
-
-    def get_sequence_output(self):
-        """Gets final hidden layer of encoder.
-
-        Returns:
-        float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
-        to the final hidden of the transformer encoder.
-        """
-        return self.sequence_output
-
-    def get_all_encoder_layers(self):
-        return self.all_encoder_layers
-
-    def get_embedding_output(self):
-        """Gets output of the embedding lookup (i.e., input to the transformer).
-
-        Returns:
-        float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
-        to the output of the embedding layer, after summing the word
-        embeddings with the positional embeddings and the token type embeddings,
-        then performing layer normalization. This is the input to the transformer.
-        """
-        return self.embedding_output
-
-    def get_embedding_table(self):
-        return self.embedding_table
-
-
-def gelu(input_tensor):
-    """Gaussian Error Linear Unit.
-
-    This is a smoother version of the RELU.
-    Original paper: https://arxiv.org/abs/1606.08415
-
-    Args:
-        input_tensor: float Tensor to perform activation.
-
-    Returns:
-        `input_tensor` with the GELU activation applied.
-    """
-    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
-    return input_tensor * cdf
-
-
-def get_activation(activation_string):
-    """Maps a string to a Python function, e.g., "relu" => `tf.nn.relu`.
-
-    Args:
-        activation_string: String name of the activation function.
-
-    Returns:
-        A Python function corresponding to the activation function. If
-        `activation_string` is None, empty, or "linear", this will return None.
-        If `activation_string` is not a string, it will return `activation_string`.
-
-    Raises:
-        ValueError: The `activation_string` does not correspond to a known
-        activation.
-    """
-
-    # We assume that anything that"s not a string is already an activation
-    # function, so we just return it.
-    if not isinstance(activation_string, six.string_types):
-        return activation_string
-
-    if not activation_string:
-        return None
-
-    act = activation_string.lower()
-    if act == "linear":
-        return None
-    elif act == "relu":
-        return tf.nn.relu
-    elif act == "gelu":
-        return gelu
-    elif act == "tanh":
-        return tf.tanh
-    else:
-        raise ValueError("Unsupported activation: %s" % act)
-
-
-def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
-    """Compute the union of the current variables and checkpoint variables."""
-    assignment_map = {}
-    initialized_variable_names = {}
-
-    name_to_variable = collections.OrderedDict()
-    for var in tvars:
-        name = var.name
-        m = re.match("^(.*):\\d+$", name)
-        if m is not None:
-            name = m.group(1)
-        name_to_variable[name] = var
-
-    init_vars = tf.train.list_variables(init_checkpoint)
-
-    assignment_map = collections.OrderedDict()
-    for x in init_vars:
-        (name, var) = (x[0], x[1])
-        if name not in name_to_variable:
-            continue
-        assignment_map[name] = name
-        initialized_variable_names[name] = 1
-        initialized_variable_names[name + ":0"] = 1
-
-    return (assignment_map, initialized_variable_names)
-
-
-def dropout(input_tensor, dropout_prob):
-    """Perform dropout.
-
-    Args:
-        input_tensor: float Tensor.
-        dropout_prob: Python float. The probability of dropping out a value (NOT of
-        *keeping* a dimension as in `tf.nn.dropout`).
-
-    Returns:
-        A version of `input_tensor` with dropout applied.
-    """
-    if dropout_prob is None or dropout_prob == 0.0:
-        return input_tensor
-
-    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)
-    return output
-
-
-def layer_norm(input_tensor, name=None):
-    """Run layer normalization on the last dimension of the tensor."""
-    return tf.contrib.layers.layer_norm(
-        inputs=input_tensor,
-        begin_norm_axis=-1,
-        begin_params_axis=-1,
-        scope=name)
-
-
-def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):
-    """Runs layer normalization followed by dropout."""
-    output_tensor = layer_norm(input_tensor, name)
-    output_tensor = dropout(output_tensor, dropout_prob)
-    return output_tensor
-
-
-def create_initializer(initializer_range=0.02):
-    """Creates a `truncated_normal_initializer` with the given range."""
-    return tf.truncated_normal_initializer(stddev=initializer_range)
-
-
-def embedding_lookup(input_ids,
-                     vocab_size,
-                     embedding_size=128,
-                     initializer_range=0.02,
-                     word_embedding_name="word_embeddings",
-                     use_one_hot_embeddings=False):
-    """Looks up words embeddings for id tensor.
-
-    Args:
-        input_ids: int32 Tensor of shape [batch_size, seq_length] containing word
-        ids.
-        vocab_size: int. Size of the embedding vocabulary.
-        embedding_size: int. Width of the word embeddings.
-        initializer_range: float. Embedding initialization range.
-        word_embedding_name: string. Name of the embedding table.
-        use_one_hot_embeddings: bool. If True, use one-hot method for word
-        embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better
-        for TPUs.
-
-    Returns:
-        float Tensor of shape [batch_size, seq_length, embedding_size].
-    """
-    # This function assumes that the input is of shape [batch_size, seq_length,
-    # num_inputs].
-    #
-    # If the input is a 2D tensor of shape [batch_size, seq_length], we
-    # reshape to [batch_size, seq_length, 1].
-    if input_ids.shape.ndims == 2:
-        input_ids = tf.expand_dims(input_ids, axis=[-1])
-
-    embedding_table = tf.get_variable(
-        name=word_embedding_name,
-        shape=[vocab_size, embedding_size],
-        initializer=create_initializer(initializer_range))
-
-    if use_one_hot_embeddings:
-        flat_input_ids = tf.reshape(input_ids, [-1])
-        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)
-        output = tf.matmul(one_hot_input_ids, embedding_table)
-    else:
-        output = tf.nn.embedding_lookup(embedding_table, input_ids)
-
-    input_shape = get_shape_list(input_ids)
-
-    output = tf.reshape(output,
-                        input_shape[0:-1] + [input_shape[-1] * embedding_size])
-    return (output, embedding_table)
-
-
-def embedding_postprocessor(input_tensor,
-                            use_token_type=False,
-                            token_type_ids=None,
-                            token_type_vocab_size=16,
-                            token_type_embedding_name="token_type_embeddings",
-                            use_position_embeddings=True,
-                            position_embedding_name="position_embeddings",
-                            initializer_range=0.02,
-                            max_position_embeddings=512,
-                            dropout_prob=0.1):
-    """Performs various post-processing on a word embedding tensor.
-
-    Args:
-        input_tensor: float Tensor of shape [batch_size, seq_length,
-        embedding_size].
-        use_token_type: bool. Whether to add embeddings for `token_type_ids`.
-        token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
-        Must be specified if `use_token_type` is True.
-        token_type_vocab_size: int. The vocabulary size of `token_type_ids`.
-        token_type_embedding_name: string. The name of the embedding table variable
-        for token type ids.
-        use_position_embeddings: bool. Whether to add position embeddings for the
-        position of each token in the sequence.
-        position_embedding_name: string. The name of the embedding table variable
-        for positional embeddings.
-        initializer_range: float. Range of the weight initialization.
-        max_position_embeddings: int. Maximum sequence length that might ever be
-        used with this model. This can be longer than the sequence length of
-        input_tensor, but cannot be shorter.
-        dropout_prob: float. Dropout probability applied to the final output tensor.
-
-    Returns:
-        float tensor with same shape as `input_tensor`.
-
-    Raises:
-        ValueError: One of the tensor shapes or input values is invalid.
-    """
-    input_shape = get_shape_list(input_tensor, expected_rank=3)
-    batch_size = input_shape[0]
-    seq_length = input_shape[1]
-    width = input_shape[2]
-
-    output = input_tensor
-
-    if use_token_type:
-        if token_type_ids is None:
-            raise ValueError("`token_type_ids` must be specified if"
-                             "`use_token_type` is True.")
-        token_type_table = tf.get_variable(
-            name=token_type_embedding_name,
-            shape=[token_type_vocab_size, width],
-            initializer=create_initializer(initializer_range))
-        # This vocab will be small so we always do one-hot here, since it is always
-        # faster for a small vocabulary.
-        flat_token_type_ids = tf.reshape(token_type_ids, [-1])
-        one_hot_ids = tf.one_hot(
-            flat_token_type_ids, depth=token_type_vocab_size)
-        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
-        token_type_embeddings = tf.reshape(token_type_embeddings,
-                                           [batch_size, seq_length, width])
-        output += token_type_embeddings
-
-    if use_position_embeddings:
-        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)
-        with tf.control_dependencies([assert_op]):
-            full_position_embeddings = tf.get_variable(
-                name=position_embedding_name,
-                shape=[max_position_embeddings, width],
-                initializer=create_initializer(initializer_range))
-            # Since the position embedding table is a learned variable, we create it
-            # using a (long) sequence length `max_position_embeddings`. The actual
-            # sequence length might be shorter than this, for faster training of
-            # tasks that do not have long sequences.
-            #
-            # So `full_position_embeddings` is effectively an embedding table
-            # for position [0, 1, 2, ..., max_position_embeddings-1], and the current
-            # sequence has positions [0, 1, 2, ... seq_length-1], so we can just
-            # perform a slice.
-            position_embeddings = tf.slice(full_position_embeddings, [0, 0],
-                                           [seq_length, -1])
-            num_dims = len(output.shape.as_list())
-
-            # Only the last two dimensions are relevant (`seq_length` and `width`), so
-            # we broadcast among the first dimensions, which is typically just
-            # the batch size.
-            position_broadcast_shape = []
-            for _ in range(num_dims - 2):
-                position_broadcast_shape.append(1)
-            position_broadcast_shape.extend([seq_length, width])
-            position_embeddings = tf.reshape(position_embeddings,
-                                             position_broadcast_shape)
-            output += position_embeddings
-
-    output = layer_norm_and_dropout(output, dropout_prob)
-    return output
-
-
-def create_attention_mask_from_input_mask(from_tensor, to_mask):
-    """Create 3D attention mask from a 2D tensor mask.
-
-    Args:
-        from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].
-        to_mask: int32 Tensor of shape [batch_size, to_seq_length].
-
-    Returns:
-        float Tensor of shape [batch_size, from_seq_length, to_seq_length].
-    """
-    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
-    batch_size = from_shape[0]
-    from_seq_length = from_shape[1]
-
-    to_shape = get_shape_list(to_mask, expected_rank=2)
-    to_seq_length = to_shape[1]
-
-    to_mask = tf.cast(
-        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)
-
-    # We don't assume that `from_tensor` is a mask (although it could be). We
-    # don't actually care if we attend *from* padding tokens (only *to* padding)
-    # tokens so we create a tensor of all ones.
-    #
-    # `broadcast_ones` = [batch_size, from_seq_length, 1]
-    broadcast_ones = tf.ones(
-        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)
-
-    # Here we broadcast along two dimensions to create the mask.
-    mask = broadcast_ones * to_mask
-
-    return mask
-
-
-def attention_layer(from_tensor,
-                    to_tensor,
-                    attention_mask=None,
-                    num_attention_heads=1,
-                    size_per_head=512,
-                    query_act=None,
-                    key_act=None,
-                    value_act=None,
-                    attention_probs_dropout_prob=0.0,
-                    initializer_range=0.02,
-                    do_return_2d_tensor=False,
-                    batch_size=None,
-                    from_seq_length=None,
-                    to_seq_length=None):
-    """Performs multi-headed attention from `from_tensor` to `to_tensor`.
-
-    This is an implementation of multi-headed attention based on "Attention
-    is all you Need". If `from_tensor` and `to_tensor` are the same, then
-    this is self-attention. Each timestep in `from_tensor` attends to the
-    corresponding sequence in `to_tensor`, and returns a fixed-with vector.
-
-    This function first projects `from_tensor` into a "query" tensor and
-    `to_tensor` into "key" and "value" tensors. These are (effectively) a list
-    of tensors of length `num_attention_heads`, where each tensor is of shape
-    [batch_size, seq_length, size_per_head].
-
-    Then, the query and key tensors are dot-producted and scaled. These are
-    softmaxed to obtain attention probabilities. The value tensors are then
-    interpolated by these probabilities, then concatenated back to a single
-    tensor and returned.
-
-    In practice, the multi-headed attention are done with transposes and
-    reshapes rather than actual separate tensors.
-
-    Args:
-        from_tensor: float Tensor of shape [batch_size, from_seq_length,
-        from_width].
-        to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].
-        attention_mask: (optional) int32 Tensor of shape [batch_size,
-        from_seq_length, to_seq_length]. The values should be 1 or 0. The
-        attention scores will effectively be set to -infinity for any positions in
-        the mask that are 0, and will be unchanged for positions that are 1.
-        num_attention_heads: int. Number of attention heads.
-        size_per_head: int. Size of each attention head.
-        query_act: (optional) Activation function for the query transform.
-        key_act: (optional) Activation function for the key transform.
-        value_act: (optional) Activation function for the value transform.
-        attention_probs_dropout_prob: (optional) float. Dropout probability of the
-        attention probabilities.
-        initializer_range: float. Range of the weight initializer.
-        do_return_2d_tensor: bool. If True, the output will be of shape [batch_size
-        * from_seq_length, num_attention_heads * size_per_head]. If False, the
-        output will be of shape [batch_size, from_seq_length, num_attention_heads
-        * size_per_head].
-        batch_size: (Optional) int. If the input is 2D, this might be the batch size
-        of the 3D version of the `from_tensor` and `to_tensor`.
-        from_seq_length: (Optional) If the input is 2D, this might be the seq length
-        of the 3D version of the `from_tensor`.
-        to_seq_length: (Optional) If the input is 2D, this might be the seq length
-        of the 3D version of the `to_tensor`.
-
-    Returns:
-        float Tensor of shape [batch_size, from_seq_length,
-        num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is
-        true, this will be of shape [batch_size * from_seq_length,
-        num_attention_heads * size_per_head]).
-
-    Raises:
-        ValueError: Any of the arguments or tensor shapes are invalid.
-    """
-
-    def transpose_for_scores(input_tensor, batch_size, num_attention_heads,
-                             seq_length, width):
-        output_tensor = tf.reshape(
-            input_tensor, [batch_size, seq_length, num_attention_heads, width])
-
-        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])
-        return output_tensor
-
-    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
-    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])
-
-    if len(from_shape) != len(to_shape):
-        raise ValueError(
-            "The rank of `from_tensor` must match the rank of `to_tensor`.")
-
-    if len(from_shape) == 3:
-        batch_size = from_shape[0]
-        from_seq_length = from_shape[1]
-        to_seq_length = to_shape[1]
-    elif len(from_shape) == 2:
-        if (batch_size is None or from_seq_length is None
-                or to_seq_length is None):
-            raise ValueError(
-                "When passing in rank 2 tensors to attention_layer, the values "
-                "for `batch_size`, `from_seq_length`, and `to_seq_length` "
-                "must all be specified.")
-
-    # Scalar dimensions referenced here:
-    #   B = batch size (number of sequences)
-    #   F = `from_tensor` sequence length
-    #   T = `to_tensor` sequence length
-    #   N = `num_attention_heads`
-    #   H = `size_per_head`
-
-    from_tensor_2d = reshape_to_matrix(from_tensor)
-    to_tensor_2d = reshape_to_matrix(to_tensor)
-
-    # `query_layer` = [B*F, N*H]
-    query_layer = tf.layers.dense(
-        from_tensor_2d,
-        num_attention_heads * size_per_head,
-        activation=query_act,
-        name="query",
-        kernel_initializer=create_initializer(initializer_range))
-
-    # `key_layer` = [B*T, N*H]
-    key_layer = tf.layers.dense(
-        to_tensor_2d,
-        num_attention_heads * size_per_head,
-        activation=key_act,
-        name="key",
-        kernel_initializer=create_initializer(initializer_range))
-
-    # `value_layer` = [B*T, N*H]
-    value_layer = tf.layers.dense(
-        to_tensor_2d,
-        num_attention_heads * size_per_head,
-        activation=value_act,
-        name="value",
-        kernel_initializer=create_initializer(initializer_range))
-
-    # `query_layer` = [B, N, F, H]
-    query_layer = transpose_for_scores(query_layer, batch_size,
-                                       num_attention_heads, from_seq_length,
-                                       size_per_head)
-
-    # `key_layer` = [B, N, T, H]
-    key_layer = transpose_for_scores(key_layer, batch_size,
-                                     num_attention_heads, to_seq_length,
-                                     size_per_head)
-
-    # Take the dot product between "query" and "key" to get the raw
-    # attention scores.
-    # `attention_scores` = [B, N, F, T]
-    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
-    attention_scores = tf.multiply(attention_scores,
-                                   1.0 / math.sqrt(float(size_per_head)))
-
-    if attention_mask is not None:
-        # `attention_mask` = [B, 1, F, T]
-        attention_mask = tf.expand_dims(attention_mask, axis=[1])
-
-        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
-        # masked positions, this operation will create a tensor which is 0.0 for
-        # positions we want to attend and -10000.0 for masked positions.
-        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0
-
-        # Since we are adding it to the raw scores before the softmax, this is
-        # effectively the same as removing these entirely.
-        attention_scores += adder
-
-    # Normalize the attention scores to probabilities.
-    # `attention_probs` = [B, N, F, T]
-    attention_probs = tf.nn.softmax(attention_scores)
-
-    # This is actually dropping out entire tokens to attend to, which might
-    # seem a bit unusual, but is taken from the original Transformer paper.
-    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)
-
-    # `value_layer` = [B, T, N, H]
-    value_layer = tf.reshape(
-        value_layer,
-        [batch_size, to_seq_length, num_attention_heads, size_per_head])
-
-    # `value_layer` = [B, N, T, H]
-    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])
-
-    # `context_layer` = [B, N, F, H]
-    context_layer = tf.matmul(attention_probs, value_layer)
-
-    # `context_layer` = [B, F, N, H]
-    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])
-
-    if do_return_2d_tensor:
-        # `context_layer` = [B*F, N*V]
-        context_layer = tf.reshape(context_layer, [
-            batch_size * from_seq_length, num_attention_heads * size_per_head
-        ])
-    else:
-        # `context_layer` = [B, F, N*V]
-        context_layer = tf.reshape(
-            context_layer,
-            [batch_size, from_seq_length, num_attention_heads * size_per_head])
-
-    return context_layer
-
-
-def transformer_model(input_tensor,
-                      attention_mask=None,
-                      hidden_size=768,
-                      num_hidden_layers=12,
-                      num_attention_heads=12,
-                      intermediate_size=3072,
-                      intermediate_act_fn=gelu,
-                      hidden_dropout_prob=0.1,
-                      attention_probs_dropout_prob=0.1,
-                      initializer_range=0.02,
-                      do_return_all_layers=False):
-    """Multi-headed, multi-layer Transformer from "Attention is All You Need".
-
-    This is almost an exact implementation of the original Transformer encoder.
-
-    See the original paper:
-    https://arxiv.org/abs/1706.03762
-
-    Also see:
-    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py
-
-    Args:
-        input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
-        attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
-        seq_length], with 1 for positions that can be attended to and 0 in
-        positions that should not be.
-        hidden_size: int. Hidden size of the Transformer.
-        num_hidden_layers: int. Number of layers (blocks) in the Transformer.
-        num_attention_heads: int. Number of attention heads in the Transformer.
-        intermediate_size: int. The size of the "intermediate" (a.k.a., feed
-        forward) layer.
-        intermediate_act_fn: function. The non-linear activation function to apply
-        to the output of the intermediate/feed-forward layer.
-        hidden_dropout_prob: float. Dropout probability for the hidden layers.
-        attention_probs_dropout_prob: float. Dropout probability of the attention
-        probabilities.
-        initializer_range: float. Range of the initializer (stddev of truncated
-        normal).
-        do_return_all_layers: Whether to also return all layers or just the final
-        layer.
-
-    Returns:
-        float Tensor of shape [batch_size, seq_length, hidden_size], the final
-        hidden layer of the Transformer.
-
-    Raises:
-        ValueError: A Tensor shape or parameter is invalid.
-    """
-    if hidden_size % num_attention_heads != 0:
-        raise ValueError(
-            "The hidden size (%d) is not a multiple of the number of attention "
-            "heads (%d)" % (hidden_size, num_attention_heads))
-
-    attention_head_size = int(hidden_size / num_attention_heads)
-    input_shape = get_shape_list(input_tensor, expected_rank=3)
-    batch_size = input_shape[0]
-    seq_length = input_shape[1]
-    input_width = input_shape[2]
-
-    # The Transformer performs sum residuals on all layers so the input needs
-    # to be the same as the hidden size.
-    if input_width != hidden_size:
-        raise ValueError(
-            "The width of the input tensor (%d) != hidden size (%d)" %
-            (input_width, hidden_size))
-
-    # We keep the representation as a 2D tensor to avoid re-shaping it back and
-    # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on
-    # the GPU/CPU but may not be free on the TPU, so we want to minimize them to
-    # help the optimizer.
-    prev_output = reshape_to_matrix(input_tensor)
-
-    all_layer_outputs = []
-    for layer_idx in range(num_hidden_layers):
-        with tf.variable_scope("layer_%d" % layer_idx):
-            layer_input = prev_output
-
-            with tf.variable_scope("attention"):
-                attention_heads = []
-                with tf.variable_scope("self"):
-                    attention_head = attention_layer(
-                        from_tensor=layer_input,
-                        to_tensor=layer_input,
-                        attention_mask=attention_mask,
-                        num_attention_heads=num_attention_heads,
-                        size_per_head=attention_head_size,
-                        attention_probs_dropout_prob=
-                        attention_probs_dropout_prob,
-                        initializer_range=initializer_range,
-                        do_return_2d_tensor=True,
-                        batch_size=batch_size,
-                        from_seq_length=seq_length,
-                        to_seq_length=seq_length)
-                    attention_heads.append(attention_head)
-
-                attention_output = None
-                if len(attention_heads) == 1:
-                    attention_output = attention_heads[0]
-                else:
-                    # In the case where we have other sequences, we just concatenate
-                    # them to the self-attention head before the projection.
-                    attention_output = tf.concat(attention_heads, axis=-1)
-
-                # Run a linear projection of `hidden_size` then add a residual
-                # with `layer_input`.
-                with tf.variable_scope("output"):
-                    attention_output = tf.layers.dense(
-                        attention_output,
-                        hidden_size,
-                        kernel_initializer=create_initializer(
-                            initializer_range))
-                    attention_output = dropout(attention_output,
-                                               hidden_dropout_prob)
-                    attention_output = layer_norm(attention_output +
-                                                  layer_input)
-
-            # The activation is only applied to the "intermediate" hidden layer.
-            with tf.variable_scope("intermediate"):
-                intermediate_output = tf.layers.dense(
-                    attention_output,
-                    intermediate_size,
-                    activation=intermediate_act_fn,
-                    kernel_initializer=create_initializer(initializer_range))
-
-            # Down-project back to `hidden_size` then add the residual.
-            with tf.variable_scope("output"):
-                layer_output = tf.layers.dense(
-                    intermediate_output,
-                    hidden_size,
-                    kernel_initializer=create_initializer(initializer_range))
-                layer_output = dropout(layer_output, hidden_dropout_prob)
-                layer_output = layer_norm(layer_output + attention_output)
-                prev_output = layer_output
-                all_layer_outputs.append(layer_output)
-
-    if do_return_all_layers:
-        final_outputs = []
-        for layer_output in all_layer_outputs:
-            final_output = reshape_from_matrix(layer_output, input_shape)
-            final_outputs.append(final_output)
-        return final_outputs
-    else:
-        final_output = reshape_from_matrix(prev_output, input_shape)
-        return final_output
-
-
-def get_shape_list(tensor, expected_rank=None, name=None):
-    """Returns a list of the shape of tensor, preferring static dimensions.
-
-    Args:
-        tensor: A tf.Tensor object to find the shape of.
-        expected_rank: (optional) int. The expected rank of `tensor`. If this is
-        specified and the `tensor` has a different rank, and exception will be
-        thrown.
-        name: Optional name of the tensor for the error message.
-
-    Returns:
-        A list of dimensions of the shape of tensor. All static dimensions will
-        be returned as python integers, and dynamic dimensions will be returned
-        as tf.Tensor scalars.
-    """
-    if name is None:
-        name = tensor.name
-
-    if expected_rank is not None:
-        assert_rank(tensor, expected_rank, name)
-
-    shape = tensor.shape.as_list()
-
-    non_static_indexes = []
-    for (index, dim) in enumerate(shape):
-        if dim is None:
-            non_static_indexes.append(index)
-
-    if not non_static_indexes:
-        return shape
-
-    dyn_shape = tf.shape(tensor)
-    for index in non_static_indexes:
-        shape[index] = dyn_shape[index]
-    return shape
-
-
-def reshape_to_matrix(input_tensor):
-    """Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix)."""
-    ndims = input_tensor.shape.ndims
-    if ndims < 2:
-        raise ValueError("Input tensor must have at least rank 2. Shape = %s" %
-                         (input_tensor.shape))
-    if ndims == 2:
-        return input_tensor
-
-    width = input_tensor.shape[-1]
-    output_tensor = tf.reshape(input_tensor, [-1, width])
-    return output_tensor
-
-
-def reshape_from_matrix(output_tensor, orig_shape_list):
-    """Reshapes a rank 2 tensor back to its original rank >= 2 tensor."""
-    if len(orig_shape_list) == 2:
-        return output_tensor
-
-    output_shape = get_shape_list(output_tensor)
-
-    orig_dims = orig_shape_list[0:-1]
-    width = output_shape[-1]
-
-    return tf.reshape(output_tensor, orig_dims + [width])
-
-
-def assert_rank(tensor, expected_rank, name=None):
-    """Raises an exception if the tensor rank is not of the expected rank.
-
-    Args:
-        tensor: A tf.Tensor to check the rank of.
-        expected_rank: Python integer or list of integers, expected rank.
-        name: Optional name of the tensor for the error message.
-
-    Raises:
-        ValueError: If the expected shape doesn't match the actual shape.
-    """
-    if name is None:
-        name = tensor.name
-
-    expected_rank_dict = {}
-    if isinstance(expected_rank, six.integer_types):
-        expected_rank_dict[expected_rank] = True
-    else:
-        for x in expected_rank:
-            expected_rank_dict[x] = True
-
-    actual_rank = tensor.shape.ndims
-    if actual_rank not in expected_rank_dict:
-        scope_name = tf.get_variable_scope().name
-        raise ValueError(
-            "For the tensor `%s` in scope `%s`, the actual rank "
-            "`%d` (shape = %s) is not equal to the expected rank `%s`" %
-            (name, scope_name, actual_rank, str(tensor.shape),
-             str(expected_rank)))
+# coding=utf-8
+
+"""The main BERT model and related functions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import copy
+import json
+import math
+import re
+import six
+import tensorflow as tf
+
+
+class BertConfig(object):
+    """Configuration for `BertModel`."""
+
+    def __init__(self,
+                 vocab_size,
+                 hidden_size=768,
+                 num_hidden_layers=12,
+                 num_attention_heads=12,
+                 intermediate_size=3072,
+                 hidden_act="gelu",
+                 hidden_dropout_prob=0.1,
+                 attention_probs_dropout_prob=0.1,
+                 max_position_embeddings=512,
+                 type_vocab_size=16,
+                 initializer_range=0.02):
+        """Constructs BertConfig.
+
+        Args:
+        vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.
+        hidden_size: Size of the encoder layers and the pooler layer.
+        num_hidden_layers: Number of hidden layers in the Transformer encoder.
+        num_attention_heads: Number of attention heads for each attention layer in
+            the Transformer encoder.
+        intermediate_size: The size of the "intermediate" (i.e., feed-forward)
+            layer in the Transformer encoder.
+        hidden_act: The non-linear activation function (function or string) in the
+            encoder and pooler.
+        hidden_dropout_prob: The dropout probability for all fully connected
+            layers in the embeddings, encoder, and pooler.
+        attention_probs_dropout_prob: The dropout ratio for the attention
+            probabilities.
+        max_position_embeddings: The maximum sequence length that this model might
+            ever be used with. Typically set this to something large just in case
+            (e.g., 512 or 1024 or 2048).
+        type_vocab_size: The vocabulary size of the `token_type_ids` passed into
+            `BertModel`.
+        initializer_range: The stdev of the truncated_normal_initializer for
+            initializing all weight matrices.
+        """
+        self.vocab_size = vocab_size
+        self.hidden_size = hidden_size
+        self.num_hidden_layers = num_hidden_layers
+        self.num_attention_heads = num_attention_heads
+        self.hidden_act = hidden_act
+        self.intermediate_size = intermediate_size
+        self.hidden_dropout_prob = hidden_dropout_prob
+        self.attention_probs_dropout_prob = attention_probs_dropout_prob
+        self.max_position_embeddings = max_position_embeddings
+        self.type_vocab_size = type_vocab_size
+        self.initializer_range = initializer_range
+
+    @classmethod
+    def from_dict(cls, json_object):
+        """Constructs a `BertConfig` from a Python dictionary of parameters."""
+        config = BertConfig(vocab_size=None)
+        for (key, value) in six.iteritems(json_object):
+            config.__dict__[key] = value
+        return config
+
+    @classmethod
+    def from_json_file(cls, json_file):
+        """Constructs a `BertConfig` from a json file of parameters."""
+        with tf.gfile.GFile(json_file, "r") as reader:
+            text = reader.read()
+        return cls.from_dict(json.loads(text))
+
+    def to_dict(self):
+        """Serializes this instance to a Python dictionary."""
+        output = copy.deepcopy(self.__dict__)
+        return output
+
+    def to_json_string(self):
+        """Serializes this instance to a JSON string."""
+        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + "\n"
+
+
+class BertModel(object):
+    """BERT model ("Bidirectional Embedding Representations from a Transformer").
+
+    Example usage:
+
+    ```python
+    # Already been converted into WordPiece token ids
+    input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])
+    input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
+    token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])
+
+    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,
+        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)
+
+    model = modeling.BertModel(config=config, is_training=True,
+        input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)
+
+    label_embeddings = tf.get_variable(...)
+    logits = tf.matmul(pooled_output, label_embeddings)
+    ...
+    ```
+    """
+    def __init__(self,
+                 config,
+                 is_training,
+                 input_ids,
+                 input_mask=None,
+                 token_type_ids=None,
+                 use_one_hot_embeddings=True,
+                 scope=None):
+        """Constructor for BertModel.
+
+        Args:
+        config: `BertConfig` instance.
+        is_training: bool. rue for training model, false for eval model. Controls
+            whether dropout will be applied.
+        input_ids: int32 Tensor of shape [batch_size, seq_length].
+        input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].
+        token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
+        use_one_hot_embeddings: (optional) bool. Whether to use one-hot word
+            embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,
+            it is must faster if this is True, on the CPU or GPU, it is faster if
+            this is False.
+        scope: (optional) variable scope. Defaults to "bert".
+
+        Raises:
+        ValueError: The config is invalid or one of the input tensor shapes
+            is invalid.
+        """
+        config = copy.deepcopy(config)
+        if not is_training:
+            config.hidden_dropout_prob = 0.0
+            config.attention_probs_dropout_prob = 0.0
+
+        input_shape = get_shape_list(input_ids, expected_rank=2)
+        batch_size = input_shape[0]
+        seq_length = input_shape[1]
+
+        if input_mask is None:
+            input_mask = tf.ones(
+                shape=[batch_size, seq_length], dtype=tf.int32)
+
+        if token_type_ids is None:
+            token_type_ids = tf.zeros(
+                shape=[batch_size, seq_length], dtype=tf.int32)
+
+        with tf.variable_scope(scope, default_name="bert"):
+            with tf.variable_scope("embeddings"):
+                # Perform embedding lookup on the word ids.
+                (self.embedding_output,
+                 self.embedding_table) = embedding_lookup(
+                     input_ids=input_ids,
+                     vocab_size=config.vocab_size,
+                     embedding_size=config.hidden_size,
+                     initializer_range=config.initializer_range,
+                     word_embedding_name="word_embeddings",
+                     use_one_hot_embeddings=use_one_hot_embeddings)
+
+                # Add positional embeddings and token type embeddings, then layer
+                # normalize and perform dropout.
+                self.embedding_output = embedding_postprocessor(
+                    input_tensor=self.embedding_output,
+                    use_token_type=True,
+                    token_type_ids=token_type_ids,
+                    token_type_vocab_size=config.type_vocab_size,
+                    token_type_embedding_name="token_type_embeddings",
+                    use_position_embeddings=True,
+                    position_embedding_name="position_embeddings",
+                    initializer_range=config.initializer_range,
+                    max_position_embeddings=config.max_position_embeddings,
+                    dropout_prob=config.hidden_dropout_prob)
+
+            with tf.variable_scope("encoder"):
+                # This converts a 2D mask of shape [batch_size, seq_length] to a 3D
+                # mask of shape [batch_size, seq_length, seq_length] which is used
+                # for the attention scores.
+                attention_mask = create_attention_mask_from_input_mask(
+                    input_ids, input_mask)
+
+                # Run the stacked transformer.
+                # `sequence_output` shape = [batch_size, seq_length, hidden_size].
+                self.all_encoder_layers = transformer_model(
+                    input_tensor=self.embedding_output,
+                    attention_mask=attention_mask,
+                    hidden_size=config.hidden_size,
+                    num_hidden_layers=config.num_hidden_layers,
+                    num_attention_heads=config.num_attention_heads,
+                    intermediate_size=config.intermediate_size,
+                    intermediate_act_fn=get_activation(config.hidden_act),
+                    hidden_dropout_prob=config.hidden_dropout_prob,
+                    attention_probs_dropout_prob=config.
+                    attention_probs_dropout_prob,
+                    initializer_range=config.initializer_range,
+                    do_return_all_layers=True)
+
+            self.sequence_output = self.all_encoder_layers[-1]
+            # The "pooler" converts the encoded sequence tensor of shape
+            # [batch_size, seq_length, hidden_size] to a tensor of shape
+            # [batch_size, hidden_size]. This is necessary for segment-level
+            # (or segment-pair-level) classification tasks where we need a fixed
+            # dimensional representation of the segment.
+            # with tf.variable_scope("pooler"):
+            #     # We "pool" the model by simply taking the hidden state corresponding
+            #     # to the first token. We assume that this has been pre-trained
+            #     first_token_tensor = tf.squeeze(
+            #         self.sequence_output[:, 0:1, :], axis=1)
+            #     self.pooled_output = tf.layers.dense(
+            #         first_token_tensor,
+            #         config.hidden_size,
+            #         activation=tf.tanh,
+            #         kernel_initializer=create_initializer(
+            #             config.initializer_range))
+
+    def get_pooled_output(self):
+        return self.pooled_output
+
+    def get_sequence_output(self):
+        """Gets final hidden layer of encoder.
+
+        Returns:
+        float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
+        to the final hidden of the transformer encoder.
+        """
+        return self.sequence_output
+
+    def get_all_encoder_layers(self):
+        return self.all_encoder_layers
+
+    def get_embedding_output(self):
+        """Gets output of the embedding lookup (i.e., input to the transformer).
+
+        Returns:
+        float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
+        to the output of the embedding layer, after summing the word
+        embeddings with the positional embeddings and the token type embeddings,
+        then performing layer normalization. This is the input to the transformer.
+        """
+        return self.embedding_output
+
+    def get_embedding_table(self):
+        return self.embedding_table
+
+
+def gelu(input_tensor):
+    """Gaussian Error Linear Unit.
+
+    This is a smoother version of the RELU.
+    Original paper: https://arxiv.org/abs/1606.08415
+
+    Args:
+        input_tensor: float Tensor to perform activation.
+
+    Returns:
+        `input_tensor` with the GELU activation applied.
+    """
+    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))
+    return input_tensor * cdf
+
+
+def get_activation(activation_string):
+    """Maps a string to a Python function, e.g., "relu" => `tf.nn.relu`.
+
+    Args:
+        activation_string: String name of the activation function.
+
+    Returns:
+        A Python function corresponding to the activation function. If
+        `activation_string` is None, empty, or "linear", this will return None.
+        If `activation_string` is not a string, it will return `activation_string`.
+
+    Raises:
+        ValueError: The `activation_string` does not correspond to a known
+        activation.
+    """
+
+    # We assume that anything that"s not a string is already an activation
+    # function, so we just return it.
+    if not isinstance(activation_string, six.string_types):
+        return activation_string
+
+    if not activation_string:
+        return None
+
+    act = activation_string.lower()
+    if act == "linear":
+        return None
+    elif act == "relu":
+        return tf.nn.relu
+    elif act == "gelu":
+        return gelu
+    elif act == "tanh":
+        return tf.tanh
+    else:
+        raise ValueError("Unsupported activation: %s" % act)
+
+
+def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
+    """Compute the union of the current variables and checkpoint variables."""
+    assignment_map = {}
+    initialized_variable_names = {}
+
+    name_to_variable = collections.OrderedDict()
+    for var in tvars:
+        name = var.name
+        m = re.match("^(.*):\\d+$", name)
+        if m is not None:
+            name = m.group(1)
+        name_to_variable[name] = var
+
+    init_vars = tf.train.list_variables(init_checkpoint)
+
+    assignment_map = collections.OrderedDict()
+    for x in init_vars:
+        (name, var) = (x[0], x[1])
+        if name not in name_to_variable:
+            continue
+        assignment_map[name] = name
+        initialized_variable_names[name] = 1
+        initialized_variable_names[name + ":0"] = 1
+
+    return (assignment_map, initialized_variable_names)
+
+
+def dropout(input_tensor, dropout_prob):
+    """Perform dropout.
+
+    Args:
+        input_tensor: float Tensor.
+        dropout_prob: Python float. The probability of dropping out a value (NOT of
+        *keeping* a dimension as in `tf.nn.dropout`).
+
+    Returns:
+        A version of `input_tensor` with dropout applied.
+    """
+    if dropout_prob is None or dropout_prob == 0.0:
+        return input_tensor
+
+    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)
+    return output
+
+
+def layer_norm(input_tensor, name=None):
+    """Run layer normalization on the last dimension of the tensor."""
+    return tf.contrib.layers.layer_norm(
+        inputs=input_tensor,
+        begin_norm_axis=-1,
+        begin_params_axis=-1,
+        scope=name)
+
+
+def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):
+    """Runs layer normalization followed by dropout."""
+    output_tensor = layer_norm(input_tensor, name)
+    output_tensor = dropout(output_tensor, dropout_prob)
+    return output_tensor
+
+
+def create_initializer(initializer_range=0.02):
+    """Creates a `truncated_normal_initializer` with the given range."""
+    return tf.truncated_normal_initializer(stddev=initializer_range)
+
+
+def embedding_lookup(input_ids,
+                     vocab_size,
+                     embedding_size=128,
+                     initializer_range=0.02,
+                     word_embedding_name="word_embeddings",
+                     use_one_hot_embeddings=False):
+    """Looks up words embeddings for id tensor.
+
+    Args:
+        input_ids: int32 Tensor of shape [batch_size, seq_length] containing word
+        ids.
+        vocab_size: int. Size of the embedding vocabulary.
+        embedding_size: int. Width of the word embeddings.
+        initializer_range: float. Embedding initialization range.
+        word_embedding_name: string. Name of the embedding table.
+        use_one_hot_embeddings: bool. If True, use one-hot method for word
+        embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better
+        for TPUs.
+
+    Returns:
+        float Tensor of shape [batch_size, seq_length, embedding_size].
+    """
+    # This function assumes that the input is of shape [batch_size, seq_length,
+    # num_inputs].
+    #
+    # If the input is a 2D tensor of shape [batch_size, seq_length], we
+    # reshape to [batch_size, seq_length, 1].
+    if input_ids.shape.ndims == 2:
+        input_ids = tf.expand_dims(input_ids, axis=[-1])
+
+    embedding_table = tf.get_variable(
+        name=word_embedding_name,
+        shape=[vocab_size, embedding_size],
+        initializer=create_initializer(initializer_range))
+
+    if use_one_hot_embeddings:
+        flat_input_ids = tf.reshape(input_ids, [-1])
+        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)
+        output = tf.matmul(one_hot_input_ids, embedding_table)
+    else:
+        output = tf.nn.embedding_lookup(embedding_table, input_ids)
+
+    input_shape = get_shape_list(input_ids)
+
+    output = tf.reshape(output,
+                        input_shape[0:-1] + [input_shape[-1] * embedding_size])
+    return (output, embedding_table)
+
+
+def embedding_postprocessor(input_tensor,
+                            use_token_type=False,
+                            token_type_ids=None,
+                            token_type_vocab_size=16,
+                            token_type_embedding_name="token_type_embeddings",
+                            use_position_embeddings=True,
+                            position_embedding_name="position_embeddings",
+                            initializer_range=0.02,
+                            max_position_embeddings=512,
+                            dropout_prob=0.1):
+    """Performs various post-processing on a word embedding tensor.
+
+    Args:
+        input_tensor: float Tensor of shape [batch_size, seq_length,
+        embedding_size].
+        use_token_type: bool. Whether to add embeddings for `token_type_ids`.
+        token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
+        Must be specified if `use_token_type` is True.
+        token_type_vocab_size: int. The vocabulary size of `token_type_ids`.
+        token_type_embedding_name: string. The name of the embedding table variable
+        for token type ids.
+        use_position_embeddings: bool. Whether to add position embeddings for the
+        position of each token in the sequence.
+        position_embedding_name: string. The name of the embedding table variable
+        for positional embeddings.
+        initializer_range: float. Range of the weight initialization.
+        max_position_embeddings: int. Maximum sequence length that might ever be
+        used with this model. This can be longer than the sequence length of
+        input_tensor, but cannot be shorter.
+        dropout_prob: float. Dropout probability applied to the final output tensor.
+
+    Returns:
+        float tensor with same shape as `input_tensor`.
+
+    Raises:
+        ValueError: One of the tensor shapes or input values is invalid.
+    """
+    input_shape = get_shape_list(input_tensor, expected_rank=3)
+    batch_size = input_shape[0]
+    seq_length = input_shape[1]
+    width = input_shape[2]
+
+    output = input_tensor
+
+    if use_token_type:
+        if token_type_ids is None:
+            raise ValueError("`token_type_ids` must be specified if"
+                             "`use_token_type` is True.")
+        token_type_table = tf.get_variable(
+            name=token_type_embedding_name,
+            shape=[token_type_vocab_size, width],
+            initializer=create_initializer(initializer_range))
+        # This vocab will be small so we always do one-hot here, since it is always
+        # faster for a small vocabulary.
+        flat_token_type_ids = tf.reshape(token_type_ids, [-1])
+        one_hot_ids = tf.one_hot(
+            flat_token_type_ids, depth=token_type_vocab_size)
+        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
+        token_type_embeddings = tf.reshape(token_type_embeddings,
+                                           [batch_size, seq_length, width])
+        output += token_type_embeddings
+
+    if use_position_embeddings:
+        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)
+        with tf.control_dependencies([assert_op]):
+            full_position_embeddings = tf.get_variable(
+                name=position_embedding_name,
+                shape=[max_position_embeddings, width],
+                initializer=create_initializer(initializer_range))
+            # Since the position embedding table is a learned variable, we create it
+            # using a (long) sequence length `max_position_embeddings`. The actual
+            # sequence length might be shorter than this, for faster training of
+            # tasks that do not have long sequences.
+            #
+            # So `full_position_embeddings` is effectively an embedding table
+            # for position [0, 1, 2, ..., max_position_embeddings-1], and the current
+            # sequence has positions [0, 1, 2, ... seq_length-1], so we can just
+            # perform a slice.
+            position_embeddings = tf.slice(full_position_embeddings, [0, 0],
+                                           [seq_length, -1])
+            num_dims = len(output.shape.as_list())
+
+            # Only the last two dimensions are relevant (`seq_length` and `width`), so
+            # we broadcast among the first dimensions, which is typically just
+            # the batch size.
+            position_broadcast_shape = []
+            for _ in range(num_dims - 2):
+                position_broadcast_shape.append(1)
+            position_broadcast_shape.extend([seq_length, width])
+            position_embeddings = tf.reshape(position_embeddings,
+                                             position_broadcast_shape)
+            output += position_embeddings
+
+    output = layer_norm_and_dropout(output, dropout_prob)
+    return output
+
+
+def create_attention_mask_from_input_mask(from_tensor, to_mask):
+    """Create 3D attention mask from a 2D tensor mask.
+
+    Args:
+        from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].
+        to_mask: int32 Tensor of shape [batch_size, to_seq_length].
+
+    Returns:
+        float Tensor of shape [batch_size, from_seq_length, to_seq_length].
+    """
+    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
+    batch_size = from_shape[0]
+    from_seq_length = from_shape[1]
+
+    to_shape = get_shape_list(to_mask, expected_rank=2)
+    to_seq_length = to_shape[1]
+
+    to_mask = tf.cast(
+        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)
+
+    # We don't assume that `from_tensor` is a mask (although it could be). We
+    # don't actually care if we attend *from* padding tokens (only *to* padding)
+    # tokens so we create a tensor of all ones.
+    #
+    # `broadcast_ones` = [batch_size, from_seq_length, 1]
+    broadcast_ones = tf.ones(
+        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)
+
+    # Here we broadcast along two dimensions to create the mask.
+    mask = broadcast_ones * to_mask
+
+    return mask
+
+
+def attention_layer(from_tensor,
+                    to_tensor,
+                    attention_mask=None,
+                    num_attention_heads=1,
+                    size_per_head=512,
+                    query_act=None,
+                    key_act=None,
+                    value_act=None,
+                    attention_probs_dropout_prob=0.0,
+                    initializer_range=0.02,
+                    do_return_2d_tensor=False,
+                    batch_size=None,
+                    from_seq_length=None,
+                    to_seq_length=None):
+    """Performs multi-headed attention from `from_tensor` to `to_tensor`.
+
+    This is an implementation of multi-headed attention based on "Attention
+    is all you Need". If `from_tensor` and `to_tensor` are the same, then
+    this is self-attention. Each timestep in `from_tensor` attends to the
+    corresponding sequence in `to_tensor`, and returns a fixed-with vector.
+
+    This function first projects `from_tensor` into a "query" tensor and
+    `to_tensor` into "key" and "value" tensors. These are (effectively) a list
+    of tensors of length `num_attention_heads`, where each tensor is of shape
+    [batch_size, seq_length, size_per_head].
+
+    Then, the query and key tensors are dot-producted and scaled. These are
+    softmaxed to obtain attention probabilities. The value tensors are then
+    interpolated by these probabilities, then concatenated back to a single
+    tensor and returned.
+
+    In practice, the multi-headed attention are done with transposes and
+    reshapes rather than actual separate tensors.
+
+    Args:
+        from_tensor: float Tensor of shape [batch_size, from_seq_length,
+        from_width].
+        to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].
+        attention_mask: (optional) int32 Tensor of shape [batch_size,
+        from_seq_length, to_seq_length]. The values should be 1 or 0. The
+        attention scores will effectively be set to -infinity for any positions in
+        the mask that are 0, and will be unchanged for positions that are 1.
+        num_attention_heads: int. Number of attention heads.
+        size_per_head: int. Size of each attention head.
+        query_act: (optional) Activation function for the query transform.
+        key_act: (optional) Activation function for the key transform.
+        value_act: (optional) Activation function for the value transform.
+        attention_probs_dropout_prob: (optional) float. Dropout probability of the
+        attention probabilities.
+        initializer_range: float. Range of the weight initializer.
+        do_return_2d_tensor: bool. If True, the output will be of shape [batch_size
+        * from_seq_length, num_attention_heads * size_per_head]. If False, the
+        output will be of shape [batch_size, from_seq_length, num_attention_heads
+        * size_per_head].
+        batch_size: (Optional) int. If the input is 2D, this might be the batch size
+        of the 3D version of the `from_tensor` and `to_tensor`.
+        from_seq_length: (Optional) If the input is 2D, this might be the seq length
+        of the 3D version of the `from_tensor`.
+        to_seq_length: (Optional) If the input is 2D, this might be the seq length
+        of the 3D version of the `to_tensor`.
+
+    Returns:
+        float Tensor of shape [batch_size, from_seq_length,
+        num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is
+        true, this will be of shape [batch_size * from_seq_length,
+        num_attention_heads * size_per_head]).
+
+    Raises:
+        ValueError: Any of the arguments or tensor shapes are invalid.
+    """
+
+    def transpose_for_scores(input_tensor, batch_size, num_attention_heads,
+                             seq_length, width):
+        output_tensor = tf.reshape(
+            input_tensor, [batch_size, seq_length, num_attention_heads, width])
+
+        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])
+        return output_tensor
+
+    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
+    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])
+
+    if len(from_shape) != len(to_shape):
+        raise ValueError(
+            "The rank of `from_tensor` must match the rank of `to_tensor`.")
+
+    if len(from_shape) == 3:
+        batch_size = from_shape[0]
+        from_seq_length = from_shape[1]
+        to_seq_length = to_shape[1]
+    elif len(from_shape) == 2:
+        if (batch_size is None or from_seq_length is None
+                or to_seq_length is None):
+            raise ValueError(
+                "When passing in rank 2 tensors to attention_layer, the values "
+                "for `batch_size`, `from_seq_length`, and `to_seq_length` "
+                "must all be specified.")
+
+    # Scalar dimensions referenced here:
+    #   B = batch size (number of sequences)
+    #   F = `from_tensor` sequence length
+    #   T = `to_tensor` sequence length
+    #   N = `num_attention_heads`
+    #   H = `size_per_head`
+
+    from_tensor_2d = reshape_to_matrix(from_tensor)
+    to_tensor_2d = reshape_to_matrix(to_tensor)
+
+    # `query_layer` = [B*F, N*H]
+    query_layer = tf.layers.dense(
+        from_tensor_2d,
+        num_attention_heads * size_per_head,
+        activation=query_act,
+        name="query",
+        kernel_initializer=create_initializer(initializer_range))
+
+    # `key_layer` = [B*T, N*H]
+    key_layer = tf.layers.dense(
+        to_tensor_2d,
+        num_attention_heads * size_per_head,
+        activation=key_act,
+        name="key",
+        kernel_initializer=create_initializer(initializer_range))
+
+    # `value_layer` = [B*T, N*H]
+    value_layer = tf.layers.dense(
+        to_tensor_2d,
+        num_attention_heads * size_per_head,
+        activation=value_act,
+        name="value",
+        kernel_initializer=create_initializer(initializer_range))
+
+    # `query_layer` = [B, N, F, H]
+    query_layer = transpose_for_scores(query_layer, batch_size,
+                                       num_attention_heads, from_seq_length,
+                                       size_per_head)
+
+    # `key_layer` = [B, N, T, H]
+    key_layer = transpose_for_scores(key_layer, batch_size,
+                                     num_attention_heads, to_seq_length,
+                                     size_per_head)
+
+    # Take the dot product between "query" and "key" to get the raw
+    # attention scores.
+    # `attention_scores` = [B, N, F, T]
+    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
+    attention_scores = tf.multiply(attention_scores,
+                                   1.0 / math.sqrt(float(size_per_head)))
+
+    if attention_mask is not None:
+        # `attention_mask` = [B, 1, F, T]
+        attention_mask = tf.expand_dims(attention_mask, axis=[1])
+
+        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+        # masked positions, this operation will create a tensor which is 0.0 for
+        # positions we want to attend and -10000.0 for masked positions.
+        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0
+
+        # Since we are adding it to the raw scores before the softmax, this is
+        # effectively the same as removing these entirely.
+        attention_scores += adder
+
+    # Normalize the attention scores to probabilities.
+    # `attention_probs` = [B, N, F, T]
+    attention_probs = tf.nn.softmax(attention_scores)
+
+    # This is actually dropping out entire tokens to attend to, which might
+    # seem a bit unusual, but is taken from the original Transformer paper.
+    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)
+
+    # `value_layer` = [B, T, N, H]
+    value_layer = tf.reshape(
+        value_layer,
+        [batch_size, to_seq_length, num_attention_heads, size_per_head])
+
+    # `value_layer` = [B, N, T, H]
+    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])
+
+    # `context_layer` = [B, N, F, H]
+    context_layer = tf.matmul(attention_probs, value_layer)
+
+    # `context_layer` = [B, F, N, H]
+    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])
+
+    if do_return_2d_tensor:
+        # `context_layer` = [B*F, N*V]
+        context_layer = tf.reshape(context_layer, [
+            batch_size * from_seq_length, num_attention_heads * size_per_head
+        ])
+    else:
+        # `context_layer` = [B, F, N*V]
+        context_layer = tf.reshape(
+            context_layer,
+            [batch_size, from_seq_length, num_attention_heads * size_per_head])
+
+    return context_layer
+
+
+def transformer_model(input_tensor,
+                      attention_mask=None,
+                      hidden_size=768,
+                      num_hidden_layers=12,
+                      num_attention_heads=12,
+                      intermediate_size=3072,
+                      intermediate_act_fn=gelu,
+                      hidden_dropout_prob=0.1,
+                      attention_probs_dropout_prob=0.1,
+                      initializer_range=0.02,
+                      do_return_all_layers=False):
+    """Multi-headed, multi-layer Transformer from "Attention is All You Need".
+
+    This is almost an exact implementation of the original Transformer encoder.
+
+    See the original paper:
+    https://arxiv.org/abs/1706.03762
+
+    Also see:
+    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py
+
+    Args:
+        input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
+        attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
+        seq_length], with 1 for positions that can be attended to and 0 in
+        positions that should not be.
+        hidden_size: int. Hidden size of the Transformer.
+        num_hidden_layers: int. Number of layers (blocks) in the Transformer.
+        num_attention_heads: int. Number of attention heads in the Transformer.
+        intermediate_size: int. The size of the "intermediate" (a.k.a., feed
+        forward) layer.
+        intermediate_act_fn: function. The non-linear activation function to apply
+        to the output of the intermediate/feed-forward layer.
+        hidden_dropout_prob: float. Dropout probability for the hidden layers.
+        attention_probs_dropout_prob: float. Dropout probability of the attention
+        probabilities.
+        initializer_range: float. Range of the initializer (stddev of truncated
+        normal).
+        do_return_all_layers: Whether to also return all layers or just the final
+        layer.
+
+    Returns:
+        float Tensor of shape [batch_size, seq_length, hidden_size], the final
+        hidden layer of the Transformer.
+
+    Raises:
+        ValueError: A Tensor shape or parameter is invalid.
+    """
+    if hidden_size % num_attention_heads != 0:
+        raise ValueError(
+            "The hidden size (%d) is not a multiple of the number of attention "
+            "heads (%d)" % (hidden_size, num_attention_heads))
+
+    attention_head_size = int(hidden_size / num_attention_heads)
+    input_shape = get_shape_list(input_tensor, expected_rank=3)
+    batch_size = input_shape[0]
+    seq_length = input_shape[1]
+    input_width = input_shape[2]
+
+    # The Transformer performs sum residuals on all layers so the input needs
+    # to be the same as the hidden size.
+    if input_width != hidden_size:
+        raise ValueError(
+            "The width of the input tensor (%d) != hidden size (%d)" %
+            (input_width, hidden_size))
+
+    # We keep the representation as a 2D tensor to avoid re-shaping it back and
+    # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on
+    # the GPU/CPU but may not be free on the TPU, so we want to minimize them to
+    # help the optimizer.
+    prev_output = reshape_to_matrix(input_tensor)
+
+    all_layer_outputs = []
+    for layer_idx in range(num_hidden_layers):
+        with tf.variable_scope("layer_%d" % layer_idx):
+            layer_input = prev_output
+
+            with tf.variable_scope("attention"):
+                attention_heads = []
+                with tf.variable_scope("self"):
+                    attention_head = attention_layer(
+                        from_tensor=layer_input,
+                        to_tensor=layer_input,
+                        attention_mask=attention_mask,
+                        num_attention_heads=num_attention_heads,
+                        size_per_head=attention_head_size,
+                        attention_probs_dropout_prob=
+                        attention_probs_dropout_prob,
+                        initializer_range=initializer_range,
+                        do_return_2d_tensor=True,
+                        batch_size=batch_size,
+                        from_seq_length=seq_length,
+                        to_seq_length=seq_length)
+                    attention_heads.append(attention_head)
+
+                attention_output = None
+                if len(attention_heads) == 1:
+                    attention_output = attention_heads[0]
+                else:
+                    # In the case where we have other sequences, we just concatenate
+                    # them to the self-attention head before the projection.
+                    attention_output = tf.concat(attention_heads, axis=-1)
+
+                # Run a linear projection of `hidden_size` then add a residual
+                # with `layer_input`.
+                with tf.variable_scope("output"):
+                    attention_output = tf.layers.dense(
+                        attention_output,
+                        hidden_size,
+                        kernel_initializer=create_initializer(
+                            initializer_range))
+                    attention_output = dropout(attention_output,
+                                               hidden_dropout_prob)
+                    attention_output = layer_norm(attention_output +
+                                                  layer_input)
+
+            # The activation is only applied to the "intermediate" hidden layer.
+            with tf.variable_scope("intermediate"):
+                intermediate_output = tf.layers.dense(
+                    attention_output,
+                    intermediate_size,
+                    activation=intermediate_act_fn,
+                    kernel_initializer=create_initializer(initializer_range))
+
+            # Down-project back to `hidden_size` then add the residual.
+            with tf.variable_scope("output"):
+                layer_output = tf.layers.dense(
+                    intermediate_output,
+                    hidden_size,
+                    kernel_initializer=create_initializer(initializer_range))
+                layer_output = dropout(layer_output, hidden_dropout_prob)
+                layer_output = layer_norm(layer_output + attention_output)
+                prev_output = layer_output
+                all_layer_outputs.append(layer_output)
+
+    if do_return_all_layers:
+        final_outputs = []
+        for layer_output in all_layer_outputs:
+            final_output = reshape_from_matrix(layer_output, input_shape)
+            final_outputs.append(final_output)
+        return final_outputs
+    else:
+        final_output = reshape_from_matrix(prev_output, input_shape)
+        return final_output
+
+
+def get_shape_list(tensor, expected_rank=None, name=None):
+    """Returns a list of the shape of tensor, preferring static dimensions.
+
+    Args:
+        tensor: A tf.Tensor object to find the shape of.
+        expected_rank: (optional) int. The expected rank of `tensor`. If this is
+        specified and the `tensor` has a different rank, and exception will be
+        thrown.
+        name: Optional name of the tensor for the error message.
+
+    Returns:
+        A list of dimensions of the shape of tensor. All static dimensions will
+        be returned as python integers, and dynamic dimensions will be returned
+        as tf.Tensor scalars.
+    """
+    if name is None:
+        name = tensor.name
+
+    if expected_rank is not None:
+        assert_rank(tensor, expected_rank, name)
+
+    shape = tensor.shape.as_list()
+
+    non_static_indexes = []
+    for (index, dim) in enumerate(shape):
+        if dim is None:
+            non_static_indexes.append(index)
+
+    if not non_static_indexes:
+        return shape
+
+    dyn_shape = tf.shape(tensor)
+    for index in non_static_indexes:
+        shape[index] = dyn_shape[index]
+    return shape
+
+
+def reshape_to_matrix(input_tensor):
+    """Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix)."""
+    ndims = input_tensor.shape.ndims
+    if ndims < 2:
+        raise ValueError("Input tensor must have at least rank 2. Shape = %s" %
+                         (input_tensor.shape))
+    if ndims == 2:
+        return input_tensor
+
+    width = input_tensor.shape[-1]
+    output_tensor = tf.reshape(input_tensor, [-1, width])
+    return output_tensor
+
+
+def reshape_from_matrix(output_tensor, orig_shape_list):
+    """Reshapes a rank 2 tensor back to its original rank >= 2 tensor."""
+    if len(orig_shape_list) == 2:
+        return output_tensor
+
+    output_shape = get_shape_list(output_tensor)
+
+    orig_dims = orig_shape_list[0:-1]
+    width = output_shape[-1]
+
+    return tf.reshape(output_tensor, orig_dims + [width])
+
+
+def assert_rank(tensor, expected_rank, name=None):
+    """Raises an exception if the tensor rank is not of the expected rank.
+
+    Args:
+        tensor: A tf.Tensor to check the rank of.
+        expected_rank: Python integer or list of integers, expected rank.
+        name: Optional name of the tensor for the error message.
+
+    Raises:
+        ValueError: If the expected shape doesn't match the actual shape.
+    """
+    if name is None:
+        name = tensor.name
+
+    expected_rank_dict = {}
+    if isinstance(expected_rank, six.integer_types):
+        expected_rank_dict[expected_rank] = True
+    else:
+        for x in expected_rank:
+            expected_rank_dict[x] = True
+
+    actual_rank = tensor.shape.ndims
+    if actual_rank not in expected_rank_dict:
+        scope_name = tf.get_variable_scope().name
+        raise ValueError(
+            "For the tensor `%s` in scope `%s`, the actual rank "
+            "`%d` (shape = %s) is not equal to the expected rank `%s`" %
+            (name, scope_name, actual_rank, str(tensor.shape),
+             str(expected_rank)))
```

## skrec/recommender/BERT4Rec/bert4rec_utils.py

 * *Ordering differences only*

```diff
@@ -1,288 +1,288 @@
-import pickle
-from collections import defaultdict
-import numpy as np
-import tensorflow as tf
-from . import modeling, optimization
-from ...utils.py import MetricReport
-from ...utils.py.cython import eval_score_matrix
-
-
-class EvalHooks(tf.train.SessionRunHook):
-    def __init__(self, config, user_history_filename, evaluator, logger):
-        if user_history_filename is not None:
-            # print('load user history from :' + user_history_filename)
-            with open(user_history_filename, 'rb') as input_file:
-                user_history = pickle.load(input_file)
-        self.config = config
-        self.max_pre_seq = int(round(self.config.max_seq_len*self.config.masked_lm_prob))
-
-        self.user_train_dict = {}
-        self.user_test_dict = {}
-        for user, items in user_history.items():
-            user = int(user.split("_")[1])
-            # items = items[0]
-            self.user_train_dict[user] = np.array(items[:-1], dtype=np.int32)
-            self.user_test_dict[user] = set(items[-1:])
-
-        self.seq_len_num = defaultdict(int)
-        for user, item_seq in self.user_train_dict.items():
-            self.seq_len_num[len(item_seq)] += 1
-
-        self._evaluator = evaluator
-        self._logger = logger
-
-        self._logger.info("metrics:".ljust(12)+f"\t{self._evaluator.metrics_str}")
-        # self._logger.info(self._evaluator.metrics_info())
-        self.counter = 0
-        self._best_result: MetricReport = None
-        self._epoch = 0
-
-    def begin(self):
-        self._eval_results = []
-
-    def end(self, session):
-        all_user_result = np.concatenate(self._eval_results, axis=0)  # (num_users, metrics_num*max_top)
-        final_result = np.mean(all_user_result, axis=0)  # (1, metrics_num*max_top)
-        final_result = np.reshape(final_result, newshape=[self._evaluator.metrics_num, self._evaluator.max_top])  # (metrics_num, max_top)
-        final_result = final_result[:, self._evaluator.top_show - 1]
-        final_result = np.reshape(final_result, newshape=[-1])
-        cur_result = MetricReport(self._evaluator.metrics_list, final_result)
-        self._logger.info(f"test:".ljust(12)+f"\t{cur_result.values_str}")
-
-        self.counter += 1
-        if self._epoch >= 80 and self.counter > self.config.early_stop:
-            self._logger.info("early stop")
-            self._logger.info("best:".ljust(12)+f"\t{self._best_result.values_str}")
-            exit(0)
-
-        if self._best_result is None or cur_result["NDCG@10"] >= self._best_result["NDCG@10"]:
-            self._best_result = cur_result
-            self.counter = 0
-        self._epoch += 1
-
-    def before_run(self, run_context):
-        variables = tf.get_collection('eval_sp')
-        return tf.train.SessionRunArgs(variables)
-
-    def after_run(self, run_context, run_values):
-        masked_lm_log_probs, info = run_values.results
-        masked_lm_log_probs = masked_lm_log_probs.reshape(
-            (-1, self.max_pre_seq, masked_lm_log_probs.shape[1]))
-
-        bat_ratings = np.array(masked_lm_log_probs[:, 0, :], dtype=np.float32, copy=True)
-        batch_users = np.squeeze(info)
-
-        test_items = [self.user_test_dict[u] for u in batch_users]
-
-        for idx, user in enumerate(batch_users):
-            if user in self.user_train_dict and len(self.user_train_dict[user]) > 0:
-                train_items = self.user_train_dict[user]
-                bat_ratings[idx, train_items] = -np.inf
-
-        result = eval_score_matrix(bat_ratings, test_items, self._evaluator.metrics,
-                                   top_k=self._evaluator.max_top, thread_num=self._evaluator.num_thread)  # (B,k*metric_num)
-
-        self._eval_results.append(result)
-
-
-def model_fn_builder(bert_config, init_checkpoint, learning_rate,
-                     num_train_steps, num_warmup_steps, use_tpu,
-                     use_one_hot_embeddings, item_size):
-    """Returns `model_fn` closure for TPUEstimator."""
-
-    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
-        """The `model_fn` for TPUEstimator."""
-
-        info = features["info"]
-        input_ids = features["input_ids"]
-        input_mask = features["input_mask"]
-        masked_lm_positions = features["masked_lm_positions"]
-        masked_lm_ids = features["masked_lm_ids"]
-        masked_lm_weights = features["masked_lm_weights"]
-
-        is_training = (mode == tf.estimator.ModeKeys.TRAIN)
-
-        model = modeling.BertModel(
-            config=bert_config,
-            is_training=is_training,
-            input_ids=input_ids,
-            input_mask=input_mask,
-            token_type_ids=None,
-            use_one_hot_embeddings=use_one_hot_embeddings)
-
-        (masked_lm_loss,
-         masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(
-            bert_config,
-            model.get_sequence_output(),
-            model.get_embedding_table(), masked_lm_positions, masked_lm_ids,
-            masked_lm_weights)
-
-        total_loss = masked_lm_loss
-
-        tvars = tf.trainable_variables()
-
-        initialized_variable_names = {}
-        scaffold_fn = None
-        if init_checkpoint:
-            (assignment_map, initialized_variable_names
-             ) = modeling.get_assignment_map_from_checkpoint(
-                tvars, init_checkpoint)
-            if use_tpu:
-
-                def tpu_scaffold():
-                    tf.train.init_from_checkpoint(init_checkpoint,
-                                                  assignment_map)
-                    return tf.train.Scaffold()
-
-                scaffold_fn = tpu_scaffold
-            else:
-                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
-
-        if mode == tf.estimator.ModeKeys.TRAIN:
-            train_op = optimization.create_optimizer(total_loss, learning_rate,
-                                                     num_train_steps,
-                                                     num_warmup_steps, use_tpu)
-
-            output_spec = tf.estimator.EstimatorSpec(
-                mode=mode,
-                loss=total_loss,
-                train_op=train_op,
-                scaffold=scaffold_fn)
-        elif mode == tf.estimator.ModeKeys.EVAL:
-            tf.add_to_collection('eval_sp', masked_lm_log_probs)
-            tf.add_to_collection('eval_sp', info)
-
-            output_spec = tf.estimator.EstimatorSpec(
-                mode=mode,
-                loss=total_loss,
-                eval_metric_ops=None,
-                scaffold=scaffold_fn)
-        else:
-            raise ValueError("Only TRAIN and EVAL modes are supported: %s" % mode)
-
-        return output_spec
-
-    return model_fn
-
-
-def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,
-                         label_ids, label_weights):
-    """Get loss and log probs for the masked LM."""
-    # [batch_size*label_size, dim]
-    input_tensor = gather_indexes(input_tensor, positions)
-
-    with tf.variable_scope("cls/predictions"):
-        # We apply one more non-linear transformation before the output layer.
-        # This matrix is not used after pre-training.
-        with tf.variable_scope("transform"):
-            input_tensor = tf.layers.dense(
-                input_tensor,
-                units=bert_config.hidden_size,
-                activation=modeling.get_activation(bert_config.hidden_act),
-                kernel_initializer=modeling.create_initializer(
-                    bert_config.initializer_range))
-            input_tensor = modeling.layer_norm(input_tensor)
-
-        # The output weights are the same as the input embeddings, but there is
-        # an output-only bias for each token.
-        output_bias = tf.get_variable(
-            "output_bias",
-            shape=[output_weights.shape[0]],
-            initializer=tf.zeros_initializer())
-        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
-        logits = tf.nn.bias_add(logits, output_bias)
-        # logits, (bs*label_size, vocab_size)
-        log_probs = tf.nn.log_softmax(logits, -1)
-
-        label_ids = tf.reshape(label_ids, [-1])
-        label_weights = tf.reshape(label_weights, [-1])
-
-        one_hot_labels = tf.one_hot(
-            label_ids, depth=output_weights.shape[0], dtype=tf.float32)
-
-        # The `positions` tensor might be zero-padded (if the sequence is too
-        # short to have the maximum number of predictions). The `label_weights`
-        # tensor has a value of 1.0 for every real prediction and 0.0 for the
-        # padding predictions.
-        per_example_loss = -tf.reduce_sum(
-            log_probs * one_hot_labels, axis=[-1])
-        numerator = tf.reduce_sum(label_weights * per_example_loss)
-        denominator = tf.reduce_sum(label_weights) + 1e-5
-        loss = numerator / denominator
-
-    return (loss, per_example_loss, log_probs)
-
-
-def gather_indexes(sequence_tensor, positions):
-    """Gathers the vectors at the specific positions over a minibatch."""
-    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)
-    batch_size = sequence_shape[0]
-    seq_length = sequence_shape[1]
-    width = sequence_shape[2]
-
-    flat_offsets = tf.reshape(
-        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
-    flat_positions = tf.reshape(positions + flat_offsets, [-1])
-    flat_sequence_tensor = tf.reshape(sequence_tensor,
-                                      [batch_size * seq_length, width])
-    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
-    return output_tensor
-
-
-def input_fn_builder(input_files,
-                     max_seq_length,
-                     max_predictions_per_seq,
-                     is_training,
-                     num_cpu_threads=4):
-    """Creates an `input_fn` closure to be passed to TPUEstimator."""
-
-    def input_fn(params):
-        """The actual input function."""
-        batch_size = params["batch_size"]
-
-        name_to_features = {
-            "info":
-                tf.FixedLenFeature([1], tf.int64),  #[user]
-            "input_ids":
-                tf.FixedLenFeature([max_seq_length], tf.int64),
-            "input_mask":
-                tf.FixedLenFeature([max_seq_length], tf.int64),
-            "masked_lm_positions":
-                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
-            "masked_lm_ids":
-                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
-            "masked_lm_weights":
-                tf.FixedLenFeature([max_predictions_per_seq], tf.float32)
-        }
-
-        # For training, we want a lot of parallel reading and shuffling.
-        # For eval, we want no shuffling and parallel reading doesn't matter.
-        if is_training:
-            d = tf.data.TFRecordDataset(input_files)
-            d = d.repeat()
-            d = d.shuffle(buffer_size=100)
-        else:
-            d = tf.data.TFRecordDataset(input_files)
-
-        d = d.map(
-            lambda record: _decode_record(record, name_to_features),
-            num_parallel_calls=num_cpu_threads)
-        d = d.batch(batch_size=batch_size)
-        return d
-
-    return input_fn
-
-
-def _decode_record(record, name_to_features):
-    """Decodes a record to a TensorFlow example."""
-    example = tf.parse_single_example(record, name_to_features)
-
-    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
-    # So cast all int64 to int32.
-    for name in list(example.keys()):
-        t = example[name]
-        if t.dtype == tf.int64:
-            t = tf.to_int32(t)
-        example[name] = t
-
-    return example
+import pickle
+from collections import defaultdict
+import numpy as np
+import tensorflow as tf
+from . import modeling, optimization
+from ...utils.py import MetricReport
+from ...utils.py.cython import eval_score_matrix
+
+
+class EvalHooks(tf.train.SessionRunHook):
+    def __init__(self, config, user_history_filename, evaluator, logger):
+        if user_history_filename is not None:
+            # print('load user history from :' + user_history_filename)
+            with open(user_history_filename, 'rb') as input_file:
+                user_history = pickle.load(input_file)
+        self.config = config
+        self.max_pre_seq = int(round(self.config.max_seq_len*self.config.masked_lm_prob))
+
+        self.user_train_dict = {}
+        self.user_test_dict = {}
+        for user, items in user_history.items():
+            user = int(user.split("_")[1])
+            # items = items[0]
+            self.user_train_dict[user] = np.array(items[:-1], dtype=np.int32)
+            self.user_test_dict[user] = set(items[-1:])
+
+        self.seq_len_num = defaultdict(int)
+        for user, item_seq in self.user_train_dict.items():
+            self.seq_len_num[len(item_seq)] += 1
+
+        self._evaluator = evaluator
+        self._logger = logger
+
+        self._logger.info("metrics:".ljust(12)+f"\t{self._evaluator.metrics_str}")
+        # self._logger.info(self._evaluator.metrics_info())
+        self.counter = 0
+        self._best_result: MetricReport = None
+        self._epoch = 0
+
+    def begin(self):
+        self._eval_results = []
+
+    def end(self, session):
+        all_user_result = np.concatenate(self._eval_results, axis=0)  # (num_users, metrics_num*max_top)
+        final_result = np.mean(all_user_result, axis=0)  # (1, metrics_num*max_top)
+        final_result = np.reshape(final_result, newshape=[self._evaluator.metrics_num, self._evaluator.max_top])  # (metrics_num, max_top)
+        final_result = final_result[:, self._evaluator.top_show - 1]
+        final_result = np.reshape(final_result, newshape=[-1])
+        cur_result = MetricReport(self._evaluator.metrics_list, final_result)
+        self._logger.info(f"test:".ljust(12)+f"\t{cur_result.values_str}")
+
+        self.counter += 1
+        if self._epoch >= 80 and self.counter > self.config.early_stop:
+            self._logger.info("early stop")
+            self._logger.info("best:".ljust(12)+f"\t{self._best_result.values_str}")
+            exit(0)
+
+        if self._best_result is None or cur_result["NDCG@10"] >= self._best_result["NDCG@10"]:
+            self._best_result = cur_result
+            self.counter = 0
+        self._epoch += 1
+
+    def before_run(self, run_context):
+        variables = tf.get_collection('eval_sp')
+        return tf.train.SessionRunArgs(variables)
+
+    def after_run(self, run_context, run_values):
+        masked_lm_log_probs, info = run_values.results
+        masked_lm_log_probs = masked_lm_log_probs.reshape(
+            (-1, self.max_pre_seq, masked_lm_log_probs.shape[1]))
+
+        bat_ratings = np.array(masked_lm_log_probs[:, 0, :], dtype=np.float32, copy=True)
+        batch_users = np.squeeze(info)
+
+        test_items = [self.user_test_dict[u] for u in batch_users]
+
+        for idx, user in enumerate(batch_users):
+            if user in self.user_train_dict and len(self.user_train_dict[user]) > 0:
+                train_items = self.user_train_dict[user]
+                bat_ratings[idx, train_items] = -np.inf
+
+        result = eval_score_matrix(bat_ratings, test_items, self._evaluator.metrics,
+                                   top_k=self._evaluator.max_top, thread_num=self._evaluator.num_thread)  # (B,k*metric_num)
+
+        self._eval_results.append(result)
+
+
+def model_fn_builder(bert_config, init_checkpoint, learning_rate,
+                     num_train_steps, num_warmup_steps, use_tpu,
+                     use_one_hot_embeddings, item_size):
+    """Returns `model_fn` closure for TPUEstimator."""
+
+    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
+        """The `model_fn` for TPUEstimator."""
+
+        info = features["info"]
+        input_ids = features["input_ids"]
+        input_mask = features["input_mask"]
+        masked_lm_positions = features["masked_lm_positions"]
+        masked_lm_ids = features["masked_lm_ids"]
+        masked_lm_weights = features["masked_lm_weights"]
+
+        is_training = (mode == tf.estimator.ModeKeys.TRAIN)
+
+        model = modeling.BertModel(
+            config=bert_config,
+            is_training=is_training,
+            input_ids=input_ids,
+            input_mask=input_mask,
+            token_type_ids=None,
+            use_one_hot_embeddings=use_one_hot_embeddings)
+
+        (masked_lm_loss,
+         masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(
+            bert_config,
+            model.get_sequence_output(),
+            model.get_embedding_table(), masked_lm_positions, masked_lm_ids,
+            masked_lm_weights)
+
+        total_loss = masked_lm_loss
+
+        tvars = tf.trainable_variables()
+
+        initialized_variable_names = {}
+        scaffold_fn = None
+        if init_checkpoint:
+            (assignment_map, initialized_variable_names
+             ) = modeling.get_assignment_map_from_checkpoint(
+                tvars, init_checkpoint)
+            if use_tpu:
+
+                def tpu_scaffold():
+                    tf.train.init_from_checkpoint(init_checkpoint,
+                                                  assignment_map)
+                    return tf.train.Scaffold()
+
+                scaffold_fn = tpu_scaffold
+            else:
+                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
+
+        if mode == tf.estimator.ModeKeys.TRAIN:
+            train_op = optimization.create_optimizer(total_loss, learning_rate,
+                                                     num_train_steps,
+                                                     num_warmup_steps, use_tpu)
+
+            output_spec = tf.estimator.EstimatorSpec(
+                mode=mode,
+                loss=total_loss,
+                train_op=train_op,
+                scaffold=scaffold_fn)
+        elif mode == tf.estimator.ModeKeys.EVAL:
+            tf.add_to_collection('eval_sp', masked_lm_log_probs)
+            tf.add_to_collection('eval_sp', info)
+
+            output_spec = tf.estimator.EstimatorSpec(
+                mode=mode,
+                loss=total_loss,
+                eval_metric_ops=None,
+                scaffold=scaffold_fn)
+        else:
+            raise ValueError("Only TRAIN and EVAL modes are supported: %s" % mode)
+
+        return output_spec
+
+    return model_fn
+
+
+def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,
+                         label_ids, label_weights):
+    """Get loss and log probs for the masked LM."""
+    # [batch_size*label_size, dim]
+    input_tensor = gather_indexes(input_tensor, positions)
+
+    with tf.variable_scope("cls/predictions"):
+        # We apply one more non-linear transformation before the output layer.
+        # This matrix is not used after pre-training.
+        with tf.variable_scope("transform"):
+            input_tensor = tf.layers.dense(
+                input_tensor,
+                units=bert_config.hidden_size,
+                activation=modeling.get_activation(bert_config.hidden_act),
+                kernel_initializer=modeling.create_initializer(
+                    bert_config.initializer_range))
+            input_tensor = modeling.layer_norm(input_tensor)
+
+        # The output weights are the same as the input embeddings, but there is
+        # an output-only bias for each token.
+        output_bias = tf.get_variable(
+            "output_bias",
+            shape=[output_weights.shape[0]],
+            initializer=tf.zeros_initializer())
+        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
+        logits = tf.nn.bias_add(logits, output_bias)
+        # logits, (bs*label_size, vocab_size)
+        log_probs = tf.nn.log_softmax(logits, -1)
+
+        label_ids = tf.reshape(label_ids, [-1])
+        label_weights = tf.reshape(label_weights, [-1])
+
+        one_hot_labels = tf.one_hot(
+            label_ids, depth=output_weights.shape[0], dtype=tf.float32)
+
+        # The `positions` tensor might be zero-padded (if the sequence is too
+        # short to have the maximum number of predictions). The `label_weights`
+        # tensor has a value of 1.0 for every real prediction and 0.0 for the
+        # padding predictions.
+        per_example_loss = -tf.reduce_sum(
+            log_probs * one_hot_labels, axis=[-1])
+        numerator = tf.reduce_sum(label_weights * per_example_loss)
+        denominator = tf.reduce_sum(label_weights) + 1e-5
+        loss = numerator / denominator
+
+    return (loss, per_example_loss, log_probs)
+
+
+def gather_indexes(sequence_tensor, positions):
+    """Gathers the vectors at the specific positions over a minibatch."""
+    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)
+    batch_size = sequence_shape[0]
+    seq_length = sequence_shape[1]
+    width = sequence_shape[2]
+
+    flat_offsets = tf.reshape(
+        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
+    flat_positions = tf.reshape(positions + flat_offsets, [-1])
+    flat_sequence_tensor = tf.reshape(sequence_tensor,
+                                      [batch_size * seq_length, width])
+    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
+    return output_tensor
+
+
+def input_fn_builder(input_files,
+                     max_seq_length,
+                     max_predictions_per_seq,
+                     is_training,
+                     num_cpu_threads=4):
+    """Creates an `input_fn` closure to be passed to TPUEstimator."""
+
+    def input_fn(params):
+        """The actual input function."""
+        batch_size = params["batch_size"]
+
+        name_to_features = {
+            "info":
+                tf.FixedLenFeature([1], tf.int64),  #[user]
+            "input_ids":
+                tf.FixedLenFeature([max_seq_length], tf.int64),
+            "input_mask":
+                tf.FixedLenFeature([max_seq_length], tf.int64),
+            "masked_lm_positions":
+                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
+            "masked_lm_ids":
+                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
+            "masked_lm_weights":
+                tf.FixedLenFeature([max_predictions_per_seq], tf.float32)
+        }
+
+        # For training, we want a lot of parallel reading and shuffling.
+        # For eval, we want no shuffling and parallel reading doesn't matter.
+        if is_training:
+            d = tf.data.TFRecordDataset(input_files)
+            d = d.repeat()
+            d = d.shuffle(buffer_size=100)
+        else:
+            d = tf.data.TFRecordDataset(input_files)
+
+        d = d.map(
+            lambda record: _decode_record(record, name_to_features),
+            num_parallel_calls=num_cpu_threads)
+        d = d.batch(batch_size=batch_size)
+        return d
+
+    return input_fn
+
+
+def _decode_record(record, name_to_features):
+    """Decodes a record to a TensorFlow example."""
+    example = tf.parse_single_example(record, name_to_features)
+
+    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
+    # So cast all int64 to int32.
+    for name in list(example.keys()):
+        t = example[name]
+        if t.dtype == tf.int64:
+            t = tf.to_int32(t)
+        example[name] = t
+
+    return example
```

## skrec/recommender/BERT4Rec/BERT4Rec.py

 * *Ordering differences only*

```diff
@@ -1,186 +1,186 @@
-"""
-Paper: BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer
-Author: Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang
-Reference: https://github.com/FeiSun/BERT4Rec
-"""
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["BERT4Rec"]
-
-import os
-import numpy as np
-import time
-import pickle
-from typing import Dict
-from skrec.utils.py import Config
-from skrec.io import Dataset
-from skrec.recommender.base import AbstractRecommender
-from skrec.utils.py import RankingEvaluator
-from . import modeling
-import tensorflow as tf
-from .bert4rec_utils import model_fn_builder, input_fn_builder, EvalHooks
-
-
-class BERT4RecConfig(Config):
-    def __init__(self,
-                 # prepare data
-                 max_seq_len=5,
-                 masked_lm_prob=0.4,
-                 sliding_step=1,
-                 dupe_factor=10,
-                 # bert model
-                 att_drop=0.2,
-                 h_drop=0.5,
-                 h_size=64,
-                 att_heads=2,
-                 init_range=0.02,
-                 h_act="gelu",
-                 n_layers=2,
-                 # train
-                 lr=1e-4,
-                 batch_size=256,
-                 save_ckpt_epoch=10,
-                 init_ckpt=None,
-                 epochs=3000,
-                 early_stop=80,
-                 verbose=10,
-                 pool_size=10,
-                 **kwargs):
-        super(BERT4RecConfig, self).__init__()
-        # prepare data
-        self.max_seq_len: int = max_seq_len
-        self.masked_lm_prob: float = masked_lm_prob
-        self.sliding_step: int = sliding_step
-        self.dupe_factor: int = dupe_factor
-        # bert model
-        self.att_drop: float = att_drop
-        self.h_drop: float = h_drop
-        self.h_size: int = h_size
-        self.att_heads: int = att_heads
-        # do not tune
-        self.init_range: float = init_range
-        self.h_act: str = h_act
-        self.n_layers: int = n_layers
-        # train
-        self.lr: float = lr
-        self.batch_size: int = batch_size
-        self.save_ckpt_epoch: int = save_ckpt_epoch
-        self.init_ckpt: str = init_ckpt
-        self.epochs: int = epochs
-        self.early_stop: int = early_stop
-        self.verbose: int = verbose
-        self.pool_size: int = pool_size
-
-
-class BERT4Rec(AbstractRecommender):
-    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
-        config = BERT4RecConfig(**cfg_dict)
-        super(BERT4Rec, self).__init__(dataset, config)
-        self.config = config
-        self.dataset = dataset
-        self.evaluator = evaluator
-        self._prepare_data()
-        self._build_model()
-
-    def _prepare_data(self):
-        output_dir = self.dataset.data_dir
-        tf_record_name = [f"max_seq_len={self.config.max_seq_len}",
-                          f"masked_lm_prob={self.config.masked_lm_prob}",
-                          f"sliding_step={self.config.sliding_step}",
-                          f"dupe_factor={self.config.dupe_factor}"]
-        tf_record_name = "_".join(tf_record_name)
-
-        output_dir = os.path.join(output_dir, f"_{self.__class__.__name__}_data")
-        self.train_input_file = os.path.join(output_dir, tf_record_name+'.train.tfrecord')
-        self.test_input_file = os.path.join(output_dir, tf_record_name+'.test.tfrecord')
-        vocab_filename = os.path.join(output_dir, tf_record_name+'.vocab')
-        self.user_history_filename = os.path.join(output_dir, tf_record_name+'.his')
-        num_trains_file = os.path.join(output_dir, tf_record_name+'.train.num.npy')
-
-        if not os.path.exists(self.train_input_file) or \
-                not os.path.exists(self.test_input_file) or \
-                not os.path.exists(vocab_filename) or \
-                not os.path.exists(self.user_history_filename) or \
-                not os.path.exists(num_trains_file):
-            from . import bert4rec_gen_data
-            bert4rec_gen_data.main(self.config, self.dataset, output_dir, tf_record_name)
-
-        with open(vocab_filename, 'rb') as input_file:
-            vocab = pickle.load(input_file)
-        self.item_size = len(vocab.token_to_ids)
-        self.num_instances = np.load(num_trains_file)
-
-    def _build_model(self):
-        config = self.config
-        bert_config = modeling.BertConfig(self.item_size,
-                                          hidden_size=config.h_size,
-                                          num_hidden_layers=config.n_layers,
-                                          num_attention_heads=config.att_heads,
-                                          intermediate_size=config.h_size*4,
-                                          hidden_act=config.h_act,
-                                          hidden_dropout_prob=config.h_drop,
-                                          attention_probs_dropout_prob=config.att_drop,
-                                          max_position_embeddings=config.max_seq_len,
-                                          type_vocab_size=2,
-                                          initializer_range=config.init_range)
-        timestamp = time.time()
-        checkpoint_dir = os.path.join(self.dataset.data_dir, f"_{self.__class__.__name__}_ckpt_dir")
-        checkpoint_dir = os.path.join(checkpoint_dir, f"{timestamp}")
-        if not os.path.isdir(checkpoint_dir):
-            os.makedirs(checkpoint_dir)
-
-        num_instances_per_epoch = int(self.num_instances/(config.dupe_factor+1))
-        self.steps_per_epoch = int(num_instances_per_epoch / config.batch_size)
-        num_train_steps = self.steps_per_epoch * config.epochs
-        max_pre_seq = int(round(config.max_seq_len*config.masked_lm_prob))
-
-        save_ckpt_steps = self.steps_per_epoch*config.save_ckpt_epoch
-        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
-        tf_config.gpu_options.allow_growth = True
-
-        run_config = tf.estimator.RunConfig(
-            session_config=tf_config,
-            model_dir=checkpoint_dir,
-            save_checkpoints_steps=save_ckpt_steps,
-            keep_checkpoint_max=1,
-            save_summary_steps=0)
-
-        model_fn = model_fn_builder(
-            bert_config=bert_config,
-            init_checkpoint=config.init_ckpt,
-            learning_rate=config.lr,
-            num_train_steps=num_train_steps,
-            num_warmup_steps=100,
-            use_tpu=False,
-            use_one_hot_embeddings=False,
-            item_size=self.item_size)
-
-        self.estimator = tf.estimator.Estimator(
-            model_fn=model_fn,
-            config=run_config,
-            params={
-                "batch_size": config.batch_size
-            })
-
-        self.train_input_fn = input_fn_builder(
-            input_files=[self.train_input_file],
-            max_seq_length=config.max_seq_len,
-            max_predictions_per_seq=max_pre_seq,
-            is_training=True)
-
-        self.eval_input_fn = input_fn_builder(
-            input_files=[self.test_input_file],
-            max_seq_length=config.max_seq_len,
-            max_predictions_per_seq=max_pre_seq,
-            is_training=False)
-
-    def fit(self):
-        config = self.config
-        eval_hook = EvalHooks(config, self.user_history_filename, self.evaluator, self.logger)
-        cur_steps = 0
-        for epoch in range(0, config.epochs, config.verbose):
-            self.estimator.train(input_fn=self.train_input_fn,
-                                 max_steps=cur_steps+self.steps_per_epoch*config.verbose)
-            cur_steps += self.steps_per_epoch*config.verbose
-            self.estimator.evaluate(input_fn=self.eval_input_fn, steps=None, hooks=[eval_hook])
+"""
+Paper: BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer
+Author: Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang
+Reference: https://github.com/FeiSun/BERT4Rec
+"""
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["BERT4Rec"]
+
+import os
+import numpy as np
+import time
+import pickle
+from typing import Dict
+from skrec.utils.py import Config
+from skrec.io import Dataset
+from skrec.recommender.base import AbstractRecommender
+from skrec.utils.py import RankingEvaluator
+from . import modeling
+import tensorflow as tf
+from .bert4rec_utils import model_fn_builder, input_fn_builder, EvalHooks
+
+
+class BERT4RecConfig(Config):
+    def __init__(self,
+                 # prepare data
+                 max_seq_len=5,
+                 masked_lm_prob=0.4,
+                 sliding_step=1,
+                 dupe_factor=10,
+                 # bert model
+                 att_drop=0.2,
+                 h_drop=0.5,
+                 h_size=64,
+                 att_heads=2,
+                 init_range=0.02,
+                 h_act="gelu",
+                 n_layers=2,
+                 # train
+                 lr=1e-4,
+                 batch_size=256,
+                 save_ckpt_epoch=10,
+                 init_ckpt=None,
+                 epochs=3000,
+                 early_stop=80,
+                 verbose=10,
+                 pool_size=10,
+                 **kwargs):
+        super(BERT4RecConfig, self).__init__()
+        # prepare data
+        self.max_seq_len: int = max_seq_len
+        self.masked_lm_prob: float = masked_lm_prob
+        self.sliding_step: int = sliding_step
+        self.dupe_factor: int = dupe_factor
+        # bert model
+        self.att_drop: float = att_drop
+        self.h_drop: float = h_drop
+        self.h_size: int = h_size
+        self.att_heads: int = att_heads
+        # do not tune
+        self.init_range: float = init_range
+        self.h_act: str = h_act
+        self.n_layers: int = n_layers
+        # train
+        self.lr: float = lr
+        self.batch_size: int = batch_size
+        self.save_ckpt_epoch: int = save_ckpt_epoch
+        self.init_ckpt: str = init_ckpt
+        self.epochs: int = epochs
+        self.early_stop: int = early_stop
+        self.verbose: int = verbose
+        self.pool_size: int = pool_size
+
+
+class BERT4Rec(AbstractRecommender):
+    def __init__(self, dataset: Dataset, cfg_dict: Dict, evaluator: RankingEvaluator):
+        config = BERT4RecConfig(**cfg_dict)
+        super(BERT4Rec, self).__init__(dataset, config)
+        self.config = config
+        self.dataset = dataset
+        self.evaluator = evaluator
+        self._prepare_data()
+        self._build_model()
+
+    def _prepare_data(self):
+        output_dir = self.dataset.data_dir
+        tf_record_name = [f"max_seq_len={self.config.max_seq_len}",
+                          f"masked_lm_prob={self.config.masked_lm_prob}",
+                          f"sliding_step={self.config.sliding_step}",
+                          f"dupe_factor={self.config.dupe_factor}"]
+        tf_record_name = "_".join(tf_record_name)
+
+        output_dir = os.path.join(output_dir, f"_{self.__class__.__name__}_data")
+        self.train_input_file = os.path.join(output_dir, tf_record_name+'.train.tfrecord')
+        self.test_input_file = os.path.join(output_dir, tf_record_name+'.test.tfrecord')
+        vocab_filename = os.path.join(output_dir, tf_record_name+'.vocab')
+        self.user_history_filename = os.path.join(output_dir, tf_record_name+'.his')
+        num_trains_file = os.path.join(output_dir, tf_record_name+'.train.num.npy')
+
+        if not os.path.exists(self.train_input_file) or \
+                not os.path.exists(self.test_input_file) or \
+                not os.path.exists(vocab_filename) or \
+                not os.path.exists(self.user_history_filename) or \
+                not os.path.exists(num_trains_file):
+            from . import bert4rec_gen_data
+            bert4rec_gen_data.main(self.config, self.dataset, output_dir, tf_record_name)
+
+        with open(vocab_filename, 'rb') as input_file:
+            vocab = pickle.load(input_file)
+        self.item_size = len(vocab.token_to_ids)
+        self.num_instances = np.load(num_trains_file)
+
+    def _build_model(self):
+        config = self.config
+        bert_config = modeling.BertConfig(self.item_size,
+                                          hidden_size=config.h_size,
+                                          num_hidden_layers=config.n_layers,
+                                          num_attention_heads=config.att_heads,
+                                          intermediate_size=config.h_size*4,
+                                          hidden_act=config.h_act,
+                                          hidden_dropout_prob=config.h_drop,
+                                          attention_probs_dropout_prob=config.att_drop,
+                                          max_position_embeddings=config.max_seq_len,
+                                          type_vocab_size=2,
+                                          initializer_range=config.init_range)
+        timestamp = time.time()
+        checkpoint_dir = os.path.join(self.dataset.data_dir, f"_{self.__class__.__name__}_ckpt_dir")
+        checkpoint_dir = os.path.join(checkpoint_dir, f"{timestamp}")
+        if not os.path.isdir(checkpoint_dir):
+            os.makedirs(checkpoint_dir)
+
+        num_instances_per_epoch = int(self.num_instances/(config.dupe_factor+1))
+        self.steps_per_epoch = int(num_instances_per_epoch / config.batch_size)
+        num_train_steps = self.steps_per_epoch * config.epochs
+        max_pre_seq = int(round(config.max_seq_len*config.masked_lm_prob))
+
+        save_ckpt_steps = self.steps_per_epoch*config.save_ckpt_epoch
+        tf_config = tf.ConfigProto()  # allow_soft_placement=False, log_device_placement=True
+        tf_config.gpu_options.allow_growth = True
+
+        run_config = tf.estimator.RunConfig(
+            session_config=tf_config,
+            model_dir=checkpoint_dir,
+            save_checkpoints_steps=save_ckpt_steps,
+            keep_checkpoint_max=1,
+            save_summary_steps=0)
+
+        model_fn = model_fn_builder(
+            bert_config=bert_config,
+            init_checkpoint=config.init_ckpt,
+            learning_rate=config.lr,
+            num_train_steps=num_train_steps,
+            num_warmup_steps=100,
+            use_tpu=False,
+            use_one_hot_embeddings=False,
+            item_size=self.item_size)
+
+        self.estimator = tf.estimator.Estimator(
+            model_fn=model_fn,
+            config=run_config,
+            params={
+                "batch_size": config.batch_size
+            })
+
+        self.train_input_fn = input_fn_builder(
+            input_files=[self.train_input_file],
+            max_seq_length=config.max_seq_len,
+            max_predictions_per_seq=max_pre_seq,
+            is_training=True)
+
+        self.eval_input_fn = input_fn_builder(
+            input_files=[self.test_input_file],
+            max_seq_length=config.max_seq_len,
+            max_predictions_per_seq=max_pre_seq,
+            is_training=False)
+
+    def fit(self):
+        config = self.config
+        eval_hook = EvalHooks(config, self.user_history_filename, self.evaluator, self.logger)
+        cur_steps = 0
+        for epoch in range(0, config.epochs, config.verbose):
+            self.estimator.train(input_fn=self.train_input_fn,
+                                 max_steps=cur_steps+self.steps_per_epoch*config.verbose)
+            cur_steps += self.steps_per_epoch*config.verbose
+            self.estimator.evaluate(input_fn=self.eval_input_fn, steps=None, hooks=[eval_hook])
```

## skrec/recommender/BERT4Rec/bert4rec_gen_data.py

 * *Ordering differences only*

```diff
@@ -1,508 +1,508 @@
-import os
-import collections
-import random
-import numpy as np
-
-import tensorflow as tf
-
-from .vocab import FreqVocab
-import pickle
-import multiprocessing
-import time
-from ...io import Dataset
-
-random_seed = 12345
-short_seq_prob = 0  # Probability of creating sequences which are shorter than the maximum length
-
-
-def printable_text(text):
-    """Returns text encoded in a way suitable for print or `tf.logging`."""
-
-    # These functions want `str` for both Python2 and Python3, but in one case
-    # it's a Unicode string and in the other it's a byte string.
-    if isinstance(text, str):
-        return text
-    elif isinstance(text, bytes):
-        return text.decode("utf-8", "ignore")
-    else:
-        raise ValueError("Unsupported string type: %s" % (type(text)))
-
-
-def convert_to_unicode(text):
-    """Converts `text` to Unicode (if it's not already), assuming utf-8 input."""
-    if isinstance(text, str):
-        return text
-    elif isinstance(text, bytes):
-        return text.decode("utf-8", "ignore")
-    else:
-        raise ValueError("Unsupported string type: %s" % (type(text)))
-
-
-class TrainingInstance(object):
-    """A single training instance (sentence pair)."""
-
-    def __init__(self, info, tokens, masked_lm_positions, masked_lm_labels):
-        self.info = info  # info = [user]
-        self.tokens = tokens
-        self.masked_lm_positions = masked_lm_positions
-        self.masked_lm_labels = masked_lm_labels
-
-    def __str__(self):
-        s = ""
-        s += "info: %s\n" % (" ".join([printable_text(x) for x in self.info]))
-        s += "tokens: %s\n" % (
-            " ".join([printable_text(x) for x in self.tokens]))
-        s += "masked_lm_positions: %s\n" % (
-            " ".join([str(x) for x in self.masked_lm_positions]))
-        s += "masked_lm_labels: %s\n" % (
-            " ".join([printable_text(x) for x in self.masked_lm_labels]))
-        s += "\n"
-        return s
-
-    def __repr__(self):
-        return self.__str__()
-
-
-def write_instance_to_example_files(instances, max_seq_length,
-                                    max_predictions_per_seq, vocab,
-                                    output_files):
-    """Create TF example files from `TrainingInstance`s."""
-    writers = []
-    for output_file in output_files:
-        writers.append(tf.python_io.TFRecordWriter(output_file))
-
-    writer_index = 0
-
-    total_written = 0
-    for (inst_index, instance) in enumerate(instances):
-        try:
-            input_ids = vocab.convert_tokens_to_ids(instance.tokens)
-        except:
-            print(instance)
-
-        input_mask = [1] * len(input_ids)
-        assert len(input_ids) <= max_seq_length
-
-        input_ids += [0] * (max_seq_length - len(input_ids))
-        input_mask += [0] * (max_seq_length - len(input_mask))
-
-        assert len(input_ids) == max_seq_length
-        assert len(input_mask) == max_seq_length
-
-        masked_lm_positions = list(instance.masked_lm_positions)
-        masked_lm_ids = vocab.convert_tokens_to_ids(instance.masked_lm_labels)
-        masked_lm_weights = [1.0] * len(masked_lm_ids)
-
-        masked_lm_positions += [0] * (max_predictions_per_seq - len(masked_lm_positions))
-        masked_lm_ids += [0] * (max_predictions_per_seq - len(masked_lm_ids))
-        masked_lm_weights += [0.0] * (max_predictions_per_seq - len(masked_lm_weights))
-
-        features = collections.OrderedDict()
-        features["info"] = create_int_feature(instance.info)
-        features["input_ids"] = create_int_feature(input_ids)
-        features["input_mask"] = create_int_feature(input_mask)
-        features["masked_lm_positions"] = create_int_feature(masked_lm_positions)
-        features["masked_lm_ids"] = create_int_feature(masked_lm_ids)
-        features["masked_lm_weights"] = create_float_feature(masked_lm_weights)
-
-        tf_example = tf.train.Example(
-            features=tf.train.Features(feature=features))
-
-        writers[writer_index].write(tf_example.SerializeToString())
-        writer_index = (writer_index + 1) % len(writers)
-
-        total_written += 1
-
-    for writer in writers:
-        writer.close()
-
-    tf.logging.info("Wrote %d total instances", total_written)
-    return total_written
-
-
-def create_int_feature(values):
-    feature = tf.train.Feature(
-        int64_list=tf.train.Int64List(value=list(values)))
-    return feature
-
-
-def create_float_feature(values):
-    feature = tf.train.Feature(
-        float_list=tf.train.FloatList(value=list(values)))
-    return feature
-
-
-def create_training_instances(all_documents_raw,
-                              max_seq_length,
-                              dupe_factor,
-                              short_seq_prob,
-                              masked_lm_prob,
-                              max_predictions_per_seq,
-                              rng,
-                              vocab,
-                              mask_prob,
-                              sliding_step,
-                              pool_size,
-                              force_last=False):
-    """Create `TrainingInstance`s from raw text."""
-    all_documents = {}
-
-    if force_last:
-        max_num_tokens = max_seq_length
-        for user, item_seq in all_documents_raw.items():
-            if len(item_seq) == 0:
-                print("got empty seq:" + user)
-                continue
-            all_documents[user] = [item_seq[-max_num_tokens:]]
-    else:
-        max_num_tokens = max_seq_length  # we need two sentence
-
-        assert sliding_step > 0
-        assert sliding_step < max_num_tokens
-        for user, item_seq in all_documents_raw.items():
-            if len(item_seq) == 0:
-                print("got empty seq:" + user)
-                continue
-
-            if len(item_seq) <= max_num_tokens:
-                all_documents[user] = [item_seq]
-            else:
-                beg_idx = list(range(len(item_seq)-max_num_tokens, 0, -sliding_step))
-                beg_idx.append(0)
-                all_documents[user] = [item_seq[i:i + max_num_tokens] for i in beg_idx[::-1]]
-
-    instances = []
-    if force_last:
-        for user in all_documents:
-            instances.extend(
-                create_instances_from_document_test(
-                    all_documents, user, max_seq_length))
-        print("num of instance:{}".format(len(instances)))
-    else:
-        start_time = time.perf_counter()
-        pool = multiprocessing.Pool(processes=pool_size)
-        instances = []
-        print("document num: {}".format(len(all_documents)))
-
-        def log_result(result):
-            print("callback function result type: {}, size: {} ".format(type(result), len(result)))
-            instances.extend(result)
-
-        for step in range(dupe_factor):
-            # create_instances_threading(
-            #     all_documents, user, max_seq_length, short_seq_prob,
-            #     masked_lm_prob, max_predictions_per_seq, vocab, random.Random(random.randint(1, 10000)),
-            #     mask_prob, step)
-            pool.apply_async(
-                create_instances_threading, args=(
-                    all_documents, user, max_seq_length, short_seq_prob,
-                    masked_lm_prob, max_predictions_per_seq, vocab, random.Random(random.randint(1,10000)),
-                    mask_prob, step), callback=log_result)
-        pool.close()
-        pool.join()
-
-        for user in all_documents:
-            instances.extend(mask_last(all_documents, user, max_seq_length, short_seq_prob,
-                                       masked_lm_prob, max_predictions_per_seq, vocab, rng))
-
-        print("num of instance:{}; time:{}".format(len(instances), time.perf_counter() - start_time))
-    rng.shuffle(instances)
-    return instances
-
-
-def create_instances_threading(all_documents, user, max_seq_length, short_seq_prob,
-                               masked_lm_prob, max_predictions_per_seq, vocab, rng,
-                               mask_prob, step):
-    cnt = 0
-    start_time = time.perf_counter()
-    instances = []
-    vocab_item_set = set(vocab.get_items())
-    for user in all_documents:
-        cnt += 1
-        if cnt % 5000 == 0:
-            print("step: {}, name: {}, step: {}, time: {}".format(step, multiprocessing.current_process().name, cnt, time.perf_counter()-start_time))
-            start_time = time.perf_counter()
-        instances.extend(create_instances_from_document_train(
-            all_documents, user, max_seq_length, short_seq_prob,
-            masked_lm_prob, max_predictions_per_seq, vocab_item_set, rng,
-            mask_prob))
-
-    return instances
-
-
-def mask_last(
-        all_documents, user, max_seq_length, short_seq_prob, masked_lm_prob,
-        max_predictions_per_seq, vocab, rng):
-    """Creates `TrainingInstance`s for a single document."""
-    document = all_documents[user]
-    max_num_tokens = max_seq_length
-
-    instances = []
-    info = [int(user.split("_")[1])]
-    vocab_items = vocab.get_items()
-
-    for tokens in document:
-        assert len(tokens) >= 1
-        assert len(tokens) <= max_num_tokens
-
-        (tokens, masked_lm_positions, masked_lm_labels) = create_masked_lm_predictions_force_last(tokens)
-        instance = TrainingInstance(
-            info=info,
-            tokens=tokens,
-            masked_lm_positions=masked_lm_positions,
-            masked_lm_labels=masked_lm_labels)
-        instances.append(instance)
-
-    return instances
-
-
-def create_instances_from_document_test(all_documents, user, max_seq_length):
-    """Creates `TrainingInstance`s for a single document."""
-    document = all_documents[user]
-    max_num_tokens = max_seq_length
-
-    assert len(document) == 1 and len(document[0]) <= max_num_tokens
-
-    tokens = document[0]
-    assert len(tokens) >= 1
-
-    (tokens, masked_lm_positions,
-     masked_lm_labels) = create_masked_lm_predictions_force_last(tokens)
-
-    info = [int(user.split("_")[1])]
-    instance = TrainingInstance(
-        info=info,
-        tokens=tokens,
-        masked_lm_positions=masked_lm_positions,
-        masked_lm_labels=masked_lm_labels)
-
-    return [instance]
-
-
-def create_instances_from_document_train(
-        all_documents, user, max_seq_length, short_seq_prob, masked_lm_prob,
-        max_predictions_per_seq, vocab_items, rng, mask_prob):
-    """Creates `TrainingInstance`s for a single document."""
-    document = all_documents[user]
-
-    max_num_tokens = max_seq_length
-
-    instances = []
-    info = [int(user.split("_")[1])]
-    # vocab_items = vocab.get_items()
-
-    for tokens in document:
-        assert len(tokens) >= 1
-        assert len(tokens) <= max_num_tokens
-
-        (tokens, masked_lm_positions, masked_lm_labels) = \
-            create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_items, rng, mask_prob)
-
-        instance = TrainingInstance(info=info,
-                                    tokens=tokens,
-                                    masked_lm_positions=masked_lm_positions,
-                                    masked_lm_labels=masked_lm_labels)
-        instances.append(instance)
-
-    return instances
-
-
-MaskedLmInstance = collections.namedtuple("MaskedLmInstance",
-                                          ["index", "label"])
-
-
-def create_masked_lm_predictions_force_last(tokens):
-    """Creates the predictions for the masked LM objective."""
-
-    last_index = -1
-    for (i, token) in enumerate(tokens):
-        if token == "[CLS]" or token == "[PAD]" or token == '[NO_USE]':
-            continue
-        last_index = i
-
-    assert last_index > 0
-
-    output_tokens = list(tokens)
-    output_tokens[last_index] = "[MASK]"
-
-    masked_lm_positions = [last_index]
-    masked_lm_labels = [tokens[last_index]]
-
-    return (output_tokens, masked_lm_positions, masked_lm_labels)
-
-
-def create_masked_lm_predictions(tokens, masked_lm_prob,
-                                 max_predictions_per_seq, vocab_words, rng,
-                                 mask_prob):
-    """Creates the predictions for the masked LM objective."""
-
-    cand_indexes = []
-    for (i, token) in enumerate(tokens):
-        if token not in vocab_words:
-            continue
-        cand_indexes.append(i)
-
-    rng.shuffle(cand_indexes)
-
-    output_tokens = list(tokens)
-
-    num_to_predict = min(max_predictions_per_seq,
-                         max(1, int(round(len(tokens) * masked_lm_prob))))
-
-    masked_lms = []
-    covered_indexes = set()
-    for index in cand_indexes:
-        if len(masked_lms) >= num_to_predict:
-            break
-        if index in covered_indexes:
-            continue
-        covered_indexes.add(index)
-
-        masked_token = None
-        # 80% of the time, replace with [MASK]
-        if rng.random() < mask_prob:
-            masked_token = "[MASK]"
-        else:
-            # 10% of the time, keep original
-            raise NotImplementedError
-            if rng.random() < 0.5:
-                masked_token = tokens[index]
-            # 10% of the time, replace with random word
-            else:
-                # masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]
-                raise NotImplementedError
-                masked_token = rng.choice(vocab_words)
-
-
-        output_tokens[index] = masked_token
-
-        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))
-
-    masked_lms = sorted(masked_lms, key=lambda x: x.index)
-
-    masked_lm_positions = []
-    masked_lm_labels = []
-    for p in masked_lms:
-        masked_lm_positions.append(p.index)
-        masked_lm_labels.append(p.label)
-
-    return (output_tokens, masked_lm_positions, masked_lm_labels)
-
-
-def gen_samples(data,
-                output_filename,
-                rng,
-                vocab,
-                max_seq_length,
-                dupe_factor,
-                short_seq_prob,
-                mask_prob,
-                masked_lm_prob,
-                max_predictions_per_seq,
-                sliding_step,
-                pool_size,
-                force_last=False):
-    # create train
-    instances = create_training_instances(
-        data, max_seq_length, dupe_factor, short_seq_prob, masked_lm_prob,
-        max_predictions_per_seq, rng, vocab, mask_prob, sliding_step,
-        pool_size, force_last)
-
-    tf.logging.info("*** Writing to output files ***")
-    tf.logging.info("  %s", output_filename)
-
-    total_written = write_instance_to_example_files(instances, max_seq_length,
-                                                    max_predictions_per_seq, vocab,
-                                                    [output_filename])
-    return total_written
-
-
-def main(config, dataset: Dataset, output_dir: str, tf_record_name: str):
-    tf.logging.set_verbosity(tf.logging.DEBUG)
-
-    max_seq_length = config.max_seq_len
-    max_predictions_per_seq = int(round(config.max_seq_len*config.masked_lm_prob))
-    masked_lm_prob = config.masked_lm_prob
-    mask_prob = 1.0
-    dupe_factor = config.dupe_factor
-    sliding_step = config.sliding_step
-    pool_size = config.pool_size
-
-    if not os.path.exists(output_dir):
-        os.makedirs(output_dir)
-
-    user_train = dataset.train_data.to_user_dict_by_time()
-    user_train = {user: items.tolist() for user, items in user_train.items()}
-
-    user_test = dataset.test_data.to_user_dict_by_time()
-    user_test = {user: items.tolist() for user, items in user_test.items()}
-
-    # get the max index of the data
-    user_train_data = {
-        'user_' + str(k): ['item_' + str(item) for item in v]
-        for k, v in user_train.items() if len(v) > 0
-    }
-    user_test_data = {
-        'user_' + str(u):
-            ['item_' + str(item) for item in (user_train[u] + user_test[u])]
-        for u in user_train if len(user_train[u]) > 0 and len(user_test[u]) > 0
-    }
-    rng = random.Random(random_seed)
-
-    vocab = FreqVocab(user_test_data)
-    user_test_data_output = {
-        k: vocab.convert_tokens_to_ids(v)
-        for k, v in user_test_data.items()
-    }
-
-    print('begin to generate train')
-    output_filename = os.path.join(output_dir, tf_record_name + '.train.tfrecord')
-    total_written = gen_samples(user_train_data,
-                                output_filename,
-                                rng,
-                                vocab,
-                                max_seq_length,
-                                dupe_factor,
-                                short_seq_prob,
-                                mask_prob,
-                                masked_lm_prob,
-                                max_predictions_per_seq,
-                                sliding_step,
-                                pool_size,
-                                force_last=False)
-    print('train:{}'.format(output_filename))
-    np.save(os.path.join(output_dir, tf_record_name + '.train.num.npy'), total_written)
-
-    print('begin to generate test')
-    output_filename = os.path.join(output_dir, tf_record_name + '.test.tfrecord')
-    gen_samples(
-        user_test_data,
-        output_filename,
-        rng,
-        vocab,
-        max_seq_length,
-        dupe_factor,
-        short_seq_prob,
-        mask_prob,
-        masked_lm_prob,
-        max_predictions_per_seq,
-        -1.0,
-        pool_size,
-        force_last=True)
-    print('test:{}'.format(output_filename))
-
-    print('vocab_size:{}, user_size:{}, item_size:{}, item_with_other_size:{}'.
-          format(vocab.get_vocab_size(),
-                 vocab.get_user_count(),
-                 vocab.get_item_count(),
-                 vocab.get_item_count() + vocab.get_special_token_count()))
-    vocab_file_name = os.path.join(output_dir, tf_record_name + '.vocab')
-    print('vocab pickle file: ' + vocab_file_name)
-    with open(vocab_file_name, 'wb') as output_file:
-        pickle.dump(vocab, output_file, protocol=2)
-
-    his_file_name = os.path.join(output_dir, tf_record_name + '.his')
-    print('test data pickle file: ' + his_file_name)
-    with open(his_file_name, 'wb') as output_file:
-        pickle.dump(user_test_data_output, output_file, protocol=2)
-    print('done.')
+import os
+import collections
+import random
+import numpy as np
+
+import tensorflow as tf
+
+from .vocab import FreqVocab
+import pickle
+import multiprocessing
+import time
+from ...io import Dataset
+
+random_seed = 12345
+short_seq_prob = 0  # Probability of creating sequences which are shorter than the maximum length
+
+
+def printable_text(text):
+    """Returns text encoded in a way suitable for print or `tf.logging`."""
+
+    # These functions want `str` for both Python2 and Python3, but in one case
+    # it's a Unicode string and in the other it's a byte string.
+    if isinstance(text, str):
+        return text
+    elif isinstance(text, bytes):
+        return text.decode("utf-8", "ignore")
+    else:
+        raise ValueError("Unsupported string type: %s" % (type(text)))
+
+
+def convert_to_unicode(text):
+    """Converts `text` to Unicode (if it's not already), assuming utf-8 input."""
+    if isinstance(text, str):
+        return text
+    elif isinstance(text, bytes):
+        return text.decode("utf-8", "ignore")
+    else:
+        raise ValueError("Unsupported string type: %s" % (type(text)))
+
+
+class TrainingInstance(object):
+    """A single training instance (sentence pair)."""
+
+    def __init__(self, info, tokens, masked_lm_positions, masked_lm_labels):
+        self.info = info  # info = [user]
+        self.tokens = tokens
+        self.masked_lm_positions = masked_lm_positions
+        self.masked_lm_labels = masked_lm_labels
+
+    def __str__(self):
+        s = ""
+        s += "info: %s\n" % (" ".join([printable_text(x) for x in self.info]))
+        s += "tokens: %s\n" % (
+            " ".join([printable_text(x) for x in self.tokens]))
+        s += "masked_lm_positions: %s\n" % (
+            " ".join([str(x) for x in self.masked_lm_positions]))
+        s += "masked_lm_labels: %s\n" % (
+            " ".join([printable_text(x) for x in self.masked_lm_labels]))
+        s += "\n"
+        return s
+
+    def __repr__(self):
+        return self.__str__()
+
+
+def write_instance_to_example_files(instances, max_seq_length,
+                                    max_predictions_per_seq, vocab,
+                                    output_files):
+    """Create TF example files from `TrainingInstance`s."""
+    writers = []
+    for output_file in output_files:
+        writers.append(tf.python_io.TFRecordWriter(output_file))
+
+    writer_index = 0
+
+    total_written = 0
+    for (inst_index, instance) in enumerate(instances):
+        try:
+            input_ids = vocab.convert_tokens_to_ids(instance.tokens)
+        except:
+            print(instance)
+
+        input_mask = [1] * len(input_ids)
+        assert len(input_ids) <= max_seq_length
+
+        input_ids += [0] * (max_seq_length - len(input_ids))
+        input_mask += [0] * (max_seq_length - len(input_mask))
+
+        assert len(input_ids) == max_seq_length
+        assert len(input_mask) == max_seq_length
+
+        masked_lm_positions = list(instance.masked_lm_positions)
+        masked_lm_ids = vocab.convert_tokens_to_ids(instance.masked_lm_labels)
+        masked_lm_weights = [1.0] * len(masked_lm_ids)
+
+        masked_lm_positions += [0] * (max_predictions_per_seq - len(masked_lm_positions))
+        masked_lm_ids += [0] * (max_predictions_per_seq - len(masked_lm_ids))
+        masked_lm_weights += [0.0] * (max_predictions_per_seq - len(masked_lm_weights))
+
+        features = collections.OrderedDict()
+        features["info"] = create_int_feature(instance.info)
+        features["input_ids"] = create_int_feature(input_ids)
+        features["input_mask"] = create_int_feature(input_mask)
+        features["masked_lm_positions"] = create_int_feature(masked_lm_positions)
+        features["masked_lm_ids"] = create_int_feature(masked_lm_ids)
+        features["masked_lm_weights"] = create_float_feature(masked_lm_weights)
+
+        tf_example = tf.train.Example(
+            features=tf.train.Features(feature=features))
+
+        writers[writer_index].write(tf_example.SerializeToString())
+        writer_index = (writer_index + 1) % len(writers)
+
+        total_written += 1
+
+    for writer in writers:
+        writer.close()
+
+    tf.logging.info("Wrote %d total instances", total_written)
+    return total_written
+
+
+def create_int_feature(values):
+    feature = tf.train.Feature(
+        int64_list=tf.train.Int64List(value=list(values)))
+    return feature
+
+
+def create_float_feature(values):
+    feature = tf.train.Feature(
+        float_list=tf.train.FloatList(value=list(values)))
+    return feature
+
+
+def create_training_instances(all_documents_raw,
+                              max_seq_length,
+                              dupe_factor,
+                              short_seq_prob,
+                              masked_lm_prob,
+                              max_predictions_per_seq,
+                              rng,
+                              vocab,
+                              mask_prob,
+                              sliding_step,
+                              pool_size,
+                              force_last=False):
+    """Create `TrainingInstance`s from raw text."""
+    all_documents = {}
+
+    if force_last:
+        max_num_tokens = max_seq_length
+        for user, item_seq in all_documents_raw.items():
+            if len(item_seq) == 0:
+                print("got empty seq:" + user)
+                continue
+            all_documents[user] = [item_seq[-max_num_tokens:]]
+    else:
+        max_num_tokens = max_seq_length  # we need two sentence
+
+        assert sliding_step > 0
+        assert sliding_step < max_num_tokens
+        for user, item_seq in all_documents_raw.items():
+            if len(item_seq) == 0:
+                print("got empty seq:" + user)
+                continue
+
+            if len(item_seq) <= max_num_tokens:
+                all_documents[user] = [item_seq]
+            else:
+                beg_idx = list(range(len(item_seq)-max_num_tokens, 0, -sliding_step))
+                beg_idx.append(0)
+                all_documents[user] = [item_seq[i:i + max_num_tokens] for i in beg_idx[::-1]]
+
+    instances = []
+    if force_last:
+        for user in all_documents:
+            instances.extend(
+                create_instances_from_document_test(
+                    all_documents, user, max_seq_length))
+        print("num of instance:{}".format(len(instances)))
+    else:
+        start_time = time.perf_counter()
+        pool = multiprocessing.Pool(processes=pool_size)
+        instances = []
+        print("document num: {}".format(len(all_documents)))
+
+        def log_result(result):
+            print("callback function result type: {}, size: {} ".format(type(result), len(result)))
+            instances.extend(result)
+
+        for step in range(dupe_factor):
+            # create_instances_threading(
+            #     all_documents, user, max_seq_length, short_seq_prob,
+            #     masked_lm_prob, max_predictions_per_seq, vocab, random.Random(random.randint(1, 10000)),
+            #     mask_prob, step)
+            pool.apply_async(
+                create_instances_threading, args=(
+                    all_documents, user, max_seq_length, short_seq_prob,
+                    masked_lm_prob, max_predictions_per_seq, vocab, random.Random(random.randint(1,10000)),
+                    mask_prob, step), callback=log_result)
+        pool.close()
+        pool.join()
+
+        for user in all_documents:
+            instances.extend(mask_last(all_documents, user, max_seq_length, short_seq_prob,
+                                       masked_lm_prob, max_predictions_per_seq, vocab, rng))
+
+        print("num of instance:{}; time:{}".format(len(instances), time.perf_counter() - start_time))
+    rng.shuffle(instances)
+    return instances
+
+
+def create_instances_threading(all_documents, user, max_seq_length, short_seq_prob,
+                               masked_lm_prob, max_predictions_per_seq, vocab, rng,
+                               mask_prob, step):
+    cnt = 0
+    start_time = time.perf_counter()
+    instances = []
+    vocab_item_set = set(vocab.get_items())
+    for user in all_documents:
+        cnt += 1
+        if cnt % 5000 == 0:
+            print("step: {}, name: {}, step: {}, time: {}".format(step, multiprocessing.current_process().name, cnt, time.perf_counter()-start_time))
+            start_time = time.perf_counter()
+        instances.extend(create_instances_from_document_train(
+            all_documents, user, max_seq_length, short_seq_prob,
+            masked_lm_prob, max_predictions_per_seq, vocab_item_set, rng,
+            mask_prob))
+
+    return instances
+
+
+def mask_last(
+        all_documents, user, max_seq_length, short_seq_prob, masked_lm_prob,
+        max_predictions_per_seq, vocab, rng):
+    """Creates `TrainingInstance`s for a single document."""
+    document = all_documents[user]
+    max_num_tokens = max_seq_length
+
+    instances = []
+    info = [int(user.split("_")[1])]
+    vocab_items = vocab.get_items()
+
+    for tokens in document:
+        assert len(tokens) >= 1
+        assert len(tokens) <= max_num_tokens
+
+        (tokens, masked_lm_positions, masked_lm_labels) = create_masked_lm_predictions_force_last(tokens)
+        instance = TrainingInstance(
+            info=info,
+            tokens=tokens,
+            masked_lm_positions=masked_lm_positions,
+            masked_lm_labels=masked_lm_labels)
+        instances.append(instance)
+
+    return instances
+
+
+def create_instances_from_document_test(all_documents, user, max_seq_length):
+    """Creates `TrainingInstance`s for a single document."""
+    document = all_documents[user]
+    max_num_tokens = max_seq_length
+
+    assert len(document) == 1 and len(document[0]) <= max_num_tokens
+
+    tokens = document[0]
+    assert len(tokens) >= 1
+
+    (tokens, masked_lm_positions,
+     masked_lm_labels) = create_masked_lm_predictions_force_last(tokens)
+
+    info = [int(user.split("_")[1])]
+    instance = TrainingInstance(
+        info=info,
+        tokens=tokens,
+        masked_lm_positions=masked_lm_positions,
+        masked_lm_labels=masked_lm_labels)
+
+    return [instance]
+
+
+def create_instances_from_document_train(
+        all_documents, user, max_seq_length, short_seq_prob, masked_lm_prob,
+        max_predictions_per_seq, vocab_items, rng, mask_prob):
+    """Creates `TrainingInstance`s for a single document."""
+    document = all_documents[user]
+
+    max_num_tokens = max_seq_length
+
+    instances = []
+    info = [int(user.split("_")[1])]
+    # vocab_items = vocab.get_items()
+
+    for tokens in document:
+        assert len(tokens) >= 1
+        assert len(tokens) <= max_num_tokens
+
+        (tokens, masked_lm_positions, masked_lm_labels) = \
+            create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_items, rng, mask_prob)
+
+        instance = TrainingInstance(info=info,
+                                    tokens=tokens,
+                                    masked_lm_positions=masked_lm_positions,
+                                    masked_lm_labels=masked_lm_labels)
+        instances.append(instance)
+
+    return instances
+
+
+MaskedLmInstance = collections.namedtuple("MaskedLmInstance",
+                                          ["index", "label"])
+
+
+def create_masked_lm_predictions_force_last(tokens):
+    """Creates the predictions for the masked LM objective."""
+
+    last_index = -1
+    for (i, token) in enumerate(tokens):
+        if token == "[CLS]" or token == "[PAD]" or token == '[NO_USE]':
+            continue
+        last_index = i
+
+    assert last_index > 0
+
+    output_tokens = list(tokens)
+    output_tokens[last_index] = "[MASK]"
+
+    masked_lm_positions = [last_index]
+    masked_lm_labels = [tokens[last_index]]
+
+    return (output_tokens, masked_lm_positions, masked_lm_labels)
+
+
+def create_masked_lm_predictions(tokens, masked_lm_prob,
+                                 max_predictions_per_seq, vocab_words, rng,
+                                 mask_prob):
+    """Creates the predictions for the masked LM objective."""
+
+    cand_indexes = []
+    for (i, token) in enumerate(tokens):
+        if token not in vocab_words:
+            continue
+        cand_indexes.append(i)
+
+    rng.shuffle(cand_indexes)
+
+    output_tokens = list(tokens)
+
+    num_to_predict = min(max_predictions_per_seq,
+                         max(1, int(round(len(tokens) * masked_lm_prob))))
+
+    masked_lms = []
+    covered_indexes = set()
+    for index in cand_indexes:
+        if len(masked_lms) >= num_to_predict:
+            break
+        if index in covered_indexes:
+            continue
+        covered_indexes.add(index)
+
+        masked_token = None
+        # 80% of the time, replace with [MASK]
+        if rng.random() < mask_prob:
+            masked_token = "[MASK]"
+        else:
+            # 10% of the time, keep original
+            raise NotImplementedError
+            if rng.random() < 0.5:
+                masked_token = tokens[index]
+            # 10% of the time, replace with random word
+            else:
+                # masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]
+                raise NotImplementedError
+                masked_token = rng.choice(vocab_words)
+
+
+        output_tokens[index] = masked_token
+
+        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))
+
+    masked_lms = sorted(masked_lms, key=lambda x: x.index)
+
+    masked_lm_positions = []
+    masked_lm_labels = []
+    for p in masked_lms:
+        masked_lm_positions.append(p.index)
+        masked_lm_labels.append(p.label)
+
+    return (output_tokens, masked_lm_positions, masked_lm_labels)
+
+
+def gen_samples(data,
+                output_filename,
+                rng,
+                vocab,
+                max_seq_length,
+                dupe_factor,
+                short_seq_prob,
+                mask_prob,
+                masked_lm_prob,
+                max_predictions_per_seq,
+                sliding_step,
+                pool_size,
+                force_last=False):
+    # create train
+    instances = create_training_instances(
+        data, max_seq_length, dupe_factor, short_seq_prob, masked_lm_prob,
+        max_predictions_per_seq, rng, vocab, mask_prob, sliding_step,
+        pool_size, force_last)
+
+    tf.logging.info("*** Writing to output files ***")
+    tf.logging.info("  %s", output_filename)
+
+    total_written = write_instance_to_example_files(instances, max_seq_length,
+                                                    max_predictions_per_seq, vocab,
+                                                    [output_filename])
+    return total_written
+
+
+def main(config, dataset: Dataset, output_dir: str, tf_record_name: str):
+    tf.logging.set_verbosity(tf.logging.DEBUG)
+
+    max_seq_length = config.max_seq_len
+    max_predictions_per_seq = int(round(config.max_seq_len*config.masked_lm_prob))
+    masked_lm_prob = config.masked_lm_prob
+    mask_prob = 1.0
+    dupe_factor = config.dupe_factor
+    sliding_step = config.sliding_step
+    pool_size = config.pool_size
+
+    if not os.path.exists(output_dir):
+        os.makedirs(output_dir)
+
+    user_train = dataset.train_data.to_user_dict_by_time()
+    user_train = {user: items.tolist() for user, items in user_train.items()}
+
+    user_test = dataset.test_data.to_user_dict_by_time()
+    user_test = {user: items.tolist() for user, items in user_test.items()}
+
+    # get the max index of the data
+    user_train_data = {
+        'user_' + str(k): ['item_' + str(item) for item in v]
+        for k, v in user_train.items() if len(v) > 0
+    }
+    user_test_data = {
+        'user_' + str(u):
+            ['item_' + str(item) for item in (user_train[u] + user_test[u])]
+        for u in user_train if len(user_train[u]) > 0 and len(user_test[u]) > 0
+    }
+    rng = random.Random(random_seed)
+
+    vocab = FreqVocab(user_test_data)
+    user_test_data_output = {
+        k: vocab.convert_tokens_to_ids(v)
+        for k, v in user_test_data.items()
+    }
+
+    print('begin to generate train')
+    output_filename = os.path.join(output_dir, tf_record_name + '.train.tfrecord')
+    total_written = gen_samples(user_train_data,
+                                output_filename,
+                                rng,
+                                vocab,
+                                max_seq_length,
+                                dupe_factor,
+                                short_seq_prob,
+                                mask_prob,
+                                masked_lm_prob,
+                                max_predictions_per_seq,
+                                sliding_step,
+                                pool_size,
+                                force_last=False)
+    print('train:{}'.format(output_filename))
+    np.save(os.path.join(output_dir, tf_record_name + '.train.num.npy'), total_written)
+
+    print('begin to generate test')
+    output_filename = os.path.join(output_dir, tf_record_name + '.test.tfrecord')
+    gen_samples(
+        user_test_data,
+        output_filename,
+        rng,
+        vocab,
+        max_seq_length,
+        dupe_factor,
+        short_seq_prob,
+        mask_prob,
+        masked_lm_prob,
+        max_predictions_per_seq,
+        -1.0,
+        pool_size,
+        force_last=True)
+    print('test:{}'.format(output_filename))
+
+    print('vocab_size:{}, user_size:{}, item_size:{}, item_with_other_size:{}'.
+          format(vocab.get_vocab_size(),
+                 vocab.get_user_count(),
+                 vocab.get_item_count(),
+                 vocab.get_item_count() + vocab.get_special_token_count()))
+    vocab_file_name = os.path.join(output_dir, tf_record_name + '.vocab')
+    print('vocab pickle file: ' + vocab_file_name)
+    with open(vocab_file_name, 'wb') as output_file:
+        pickle.dump(vocab, output_file, protocol=2)
+
+    his_file_name = os.path.join(output_dir, tf_record_name + '.his')
+    print('test data pickle file: ' + his_file_name)
+    with open(his_file_name, 'wb') as output_file:
+        pickle.dump(user_test_data_output, output_file, protocol=2)
+    print('done.')
```

## skrec/recommender/BERT4Rec/vocab.py

 * *Ordering differences only*

```diff
@@ -1,73 +1,73 @@
-from collections import Counter
-
-
-def convert_by_vocab(vocab, items):
-    """Converts a sequence of [tokens|ids] using the vocab."""
-    output = []
-    for item in items:
-        output.append(vocab[item])
-    return output
-
-
-class FreqVocab(object):
-    """Runs end-to-end tokenziation."""
-
-    def __init__(self, user_to_list):
-        # layout of the  ulary
-        # item_id based on freq
-        # special token
-        # user_id based on nothing
-        self.counter = Counter()  # sorted(self.items(), key=_itemgetter(1), reverse=True)
-        self.user_set = set()
-        for u, item_list in user_to_list.items():
-            self.counter.update(item_list)
-            self.user_set.add(str(u))
-
-        self.user_count = len(self.user_set)
-        self.item_count = len(self.counter.keys())
-        self.special_tokens = {"[pad]", "[MASK]", '[NO_USE]'}
-        self.token_to_ids = {}  # index begin from 1
-        #first items
-        for token, count in self.counter.most_common():
-            self.token_to_ids[token] = len(self.token_to_ids) + 1
-
-        # then special tokens
-        for token in self.special_tokens:
-            self.token_to_ids[token] = len(self.token_to_ids) + 1
-
-        # then user
-#         for user in self.user_set:
-#             self.token_to_ids[user] = len(self.token_to_ids) + 1
-
-        self.id_to_tokens = {v: k for k, v in self.token_to_ids.items()}
-        self.vocab_words = list(self.token_to_ids.keys())
-
-    def convert_tokens_to_ids(self, tokens):
-        return convert_by_vocab(self.token_to_ids, tokens)
-
-    def convert_ids_to_tokens(self, ids):
-        return convert_by_vocab(self.id_to_tokens, ids)
-
-    def get_vocab_words(self):
-        return self.vocab_words  # not in order
-
-    def get_item_count(self):
-        return self.item_count
-
-    def get_user_count(self):
-        return self.user_count
-
-    def get_items(self):
-        return list(self.counter.keys())
-
-    def get_users(self):
-        return self.user_set
-
-    def get_special_token_count(self):
-        return len(self.special_tokens)
-
-    def get_special_token(self):
-        return self.special_tokens
-
-    def get_vocab_size(self):
-        return self.get_item_count() + self.get_special_token_count() + 1 #self.get_user_count()
+from collections import Counter
+
+
+def convert_by_vocab(vocab, items):
+    """Converts a sequence of [tokens|ids] using the vocab."""
+    output = []
+    for item in items:
+        output.append(vocab[item])
+    return output
+
+
+class FreqVocab(object):
+    """Runs end-to-end tokenziation."""
+
+    def __init__(self, user_to_list):
+        # layout of the  ulary
+        # item_id based on freq
+        # special token
+        # user_id based on nothing
+        self.counter = Counter()  # sorted(self.items(), key=_itemgetter(1), reverse=True)
+        self.user_set = set()
+        for u, item_list in user_to_list.items():
+            self.counter.update(item_list)
+            self.user_set.add(str(u))
+
+        self.user_count = len(self.user_set)
+        self.item_count = len(self.counter.keys())
+        self.special_tokens = {"[pad]", "[MASK]", '[NO_USE]'}
+        self.token_to_ids = {}  # index begin from 1
+        #first items
+        for token, count in self.counter.most_common():
+            self.token_to_ids[token] = len(self.token_to_ids) + 1
+
+        # then special tokens
+        for token in self.special_tokens:
+            self.token_to_ids[token] = len(self.token_to_ids) + 1
+
+        # then user
+#         for user in self.user_set:
+#             self.token_to_ids[user] = len(self.token_to_ids) + 1
+
+        self.id_to_tokens = {v: k for k, v in self.token_to_ids.items()}
+        self.vocab_words = list(self.token_to_ids.keys())
+
+    def convert_tokens_to_ids(self, tokens):
+        return convert_by_vocab(self.token_to_ids, tokens)
+
+    def convert_ids_to_tokens(self, ids):
+        return convert_by_vocab(self.id_to_tokens, ids)
+
+    def get_vocab_words(self):
+        return self.vocab_words  # not in order
+
+    def get_item_count(self):
+        return self.item_count
+
+    def get_user_count(self):
+        return self.user_count
+
+    def get_items(self):
+        return list(self.counter.keys())
+
+    def get_users(self):
+        return self.user_set
+
+    def get_special_token_count(self):
+        return len(self.special_tokens)
+
+    def get_special_token(self):
+        return self.special_tokens
+
+    def get_vocab_size(self):
+        return self.get_item_count() + self.get_special_token_count() + 1 #self.get_user_count()
```

## skrec/recommender/BERT4Rec/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-from .BERT4Rec import BERT4Rec, BERT4RecConfig
-
-__all__ = [BERT4Rec, BERT4RecConfig]
+from .BERT4Rec import BERT4Rec, BERT4RecConfig
+
+__all__ = [BERT4Rec, BERT4RecConfig]
```

## skrec/recommender/BERT4Rec/optimization.py

 * *Ordering differences only*

```diff
@@ -1,171 +1,171 @@
-# coding=utf-8
-# Copyright 2018 The Google AI Language Team Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-"""Functions and classes related to optimization (weight updates)."""
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import re
-import tensorflow as tf
-
-
-def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps,
-                     use_tpu):
-    """Creates an optimizer training op."""
-    global_step = tf.train.get_or_create_global_step()
-
-    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)
-
-    # Implements linear decay of the learning rate.
-    learning_rate = tf.train.polynomial_decay(
-        learning_rate,
-        global_step,
-        num_train_steps,
-        end_learning_rate=0.0,
-        power=1.0,
-        cycle=False)
-
-    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the
-    # learning rate will be `global_step/num_warmup_steps * init_lr`.
-    if num_warmup_steps:
-        global_steps_int = tf.cast(global_step, tf.int32)
-        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)
-
-        global_steps_float = tf.cast(global_steps_int, tf.float32)
-        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)
-
-        warmup_percent_done = global_steps_float / warmup_steps_float
-        warmup_learning_rate = init_lr * warmup_percent_done
-
-        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)
-        learning_rate = ((1.0 - is_warmup) * learning_rate +
-                         is_warmup * warmup_learning_rate)
-
-    # It is recommended that you use this optimizer for fine tuning, since this
-    # is how the model was trained (note that the Adam m/v variables are NOT
-    # loaded from init_checkpoint.)
-    optimizer = AdamWeightDecayOptimizer(
-        learning_rate=learning_rate,
-        weight_decay_rate=0.01,
-        beta_1=0.9,
-        beta_2=0.999,
-        epsilon=1e-6,
-        exclude_from_weight_decay=["LayerNorm", "layer_norm", "bias"])
-
-    if use_tpu:
-        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)
-
-    tvars = tf.trainable_variables()
-    grads = tf.gradients(loss, tvars)
-
-    # This is how the model was pre-trained.
-    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=5.0)
-
-    train_op = optimizer.apply_gradients(
-        zip(grads, tvars), global_step=global_step)
-
-    new_global_step = global_step + 1
-    train_op = tf.group(train_op, [global_step.assign(new_global_step)])
-    return train_op
-
-
-class AdamWeightDecayOptimizer(tf.train.Optimizer):
-    """A basic Adam optimizer that includes "correct" L2 weight decay."""
-
-    def __init__(self,
-                 learning_rate,
-                 weight_decay_rate=0.0,
-                 beta_1=0.9,
-                 beta_2=0.999,
-                 epsilon=1e-6,
-                 exclude_from_weight_decay=None,
-                 name="AdamWeightDecayOptimizer"):
-        """Constructs a AdamWeightDecayOptimizer."""
-        super(AdamWeightDecayOptimizer, self).__init__(False, name)
-
-        self.learning_rate = learning_rate
-        self.weight_decay_rate = weight_decay_rate
-        self.beta_1 = beta_1
-        self.beta_2 = beta_2
-        self.epsilon = epsilon
-        self.exclude_from_weight_decay = exclude_from_weight_decay
-
-    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
-        """See base class."""
-        assignments = []
-        for (grad, param) in grads_and_vars:
-            if grad is None or param is None:
-                continue
-
-            param_name = self._get_variable_name(param.name)
-
-            m = tf.get_variable(
-                name=param_name + "/adam_m",
-                shape=param.shape.as_list(),
-                dtype=tf.float32,
-                trainable=False,
-                initializer=tf.zeros_initializer())
-            v = tf.get_variable(
-                name=param_name + "/adam_v",
-                shape=param.shape.as_list(),
-                dtype=tf.float32,
-                trainable=False,
-                initializer=tf.zeros_initializer())
-
-            # Standard Adam update.
-            next_m = (tf.multiply(self.beta_1, m) +
-                      tf.multiply(1.0 - self.beta_1, grad))
-            next_v = (tf.multiply(self.beta_2, v) +
-                      tf.multiply(1.0 - self.beta_2, tf.square(grad)))
-
-            update = next_m / (tf.sqrt(next_v) + self.epsilon)
-
-            # Just adding the square of the weights to the loss function is *not*
-            # the correct way of using L2 regularization/weight decay with Adam,
-            # since that will interact with the m and v parameters in strange ways.
-            #
-            # Instead we want ot decay the weights in a manner that doesn't interact
-            # with the m/v parameters. This is equivalent to adding the square
-            # of the weights to the loss with plain (non-momentum) SGD.
-            if self._do_use_weight_decay(param_name):
-                update += self.weight_decay_rate * param
-
-            update_with_lr = self.learning_rate * update
-
-            next_param = param - update_with_lr
-
-            assignments.extend(
-                [param.assign(next_param),
-                 m.assign(next_m),
-                 v.assign(next_v)])
-        return tf.group(*assignments, name=name)
-
-    def _do_use_weight_decay(self, param_name):
-        """Whether to use L2 weight decay for `param_name`."""
-        if not self.weight_decay_rate:
-            return False
-        if self.exclude_from_weight_decay:
-            for r in self.exclude_from_weight_decay:
-                if re.search(r, param_name) is not None:
-                    return False
-        return True
-
-    def _get_variable_name(self, param_name):
-        """Get the variable name from the tensor name."""
-        m = re.match("^(.*):\\d+$", param_name)
-        if m is not None:
-            param_name = m.group(1)
-        return param_name
+# coding=utf-8
+# Copyright 2018 The Google AI Language Team Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Functions and classes related to optimization (weight updates)."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import re
+import tensorflow as tf
+
+
+def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps,
+                     use_tpu):
+    """Creates an optimizer training op."""
+    global_step = tf.train.get_or_create_global_step()
+
+    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)
+
+    # Implements linear decay of the learning rate.
+    learning_rate = tf.train.polynomial_decay(
+        learning_rate,
+        global_step,
+        num_train_steps,
+        end_learning_rate=0.0,
+        power=1.0,
+        cycle=False)
+
+    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the
+    # learning rate will be `global_step/num_warmup_steps * init_lr`.
+    if num_warmup_steps:
+        global_steps_int = tf.cast(global_step, tf.int32)
+        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)
+
+        global_steps_float = tf.cast(global_steps_int, tf.float32)
+        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)
+
+        warmup_percent_done = global_steps_float / warmup_steps_float
+        warmup_learning_rate = init_lr * warmup_percent_done
+
+        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)
+        learning_rate = ((1.0 - is_warmup) * learning_rate +
+                         is_warmup * warmup_learning_rate)
+
+    # It is recommended that you use this optimizer for fine tuning, since this
+    # is how the model was trained (note that the Adam m/v variables are NOT
+    # loaded from init_checkpoint.)
+    optimizer = AdamWeightDecayOptimizer(
+        learning_rate=learning_rate,
+        weight_decay_rate=0.01,
+        beta_1=0.9,
+        beta_2=0.999,
+        epsilon=1e-6,
+        exclude_from_weight_decay=["LayerNorm", "layer_norm", "bias"])
+
+    if use_tpu:
+        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)
+
+    tvars = tf.trainable_variables()
+    grads = tf.gradients(loss, tvars)
+
+    # This is how the model was pre-trained.
+    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=5.0)
+
+    train_op = optimizer.apply_gradients(
+        zip(grads, tvars), global_step=global_step)
+
+    new_global_step = global_step + 1
+    train_op = tf.group(train_op, [global_step.assign(new_global_step)])
+    return train_op
+
+
+class AdamWeightDecayOptimizer(tf.train.Optimizer):
+    """A basic Adam optimizer that includes "correct" L2 weight decay."""
+
+    def __init__(self,
+                 learning_rate,
+                 weight_decay_rate=0.0,
+                 beta_1=0.9,
+                 beta_2=0.999,
+                 epsilon=1e-6,
+                 exclude_from_weight_decay=None,
+                 name="AdamWeightDecayOptimizer"):
+        """Constructs a AdamWeightDecayOptimizer."""
+        super(AdamWeightDecayOptimizer, self).__init__(False, name)
+
+        self.learning_rate = learning_rate
+        self.weight_decay_rate = weight_decay_rate
+        self.beta_1 = beta_1
+        self.beta_2 = beta_2
+        self.epsilon = epsilon
+        self.exclude_from_weight_decay = exclude_from_weight_decay
+
+    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
+        """See base class."""
+        assignments = []
+        for (grad, param) in grads_and_vars:
+            if grad is None or param is None:
+                continue
+
+            param_name = self._get_variable_name(param.name)
+
+            m = tf.get_variable(
+                name=param_name + "/adam_m",
+                shape=param.shape.as_list(),
+                dtype=tf.float32,
+                trainable=False,
+                initializer=tf.zeros_initializer())
+            v = tf.get_variable(
+                name=param_name + "/adam_v",
+                shape=param.shape.as_list(),
+                dtype=tf.float32,
+                trainable=False,
+                initializer=tf.zeros_initializer())
+
+            # Standard Adam update.
+            next_m = (tf.multiply(self.beta_1, m) +
+                      tf.multiply(1.0 - self.beta_1, grad))
+            next_v = (tf.multiply(self.beta_2, v) +
+                      tf.multiply(1.0 - self.beta_2, tf.square(grad)))
+
+            update = next_m / (tf.sqrt(next_v) + self.epsilon)
+
+            # Just adding the square of the weights to the loss function is *not*
+            # the correct way of using L2 regularization/weight decay with Adam,
+            # since that will interact with the m and v parameters in strange ways.
+            #
+            # Instead we want ot decay the weights in a manner that doesn't interact
+            # with the m/v parameters. This is equivalent to adding the square
+            # of the weights to the loss with plain (non-momentum) SGD.
+            if self._do_use_weight_decay(param_name):
+                update += self.weight_decay_rate * param
+
+            update_with_lr = self.learning_rate * update
+
+            next_param = param - update_with_lr
+
+            assignments.extend(
+                [param.assign(next_param),
+                 m.assign(next_m),
+                 v.assign(next_v)])
+        return tf.group(*assignments, name=name)
+
+    def _do_use_weight_decay(self, param_name):
+        """Whether to use L2 weight decay for `param_name`."""
+        if not self.weight_decay_rate:
+            return False
+        if self.exclude_from_weight_decay:
+            for r in self.exclude_from_weight_decay:
+                if re.search(r, param_name) is not None:
+                    return False
+        return True
+
+    def _get_variable_name(self, param_name):
+        """Get the variable name from the tensor name."""
+        m = re.match("^(.*):\\d+$", param_name)
+        if m is not None:
+            param_name = m.group(1)
+        return param_name
```

## skrec/io/data_iterator.py

 * *Ordering differences only*

```diff
@@ -1,342 +1,342 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["PointwiseIterator", "PairwiseIterator",
-           "SequentialPointwiseIterator", "SequentialPairwiseIterator",
-           "UserVecIterator", "ItemVecIterator"
-           ]
-
-from typing import Dict
-from collections import Iterable
-from collections import OrderedDict
-import numpy as np
-from ..utils.py import OrderedDefaultDict
-from ..utils.py import BatchIterator
-from ..utils.py import randint_choice
-from ..utils.py import pad_sequences
-from .dataset import ImplicitFeedback
-
-
-class _Iterator(object):
-    def __iter__(self):
-        raise NotImplementedError
-
-    def __len__(self):
-        raise NotImplementedError
-
-
-def _generate_positive_items(user_pos_dict: Dict[int, np.ndarray]):
-    assert user_pos_dict, "'user_pos_dict' cannot be empty."
-
-    list_users, list_items = [], []
-    user_n_pos = OrderedDict()
-
-    for user, items in user_pos_dict.items():
-        list_items.append(items)
-        list_users.append(np.full_like(items, user))
-        user_n_pos[user] = len(items)
-    users_ary = np.concatenate(list_users, axis=0)
-    items_ary = np.concatenate(list_items, axis=0)
-    return user_n_pos, users_ary, items_ary
-
-
-def _generative_time_order_positive_items(user_pos_dict: Dict[int, np.ndarray],
-                                          num_previous: int=1, num_next: int=1, pad=None):
-    assert user_pos_dict, "'user_pos_dict' cannot be empty."
-    assert num_previous >= 1
-    assert num_next >= 1
-    # TODO .
-    users_list, seqs_list = [], []
-    user_n_pos = OrderedDefaultDict(int)
-
-    tot_len = num_previous + num_next
-    for user, seq_items in user_pos_dict.items():
-        # user_n_pos[user] = 0
-        for idx in range(len(seq_items), 0, -1):
-            cur_seqs = seq_items[:idx]
-            if len(cur_seqs) >= tot_len:  # not pad
-                seqs_list.append(cur_seqs[-tot_len:])
-                users_list.append(user)
-                user_n_pos[user] += 1
-            elif pad is not None and len(cur_seqs) > num_next:  # pad
-                seqs_list.append(cur_seqs[-tot_len:])
-                users_list.append(user)
-                user_n_pos[user] += 1
-            else:  # next user
-                break
-
-    if pad is not None and tot_len > 2:
-        seqs_ary = pad_sequences(seqs_list, value=pad, max_len=tot_len,
-                                 padding='pre', truncating='pre', dtype=np.int32)
-    else:
-        seqs_ary = np.int32(seqs_list)
-
-    previous_items, next_items = np.split(seqs_ary, [num_previous], axis=-1)
-    users_ary = np.int32(users_list)
-    return user_n_pos, users_ary, previous_items, next_items
-
-
-def _sampling_negative_items(user_n_pos: OrderedDict, num_neg: int, num_items: int,
-                             user_pos_dict: Dict[int, np.ndarray]):
-    assert num_neg > 0, "'num_neg' must be a positive integer."
-
-    neg_items_list = []
-    for user, n_pos in user_n_pos.items():
-        neg_items = randint_choice(num_items, size=n_pos*num_neg, exclusion=user_pos_dict[user])
-        neg_items = neg_items if isinstance(neg_items, Iterable) else np.int32([neg_items])  # only one item
-        neg_items = np.reshape(neg_items, newshape=[n_pos, num_neg])
-        neg_items_list.append(neg_items)
-
-    return np.concatenate(neg_items_list, axis=0)
-
-
-class PointwiseIterator(_Iterator):
-    """Sample negative items and iterate dataset with pointwise training instances.
-
-    The training instances consist of `batch_users`, `batch_items` and
-    `batch_labels`, which are lists of users, items and labels. All lengths of
-    them are `batch_size`.
-    Positive and negative items are labeled as `1.` and  `0.`, respectively.
-    """
-    def __init__(self, dataset: ImplicitFeedback,
-                 num_neg: int=1, batch_size: int=1024,
-                 shuffle: bool=True, drop_last: bool=False):
-        """Initializes a new `PointwiseIterator` instance.
-
-        Args:
-            dataset (ImplicitFeedback): An instance of `Interaction`.
-            num_neg (int): How many negative items for each positive item.
-                Defaults to `1`.
-            batch_size (int): How many samples per batch to load.
-                Defaults to `1`.
-            shuffle (bool): Whether reshuffling the samples at every epoch.
-                Defaults to `False`.
-            drop_last (bool): Whether dropping the last incomplete batch.
-                Defaults to `False`.
-        """
-        super(PointwiseIterator, self).__init__()
-        assert num_neg > 0, "'num_neg' must be a positive integer."
-
-        self.num_neg = num_neg
-        self.batch_size = batch_size
-        self.shuffle = shuffle
-        self.drop_last = drop_last
-        self.num_items = dataset.num_items
-        self.user_pos_dict = dataset.to_user_dict()
-
-        self.user_n_pos, users_ary, self.pos_items = \
-            _generate_positive_items(self.user_pos_dict)
-
-        self.all_users = np.tile(users_ary, self.num_neg + 1)
-        num_pos_items = len(self.pos_items)
-        pos_labels = np.ones(num_pos_items, dtype=np.float32)
-        neg_labels = np.zeros(num_pos_items*self.num_neg, dtype=np.float32)
-        self.all_labels = np.concatenate([pos_labels, neg_labels], axis=0)
-
-    def __len__(self):
-        n_sample = len(self.all_users)
-        if self.drop_last:
-            return n_sample // self.batch_size
-        else:
-            return (n_sample + self.batch_size - 1) // self.batch_size
-
-    def __iter__(self):
-        neg_items = _sampling_negative_items(self.user_n_pos, self.num_neg,
-                                             self.num_items, self.user_pos_dict)
-        # neg_items.shape: [-1, num_neg]
-
-        neg_items = neg_items.transpose().reshape([-1])
-        all_items = np.concatenate([self.pos_items, neg_items], axis=0)
-
-        data_iter = BatchIterator(self.all_users, all_items, self.all_labels,
-                                  batch_size=self.batch_size,
-                                  shuffle=self.shuffle, drop_last=self.drop_last)
-
-        for bat_users, bat_items, bat_labels in data_iter:
-            yield np.asarray(bat_users), np.asarray(bat_items), np.asarray(bat_labels)
-
-
-class PairwiseIterator(_Iterator):
-    def __init__(self, dataset: ImplicitFeedback,
-                 batch_size: int=1024, shuffle: bool=True, drop_last: bool=False):
-        """Initializes a new `PairwiseSampler` instance.
-
-        Args:
-            dataset (data.Interaction): An instance of `data.Interaction`.
-            batch_size (int): How many samples per batch to load.
-                Defaults to `1`.
-            shuffle (bool): Whether reshuffling the samples at every epoch.
-                Defaults to `False`.
-            drop_last (bool): Whether dropping the last incomplete batch.
-                Defaults to `False`.
-        """
-        super(PairwiseIterator, self).__init__()
-
-        self.batch_size = batch_size
-        self.shuffle = shuffle
-        self.drop_last = drop_last
-        self.num_items = dataset.num_items
-        self.user_pos_dict = dataset.to_user_dict()
-
-        self.user_n_pos, self.all_users, self.pos_items = \
-            _generate_positive_items(self.user_pos_dict)
-
-    def __len__(self):
-        n_sample = len(self.all_users)
-        if self.drop_last:
-            return n_sample // self.batch_size
-        else:
-            return (n_sample + self.batch_size - 1) // self.batch_size
-
-    def __iter__(self):
-        neg_items = _sampling_negative_items(self.user_n_pos, 1, self.num_items,
-                                             self.user_pos_dict).squeeze()
-
-        data_iter = BatchIterator(self.all_users, self.pos_items, neg_items,
-                                  batch_size=self.batch_size,
-                                  shuffle=self.shuffle, drop_last=self.drop_last)
-        for bat_users, bat_pos_items, bat_neg_items in data_iter:
-            yield np.asarray(bat_users), np.asarray(bat_pos_items), np.asarray(bat_neg_items)
-
-
-class SequentialPointwiseIterator(_Iterator):
-    def __init__(self, dataset: ImplicitFeedback,
-                 num_previous: int=1, num_next: int=1, num_neg: int=1,
-                 pad: int=None, batch_size: int=1024,
-                 shuffle: bool=True, drop_last: bool=False):
-        super(SequentialPointwiseIterator, self).__init__()
-        assert num_previous >= 1
-        assert num_next >= 1
-        assert num_neg >= 1
-        # TODO , .
-        self.num_previous = num_previous
-        self.num_next = num_next
-        self.num_neg = num_neg
-        self.pad = pad
-        self.batch_size = batch_size
-        self.shuffle = shuffle
-        self.drop_last = drop_last
-        self.num_items = dataset.num_items
-        self.user_pos_dict = dataset.to_user_dict_by_time()
-
-        self.user_n_pos, users_ary, item_seqs_ary, self.pos_next_items = \
-            _generative_time_order_positive_items(self.user_pos_dict, num_previous=num_previous,
-                                                  num_next=num_next, pad=pad)
-
-        self.all_users = np.tile(users_ary, self.num_neg + 1)
-        self.all_item_seqs = np.tile(item_seqs_ary, [self.num_neg + 1, 1]).squeeze()
-
-        len_pos = len(self.pos_next_items)
-        pos_labels = np.ones([len_pos, num_next], dtype=np.float32)
-        neg_labels = np.zeros([len_pos*self.num_neg, num_next], dtype=np.float32)
-        self.all_labels = np.concatenate([pos_labels, neg_labels], axis=0).squeeze()
-
-    def __len__(self):
-        n_sample = len(self.all_users)
-        if self.drop_last:
-            return n_sample // self.batch_size
-        else:
-            return (n_sample + self.batch_size - 1) // self.batch_size
-
-    def __iter__(self):
-        neg_next_items = _sampling_negative_items(self.user_n_pos, self.num_neg * self.num_next,
-                                                  self.num_items, self.user_pos_dict)
-
-        neg_next_items = np.concatenate(np.split(neg_next_items, self.num_neg, axis=-1), axis=0)
-        all_next_items = np.concatenate([self.pos_next_items, neg_next_items], axis=0).squeeze()
-
-        data_iter = BatchIterator(self.all_users, self.all_item_seqs,
-                                  all_next_items, self.all_labels,
-                                  batch_size=self.batch_size,
-                                  shuffle=self.shuffle, drop_last=self.drop_last)
-
-        for bat_users, bat_item_seqs, bat_next_items, bat_labels in data_iter:
-            yield np.asarray(bat_users), np.asarray(bat_item_seqs), \
-                  np.asarray(bat_next_items), np.asarray(bat_labels)
-
-
-class SequentialPairwiseIterator(_Iterator):
-    def __init__(self, dataset: ImplicitFeedback,
-                 num_previous: int = 1, num_next: int = 1,
-                 pad: int = None, batch_size: int = 1024,
-                 shuffle: bool = True, drop_last: bool = False):
-        assert num_previous >= 1
-        assert num_next >= 1
-        # TODO , .
-        self.num_previous = num_previous
-        self.num_next = num_next
-        self.pad = pad
-        self.batch_size = batch_size
-        self.shuffle = shuffle
-        self.drop_last = drop_last
-        self.num_items = dataset.num_items
-        self.user_pos_dict = dataset.to_user_dict_by_time()
-
-        self.user_n_pos, self.all_users, all_item_seqs, pos_next_items = \
-            _generative_time_order_positive_items(self.user_pos_dict, num_previous=num_previous,
-                                                  num_next=num_next, pad=pad)
-
-        self.all_item_seqs = all_item_seqs.squeeze()
-        self.pos_next_items = pos_next_items.squeeze()
-
-    def __len__(self):
-        n_sample = len(self.all_users)
-        if self.drop_last:
-            return n_sample // self.batch_size
-        else:
-            return (n_sample + self.batch_size - 1) // self.batch_size
-
-    def __iter__(self):
-        neg_next_items = _sampling_negative_items(self.user_n_pos, self.num_next, self.num_items,
-                                                  self.user_pos_dict).squeeze()
-
-        data_iter = BatchIterator(self.all_users, self.all_item_seqs,
-                                  self.pos_next_items, neg_next_items,
-                                  batch_size=self.batch_size,
-                                  shuffle=self.shuffle, drop_last=self.drop_last)
-
-        for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
-            yield np.asarray(bat_users), np.asarray(bat_item_seqs), \
-                  np.asarray(bat_pos_items), np.asarray(bat_neg_items)
-
-
-class UserVecIterator(_Iterator):
-    def __init__(self, dataset: ImplicitFeedback, batch_size: int=1024,
-                 shuffle: bool=True, drop_last: bool=False):
-        super(UserVecIterator, self).__init__()
-
-        self.batch_size = batch_size
-        self.shuffle = shuffle
-        self.drop_last = drop_last
-        self.user_csr_matrix = dataset.to_csr_matrix()
-        all_users = np.arange(dataset.num_users, dtype=np.int32)
-        self.user_iter = BatchIterator(all_users, batch_size=self.batch_size,
-                                       shuffle=self.shuffle, drop_last=self.drop_last)
-
-    def __len__(self):
-        return len(self.user_iter)
-
-    def __iter__(self):
-        for bat_users in self.user_iter:
-            yield self.user_csr_matrix[bat_users].toarray()
-
-
-class ItemVecIterator(_Iterator):
-    def __init__(self, dataset: ImplicitFeedback, batch_size: int=1024,
-                 shuffle: bool=True, drop_last: bool=False):
-        super(ItemVecIterator, self).__init__()
-
-        self.batch_size = batch_size
-        self.shuffle = shuffle
-        self.drop_last = drop_last
-        self.item_csr_matrix = dataset.to_csr_matrix().transpose().tocsr()
-        all_items = np.arange(dataset.num_items, dtype=np.int32)
-        self.item_iter = BatchIterator(all_items, batch_size=self.batch_size,
-                                       shuffle=self.shuffle, drop_last=self.drop_last)
-
-    def __len__(self):
-        return len(self.item_iter)
-
-    def __iter__(self):
-        for bat_items in self.item_iter:
-            yield self.item_csr_matrix[bat_items].toarray()
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["PointwiseIterator", "PairwiseIterator",
+           "SequentialPointwiseIterator", "SequentialPairwiseIterator",
+           "UserVecIterator", "ItemVecIterator"
+           ]
+
+from typing import Dict
+from collections import Iterable
+from collections import OrderedDict
+import numpy as np
+from ..utils.py import OrderedDefaultDict
+from ..utils.py import BatchIterator
+from ..utils.py import randint_choice
+from ..utils.py import pad_sequences
+from .dataset import ImplicitFeedback
+
+
+class _Iterator(object):
+    def __iter__(self):
+        raise NotImplementedError
+
+    def __len__(self):
+        raise NotImplementedError
+
+
+def _generate_positive_items(user_pos_dict: Dict[int, np.ndarray]):
+    assert user_pos_dict, "'user_pos_dict' cannot be empty."
+
+    list_users, list_items = [], []
+    user_n_pos = OrderedDict()
+
+    for user, items in user_pos_dict.items():
+        list_items.append(items)
+        list_users.append(np.full_like(items, user))
+        user_n_pos[user] = len(items)
+    users_ary = np.concatenate(list_users, axis=0)
+    items_ary = np.concatenate(list_items, axis=0)
+    return user_n_pos, users_ary, items_ary
+
+
+def _generative_time_order_positive_items(user_pos_dict: Dict[int, np.ndarray],
+                                          num_previous: int=1, num_next: int=1, pad=None):
+    assert user_pos_dict, "'user_pos_dict' cannot be empty."
+    assert num_previous >= 1
+    assert num_next >= 1
+    # TODO .
+    users_list, seqs_list = [], []
+    user_n_pos = OrderedDefaultDict(int)
+
+    tot_len = num_previous + num_next
+    for user, seq_items in user_pos_dict.items():
+        # user_n_pos[user] = 0
+        for idx in range(len(seq_items), 0, -1):
+            cur_seqs = seq_items[:idx]
+            if len(cur_seqs) >= tot_len:  # not pad
+                seqs_list.append(cur_seqs[-tot_len:])
+                users_list.append(user)
+                user_n_pos[user] += 1
+            elif pad is not None and len(cur_seqs) > num_next:  # pad
+                seqs_list.append(cur_seqs[-tot_len:])
+                users_list.append(user)
+                user_n_pos[user] += 1
+            else:  # next user
+                break
+
+    if pad is not None and tot_len > 2:
+        seqs_ary = pad_sequences(seqs_list, value=pad, max_len=tot_len,
+                                 padding='pre', truncating='pre', dtype=np.int32)
+    else:
+        seqs_ary = np.int32(seqs_list)
+
+    previous_items, next_items = np.split(seqs_ary, [num_previous], axis=-1)
+    users_ary = np.int32(users_list)
+    return user_n_pos, users_ary, previous_items, next_items
+
+
+def _sampling_negative_items(user_n_pos: OrderedDict, num_neg: int, num_items: int,
+                             user_pos_dict: Dict[int, np.ndarray]):
+    assert num_neg > 0, "'num_neg' must be a positive integer."
+
+    neg_items_list = []
+    for user, n_pos in user_n_pos.items():
+        neg_items = randint_choice(num_items, size=n_pos*num_neg, exclusion=user_pos_dict[user])
+        neg_items = neg_items if isinstance(neg_items, Iterable) else np.int32([neg_items])  # only one item
+        neg_items = np.reshape(neg_items, newshape=[n_pos, num_neg])
+        neg_items_list.append(neg_items)
+
+    return np.concatenate(neg_items_list, axis=0)
+
+
+class PointwiseIterator(_Iterator):
+    """Sample negative items and iterate dataset with pointwise training instances.
+
+    The training instances consist of `batch_users`, `batch_items` and
+    `batch_labels`, which are lists of users, items and labels. All lengths of
+    them are `batch_size`.
+    Positive and negative items are labeled as `1.` and  `0.`, respectively.
+    """
+    def __init__(self, dataset: ImplicitFeedback,
+                 num_neg: int=1, batch_size: int=1024,
+                 shuffle: bool=True, drop_last: bool=False):
+        """Initializes a new `PointwiseIterator` instance.
+
+        Args:
+            dataset (ImplicitFeedback): An instance of `Interaction`.
+            num_neg (int): How many negative items for each positive item.
+                Defaults to `1`.
+            batch_size (int): How many samples per batch to load.
+                Defaults to `1`.
+            shuffle (bool): Whether reshuffling the samples at every epoch.
+                Defaults to `False`.
+            drop_last (bool): Whether dropping the last incomplete batch.
+                Defaults to `False`.
+        """
+        super(PointwiseIterator, self).__init__()
+        assert num_neg > 0, "'num_neg' must be a positive integer."
+
+        self.num_neg = num_neg
+        self.batch_size = batch_size
+        self.shuffle = shuffle
+        self.drop_last = drop_last
+        self.num_items = dataset.num_items
+        self.user_pos_dict = dataset.to_user_dict()
+
+        self.user_n_pos, users_ary, self.pos_items = \
+            _generate_positive_items(self.user_pos_dict)
+
+        self.all_users = np.tile(users_ary, self.num_neg + 1)
+        num_pos_items = len(self.pos_items)
+        pos_labels = np.ones(num_pos_items, dtype=np.float32)
+        neg_labels = np.zeros(num_pos_items*self.num_neg, dtype=np.float32)
+        self.all_labels = np.concatenate([pos_labels, neg_labels], axis=0)
+
+    def __len__(self):
+        n_sample = len(self.all_users)
+        if self.drop_last:
+            return n_sample // self.batch_size
+        else:
+            return (n_sample + self.batch_size - 1) // self.batch_size
+
+    def __iter__(self):
+        neg_items = _sampling_negative_items(self.user_n_pos, self.num_neg,
+                                             self.num_items, self.user_pos_dict)
+        # neg_items.shape: [-1, num_neg]
+
+        neg_items = neg_items.transpose().reshape([-1])
+        all_items = np.concatenate([self.pos_items, neg_items], axis=0)
+
+        data_iter = BatchIterator(self.all_users, all_items, self.all_labels,
+                                  batch_size=self.batch_size,
+                                  shuffle=self.shuffle, drop_last=self.drop_last)
+
+        for bat_users, bat_items, bat_labels in data_iter:
+            yield np.asarray(bat_users), np.asarray(bat_items), np.asarray(bat_labels)
+
+
+class PairwiseIterator(_Iterator):
+    def __init__(self, dataset: ImplicitFeedback,
+                 batch_size: int=1024, shuffle: bool=True, drop_last: bool=False):
+        """Initializes a new `PairwiseSampler` instance.
+
+        Args:
+            dataset (data.Interaction): An instance of `data.Interaction`.
+            batch_size (int): How many samples per batch to load.
+                Defaults to `1`.
+            shuffle (bool): Whether reshuffling the samples at every epoch.
+                Defaults to `False`.
+            drop_last (bool): Whether dropping the last incomplete batch.
+                Defaults to `False`.
+        """
+        super(PairwiseIterator, self).__init__()
+
+        self.batch_size = batch_size
+        self.shuffle = shuffle
+        self.drop_last = drop_last
+        self.num_items = dataset.num_items
+        self.user_pos_dict = dataset.to_user_dict()
+
+        self.user_n_pos, self.all_users, self.pos_items = \
+            _generate_positive_items(self.user_pos_dict)
+
+    def __len__(self):
+        n_sample = len(self.all_users)
+        if self.drop_last:
+            return n_sample // self.batch_size
+        else:
+            return (n_sample + self.batch_size - 1) // self.batch_size
+
+    def __iter__(self):
+        neg_items = _sampling_negative_items(self.user_n_pos, 1, self.num_items,
+                                             self.user_pos_dict).squeeze()
+
+        data_iter = BatchIterator(self.all_users, self.pos_items, neg_items,
+                                  batch_size=self.batch_size,
+                                  shuffle=self.shuffle, drop_last=self.drop_last)
+        for bat_users, bat_pos_items, bat_neg_items in data_iter:
+            yield np.asarray(bat_users), np.asarray(bat_pos_items), np.asarray(bat_neg_items)
+
+
+class SequentialPointwiseIterator(_Iterator):
+    def __init__(self, dataset: ImplicitFeedback,
+                 num_previous: int=1, num_next: int=1, num_neg: int=1,
+                 pad: int=None, batch_size: int=1024,
+                 shuffle: bool=True, drop_last: bool=False):
+        super(SequentialPointwiseIterator, self).__init__()
+        assert num_previous >= 1
+        assert num_next >= 1
+        assert num_neg >= 1
+        # TODO , .
+        self.num_previous = num_previous
+        self.num_next = num_next
+        self.num_neg = num_neg
+        self.pad = pad
+        self.batch_size = batch_size
+        self.shuffle = shuffle
+        self.drop_last = drop_last
+        self.num_items = dataset.num_items
+        self.user_pos_dict = dataset.to_user_dict_by_time()
+
+        self.user_n_pos, users_ary, item_seqs_ary, self.pos_next_items = \
+            _generative_time_order_positive_items(self.user_pos_dict, num_previous=num_previous,
+                                                  num_next=num_next, pad=pad)
+
+        self.all_users = np.tile(users_ary, self.num_neg + 1)
+        self.all_item_seqs = np.tile(item_seqs_ary, [self.num_neg + 1, 1]).squeeze()
+
+        len_pos = len(self.pos_next_items)
+        pos_labels = np.ones([len_pos, num_next], dtype=np.float32)
+        neg_labels = np.zeros([len_pos*self.num_neg, num_next], dtype=np.float32)
+        self.all_labels = np.concatenate([pos_labels, neg_labels], axis=0).squeeze()
+
+    def __len__(self):
+        n_sample = len(self.all_users)
+        if self.drop_last:
+            return n_sample // self.batch_size
+        else:
+            return (n_sample + self.batch_size - 1) // self.batch_size
+
+    def __iter__(self):
+        neg_next_items = _sampling_negative_items(self.user_n_pos, self.num_neg * self.num_next,
+                                                  self.num_items, self.user_pos_dict)
+
+        neg_next_items = np.concatenate(np.split(neg_next_items, self.num_neg, axis=-1), axis=0)
+        all_next_items = np.concatenate([self.pos_next_items, neg_next_items], axis=0).squeeze()
+
+        data_iter = BatchIterator(self.all_users, self.all_item_seqs,
+                                  all_next_items, self.all_labels,
+                                  batch_size=self.batch_size,
+                                  shuffle=self.shuffle, drop_last=self.drop_last)
+
+        for bat_users, bat_item_seqs, bat_next_items, bat_labels in data_iter:
+            yield np.asarray(bat_users), np.asarray(bat_item_seqs), \
+                  np.asarray(bat_next_items), np.asarray(bat_labels)
+
+
+class SequentialPairwiseIterator(_Iterator):
+    def __init__(self, dataset: ImplicitFeedback,
+                 num_previous: int = 1, num_next: int = 1,
+                 pad: int = None, batch_size: int = 1024,
+                 shuffle: bool = True, drop_last: bool = False):
+        assert num_previous >= 1
+        assert num_next >= 1
+        # TODO , .
+        self.num_previous = num_previous
+        self.num_next = num_next
+        self.pad = pad
+        self.batch_size = batch_size
+        self.shuffle = shuffle
+        self.drop_last = drop_last
+        self.num_items = dataset.num_items
+        self.user_pos_dict = dataset.to_user_dict_by_time()
+
+        self.user_n_pos, self.all_users, all_item_seqs, pos_next_items = \
+            _generative_time_order_positive_items(self.user_pos_dict, num_previous=num_previous,
+                                                  num_next=num_next, pad=pad)
+
+        self.all_item_seqs = all_item_seqs.squeeze()
+        self.pos_next_items = pos_next_items.squeeze()
+
+    def __len__(self):
+        n_sample = len(self.all_users)
+        if self.drop_last:
+            return n_sample // self.batch_size
+        else:
+            return (n_sample + self.batch_size - 1) // self.batch_size
+
+    def __iter__(self):
+        neg_next_items = _sampling_negative_items(self.user_n_pos, self.num_next, self.num_items,
+                                                  self.user_pos_dict).squeeze()
+
+        data_iter = BatchIterator(self.all_users, self.all_item_seqs,
+                                  self.pos_next_items, neg_next_items,
+                                  batch_size=self.batch_size,
+                                  shuffle=self.shuffle, drop_last=self.drop_last)
+
+        for bat_users, bat_item_seqs, bat_pos_items, bat_neg_items in data_iter:
+            yield np.asarray(bat_users), np.asarray(bat_item_seqs), \
+                  np.asarray(bat_pos_items), np.asarray(bat_neg_items)
+
+
+class UserVecIterator(_Iterator):
+    def __init__(self, dataset: ImplicitFeedback, batch_size: int=1024,
+                 shuffle: bool=True, drop_last: bool=False):
+        super(UserVecIterator, self).__init__()
+
+        self.batch_size = batch_size
+        self.shuffle = shuffle
+        self.drop_last = drop_last
+        self.user_csr_matrix = dataset.to_csr_matrix()
+        all_users = np.arange(dataset.num_users, dtype=np.int32)
+        self.user_iter = BatchIterator(all_users, batch_size=self.batch_size,
+                                       shuffle=self.shuffle, drop_last=self.drop_last)
+
+    def __len__(self):
+        return len(self.user_iter)
+
+    def __iter__(self):
+        for bat_users in self.user_iter:
+            yield self.user_csr_matrix[bat_users].toarray()
+
+
+class ItemVecIterator(_Iterator):
+    def __init__(self, dataset: ImplicitFeedback, batch_size: int=1024,
+                 shuffle: bool=True, drop_last: bool=False):
+        super(ItemVecIterator, self).__init__()
+
+        self.batch_size = batch_size
+        self.shuffle = shuffle
+        self.drop_last = drop_last
+        self.item_csr_matrix = dataset.to_csr_matrix().transpose().tocsr()
+        all_items = np.arange(dataset.num_items, dtype=np.int32)
+        self.item_iter = BatchIterator(all_items, batch_size=self.batch_size,
+                                       shuffle=self.shuffle, drop_last=self.drop_last)
+
+    def __len__(self):
+        return len(self.item_iter)
+
+    def __iter__(self):
+        for bat_items in self.item_iter:
+            yield self.item_csr_matrix[bat_items].toarray()
```

## skrec/io/logger.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["Logger"]
-
-import sys
-import os
-import re
-import logging
-from typing import Optional
-
-
-class RemoveColorFilter(logging.Filter):
-
-    def filter(self, record):
-        if record:
-            ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
-            record.msg = ansi_escape.sub('', str(record.msg))
-        return True
-
-
-class Logger(object):
-    """`Logger` is a simple encapsulation of python logger.
-
-    This class can show a message on standard output and write it into the
-    file named `filename` simultaneously. This is convenient for observing
-    and saving training results.
-    """
-
-    def __init__(self, filename: Optional[str]=None):
-        """Initializes a new `Logger` instance.
-
-        Args:
-            filename (str): File name to create. The directory component of this
-                file will be created automatically if it is not existing.
-        """
-        logger_name = "scikit-recommender-logger" if filename is None else filename
-        self.logger = logging.getLogger(logger_name)
-        self.logger.setLevel(logging.DEBUG)
-        formatter = logging.Formatter('%(asctime)s.%(msecs)03d: %(message)s',
-                                      datefmt='%Y-%m-%d %H:%M:%S')
-
-        # show on console
-        ch = logging.StreamHandler(sys.stdout)
-        ch.setLevel(logging.DEBUG)
-        ch.setFormatter(formatter)
-        self.logger.addHandler(ch)  # add to Handler
-
-        if filename is not None:
-            dir_name = os.path.dirname(filename)
-            if dir_name and not os.path.exists(dir_name):
-                os.makedirs(dir_name)
-
-            remove_color_filter = RemoveColorFilter()
-            # write into file
-            fh = logging.FileHandler(filename)
-            fh.setLevel(logging.DEBUG)
-            fh.setFormatter(formatter)
-            fh.addFilter(remove_color_filter)
-            self.logger.addHandler(fh)  # add to Handler
-
-    def _flush(self):
-        for handler in self.logger.handlers:
-            handler.flush()
-
-    def debug(self, message):
-        self.logger.debug(message)
-        self._flush()
-
-    def info(self, message):
-        self.logger.info(message)
-        self._flush()
-
-    def warning(self, message):
-        self.logger.warning(message)
-        self._flush()
-
-    def error(self, message):
-        self.logger.error(message)
-        self._flush()
-
-    def critical(self, message):
-        self.logger.critical(message)
-        self._flush()
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["Logger"]
+
+import sys
+import os
+import re
+import logging
+from typing import Optional
+
+
+class RemoveColorFilter(logging.Filter):
+
+    def filter(self, record):
+        if record:
+            ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
+            record.msg = ansi_escape.sub('', str(record.msg))
+        return True
+
+
+class Logger(object):
+    """`Logger` is a simple encapsulation of python logger.
+
+    This class can show a message on standard output and write it into the
+    file named `filename` simultaneously. This is convenient for observing
+    and saving training results.
+    """
+
+    def __init__(self, filename: Optional[str]=None):
+        """Initializes a new `Logger` instance.
+
+        Args:
+            filename (str): File name to create. The directory component of this
+                file will be created automatically if it is not existing.
+        """
+        logger_name = "scikit-recommender-logger" if filename is None else filename
+        self.logger = logging.getLogger(logger_name)
+        self.logger.setLevel(logging.DEBUG)
+        formatter = logging.Formatter('%(asctime)s.%(msecs)03d: %(message)s',
+                                      datefmt='%Y-%m-%d %H:%M:%S')
+
+        # show on console
+        ch = logging.StreamHandler(sys.stdout)
+        ch.setLevel(logging.DEBUG)
+        ch.setFormatter(formatter)
+        self.logger.addHandler(ch)  # add to Handler
+
+        if filename is not None:
+            dir_name = os.path.dirname(filename)
+            if dir_name and not os.path.exists(dir_name):
+                os.makedirs(dir_name)
+
+            remove_color_filter = RemoveColorFilter()
+            # write into file
+            fh = logging.FileHandler(filename)
+            fh.setLevel(logging.DEBUG)
+            fh.setFormatter(formatter)
+            fh.addFilter(remove_color_filter)
+            self.logger.addHandler(fh)  # add to Handler
+
+    def _flush(self):
+        for handler in self.logger.handlers:
+            handler.flush()
+
+    def debug(self, message):
+        self.logger.debug(message)
+        self._flush()
+
+    def info(self, message):
+        self.logger.info(message)
+        self._flush()
+
+    def warning(self, message):
+        self.logger.warning(message)
+        self._flush()
+
+    def error(self, message):
+        self.logger.error(message)
+        self._flush()
+
+    def critical(self, message):
+        self.logger.critical(message)
+        self._flush()
```

## skrec/io/dataset.py

 * *Ordering differences only*

```diff
@@ -1,358 +1,358 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["ImplicitFeedback", "Dataset"]
-
-
-import os
-import pickle
-import warnings
-from typing import Dict, Callable
-from copy import deepcopy
-from functools import wraps
-from collections import OrderedDict
-import numpy as np
-import pandas as pd
-import scipy.sparse as sp
-import weakref
-from ..utils.py import pad_sequences, md5sum
-
-_USER = "user"
-_ITEM = "item"
-_RATING = "rating"
-_TIME = "time"
-_DColumns = {"UI": [_USER, _ITEM],
-             "UIR": [_USER, _ITEM, _RATING],
-             "UIT": [_USER, _ITEM, _TIME],
-             "UIRT": [_USER, _ITEM, _RATING, _TIME]
-             }
-
-
-class Interaction(object):
-    def __init__(self):
-        self._buffer = dict()
-        self._buffer_modified_flag = False
-
-    def is_empty(self) -> bool:
-        raise NotImplementedError
-
-    def _exist_in_buffer(self, name):
-        return name in self._buffer
-
-    def _write_to_buffer(self, name, value):
-        self._buffer[name] = value
-        self._buffer_modified_flag = True
-
-    def _read_from_buffer(self, name):
-        return deepcopy(self._buffer[name])
-
-    def _clean_buffer(self):
-        self._buffer.clear()
-        self._buffer_modified_flag = True
-
-    def is_buffer_modified(self):
-        return self._buffer_modified_flag
-
-    def reset_buffer_flag(self):
-        self._buffer_modified_flag = False
-
-    def __setstate__(self, state):
-        self.__dict__ = state
-        self.reset_buffer_flag()  # reset flag after pickle.load()
-
-
-def fetch_data(data_generator):
-    # read from buffer
-    @wraps(data_generator)
-    def wrapper(self: Interaction, *args, **kwargs):
-        _data_name = data_generator.__name__
-        if self.is_empty():
-            raise ValueError("data is empty!")
-
-        if self._exist_in_buffer(_data_name) is False:
-            _data = data_generator(self, *args, **kwargs)
-            self._write_to_buffer(_data_name, _data)
-
-        return self._read_from_buffer(_data_name)
-
-    return wrapper
-
-
-class ImplicitFeedback(Interaction):
-    def __init__(self, data: pd.DataFrame=None, num_users: int=None, num_items: int=None):
-        super(ImplicitFeedback, self).__init__()
-        assert data is None or isinstance(data, pd.DataFrame)
-
-        if data is None or data.empty:
-            self._data = pd.DataFrame()
-            self.num_users = 0
-            self.num_items = 0
-            self.num_ratings = 0
-        else:
-            self._data = data
-            self.num_users = num_users if num_users is not None else max(data[_USER]) + 1
-            self.num_items = num_items if num_items is not None else max(data[_ITEM]) + 1
-            self.num_ratings = len(data)
-
-    def is_empty(self) -> bool:
-        return self._data is None or self._data.empty
-
-    @fetch_data
-    def to_user_item_pairs(self) -> np.ndarray:
-        ui_pairs = self._data[[_USER, _ITEM]].to_numpy(copy=True, dtype=np.int32)
-        return ui_pairs
-
-    @fetch_data
-    def to_user_item_pairs_by_time(self) -> np.ndarray:
-        if _TIME not in self._data:
-            raise ValueError("This dataset do not contain timestamp.")
-        data_uit = self._data[[_USER, _ITEM, _TIME]]
-        data_uit = data_uit.sort_values(by=["user", "time"], inplace=False)
-        data_ui = data_uit[[_USER, _ITEM]].to_numpy(copy=True, dtype=np.int32)
-        return data_ui
-
-    @fetch_data
-    def to_csr_matrix(self) -> sp.csr_matrix:
-        users, items = self._data[_USER].to_numpy(), self._data[_ITEM].to_numpy()
-        ratings = np.ones(len(users), dtype=np.float32)
-        csr_mat = sp.csr_matrix((ratings, (users, items)), shape=(self.num_users, self.num_items), copy=True)
-        return csr_mat
-
-    @fetch_data
-    def to_csc_matrix(self) -> sp.csc_matrix:
-        return self.to_csr_matrix().tocsc()
-
-    @fetch_data
-    def to_dok_matrix(self) -> sp.dok_matrix:
-        return self.to_csr_matrix().todok()
-
-    @fetch_data
-    def to_coo_matrix(self) -> sp.coo_matrix:
-        return self.to_csr_matrix().tocoo()
-
-    @fetch_data
-    def to_user_dict(self) -> Dict[int, np.ndarray]:
-        user_dict = OrderedDict()
-        user_grouped = self._data.groupby(_USER)
-        for user, user_data in user_grouped:
-            user_dict[user] = user_data[_ITEM].to_numpy(dtype=np.int32)
-        return user_dict
-
-    @fetch_data
-    def to_user_dict_by_time(self) -> Dict[int, np.ndarray]:
-        # in chronological
-        if _TIME not in self._data:
-            raise ValueError("This dataset do not contain timestamp.")
-
-        user_dict = OrderedDict()
-        user_grouped = self._data.groupby(_USER)
-        for user, user_data in user_grouped:
-            user_data = user_data.sort_values(by=[_TIME])
-            user_dict[user] = user_data[_ITEM].to_numpy(dtype=np.int32)
-
-        return user_dict
-
-    @fetch_data
-    def to_item_dict(self) -> Dict[int, np.ndarray]:
-        item_dict = OrderedDict()
-        item_grouped = self._data.groupby(_ITEM)
-        for item, item_data in item_grouped:
-            item_dict[item] = item_data[_USER].to_numpy(dtype=np.int32)
-
-        return item_dict
-
-    def to_truncated_seq_dict(self, max_len: int, pad_value: int=0,
-                              padding='pre', truncating='pre') -> Dict[int, np.ndarray]:
-        user_seq_dict = self.to_user_dict_by_time()
-        if max_len is None:
-            max_len = max([len(seqs) for seqs in user_seq_dict.values()])
-        item_seq_list = [item_seq[-max_len:] for item_seq in user_seq_dict.values()]
-        item_seq_arr = pad_sequences(item_seq_list, value=pad_value, max_len=max_len,
-                                     padding=padding, truncating=truncating, dtype=np.int32)
-
-        seq_dict = OrderedDict([(user, item_seq) for user, item_seq in
-                                zip(user_seq_dict.keys(), item_seq_arr)])
-        return seq_dict
-
-    def __len__(self):
-        return len(self._data)
-
-
-class Dataset(object):
-    def __init__(self, data_dir, sep, columns):
-        """Dataset
-
-        Notes:
-            The prefix name of data files is same as the data_dir, and the
-            suffix/extension names are 'train', 'test', 'user2id', 'item2id'.
-            Directory structure:
-                data_dir
-                     data_dir.train      // training data
-                     data_dir.valid      // validation data, optional
-                     data_dir.test       // test data
-                     data_dir.user2id    // user to id, optional
-                     data_dir.item2id    // item to id, optional
-
-        Args:
-            data_dir: The directory of dataset.
-            sep: The separator/delimiter of file columns.
-            columns: The format of columns, must be one of 'UI',
-                'UIR', 'UIT' and 'UIRT'
-        """
-
-        self._data_dir = data_dir
-
-        # metadata
-        self.train_data = ImplicitFeedback()
-        self.valid_data = ImplicitFeedback()
-        self.test_data = ImplicitFeedback()
-        self.user2id = None
-        self.item2id = None
-        self.id2user = None
-        self.id2item = None
-
-        # statistic
-        self.num_users = 0
-        self.num_items = 0
-        self.num_ratings = 0
-        self._my_md5 = ""
-        self._cache_file = os.path.join(self.data_dir, "_cache_" + self.data_name + ".pkl")
-        self._load_data(sep, columns)
-        weakref.finalize(self, self._destructor)
-
-    @property
-    def data_name(self):
-        return os.path.split(self.data_dir)[-1]
-
-    @property
-    def data_dir(self):
-        return self._data_dir
-
-    @property
-    def _file_prefix(self):
-        return os.path.join(self.data_dir, self.data_name)
-
-    def _load_data(self, sep, columns):
-        if os.path.exists(self._cache_file):
-            try:
-                with open(self._cache_file, 'rb') as fin:
-                    _t_data: Dataset = pickle.load(fin)
-                if _t_data._my_md5 == self._raw_md5:
-                    _t_data._data_dir = self._data_dir  # keep data path up-to-date
-                    self.__dict__ = _t_data.__dict__
-                    return
-            except EOFError as e:
-                pass
-
-        self._load_from_raw(sep, columns)
-
-    def _dump_data(self):
-        with open(self._cache_file, 'wb') as fout:
-            pickle.dump(self, fout)
-
-    def _destructor(self):
-        if self.train_data.is_buffer_modified() or \
-                self.valid_data.is_buffer_modified() or \
-                self.test_data.is_buffer_modified():
-            self._dump_data()
-
-    @property
-    def _raw_md5(self):
-        files = [self._file_prefix+postfix for postfix in (".train", ".valid", ".test")
-                 if os.path.isfile(self._file_prefix+postfix)]
-        md5summary = md5sum(*files)
-        md5summary = "_".join([md5 for md5 in md5summary if md5 is not None])
-        return md5summary
-
-    @staticmethod
-    def _read_csv(csv_file, sep, header, names, handle: Callable=lambda x: x):
-        if os.path.isfile(csv_file):
-            csv_data = pd.read_csv(csv_file, sep=sep, header=header, names=names)
-        else:
-            handle(f"'{csv_file}' does not exist.")
-            csv_data = pd.DataFrame()
-        return csv_data
-
-    @staticmethod
-    def _read_map_file(map_file, sep):
-        if os.path.isfile(map_file):
-            maps = pd.read_csv(map_file, sep=sep, header=None).to_numpy()
-            maps = OrderedDict(maps)
-            reverses = OrderedDict([(second, first) for first, second in maps.items()])
-        else:
-            maps = None
-            reverses = None
-            warnings.warn(f"'{map_file}' does not exist.")
-        return maps, reverses
-
-    def _load_from_raw(self, sep, columns):
-        if columns not in _DColumns:
-            key_str = ", ".join(_DColumns.keys())
-            raise ValueError("'columns' must be one of '%s'." % key_str)
-
-        columns = _DColumns[columns]
-
-        # load data
-        def raise_error(err: str): raise FileNotFoundError(err)
-        _train_data = self._read_csv(self._file_prefix + ".train",
-                                     sep=sep, header=None, names=columns,
-                                     handle=raise_error)
-        _valid_data = self._read_csv(self._file_prefix + ".valid",
-                                     sep=sep, header=None, names=columns,
-                                     handle=warnings.warn)
-        _test_data = self._read_csv(self._file_prefix + ".test",
-                                    sep=sep, header=None, names=columns,
-                                    handle=raise_error)
-        if _train_data.isnull().values.any():
-            warnings.warn(f"'Training data has None value, please check the file or the separator.")
-        if _valid_data.isnull().values.any():
-            warnings.warn(f"'Validation data has None value, please check the file or the separator.")
-        if _test_data.isnull().values.any():
-            warnings.warn(f"'Test data has None value, please check the file or the separator.")
-        self.user2id, self.id2user = self._read_map_file(self._file_prefix + ".user2id", sep)
-        self.item2id, self.id2item = self._read_map_file(self._file_prefix + ".item2id", sep)
-
-        # statistical information
-        data_info = [(max(data[_USER]), max(data[_ITEM]), len(data))
-                     for data in [_train_data, _valid_data, _test_data] if not data.empty]
-        self.num_users = max([d[0] for d in data_info]) + 1
-        self.num_items = max([d[1] for d in data_info]) + 1
-        self.num_ratings = sum([d[2] for d in data_info])
-
-        # convert to to the object of Interaction
-        self.train_data = ImplicitFeedback(_train_data, num_users=self.num_users, num_items=self.num_items)
-        self.valid_data = ImplicitFeedback(_valid_data, num_users=self.num_users, num_items=self.num_items)
-        self.test_data = ImplicitFeedback(_test_data, num_users=self.num_users, num_items=self.num_items)
-
-        self._my_md5 = self._raw_md5
-        self._dump_data()
-
-    @property
-    def statistic_info(self):
-        """The statistic of dataset.
-
-        Returns:
-            str: The summary of statistic information
-        """
-        if 0 in {self.num_users, self.num_items, self.num_ratings}:
-            return ""
-        else:
-            num_users, num_items = self.num_users, self.num_items
-            num_ratings = self.num_ratings
-            sparsity = 1 - 1.0 * num_ratings / (num_users * num_items)
-
-            statistic = ["Dataset statistic information:",
-                         "Name: %s" % self.data_name,
-                         f"The number of users: {num_users}",
-                         f"The number of items: {num_items}",
-                         f"The number of ratings: {num_ratings}",
-                         f"Average actions of users: {(1.0 * num_ratings / num_users):.2f}",
-                         f"Average actions of items: {(1.0 * num_ratings / num_items):.2f}",
-                         f"The sparsity of the dataset: {(sparsity * 100):.6f}%%",
-                         "",
-                         f"The number of training: {len(self.train_data)}",
-                         f"The number of validation: {len(self.valid_data)}",
-                         f"The number of testing: {len(self.test_data)}"
-                         ]
-            statistic = "\n".join(statistic)
-            return statistic
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["ImplicitFeedback", "Dataset"]
+
+
+import os
+import pickle
+import warnings
+from typing import Dict, Callable
+from copy import deepcopy
+from functools import wraps
+from collections import OrderedDict
+import numpy as np
+import pandas as pd
+import scipy.sparse as sp
+import weakref
+from ..utils.py import pad_sequences, md5sum
+
+_USER = "user"
+_ITEM = "item"
+_RATING = "rating"
+_TIME = "time"
+_DColumns = {"UI": [_USER, _ITEM],
+             "UIR": [_USER, _ITEM, _RATING],
+             "UIT": [_USER, _ITEM, _TIME],
+             "UIRT": [_USER, _ITEM, _RATING, _TIME]
+             }
+
+
+class Interaction(object):
+    def __init__(self):
+        self._buffer = dict()
+        self._buffer_modified_flag = False
+
+    def is_empty(self) -> bool:
+        raise NotImplementedError
+
+    def _exist_in_buffer(self, name):
+        return name in self._buffer
+
+    def _write_to_buffer(self, name, value):
+        self._buffer[name] = value
+        self._buffer_modified_flag = True
+
+    def _read_from_buffer(self, name):
+        return deepcopy(self._buffer[name])
+
+    def _clean_buffer(self):
+        self._buffer.clear()
+        self._buffer_modified_flag = True
+
+    def is_buffer_modified(self):
+        return self._buffer_modified_flag
+
+    def reset_buffer_flag(self):
+        self._buffer_modified_flag = False
+
+    def __setstate__(self, state):
+        self.__dict__ = state
+        self.reset_buffer_flag()  # reset flag after pickle.load()
+
+
+def fetch_data(data_generator):
+    # read from buffer
+    @wraps(data_generator)
+    def wrapper(self: Interaction, *args, **kwargs):
+        _data_name = data_generator.__name__
+        if self.is_empty():
+            raise ValueError("data is empty!")
+
+        if self._exist_in_buffer(_data_name) is False:
+            _data = data_generator(self, *args, **kwargs)
+            self._write_to_buffer(_data_name, _data)
+
+        return self._read_from_buffer(_data_name)
+
+    return wrapper
+
+
+class ImplicitFeedback(Interaction):
+    def __init__(self, data: pd.DataFrame=None, num_users: int=None, num_items: int=None):
+        super(ImplicitFeedback, self).__init__()
+        assert data is None or isinstance(data, pd.DataFrame)
+
+        if data is None or data.empty:
+            self._data = pd.DataFrame()
+            self.num_users = 0
+            self.num_items = 0
+            self.num_ratings = 0
+        else:
+            self._data = data
+            self.num_users = num_users if num_users is not None else max(data[_USER]) + 1
+            self.num_items = num_items if num_items is not None else max(data[_ITEM]) + 1
+            self.num_ratings = len(data)
+
+    def is_empty(self) -> bool:
+        return self._data is None or self._data.empty
+
+    @fetch_data
+    def to_user_item_pairs(self) -> np.ndarray:
+        ui_pairs = self._data[[_USER, _ITEM]].to_numpy(copy=True, dtype=np.int32)
+        return ui_pairs
+
+    @fetch_data
+    def to_user_item_pairs_by_time(self) -> np.ndarray:
+        if _TIME not in self._data:
+            raise ValueError("This dataset do not contain timestamp.")
+        data_uit = self._data[[_USER, _ITEM, _TIME]]
+        data_uit = data_uit.sort_values(by=["user", "time"], inplace=False)
+        data_ui = data_uit[[_USER, _ITEM]].to_numpy(copy=True, dtype=np.int32)
+        return data_ui
+
+    @fetch_data
+    def to_csr_matrix(self) -> sp.csr_matrix:
+        users, items = self._data[_USER].to_numpy(), self._data[_ITEM].to_numpy()
+        ratings = np.ones(len(users), dtype=np.float32)
+        csr_mat = sp.csr_matrix((ratings, (users, items)), shape=(self.num_users, self.num_items), copy=True)
+        return csr_mat
+
+    @fetch_data
+    def to_csc_matrix(self) -> sp.csc_matrix:
+        return self.to_csr_matrix().tocsc()
+
+    @fetch_data
+    def to_dok_matrix(self) -> sp.dok_matrix:
+        return self.to_csr_matrix().todok()
+
+    @fetch_data
+    def to_coo_matrix(self) -> sp.coo_matrix:
+        return self.to_csr_matrix().tocoo()
+
+    @fetch_data
+    def to_user_dict(self) -> Dict[int, np.ndarray]:
+        user_dict = OrderedDict()
+        user_grouped = self._data.groupby(_USER)
+        for user, user_data in user_grouped:
+            user_dict[user] = user_data[_ITEM].to_numpy(dtype=np.int32)
+        return user_dict
+
+    @fetch_data
+    def to_user_dict_by_time(self) -> Dict[int, np.ndarray]:
+        # in chronological
+        if _TIME not in self._data:
+            raise ValueError("This dataset do not contain timestamp.")
+
+        user_dict = OrderedDict()
+        user_grouped = self._data.groupby(_USER)
+        for user, user_data in user_grouped:
+            user_data = user_data.sort_values(by=[_TIME])
+            user_dict[user] = user_data[_ITEM].to_numpy(dtype=np.int32)
+
+        return user_dict
+
+    @fetch_data
+    def to_item_dict(self) -> Dict[int, np.ndarray]:
+        item_dict = OrderedDict()
+        item_grouped = self._data.groupby(_ITEM)
+        for item, item_data in item_grouped:
+            item_dict[item] = item_data[_USER].to_numpy(dtype=np.int32)
+
+        return item_dict
+
+    def to_truncated_seq_dict(self, max_len: int, pad_value: int=0,
+                              padding='pre', truncating='pre') -> Dict[int, np.ndarray]:
+        user_seq_dict = self.to_user_dict_by_time()
+        if max_len is None:
+            max_len = max([len(seqs) for seqs in user_seq_dict.values()])
+        item_seq_list = [item_seq[-max_len:] for item_seq in user_seq_dict.values()]
+        item_seq_arr = pad_sequences(item_seq_list, value=pad_value, max_len=max_len,
+                                     padding=padding, truncating=truncating, dtype=np.int32)
+
+        seq_dict = OrderedDict([(user, item_seq) for user, item_seq in
+                                zip(user_seq_dict.keys(), item_seq_arr)])
+        return seq_dict
+
+    def __len__(self):
+        return len(self._data)
+
+
+class Dataset(object):
+    def __init__(self, data_dir, sep, columns):
+        """Dataset
+
+        Notes:
+            The prefix name of data files is same as the data_dir, and the
+            suffix/extension names are 'train', 'test', 'user2id', 'item2id'.
+            Directory structure:
+                data_dir
+                     data_dir.train      // training data
+                     data_dir.valid      // validation data, optional
+                     data_dir.test       // test data
+                     data_dir.user2id    // user to id, optional
+                     data_dir.item2id    // item to id, optional
+
+        Args:
+            data_dir: The directory of dataset.
+            sep: The separator/delimiter of file columns.
+            columns: The format of columns, must be one of 'UI',
+                'UIR', 'UIT' and 'UIRT'
+        """
+
+        self._data_dir = data_dir
+
+        # metadata
+        self.train_data = ImplicitFeedback()
+        self.valid_data = ImplicitFeedback()
+        self.test_data = ImplicitFeedback()
+        self.user2id = None
+        self.item2id = None
+        self.id2user = None
+        self.id2item = None
+
+        # statistic
+        self.num_users = 0
+        self.num_items = 0
+        self.num_ratings = 0
+        self._my_md5 = ""
+        self._cache_file = os.path.join(self.data_dir, "_cache_" + self.data_name + ".pkl")
+        self._load_data(sep, columns)
+        weakref.finalize(self, self._destructor)
+
+    @property
+    def data_name(self):
+        return os.path.split(self.data_dir)[-1]
+
+    @property
+    def data_dir(self):
+        return self._data_dir
+
+    @property
+    def _file_prefix(self):
+        return os.path.join(self.data_dir, self.data_name)
+
+    def _load_data(self, sep, columns):
+        if os.path.exists(self._cache_file):
+            try:
+                with open(self._cache_file, 'rb') as fin:
+                    _t_data: Dataset = pickle.load(fin)
+                if _t_data._my_md5 == self._raw_md5:
+                    _t_data._data_dir = self._data_dir  # keep data path up-to-date
+                    self.__dict__ = _t_data.__dict__
+                    return
+            except EOFError as e:
+                pass
+
+        self._load_from_raw(sep, columns)
+
+    def _dump_data(self):
+        with open(self._cache_file, 'wb') as fout:
+            pickle.dump(self, fout)
+
+    def _destructor(self):
+        if self.train_data.is_buffer_modified() or \
+                self.valid_data.is_buffer_modified() or \
+                self.test_data.is_buffer_modified():
+            self._dump_data()
+
+    @property
+    def _raw_md5(self):
+        files = [self._file_prefix+postfix for postfix in (".train", ".valid", ".test")
+                 if os.path.isfile(self._file_prefix+postfix)]
+        md5summary = md5sum(*files)
+        md5summary = "_".join([md5 for md5 in md5summary if md5 is not None])
+        return md5summary
+
+    @staticmethod
+    def _read_csv(csv_file, sep, header, names, handle: Callable=lambda x: x):
+        if os.path.isfile(csv_file):
+            csv_data = pd.read_csv(csv_file, sep=sep, header=header, names=names)
+        else:
+            handle(f"'{csv_file}' does not exist.")
+            csv_data = pd.DataFrame()
+        return csv_data
+
+    @staticmethod
+    def _read_map_file(map_file, sep):
+        if os.path.isfile(map_file):
+            maps = pd.read_csv(map_file, sep=sep, header=None).to_numpy()
+            maps = OrderedDict(maps)
+            reverses = OrderedDict([(second, first) for first, second in maps.items()])
+        else:
+            maps = None
+            reverses = None
+            warnings.warn(f"'{map_file}' does not exist.")
+        return maps, reverses
+
+    def _load_from_raw(self, sep, columns):
+        if columns not in _DColumns:
+            key_str = ", ".join(_DColumns.keys())
+            raise ValueError("'columns' must be one of '%s'." % key_str)
+
+        columns = _DColumns[columns]
+
+        # load data
+        def raise_error(err: str): raise FileNotFoundError(err)
+        _train_data = self._read_csv(self._file_prefix + ".train",
+                                     sep=sep, header=None, names=columns,
+                                     handle=raise_error)
+        _valid_data = self._read_csv(self._file_prefix + ".valid",
+                                     sep=sep, header=None, names=columns,
+                                     handle=warnings.warn)
+        _test_data = self._read_csv(self._file_prefix + ".test",
+                                    sep=sep, header=None, names=columns,
+                                    handle=raise_error)
+        if _train_data.isnull().values.any():
+            warnings.warn(f"'Training data has None value, please check the file or the separator.")
+        if _valid_data.isnull().values.any():
+            warnings.warn(f"'Validation data has None value, please check the file or the separator.")
+        if _test_data.isnull().values.any():
+            warnings.warn(f"'Test data has None value, please check the file or the separator.")
+        self.user2id, self.id2user = self._read_map_file(self._file_prefix + ".user2id", sep)
+        self.item2id, self.id2item = self._read_map_file(self._file_prefix + ".item2id", sep)
+
+        # statistical information
+        data_info = [(max(data[_USER]), max(data[_ITEM]), len(data))
+                     for data in [_train_data, _valid_data, _test_data] if not data.empty]
+        self.num_users = max([d[0] for d in data_info]) + 1
+        self.num_items = max([d[1] for d in data_info]) + 1
+        self.num_ratings = sum([d[2] for d in data_info])
+
+        # convert to to the object of Interaction
+        self.train_data = ImplicitFeedback(_train_data, num_users=self.num_users, num_items=self.num_items)
+        self.valid_data = ImplicitFeedback(_valid_data, num_users=self.num_users, num_items=self.num_items)
+        self.test_data = ImplicitFeedback(_test_data, num_users=self.num_users, num_items=self.num_items)
+
+        self._my_md5 = self._raw_md5
+        self._dump_data()
+
+    @property
+    def statistic_info(self):
+        """The statistic of dataset.
+
+        Returns:
+            str: The summary of statistic information
+        """
+        if 0 in {self.num_users, self.num_items, self.num_ratings}:
+            return ""
+        else:
+            num_users, num_items = self.num_users, self.num_items
+            num_ratings = self.num_ratings
+            sparsity = 1 - 1.0 * num_ratings / (num_users * num_items)
+
+            statistic = ["Dataset statistic information:",
+                         "Name: %s" % self.data_name,
+                         f"The number of users: {num_users}",
+                         f"The number of items: {num_items}",
+                         f"The number of ratings: {num_ratings}",
+                         f"Average actions of users: {(1.0 * num_ratings / num_users):.2f}",
+                         f"Average actions of items: {(1.0 * num_ratings / num_items):.2f}",
+                         f"The sparsity of the dataset: {(sparsity * 100):.6f}%%",
+                         "",
+                         f"The number of training: {len(self.train_data)}",
+                         f"The number of validation: {len(self.valid_data)}",
+                         f"The number of testing: {len(self.test_data)}"
+                         ]
+            statistic = "\n".join(statistic)
+            return statistic
```

## skrec/io/__init__.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-from .dataset import Dataset
-from .dataset import ImplicitFeedback
-
-from .data_iterator import PointwiseIterator
-from .data_iterator import PairwiseIterator
-from .data_iterator import SequentialPointwiseIterator
-from .data_iterator import SequentialPairwiseIterator
-from .data_iterator import UserVecIterator
-from .data_iterator import ItemVecIterator
-
-from .logger import Logger
-
-from .movielens import MovieLens100k
-from .preprocessor import Preprocessor
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+from .dataset import Dataset
+from .dataset import ImplicitFeedback
+
+from .data_iterator import PointwiseIterator
+from .data_iterator import PairwiseIterator
+from .data_iterator import SequentialPointwiseIterator
+from .data_iterator import SequentialPairwiseIterator
+from .data_iterator import UserVecIterator
+from .data_iterator import ItemVecIterator
+
+from .logger import Logger
+
+from .movielens import MovieLens100k
+from .preprocessor import Preprocessor
```

## skrec/io/preprocessor.py

 * *Ordering differences only*

```diff
@@ -1,334 +1,334 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["Preprocessor"]
-
-import os
-import math
-import pandas as pd
-from ..utils.py import typeassert
-from .logger import Logger
-from collections import OrderedDict
-
-
-class Preprocessor(object):
-    _USER = "user"
-    _ITEM = "item"
-    _RATING = "rating"
-    _TIME = "time"
-
-    def __init__(self):
-        """A class for data preprocessing
-        """
-
-        self._column_dict = {"UI": [self._USER, self._ITEM],
-                             "UIR": [self._USER, self._ITEM, self._RATING],
-                             "UIT": [self._USER, self._ITEM, self._TIME],
-                             "UIRT": [self._USER, self._ITEM, self._RATING, self._TIME]}
-        self._column_name = None
-        self._config = OrderedDict()
-        self.all_data = None
-        self.train_data = None
-        self.valid_data = None
-        self.test_data = None
-        self.user2id = None
-        self.item2id = None
-        self._dir_path = None
-        self._data_name = ""
-        self._split_manner = ""
-        self._user_min = 0
-        self._item_min = 0
-
-    @typeassert(filename=str, sep=str)
-    def load_data(self, filename: str, sep: str=",", columns: str=None):
-        """Load data
-
-        Args:
-            filename (str): The path of dataset.
-            sep (str): The separator/delimiter of columns.
-            columns (str): One of 'UI', 'UIR', 'UIT' and 'UIRT'.
-
-        """
-        if not os.path.isfile(filename):
-            raise FileNotFoundError(f"There is no file named '{filename}'.")
-        if columns not in self._column_dict:
-            key_str = ", ".join(self._column_dict.keys())
-            raise ValueError(f"'columns' must be one of '{key_str}'.")
-        self._config["columns"] = columns
-
-        self._column_name = self._column_dict[columns]
-
-        print("loading data...")
-        self._config["filename"] = filename
-        self._config["sep"] = sep
-
-        self.all_data = pd.read_csv(filename, sep=sep, header=None, names=self._column_name)
-        self.all_data.dropna(inplace=True)
-
-        self._data_name = os.path.basename(filename).split(".")[0]
-        self._dir_path = os.path.dirname(filename)
-
-    def drop_duplicates(self, keep: str="last"):
-        """Drop duplicate user-item interactions.
-
-        Args:
-            keep (str): 'first' or 'last', default 'first'.
-                Drop duplicates except for the first or last occurrence.
-
-        Returns:
-            An object of pd.DataFrame without duplicates.
-
-        Raises:
-            ValueError: If 'keep' is not 'first' or 'last'.
-        """
-
-        if keep not in {'first', 'last'}:
-            raise ValueError(f"'keep' must be 'first' or 'last', but '{keep}'")
-        print("dropping duplicate interactions...")
-
-        if self._TIME in self._column_name:
-            sort_key = [self._USER, self._TIME]
-        else:
-            sort_key = [self._USER, self._ITEM]
-
-        self.all_data.sort_values(by=sort_key, inplace=True)
-
-        self.all_data.drop_duplicates(subset=[self._USER, self._ITEM], keep=keep, inplace=True)
-
-    @typeassert(user_min=int, item_min=int)
-    def filter_data(self, user_min: int=0, item_min: int=0):
-        """Filter users and items with a few interactions.
-
-        Args:
-            user_min (int): The users with less interactions than 'user_min' will be filtered.
-            item_min (int): The items with less interactions than 'item_min' will be filtered.
-        """
-        i = 0
-        while True:
-            i += 1
-            old_len_data = len(self.all_data)
-            print(f"{i} filtering items...")
-            self.filter_item(item_min)
-            print(f"{i} filtering users...")
-            self.filter_user(user_min)
-            new_len_data = len(self.all_data)
-            if old_len_data == new_len_data:  # return if data does not change
-                break
-
-    @typeassert(user_min=int)
-    def filter_user(self, user_min: int=0):
-        """Filter users with a few interactions.
-
-        Args:
-            user_min (int): The users with less interactions than 'user_min' will be filtered.
-        """
-        self._config["user_min"] = str(user_min)
-        self._user_min = user_min
-        if user_min > 0:
-            user_count = self.all_data[self._USER].value_counts(sort=False)
-            if user_count.min() < user_min:
-                filtered_idx = self.all_data[self._USER].map(lambda x: user_count[x] >= user_min)
-                self.all_data = self.all_data[filtered_idx]
-
-    @typeassert(item_min=int)
-    def filter_item(self, item_min: int=0):
-        """Filter items with a few interactions.
-
-        Args:
-            item_min (int): The items with less interactions than 'item_min' will be filtered.
-        """
-        self._config["item_min"] = str(item_min)
-        self._item_min = item_min
-        if item_min > 0:
-            item_count = self.all_data[self._ITEM].value_counts(sort=False)
-            if item_count.min() < item_min:
-                filtered_idx = self.all_data[self._ITEM].map(lambda x: item_count[x] >= item_min)
-                self.all_data = self.all_data[filtered_idx]
-
-    def remap_data_id(self):
-        """Convert user and item IDs to integers, start from 0.
-
-        """
-        self.remap_user_id()
-        self.remap_item_id()
-
-    def remap_user_id(self):
-        """Convert user IDs to integers, start from 0.
-
-        """
-        print("remapping user IDs...")
-        self._config["remap_user_id"] = "True"
-        unique_user = self.all_data[self._USER].unique()
-        self.user2id = pd.Series(data=range(len(unique_user)), index=unique_user)
-
-        self.all_data[self._USER] = self.all_data[self._USER].map(self.user2id)
-
-    def remap_item_id(self):
-        """Convert item IDs to integers, start from 0.
-
-        """
-        print("remapping item IDs...")
-        self._config["remap_item_id"] = "True"
-        unique_item = self.all_data[self._ITEM].unique()
-        self.item2id = pd.Series(data=range(len(unique_item)), index=unique_item)
-
-        self.all_data[self._ITEM] = self.all_data[self._ITEM].map(self.item2id)
-
-    @typeassert(train=float, valid=float, test=float)
-    def split_data_by_ratio(self, train: float=0.7, valid: float=0.1, test: float=0.2, by_time: bool=True):
-        """Split dataset by the given ratios.
-
-        The dataset will be split by each user.
-
-        Args:
-            train (float): The proportion of training data.
-            valid (float): The proportion of validation data.
-                '0.0' means no validation set.
-            test (float): The proportion of testing data.
-            by_time (bool): Splitting data randomly or by time.
-        """
-        if train <= 0.0:
-            raise ValueError("'train' must be a positive value.")
-        if train + valid + test != 1.0:
-            raise ValueError("The sum of 'train', 'valid' and 'test' must be equal to 1.0.")
-        print("splitting data by ratio...")
-
-        self._config["split_by"] = "ratio"
-        self._config["train"] = str(train)
-        self._config["valid"] = str(valid)
-        self._config["test"] = str(test)
-        self._config["by_time"] = str(by_time)
-
-        if by_time is False or self._TIME not in self._column_name:
-            sort_key = [self._USER, self._ITEM]
-        else:
-            sort_key = [self._USER, self._TIME]
-
-        self.all_data.sort_values(by=sort_key, inplace=True)
-
-        _by_t = "by_time" if by_time is True else "by_random"
-        self._split_manner = "ratio_" + _by_t
-        train_data = []
-        valid_data = []
-        test_data = []
-
-        user_grouped = self.all_data.groupby(by=[self._USER])
-        for user, u_data in user_grouped:
-            u_data_len = len(u_data)
-            if not by_time:
-                u_data = u_data.sample(frac=1)
-            train_end = math.ceil(train * u_data_len)
-            train_data.append(u_data.iloc[:train_end])
-            if valid != 0:
-                test_begin = train_end + math.ceil(valid * u_data_len)
-                valid_data.append(u_data.iloc[train_end:test_begin])
-            else:
-                test_begin = train_end
-            test_data.append(u_data.iloc[test_begin:])
-
-        self.train_data = pd.concat(train_data, ignore_index=True)
-        if valid != 0:
-            self.valid_data = pd.concat(valid_data, ignore_index=True)
-        self.test_data = pd.concat(test_data, ignore_index=True)
-
-    @typeassert(valid=int, test=int)
-    def split_data_by_leave_out(self, valid: int=1, test: int=1, by_time: bool=True):
-        """Split dataset by leave out certain number items.
-
-        The dataset will be split by each user.
-
-        Args:
-            valid (int): The number of items of validation set for each user.
-                Default to 1 and means leave one out.
-            test (int): The number of items of test set for each user.
-                Default to 1 and means leave one out.
-            by_time (bool): Splitting data randomly or by time.
-        """
-
-        self._config["split_by"] = "leave_out"
-        self._config["valid"] = str(valid)
-        self._config["test"] = str(test)
-        self._config["by_time"] = str(by_time)
-
-        if by_time is False or self._TIME not in self._column_name:
-            sort_key = [self._USER, self._ITEM]
-        else:
-            sort_key = [self._USER, self._TIME]
-        print("splitting data by leave out...")
-
-        self.all_data.sort_values(by=sort_key, inplace=True)
-
-        _by_t = "by_time" if by_time is True else "by_random"
-        self._split_manner = "leave_" + _by_t
-        train_data = []
-        valid_data = []
-        test_data = []
-
-        user_grouped = self.all_data.groupby(by=[self._USER])
-        for user, u_data in user_grouped:
-            if not by_time:
-                u_data = u_data.sample(frac=1)
-            train_end = -(valid+test)
-            train_data.append(u_data.iloc[:train_end])
-            if valid != 0:
-                test_begin = train_end + valid
-                valid_data.append(u_data.iloc[train_end:test_begin])
-            else:
-                test_begin = train_end
-            test_data.append(u_data.iloc[test_begin:])
-
-        self.train_data = pd.concat(train_data, ignore_index=True)
-        if valid != 0:
-            self.valid_data = pd.concat(valid_data, ignore_index=True)
-        self.test_data = pd.concat(test_data, ignore_index=True)
-
-    def save_data(self, save_dir=None):
-        """Save data to disk.
-
-        Args:
-            save_dir (str): The directory to save the dataset and information.
-
-        """
-        print("saving data to disk...")
-        dir_path = save_dir if save_dir is not None else self._dir_path
-        filename = f"{self._data_name}_{self._split_manner}_u{self._user_min}_i{self._item_min}"
-        dir_path = os.path.join(dir_path, filename)
-        if not os.path.exists(dir_path):
-            os.makedirs(dir_path)
-
-        # save data
-        filename = os.path.join(dir_path, filename)
-        sep = "\t"  # self._config["sep"]
-        if self.all_data is not None:
-            self.all_data.to_csv(filename+".all", header=False, index=False, sep=sep)
-        if self.train_data is not None:
-            self.train_data.to_csv(filename + ".train", header=False, index=False, sep=sep)
-        if self.valid_data is not None:
-            self.valid_data.to_csv(filename + ".valid", header=False, index=False, sep=sep)
-        if self.test_data is not None:
-            self.test_data.to_csv(filename + ".test", header=False, index=False, sep=sep)
-        if self.user2id is not None:
-            self.user2id.to_csv(filename + ".user2id", header=False, index=True, sep=sep)
-        if self.item2id is not None:
-            self.item2id.to_csv(filename + ".item2id", header=False, index=True, sep=sep)
-
-        # calculate statistics
-        user_num = len(self.all_data[self._USER].unique())
-        item_num = len(self.all_data[self._ITEM].unique())
-        rating_num = len(self.all_data)
-        sparsity = 1-1.0*rating_num/(user_num*item_num)
-
-        # write log file
-        logger = Logger(filename+".info")
-        data_info = "\n".join([f"{key} = {value}" for key, value in self._config.items()])
-        logger.info("\n"+data_info)
-        logger.info("Dataset statistic information:")
-        logger.info(f"The number of users: {user_num}")
-        logger.info(f"The number of items: {item_num}")
-        logger.info(f"The number of ratings: {rating_num}")
-        logger.info(f"Average actions of users: {(1.0*rating_num/user_num):.2f}")
-        logger.info(f"Average actions of items: {(1.0*rating_num/item_num):.2f}")
-        logger.info(f"The sparsity of the dataset: {(sparsity*100)}%%")
-
-        print(f"\nThe processed data has been saved in '{dir_path}'")
-        return dir_path
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["Preprocessor"]
+
+import os
+import math
+import pandas as pd
+from ..utils.py import typeassert
+from .logger import Logger
+from collections import OrderedDict
+
+
+class Preprocessor(object):
+    _USER = "user"
+    _ITEM = "item"
+    _RATING = "rating"
+    _TIME = "time"
+
+    def __init__(self):
+        """A class for data preprocessing
+        """
+
+        self._column_dict = {"UI": [self._USER, self._ITEM],
+                             "UIR": [self._USER, self._ITEM, self._RATING],
+                             "UIT": [self._USER, self._ITEM, self._TIME],
+                             "UIRT": [self._USER, self._ITEM, self._RATING, self._TIME]}
+        self._column_name = None
+        self._config = OrderedDict()
+        self.all_data = None
+        self.train_data = None
+        self.valid_data = None
+        self.test_data = None
+        self.user2id = None
+        self.item2id = None
+        self._dir_path = None
+        self._data_name = ""
+        self._split_manner = ""
+        self._user_min = 0
+        self._item_min = 0
+
+    @typeassert(filename=str, sep=str)
+    def load_data(self, filename: str, sep: str=",", columns: str=None):
+        """Load data
+
+        Args:
+            filename (str): The path of dataset.
+            sep (str): The separator/delimiter of columns.
+            columns (str): One of 'UI', 'UIR', 'UIT' and 'UIRT'.
+
+        """
+        if not os.path.isfile(filename):
+            raise FileNotFoundError(f"There is no file named '{filename}'.")
+        if columns not in self._column_dict:
+            key_str = ", ".join(self._column_dict.keys())
+            raise ValueError(f"'columns' must be one of '{key_str}'.")
+        self._config["columns"] = columns
+
+        self._column_name = self._column_dict[columns]
+
+        print("loading data...")
+        self._config["filename"] = filename
+        self._config["sep"] = sep
+
+        self.all_data = pd.read_csv(filename, sep=sep, header=None, names=self._column_name)
+        self.all_data.dropna(inplace=True)
+
+        self._data_name = os.path.basename(filename).split(".")[0]
+        self._dir_path = os.path.dirname(filename)
+
+    def drop_duplicates(self, keep: str="last"):
+        """Drop duplicate user-item interactions.
+
+        Args:
+            keep (str): 'first' or 'last', default 'first'.
+                Drop duplicates except for the first or last occurrence.
+
+        Returns:
+            An object of pd.DataFrame without duplicates.
+
+        Raises:
+            ValueError: If 'keep' is not 'first' or 'last'.
+        """
+
+        if keep not in {'first', 'last'}:
+            raise ValueError(f"'keep' must be 'first' or 'last', but '{keep}'")
+        print("dropping duplicate interactions...")
+
+        if self._TIME in self._column_name:
+            sort_key = [self._USER, self._TIME]
+        else:
+            sort_key = [self._USER, self._ITEM]
+
+        self.all_data.sort_values(by=sort_key, inplace=True)
+
+        self.all_data.drop_duplicates(subset=[self._USER, self._ITEM], keep=keep, inplace=True)
+
+    @typeassert(user_min=int, item_min=int)
+    def filter_data(self, user_min: int=0, item_min: int=0):
+        """Filter users and items with a few interactions.
+
+        Args:
+            user_min (int): The users with less interactions than 'user_min' will be filtered.
+            item_min (int): The items with less interactions than 'item_min' will be filtered.
+        """
+        i = 0
+        while True:
+            i += 1
+            old_len_data = len(self.all_data)
+            print(f"{i} filtering items...")
+            self.filter_item(item_min)
+            print(f"{i} filtering users...")
+            self.filter_user(user_min)
+            new_len_data = len(self.all_data)
+            if old_len_data == new_len_data:  # return if data does not change
+                break
+
+    @typeassert(user_min=int)
+    def filter_user(self, user_min: int=0):
+        """Filter users with a few interactions.
+
+        Args:
+            user_min (int): The users with less interactions than 'user_min' will be filtered.
+        """
+        self._config["user_min"] = str(user_min)
+        self._user_min = user_min
+        if user_min > 0:
+            user_count = self.all_data[self._USER].value_counts(sort=False)
+            if user_count.min() < user_min:
+                filtered_idx = self.all_data[self._USER].map(lambda x: user_count[x] >= user_min)
+                self.all_data = self.all_data[filtered_idx]
+
+    @typeassert(item_min=int)
+    def filter_item(self, item_min: int=0):
+        """Filter items with a few interactions.
+
+        Args:
+            item_min (int): The items with less interactions than 'item_min' will be filtered.
+        """
+        self._config["item_min"] = str(item_min)
+        self._item_min = item_min
+        if item_min > 0:
+            item_count = self.all_data[self._ITEM].value_counts(sort=False)
+            if item_count.min() < item_min:
+                filtered_idx = self.all_data[self._ITEM].map(lambda x: item_count[x] >= item_min)
+                self.all_data = self.all_data[filtered_idx]
+
+    def remap_data_id(self):
+        """Convert user and item IDs to integers, start from 0.
+
+        """
+        self.remap_user_id()
+        self.remap_item_id()
+
+    def remap_user_id(self):
+        """Convert user IDs to integers, start from 0.
+
+        """
+        print("remapping user IDs...")
+        self._config["remap_user_id"] = "True"
+        unique_user = self.all_data[self._USER].unique()
+        self.user2id = pd.Series(data=range(len(unique_user)), index=unique_user)
+
+        self.all_data[self._USER] = self.all_data[self._USER].map(self.user2id)
+
+    def remap_item_id(self):
+        """Convert item IDs to integers, start from 0.
+
+        """
+        print("remapping item IDs...")
+        self._config["remap_item_id"] = "True"
+        unique_item = self.all_data[self._ITEM].unique()
+        self.item2id = pd.Series(data=range(len(unique_item)), index=unique_item)
+
+        self.all_data[self._ITEM] = self.all_data[self._ITEM].map(self.item2id)
+
+    @typeassert(train=float, valid=float, test=float)
+    def split_data_by_ratio(self, train: float=0.7, valid: float=0.1, test: float=0.2, by_time: bool=True):
+        """Split dataset by the given ratios.
+
+        The dataset will be split by each user.
+
+        Args:
+            train (float): The proportion of training data.
+            valid (float): The proportion of validation data.
+                '0.0' means no validation set.
+            test (float): The proportion of testing data.
+            by_time (bool): Splitting data randomly or by time.
+        """
+        if train <= 0.0:
+            raise ValueError("'train' must be a positive value.")
+        if train + valid + test != 1.0:
+            raise ValueError("The sum of 'train', 'valid' and 'test' must be equal to 1.0.")
+        print("splitting data by ratio...")
+
+        self._config["split_by"] = "ratio"
+        self._config["train"] = str(train)
+        self._config["valid"] = str(valid)
+        self._config["test"] = str(test)
+        self._config["by_time"] = str(by_time)
+
+        if by_time is False or self._TIME not in self._column_name:
+            sort_key = [self._USER, self._ITEM]
+        else:
+            sort_key = [self._USER, self._TIME]
+
+        self.all_data.sort_values(by=sort_key, inplace=True)
+
+        _by_t = "by_time" if by_time is True else "by_random"
+        self._split_manner = "ratio_" + _by_t
+        train_data = []
+        valid_data = []
+        test_data = []
+
+        user_grouped = self.all_data.groupby(by=[self._USER])
+        for user, u_data in user_grouped:
+            u_data_len = len(u_data)
+            if not by_time:
+                u_data = u_data.sample(frac=1)
+            train_end = math.ceil(train * u_data_len)
+            train_data.append(u_data.iloc[:train_end])
+            if valid != 0:
+                test_begin = train_end + math.ceil(valid * u_data_len)
+                valid_data.append(u_data.iloc[train_end:test_begin])
+            else:
+                test_begin = train_end
+            test_data.append(u_data.iloc[test_begin:])
+
+        self.train_data = pd.concat(train_data, ignore_index=True)
+        if valid != 0:
+            self.valid_data = pd.concat(valid_data, ignore_index=True)
+        self.test_data = pd.concat(test_data, ignore_index=True)
+
+    @typeassert(valid=int, test=int)
+    def split_data_by_leave_out(self, valid: int=1, test: int=1, by_time: bool=True):
+        """Split dataset by leave out certain number items.
+
+        The dataset will be split by each user.
+
+        Args:
+            valid (int): The number of items of validation set for each user.
+                Default to 1 and means leave one out.
+            test (int): The number of items of test set for each user.
+                Default to 1 and means leave one out.
+            by_time (bool): Splitting data randomly or by time.
+        """
+
+        self._config["split_by"] = "leave_out"
+        self._config["valid"] = str(valid)
+        self._config["test"] = str(test)
+        self._config["by_time"] = str(by_time)
+
+        if by_time is False or self._TIME not in self._column_name:
+            sort_key = [self._USER, self._ITEM]
+        else:
+            sort_key = [self._USER, self._TIME]
+        print("splitting data by leave out...")
+
+        self.all_data.sort_values(by=sort_key, inplace=True)
+
+        _by_t = "by_time" if by_time is True else "by_random"
+        self._split_manner = "leave_" + _by_t
+        train_data = []
+        valid_data = []
+        test_data = []
+
+        user_grouped = self.all_data.groupby(by=[self._USER])
+        for user, u_data in user_grouped:
+            if not by_time:
+                u_data = u_data.sample(frac=1)
+            train_end = -(valid+test)
+            train_data.append(u_data.iloc[:train_end])
+            if valid != 0:
+                test_begin = train_end + valid
+                valid_data.append(u_data.iloc[train_end:test_begin])
+            else:
+                test_begin = train_end
+            test_data.append(u_data.iloc[test_begin:])
+
+        self.train_data = pd.concat(train_data, ignore_index=True)
+        if valid != 0:
+            self.valid_data = pd.concat(valid_data, ignore_index=True)
+        self.test_data = pd.concat(test_data, ignore_index=True)
+
+    def save_data(self, save_dir=None):
+        """Save data to disk.
+
+        Args:
+            save_dir (str): The directory to save the dataset and information.
+
+        """
+        print("saving data to disk...")
+        dir_path = save_dir if save_dir is not None else self._dir_path
+        filename = f"{self._data_name}_{self._split_manner}_u{self._user_min}_i{self._item_min}"
+        dir_path = os.path.join(dir_path, filename)
+        if not os.path.exists(dir_path):
+            os.makedirs(dir_path)
+
+        # save data
+        filename = os.path.join(dir_path, filename)
+        sep = "\t"  # self._config["sep"]
+        if self.all_data is not None:
+            self.all_data.to_csv(filename+".all", header=False, index=False, sep=sep)
+        if self.train_data is not None:
+            self.train_data.to_csv(filename + ".train", header=False, index=False, sep=sep)
+        if self.valid_data is not None:
+            self.valid_data.to_csv(filename + ".valid", header=False, index=False, sep=sep)
+        if self.test_data is not None:
+            self.test_data.to_csv(filename + ".test", header=False, index=False, sep=sep)
+        if self.user2id is not None:
+            self.user2id.to_csv(filename + ".user2id", header=False, index=True, sep=sep)
+        if self.item2id is not None:
+            self.item2id.to_csv(filename + ".item2id", header=False, index=True, sep=sep)
+
+        # calculate statistics
+        user_num = len(self.all_data[self._USER].unique())
+        item_num = len(self.all_data[self._ITEM].unique())
+        rating_num = len(self.all_data)
+        sparsity = 1-1.0*rating_num/(user_num*item_num)
+
+        # write log file
+        logger = Logger(filename+".info")
+        data_info = "\n".join([f"{key} = {value}" for key, value in self._config.items()])
+        logger.info("\n"+data_info)
+        logger.info("Dataset statistic information:")
+        logger.info(f"The number of users: {user_num}")
+        logger.info(f"The number of items: {item_num}")
+        logger.info(f"The number of ratings: {rating_num}")
+        logger.info(f"Average actions of users: {(1.0*rating_num/user_num):.2f}")
+        logger.info(f"Average actions of items: {(1.0*rating_num/item_num):.2f}")
+        logger.info(f"The sparsity of the dataset: {(sparsity*100)}%%")
+
+        print(f"\nThe processed data has been saved in '{dir_path}'")
+        return dir_path
```

## skrec/io/movielens.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-__author__ = "Zhongchuan Sun"
-__email__ = "zhongchuansun@gmail.com"
-
-__all__ = ["MovieLens100k"]
-
-import os
-import shutil
-from urllib import request
-from zipfile import ZipFile
-
-
-class MovieLens100k(object):
-    url = "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
-
-    @classmethod
-    def download(cls, data_dir):
-        if not os.path.exists(data_dir):
-            os.makedirs(data_dir)
-
-        filepath = os.path.join(data_dir, cls.url.split("/")[-1])
-        if not os.path.exists(filepath):
-            print("downloading ml-100k.zip ...")
-            request.urlretrieve(cls.url, filepath)
-        else:
-            print(f"File '{filepath}' already downloaded.")
-
-        return filepath
-
-    @classmethod
-    def extract(cls, zip_path):
-        filename = "ml-100k.rating"
-        _rating_path = os.path.join(os.path.dirname(zip_path), filename)
-        if not os.path.exists(_rating_path):
-            with ZipFile(zip_path, "r") as z:
-                with z.open("ml-100k/u.data") as zf, open(_rating_path, "wb") as f:
-                    print("extracting...")
-                    shutil.copyfileobj(zf, f)
-        else:
-            print(f"File '{_rating_path}' already existed.")
-        return _rating_path
-
-    @classmethod
-    def download_and_extract(cls, data_dir):
-        _zip_path = MovieLens100k.download(data_dir)
-        _rating_path = MovieLens100k.extract(_zip_path)
-        return _rating_path
+__author__ = "Zhongchuan Sun"
+__email__ = "zhongchuansun@gmail.com"
+
+__all__ = ["MovieLens100k"]
+
+import os
+import shutil
+from urllib import request
+from zipfile import ZipFile
+
+
+class MovieLens100k(object):
+    url = "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
+
+    @classmethod
+    def download(cls, data_dir):
+        if not os.path.exists(data_dir):
+            os.makedirs(data_dir)
+
+        filepath = os.path.join(data_dir, cls.url.split("/")[-1])
+        if not os.path.exists(filepath):
+            print("downloading ml-100k.zip ...")
+            request.urlretrieve(cls.url, filepath)
+        else:
+            print(f"File '{filepath}' already downloaded.")
+
+        return filepath
+
+    @classmethod
+    def extract(cls, zip_path):
+        filename = "ml-100k.rating"
+        _rating_path = os.path.join(os.path.dirname(zip_path), filename)
+        if not os.path.exists(_rating_path):
+            with ZipFile(zip_path, "r") as z:
+                with z.open("ml-100k/u.data") as zf, open(_rating_path, "wb") as f:
+                    print("extracting...")
+                    shutil.copyfileobj(zf, f)
+        else:
+            print(f"File '{_rating_path}' already existed.")
+        return _rating_path
+
+    @classmethod
+    def download_and_extract(cls, data_dir):
+        _zip_path = MovieLens100k.download(data_dir)
+        _rating_path = MovieLens100k.extract(_zip_path)
+        return _rating_path
```

## Comparing `scikit_recommender-0.0.2.dist-info/RECORD` & `scikit_recommender-0.0.3.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,58 +1,58 @@
-skrec/__init__.py,sha256=BvldUX9nLHXRRgQpF_Eaif0oe3CK5ag32Bl5ZXZnfpE,181
-skrec/utils/tf1x.py,sha256=Rj9EbxYQi_Azx8QSgho_Wm3LTLRGetZaAHjkG44bCaA,1217
-skrec/utils/__init__.py,sha256=plg0X5-8ykHLOOz0nuz3RSBEJnTGc-MM_19_AP8e_us,121
-skrec/utils/registry.py,sha256=umEAjTVcESuekep52XGXtJpXQVRtxYhuNYP8dXW63rE,1217
-skrec/utils/tf2x.py,sha256=vsob04swWmtCKf4BJ96h73t48bor8wC5KM_a3HfHvH4,82
-skrec/utils/torch.py,sha256=JGPyFAPsQLimLRY_FxgW61hL8nTXkdIqlAkqhwd2c3M,4001
-skrec/utils/common.py,sha256=nn92egmkicAq7J7o0Tx-ZZuEGzpV2gmdqE_CRmHrmVc,1222
-skrec/utils/py/config.py,sha256=7FVSq-uhFJJoEl2-lI5dZXGRimILE2rvZ4tYZausVnk,2516
-skrec/utils/py/batch_iterator.py,sha256=ng2TOil_IqpEb0L-9KD1rtzjzQHKNBJvqXAkF4iTo0o,6566
-skrec/utils/py/random.py,sha256=qmTnjQYb37DQQT8PxfRTfgMOM9vCWFMKxZ6m-ImSog0,1410
-skrec/utils/py/evaluator.py,sha256=epsQFB3scHyE04l4OrljyV_YVGt4M3pQFbaZqM-o58I,8881
-skrec/utils/py/__init__.py,sha256=FBEknlL9TxcOmtsct3XIw3AbYGlCz056bgOjgH051ew,518
-skrec/utils/py/generic.py,sha256=dKoBqXGcYc320ovq0g_q4Hjff11enhE_32uYtjXQ1RM,4748
-skrec/utils/py/decorator.py,sha256=i9zDX2tBVO5UoW6f-ohMA6EncGWCURiuT9bExYcI7_M,1627
-skrec/utils/py/cython/pyx_utils.cpython-39-x86_64-linux-gnu.so,sha256=2DbDkwAfN7qArN9UKFW_0mVV5t4FujsSuJk2QcdRIrE,159064
-skrec/utils/py/cython/pyx_sort.cpython-39-x86_64-linux-gnu.so,sha256=69bUJuyEUHlogao6I_40BbV7-5LCoAo1KXcUyeJcsnY,2927960
-skrec/utils/py/cython/__init__.py,sha256=3oS8e1QKkFr1sJpx-No1SFqH4i_6XDRk0IYP9uZ2ZPM,166
-skrec/utils/py/cython/pyx_random.cpython-39-x86_64-linux-gnu.so,sha256=oMWkEZz75Z3GpxKLBBVmHW7eh34rpv60wrnU6SBihvo,1873584
-skrec/utils/py/cython/pyx_eval_matrix.cpython-39-x86_64-linux-gnu.so,sha256=YdDekzuHZm7JU_N5PBFm2D0WOJt1ZxecY_w6Z29FSk8,1952104
-skrec/utils/py/cython/pyx_init.cpython-39-x86_64-linux-gnu.so,sha256=nmiwQcak5S7q1qzA2JfyPLIrL75lqkQ6s5O7DHKblSQ,118544
-skrec/recommender/MultVAE.py,sha256=ZD8PQ8kZiLkOAYacHXQcFUerooMVZpDUhex-C3fkEHk,8499
-skrec/recommender/base.py,sha256=-XE4xSNPlIOITB6Mt4Xh6vnMGLnIbj5YdEqPGehOsJQ,1699
-skrec/recommender/Caser.py,sha256=p3r2pwW2D7cCeQDaVysbd8V50gAIu9k5Dq433jQZnVA,9351
-skrec/recommender/CDAE.py,sha256=uNiIIl0YMGXXylXcCUQcJ_DifpkrD1VN3_aP1_loWLA,9626
-skrec/recommender/HGN.py,sha256=-BtHSspiPwjM3ukzv5XM52Z0SQp3IZ6Q5zs50H5Rs3o,9425
-skrec/recommender/SASRec.py,sha256=WT1HKHhYbnzYlifwhlsl_ZUDWwhPFw188TI-3FI6XpU,21235
-skrec/recommender/LightGCN.py,sha256=iEZ31i-UxDZiAXIIwriSYpUwMBOjIQMSbBnf9TDVyaQ,9319
-skrec/recommender/GRU4Rec.py,sha256=FBwiIRNoBJrtGzzOhgqM3gYEkYcpJx2GXctfb6Tu_DM,12555
-skrec/recommender/SRGNN.py,sha256=_eVts9ECFsLcQJhkcXUZ4hq0JYboiScHPCTXd4XBA2o,15186
-skrec/recommender/__init__.py,sha256=vsob04swWmtCKf4BJ96h73t48bor8wC5KM_a3HfHvH4,82
-skrec/recommender/GRU4RecPlus.py,sha256=utS39jM68NgSVxCzZEySSQOuBk36A_R6-MH-31izPdc,14315
-skrec/recommender/Pop.py,sha256=0uiSEoy1yqHKoB6CW6JsQheoAoxjSyuzpjLObL17Hj8,1588
-skrec/recommender/CML.py,sha256=wfTdYl-3lAGTSgtUv0FDYNnBtpTtvFbG9ZEjPgNbLyY,8609
-skrec/recommender/SGAT.py,sha256=vhQb3HOLxFCYEhGbZYWcrELBya4wD6258_UpT0XiRsA,15419
-skrec/recommender/BPRMF.py,sha256=jqrrb4Ysb7iqYTKDktXo5phLaCsNG0i6Ywvdylk5D_E,5650
-skrec/recommender/TransRec.py,sha256=t9NPgb30hf5F2xbVRGfOulstkk-o5dGvKhxcvNGdREc,6934
-skrec/recommender/FPMC.py,sha256=XFZB4s36gDD02wuBSMUPRblBYFoABKBJgcsidCi3Os8,6503
-skrec/recommender/AOBPR/pyx_aobpr_func.cpython-39-x86_64-linux-gnu.so,sha256=W9rlJwTuSF7HqhKhN0EAsBeX_OP57cExcKHJcFnY1o8,616344
-skrec/recommender/AOBPR/__init__.py,sha256=5q4sQdeTcPue4pEkj2p54pL_MTPzTm9tFGQgF8MjZtc,70
-skrec/recommender/AOBPR/AOBPR.py,sha256=PIfZk8fY98ECHGl6T_AnSKi55CZEnPXgpRJlqrV-CiY,4092
-skrec/recommender/BERT4Rec/modeling.py,sha256=wdJQLcqI0seNwjUXwdXiO4vbUjzjyT1zhdEIz-g_6MA,40615
-skrec/recommender/BERT4Rec/bert4rec_utils.py,sha256=9hd74rMqHCLLrx77Tlb64T4YRybw-uYPP64qqi4ro-8,11532
-skrec/recommender/BERT4Rec/BERT4Rec.py,sha256=0hciuw3IO4ZQgF9gUG4xCiVlUNpqQVDQSRoGgoqZA3c,7729
-skrec/recommender/BERT4Rec/bert4rec_gen_data.py,sha256=RKJ8SIqAnxs5Nmd7kDqL69fz9WZgB1bMTDVkzqJTxW4,17959
-skrec/recommender/BERT4Rec/vocab.py,sha256=Jnei4WxRs8zr1NXtUYc7DnXSXKAG2AUb_6SJ1cg3USo,2263
-skrec/recommender/BERT4Rec/__init__.py,sha256=ZL3eqVpUWoGB4aYGCsTrYeKkoSKPG_ULSF4i0HIVbYY,85
-skrec/recommender/BERT4Rec/optimization.py,sha256=3bXEBegtaibqh_HDWWPsh7JXozxlLbmIk_kFqP3nwu8,6505
-skrec/io/data_iterator.py,sha256=0JmKqiOUIDvEri-ymvV5DAexDBa4ZlKncIweudU76HQ,14302
-skrec/io/logger.py,sha256=sQubQv3TR3H3qihtwOoYHqaWBuRrmN3M7XIg-Y_vads,2539
-skrec/io/dataset.py,sha256=UQfU20EZe2PL0BJDCWFt9wGT-3hs1fKVLOqcOT3lbc4,13420
-skrec/io/__init__.py,sha256=YXXF4102S-T5hhMqNR2qB3ZoSKVr1qYai-Ic7am46ww,526
-skrec/io/preprocessor.py,sha256=QvjQifriKy47vK1tgYGyZpgQg0j3b1Z1Z8ErTSafZnw,12843
-skrec/io/movielens.py,sha256=5glRDEy5GSMjjjQ_jqP60SJL4ZWo-aUfS721s1uvu3c,1437
-scikit_recommender-0.0.2.dist-info/RECORD,,
-scikit_recommender-0.0.2.dist-info/WHEEL,sha256=gREe7-l-MJWbGZG46A7WHnwwUSxA3XJYHQvGGLzmBNU,148
-scikit_recommender-0.0.2.dist-info/top_level.txt,sha256=Qv7je5rGC74taVHO3JQGhfBjISiGCzqi9vWXz2DDmYY,6
-scikit_recommender-0.0.2.dist-info/LICENSE,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-scikit_recommender-0.0.2.dist-info/METADATA,sha256=SpDMxKDimqPoy-lv7EBT-R_G4F_6H2wZpb_OoXWruJc,7169
+skrec/__init__.py,sha256=QMjNhls-vvfgRbVu_HOdm1fik_MKzdM-n-4hg5JgtGk,192
+skrec/io/__init__.py,sha256=iymFKBKqejfEmLxGPywIznJUtit2ZmZf9uXWQI_AEN0,543
+skrec/io/data_iterator.py,sha256=2K4_XgiKia6-dQpwaBIG6i0SC7HXI9DW1upjGKXqX0s,14644
+skrec/io/dataset.py,sha256=-B6PjhSHw-3tv0NGA9xyB5mfSFAecDT7sQqzk-CxywQ,13778
+skrec/io/logger.py,sha256=WVG64DERs62qrY0XttaXhrxfCz5mjOTYSvr_QYMncWU,2623
+skrec/io/movielens.py,sha256=jh22Tmjo4oucep7CUFzNrIcu4QSe8dSHKLEOnSCzPjE,1483
+skrec/io/preprocessor.py,sha256=rQblJ7FnmqOwrwrP7WW8DI8buKdRCUR8K3E5wYMqZnw,13177
+skrec/recommender/BPRMF.py,sha256=5lKpdJfFc52cy23oP7eRme4b59mltF1-DgfRp644MVo,5794
+skrec/recommender/CDAE.py,sha256=9HEyutITGC5S2NwjboKfeVSTlgO4skc6IHSq0X-C2CY,9858
+skrec/recommender/CML.py,sha256=iy9FKt75Zkaje9d2Es7hNdStKhYEM9QMlCqizovxJUA,8800
+skrec/recommender/Caser.py,sha256=QSBvfnLkVBOQnDD3TkYpwzNuYFPNPIfzL5D8cfS8vus,9587
+skrec/recommender/FPMC.py,sha256=M4BDCD1MBYpbd6CEzXVHffIK3KAuaJl0V5Gt04ILmCo,6660
+skrec/recommender/GRU4Rec.py,sha256=xg10XqZ54KmtLRvl_kPmv8l9GqgV2xD-QHJzNXXC1hg,12848
+skrec/recommender/GRU4RecPlus.py,sha256=TNXkMqQhZgyUlnbHxPhxNm-ZOMWGZdhELsId8HyyseM,14646
+skrec/recommender/HGN.py,sha256=lYHYaEdZETXzZaTpylhWoAq0lnEY6fRmKZgzpC6y-hg,9658
+skrec/recommender/LightGCN.py,sha256=PaoMwA5fNGQTqE51WS1nscrKVCelFjgZHjaFijgMkG8,9540
+skrec/recommender/MultVAE.py,sha256=VR7pMvNyfBTamCPWx8-15wyDLFdqjPkXDu4mSEkHaLw,8726
+skrec/recommender/Pop.py,sha256=C-RSsw5Ohia09XJJ9k_HloWQI3qRIJp62RvUd17KJQM,1635
+skrec/recommender/SASRec.py,sha256=k7W_5LamYv2L_dAgiy0hV7mF-eX0ImcmtVhb5-Jxn-U,21741
+skrec/recommender/SGAT.py,sha256=XAjV-b7HTzb4IJtsFZPdHfPEFjdJvnoNYbowpjQ64BU,15764
+skrec/recommender/SRGNN.py,sha256=f5a0HJAB8JwC0pB9-vyvv0Vq9iszn2moKE-TaHDoPiA,15481
+skrec/recommender/TransRec.py,sha256=ydNdtSEONG8938ZAY-MjAUeC9toxBztzFunEK9HMp8w,7098
+skrec/recommender/__init__.py,sha256=t6yJ0YYonIqB3iPvW-kxMDxVaCEKUk9-kCn7Reng2YY,86
+skrec/recommender/base.py,sha256=-XE4xSNPlIOITB6Mt4Xh6vnMGLnIbj5YdEqPGehOsJQ,1699
+skrec/recommender/AOBPR/AOBPR.py,sha256=LxFT_2e-3Nn-bpgnUbtJhWkvyB70DB9oATFRWaG6hPk,4196
+skrec/recommender/AOBPR/__init__.py,sha256=JLZkU35sL9Bh0FYDKu5ywbZpDoG-cRDmXTLUTu-8CcA,73
+skrec/recommender/AOBPR/pyx_aobpr_func.cp39-win_amd64.pyd,sha256=suNnRZMlg8vz7tZcAcuy4EQGIYDuaPPLmZn5rFFRlTY,68608
+skrec/recommender/BERT4Rec/BERT4Rec.py,sha256=8QbyqZbh_r-AAELZh9iODcOi5jYsNlsNP4C_GkRuCjk,7915
+skrec/recommender/BERT4Rec/__init__.py,sha256=hG_eS0hjzDhS0GpM5O79HvKapayU8ONaXtwK3Qi1B4g,88
+skrec/recommender/BERT4Rec/bert4rec_gen_data.py,sha256=7FtsnMo8zpgR1k61sCQXYEnXO50L4zMq_ehGbagW1-s,18467
+skrec/recommender/BERT4Rec/bert4rec_utils.py,sha256=nHw48GNalHP4fc6p96xW3Tcl8OeJnVMz_F4LLR36Lcg,11820
+skrec/recommender/BERT4Rec/modeling.py,sha256=lweMl86-bomccYtPk5nDnhAJycDxuKnZ8YNTzYKX6hA,41607
+skrec/recommender/BERT4Rec/optimization.py,sha256=6AOX2WnSfYEv6QUpLovjnMkgikiNAHTifiI20TuSjzw,6676
+skrec/recommender/BERT4Rec/vocab.py,sha256=94NBNOeZ9zRWaN7eviJViKpBET9JUFdvcboU7oRy3hc,2336
+skrec/utils/__init__.py,sha256=gYGwsGAhJkZ3pBjjOV5cgQgtbEA-XdkKiUmmQp-KluM,127
+skrec/utils/common.py,sha256=0hFHkSFOHh62Ce31OqBP6hPOtmH92CcXHSBIcnJdtrM,1261
+skrec/utils/registry.py,sha256=5puR2tKyal24PE0qOiVdX5mYyYqvHd6b2w-AbtypaF4,1255
+skrec/utils/tf1x.py,sha256=4bhZA5RnibIEJyybCgbECKQm2N0v2CIVm-dRRaVfyzA,1265
+skrec/utils/tf2x.py,sha256=t6yJ0YYonIqB3iPvW-kxMDxVaCEKUk9-kCn7Reng2YY,86
+skrec/utils/torch.py,sha256=-plQMx_nBHuN9iWy7ucIAQnMj-iFsR3sSWaZZk1GepM,4122
+skrec/utils/py/__init__.py,sha256=dkxRcGGV5IcJ2Jho3n4UXcKZmM6FP37bbw9etpC36oA,538
+skrec/utils/py/batch_iterator.py,sha256=RbT77WstQDz0z6nwbacxp4GnLM8HSfSO2Jpy8z9vM6w,6779
+skrec/utils/py/config.py,sha256=QXNUNgAewsXb53f9h2NSLSWz7WSNsRYFKj24cfBGDuA,2593
+skrec/utils/py/decorator.py,sha256=7Kjf_8lSKOU044cWWC4PIdphixllQ1YO9kms3iOwzDg,1678
+skrec/utils/py/evaluator.py,sha256=brqzeg_sqGx9hTmljSP6i3N546vSm_azuFWkYNvxfd0,9089
+skrec/utils/py/generic.py,sha256=3lZCn6_wOCmjMKSUHdog12jLhXT9lqIggTevZpwQ_aE,4876
+skrec/utils/py/random.py,sha256=kPcQKKPh6qmX82nasFKOAu6qZ-WxrgnXkiU2m-LDwaY,1450
+skrec/utils/py/cython/__init__.py,sha256=3yiwEYylUE5R6JL2RiIPvvgnuIjRsk8V6OKvyGJND54,173
+skrec/utils/py/cython/pyx_eval_matrix.cp39-win_amd64.pyd,sha256=G4r6YWeItLbmBzYHy_ccFu62_IPX_GCxuRgh2U-WiQ0,84992
+skrec/utils/py/cython/pyx_init.cp39-win_amd64.pyd,sha256=vjp-5La-9jl17RlJWpnZ1H0TyRyyK_J-0Z5gFJ3A-sM,26112
+skrec/utils/py/cython/pyx_random.cp39-win_amd64.pyd,sha256=TYZ766WPZRdKDs9DaevF0hJvPvchocYS0u9RTwWXdqI,109568
+skrec/utils/py/cython/pyx_sort.cp39-win_amd64.pyd,sha256=KN2uuKndIOAo0i89tLFvyuVto9yKfgYUmZ-0FmEyxMM,115712
+skrec/utils/py/cython/pyx_utils.cp39-win_amd64.pyd,sha256=OTyiRdCGKerSGkD8s00tzdvLW_SmDJ5IK66gqMuVDS8,30720
+scikit_recommender-0.0.3.dist-info/LICENSE,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+scikit_recommender-0.0.3.dist-info/METADATA,sha256=K7eocNlSsfFd13AgYiUOCQpRFD8W7D0RURe7dCZnX_Y,7326
+scikit_recommender-0.0.3.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+scikit_recommender-0.0.3.dist-info/top_level.txt,sha256=Qv7je5rGC74taVHO3JQGhfBjISiGCzqi9vWXz2DDmYY,6
+scikit_recommender-0.0.3.dist-info/RECORD,,
```

## Comparing `scikit_recommender-0.0.2.dist-info/METADATA` & `scikit_recommender-0.0.3.dist-info/METADATA`

 * *Files 7% similar despite different names*

```diff
@@ -1,131 +1,134 @@
-Metadata-Version: 2.1
-Name: scikit-recommender
-Version: 0.0.2
-Summary: A science toolkit for recommender systems
-Home-page: https://github.com/ZhongchuanSun/scikit-recommender
-Author: ZhongchuanSun
-Author-email: zhongchuansun@gmail.com
-Platform: Windows
-Platform: Linux
-Platform: Mac OS
-Classifier: Development Status :: 4 - Beta
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: Unix
-Classifier: Operating System :: MacOS
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: Implementation :: CPython
-Classifier: Topic :: Scientific/Engineering
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Scientific/Engineering :: Information Analysis
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Requires-Python: >=3.6
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numpy (>=1.17)
-Requires-Dist: scipy
-Requires-Dist: pandas
-Requires-Dist: colorama
-
-<!-- Add banner here -->
-
-# Scikit-Recommender
-
-<!-- Describe your project in brief -->
-Scikit-Recommender is an open source library for researchers of recommender systems.
-
-## Highlighted Features
-
-- Various recommendation models
-- Parse arguments from command line and ini-style files
-- Diverse data preprocessing
-- Fast negative sampling
-- Fast model evaluation
-- Convenient record logging
-- Flexible batch data iterator
-
-## Installation
-<!-- ## Install Scikit-Recommender -->
-
-You have three ways to use Scikit-Recommender:
-
-1. Install from PyPI
-2. Install from Source
-3. Run without Installation
-
-### Install from PyPI
-
-Binary installers are available at the [Python package index](https://pypi.org/project/scikit-recommender/) and you can install the package from pip.
-
-```sh
-pip install scikit-recommender
-```
-
-### Install from Source
-
-Installing from source requires Cython and the current code works well with the version 0.29.20.
-
-To build scikit-recommender from source you need Cython:
-
-```sh
-pip install cython==0.29.20
-```
-
-Then, the scikit-recommender can be installed by executing:
-
-```sh
-git clone https://github.com/ZhongchuanSun/scikit-recommender.git
-cd scikit-recommender
-python setup.py install
-```
-
-### Run without Installation
-
-Alternatively, You can also run the sources without installation.
-Please compile the cython codes before running:
-
-```sh
-git clone https://github.com/ZhongchuanSun/scikit-recommender.git
-cd scikit-recommender
-python setup.py build_ext --inplace
-```
-
-## Usage
-
-After installing or compiling this package, now you can run the [run_skrec.py]([./run_skrec.py](https://github.com/ZhongchuanSun/scikit-recommender/blob/master/run_skrec.py)):
-
-```sh
-python run_skrec.py
-```
-
-You can also find examples in [tutorial.ipynb](https://github.com/ZhongchuanSun/scikit-recommender/blob/master/tutorial.ipynb).
-
-## Models
-
-| Recommender | Implementation | Paper | &nbsp; Publication &nbsp;|
-|:-:|:-:|---|:-:|
-| [BPRMF](skrec/recommender/BPRMF.py)                   | PyTorch           | [Steffen Rendle et al., BPR: Bayesian Personalized Ranking from Implicit Feedback.](https://dl.acm.org/doi/10.5555/1795114.1795167) | UAI 2009 |
-| [AOBPR](skrec/recommender/AOBPR/AOBPR.py)             | C/Cython          | [Steffen Rendle et al., Improving Pairwise Learning for Item Recommendation from Implicit Feedback.](https://dl.acm.org/doi/10.1145/2556195.2556248) | WSDM 2014 |
-| [BERT4Rec](skrec/recommender/BERT4Rec/BERT4Rec.py)    | TensorFlow (1.14) | [Fei Sun et al., BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer.](https://dl.acm.org/doi/abs/10.1145/3357384.3357895) | CIKM 2019 |
-| [LightGCN](skrec/recommender/LightGCN.py)             | PyTorch           | [Xiangnan He et al., LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation.](https://dl.acm.org/doi/10.1145/3397271.3401063)| SIGIR 2020 |
-| [SASRec](skrec/recommender/SASRec.py)                 | TensorFlow (1.14) | [Wangcheng Kang et al., Self-Attentive Sequential Recommendation.](https://ieeexplore.ieee.org/abstract/document/8594844) | ICDM 2018 |
-| [HGN](skrec/recommender/HGN.py)                       |  PyTorch          | [Chen Ma et al., Hierarchical Gating Networks for Sequential Recommendation.](https://dl.acm.org/doi/10.1145/3292500.3330984) | KDD 2019 |
-| [TransRec](skrec/recommender/TransRec.py)             | PyTorch           | [Ruining He et al., Translation-based Recommendation.](https://dl.acm.org/doi/10.1145/3109859.3109882) | RecSys 2017 |
-| [SRGNN](skrec/recommender/SRGNN.py)                   | TensorFlow (1.14) | [Shu Wu et al., Session-Based Recommendation with Graph Neural Networks.](https://ojs.aaai.org/index.php/AAAI/article/view/3804) | AAAI 2019 |
-| [FPMC](skrec/recommender/FPMC.py)                     | PyTorch           | [Steffen Rendle et al., Factorizing Personalized Markov Chains for Next-Basket Recommendation.](https://dl.acm.org/doi/10.1145/1772690.1772773)  | WWW 2010 |
-| [Pop](skrec/recommender/Pop.py)                       | Python            | Make recommendations based on item popularity. |
-| [GRU4Rec](skrec/recommender/GRU4Rec.py)               | TensorFlow (1.14) | [Balzs Hidasi et al., Session-based Recommendations with Recurrent Neural Networks.](https://arxiv.org/abs/1511.06939) | ICLR 2016 |
-| [GRU4RecPlus](skrec/recommender/GRU4RecPlus.py)       | TensorFlow (1.14) | [Balzs Hidasi et al., Recurrent Neural Networks with Top-k Gains for Session-based Recommendations.](https://dl.acm.org/doi/10.1145/3269206.3271761) | CIKM 2018 |
-| [Caser](skrec/recommender/Caser.py)                   | PyTorch           |[Jiaxi Tang et al., Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.](https://dl.acm.org/doi/10.1145/3159652.3159656) | WSDM 2018 |
-| [CML](skrec/recommender/CML.py)                       | TensorFlow (1.14) | [Cheng-Kang Hsieh et al., Collaborative Metric Learning.](https://dl.acm.org/doi/10.1145/3038912.3052639) | WWW 2017 |
-| [MultiVAE](skrec/recommender/MultVAE.py)              | PyTorch           | [Dawen Liang, et al., Variational Autoencoders for Collaborative Filtering.](https://dl.acm.org/doi/10.1145/3178876.3186150) | WWW 2018 |
-| [CDAE](skrec/recommender/CDAE.py)                     | PyTorch           | [Yao Wu et al., Collaborative Denoising Auto-Encoders for Top-n Recommender Systems.](https://dl.acm.org/doi/10.1145/2835776.2835837) | WSDM 2016 |
-| [SGAT](skrec/recommender/SGAT.py)                     | TensorFlow (1.14) | [Zhongchuan Sun, et al., Sequential Graph Collaborative Filtering](https://www.sciencedirect.com/science/article/pii/S0020025522001049) | Information Sciences 2022 |
+Metadata-Version: 2.1
+Name: scikit-recommender
+Version: 0.0.3
+Summary: A science toolkit for recommender systems
+Home-page: https://github.com/ZhongchuanSun/scikit-recommender
+Author: ZhongchuanSun
+Author-email: zhongchuansun@gmail.com
+License: UNKNOWN
+Platform: Windows
+Platform: Linux
+Platform: Mac OS
+Classifier: Development Status :: 4 - Beta
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Science/Research
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: Unix
+Classifier: Operating System :: MacOS
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Topic :: Scientific/Engineering
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Scientific/Engineering :: Information Analysis
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Requires-Python: >=3.6
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: numpy (>=1.17)
+Requires-Dist: scipy
+Requires-Dist: pandas
+Requires-Dist: colorama
+
+<!-- Add banner here -->
+
+# Scikit-Recommender
+
+<!-- Describe your project in brief -->
+Scikit-Recommender is an open source library for researchers of recommender systems.
+
+## Highlighted Features
+
+- Various recommendation models
+- Parse arguments from command line and ini-style files
+- Diverse data preprocessing
+- Fast negative sampling
+- Fast model evaluation
+- Convenient record logging
+- Flexible batch data iterator
+
+## Installation
+<!-- ## Install Scikit-Recommender -->
+
+You have three ways to use Scikit-Recommender:
+
+1. Install from PyPI
+2. Install from Source
+3. Run without Installation
+
+### Install from PyPI
+
+Binary installers are available at the [Python package index](https://pypi.org/project/scikit-recommender/) and you can install the package from pip.
+
+```sh
+pip install scikit-recommender
+```
+
+### Install from Source
+
+Installing from source requires Cython and the current code works well with the version 0.29.20.
+
+To build scikit-recommender from source you need Cython:
+
+```sh
+pip install cython==0.29.20
+```
+
+Then, the scikit-recommender can be installed by executing:
+
+```sh
+git clone https://github.com/ZhongchuanSun/scikit-recommender.git
+cd scikit-recommender
+python setup.py install
+```
+
+### Run without Installation
+
+Alternatively, You can also run the sources without installation.
+Please compile the cython codes before running:
+
+```sh
+git clone https://github.com/ZhongchuanSun/scikit-recommender.git
+cd scikit-recommender
+python setup.py build_ext --inplace
+```
+
+## Usage
+
+After installing or compiling this package, now you can run the [run_skrec.py]([./run_skrec.py](https://github.com/ZhongchuanSun/scikit-recommender/blob/master/run_skrec.py)):
+
+```sh
+python run_skrec.py
+```
+
+You can also find examples in [tutorial.ipynb](https://github.com/ZhongchuanSun/scikit-recommender/blob/master/tutorial.ipynb).
+
+## Models
+
+| Recommender | Implementation | Paper | &nbsp; Publication &nbsp;|
+|:-:|:-:|---|:-:|
+| [BPRMF](skrec/recommender/BPRMF.py)                   | PyTorch           | [Steffen Rendle et al., BPR: Bayesian Personalized Ranking from Implicit Feedback.](https://dl.acm.org/doi/10.5555/1795114.1795167) | UAI 2009 |
+| [AOBPR](skrec/recommender/AOBPR/AOBPR.py)             | C/Cython          | [Steffen Rendle et al., Improving Pairwise Learning for Item Recommendation from Implicit Feedback.](https://dl.acm.org/doi/10.1145/2556195.2556248) | WSDM 2014 |
+| [BERT4Rec](skrec/recommender/BERT4Rec/BERT4Rec.py)    | TensorFlow (1.14) | [Fei Sun et al., BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer.](https://dl.acm.org/doi/abs/10.1145/3357384.3357895) | CIKM 2019 |
+| [LightGCN](skrec/recommender/LightGCN.py)             | PyTorch           | [Xiangnan He et al., LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation.](https://dl.acm.org/doi/10.1145/3397271.3401063)| SIGIR 2020 |
+| [SASRec](skrec/recommender/SASRec.py)                 | TensorFlow (1.14) | [Wangcheng Kang et al., Self-Attentive Sequential Recommendation.](https://ieeexplore.ieee.org/abstract/document/8594844) | ICDM 2018 |
+| [HGN](skrec/recommender/HGN.py)                       |  PyTorch          | [Chen Ma et al., Hierarchical Gating Networks for Sequential Recommendation.](https://dl.acm.org/doi/10.1145/3292500.3330984) | KDD 2019 |
+| [TransRec](skrec/recommender/TransRec.py)             | PyTorch           | [Ruining He et al., Translation-based Recommendation.](https://dl.acm.org/doi/10.1145/3109859.3109882) | RecSys 2017 |
+| [SRGNN](skrec/recommender/SRGNN.py)                   | TensorFlow (1.14) | [Shu Wu et al., Session-Based Recommendation with Graph Neural Networks.](https://ojs.aaai.org/index.php/AAAI/article/view/3804) | AAAI 2019 |
+| [FPMC](skrec/recommender/FPMC.py)                     | PyTorch           | [Steffen Rendle et al., Factorizing Personalized Markov Chains for Next-Basket Recommendation.](https://dl.acm.org/doi/10.1145/1772690.1772773)  | WWW 2010 |
+| [Pop](skrec/recommender/Pop.py)                       | Python            | Make recommendations based on item popularity. |
+| [GRU4Rec](skrec/recommender/GRU4Rec.py)               | TensorFlow (1.14) | [Balzs Hidasi et al., Session-based Recommendations with Recurrent Neural Networks.](https://arxiv.org/abs/1511.06939) | ICLR 2016 |
+| [GRU4RecPlus](skrec/recommender/GRU4RecPlus.py)       | TensorFlow (1.14) | [Balzs Hidasi et al., Recurrent Neural Networks with Top-k Gains for Session-based Recommendations.](https://dl.acm.org/doi/10.1145/3269206.3271761) | CIKM 2018 |
+| [Caser](skrec/recommender/Caser.py)                   | PyTorch           |[Jiaxi Tang et al., Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.](https://dl.acm.org/doi/10.1145/3159652.3159656) | WSDM 2018 |
+| [CML](skrec/recommender/CML.py)                       | TensorFlow (1.14) | [Cheng-Kang Hsieh et al., Collaborative Metric Learning.](https://dl.acm.org/doi/10.1145/3038912.3052639) | WWW 2017 |
+| [MultiVAE](skrec/recommender/MultVAE.py)              | PyTorch           | [Dawen Liang, et al., Variational Autoencoders for Collaborative Filtering.](https://dl.acm.org/doi/10.1145/3178876.3186150) | WWW 2018 |
+| [CDAE](skrec/recommender/CDAE.py)                     | PyTorch           | [Yao Wu et al., Collaborative Denoising Auto-Encoders for Top-n Recommender Systems.](https://dl.acm.org/doi/10.1145/2835776.2835837) | WSDM 2016 |
+| [SGAT](skrec/recommender/SGAT.py)                     | TensorFlow (1.14) | [Zhongchuan Sun, et al., Sequential Graph Collaborative Filtering](https://www.sciencedirect.com/science/article/pii/S0020025522001049) | Information Sciences 2022 |
+
+
```

